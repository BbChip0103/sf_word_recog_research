{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, \\\n",
    "                                    BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wav):\n",
    "    wav = sklearn.preprocessing.maxabs_scale(wav)\n",
    "    wav_mfcc = librosa.feature.mfcc(y=wav, n_mfcc=13)\n",
    "    wav_mfcc_std = StandardScaler().fit_transform(wav_mfcc)\n",
    "    wav_mfcc_std_mean = wav_mfcc_std.mean(axis=1)\n",
    "\n",
    "    features = np.concatenate([wav_mfcc_std_mean])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16400     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 1,206,032\n",
      "Trainable params: 1,202,192\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv1D (kernel_size=3, filters=128, strides=3, padding='valid',\n",
    "                  kernel_initializer='he_uniform', input_shape=input_shape))                  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 8\n",
    "# model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 9\n",
    "# model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 10\n",
    "model.add(Conv1D (kernel_size=3, filters=512, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 11\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 12\n",
    "model.add(Dense(output_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1239 - acc: 0.6788\n",
      "Epoch 00001: val_loss improved from inf to 0.42706, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/001-0.4271.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.1239 - acc: 0.6788 - val_loss: 0.4271 - val_acc: 0.8693\n",
      "Epoch 2/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8680\n",
      "Epoch 00002: val_loss improved from 0.42706 to 0.29260, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/002-0.2926.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.4364 - acc: 0.8680 - val_loss: 0.2926 - val_acc: 0.9110\n",
      "Epoch 3/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9011\n",
      "Epoch 00003: val_loss did not improve from 0.29260\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3190 - acc: 0.9011 - val_loss: 0.3081 - val_acc: 0.9073\n",
      "Epoch 4/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9163\n",
      "Epoch 00004: val_loss improved from 0.29260 to 0.26910, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/004-0.2691.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2711 - acc: 0.9163 - val_loss: 0.2691 - val_acc: 0.9159\n",
      "Epoch 5/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9317\n",
      "Epoch 00005: val_loss did not improve from 0.26910\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2164 - acc: 0.9317 - val_loss: 0.4460 - val_acc: 0.8791\n",
      "Epoch 6/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9390\n",
      "Epoch 00006: val_loss improved from 0.26910 to 0.22649, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/006-0.2265.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1936 - acc: 0.9390 - val_loss: 0.2265 - val_acc: 0.9297\n",
      "Epoch 7/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9477\n",
      "Epoch 00007: val_loss improved from 0.22649 to 0.22562, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/007-0.2256.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.1613 - acc: 0.9477 - val_loss: 0.2256 - val_acc: 0.9359\n",
      "Epoch 8/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9543\n",
      "Epoch 00008: val_loss did not improve from 0.22562\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1410 - acc: 0.9543 - val_loss: 0.3146 - val_acc: 0.9085\n",
      "Epoch 9/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9549\n",
      "Epoch 00009: val_loss improved from 0.22562 to 0.18813, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/009-0.1881.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1407 - acc: 0.9549 - val_loss: 0.1881 - val_acc: 0.9462\n",
      "Epoch 10/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9636\n",
      "Epoch 00010: val_loss improved from 0.18813 to 0.18754, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/010-0.1875.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1132 - acc: 0.9636 - val_loss: 0.1875 - val_acc: 0.9497\n",
      "Epoch 11/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9646\n",
      "Epoch 00011: val_loss improved from 0.18754 to 0.15615, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/011-0.1562.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1077 - acc: 0.9646 - val_loss: 0.1562 - val_acc: 0.9555\n",
      "Epoch 12/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9711\n",
      "Epoch 00012: val_loss did not improve from 0.15615\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0919 - acc: 0.9711 - val_loss: 0.1962 - val_acc: 0.9464\n",
      "Epoch 13/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9718\n",
      "Epoch 00013: val_loss improved from 0.15615 to 0.14090, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/013-0.1409.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0831 - acc: 0.9718 - val_loss: 0.1409 - val_acc: 0.9569\n",
      "Epoch 14/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9747\n",
      "Epoch 00014: val_loss did not improve from 0.14090\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0786 - acc: 0.9747 - val_loss: 0.1744 - val_acc: 0.9497\n",
      "Epoch 15/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9747\n",
      "Epoch 00015: val_loss improved from 0.14090 to 0.13764, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/015-0.1376.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0755 - acc: 0.9747 - val_loss: 0.1376 - val_acc: 0.9611\n",
      "Epoch 16/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9787\n",
      "Epoch 00016: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0654 - acc: 0.9787 - val_loss: 0.1721 - val_acc: 0.9539\n",
      "Epoch 17/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9760\n",
      "Epoch 00017: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0738 - acc: 0.9760 - val_loss: 0.1402 - val_acc: 0.9592\n",
      "Epoch 18/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9817\n",
      "Epoch 00018: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0557 - acc: 0.9817 - val_loss: 0.1887 - val_acc: 0.9539\n",
      "Epoch 19/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9821\n",
      "Epoch 00019: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0550 - acc: 0.9820 - val_loss: 0.1769 - val_acc: 0.9541\n",
      "Epoch 20/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9820\n",
      "Epoch 00020: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0543 - acc: 0.9820 - val_loss: 0.2505 - val_acc: 0.9390\n",
      "Epoch 21/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9809\n",
      "Epoch 00021: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0575 - acc: 0.9809 - val_loss: 0.4287 - val_acc: 0.9180\n",
      "Epoch 22/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9740\n",
      "Epoch 00022: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0796 - acc: 0.9741 - val_loss: 0.1422 - val_acc: 0.9623\n",
      "Epoch 23/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9849\n",
      "Epoch 00023: val_loss did not improve from 0.13764\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0474 - acc: 0.9849 - val_loss: 0.1504 - val_acc: 0.9583\n",
      "Epoch 24/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 00024: val_loss improved from 0.13764 to 0.12716, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/024-0.1272.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0367 - acc: 0.9886 - val_loss: 0.1272 - val_acc: 0.9683\n",
      "Epoch 25/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9897\n",
      "Epoch 00025: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0321 - acc: 0.9897 - val_loss: 0.1536 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9915\n",
      "Epoch 00026: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0280 - acc: 0.9915 - val_loss: 0.1399 - val_acc: 0.9632\n",
      "Epoch 27/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 00027: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0273 - acc: 0.9915 - val_loss: 0.6944 - val_acc: 0.8668\n",
      "Epoch 28/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9815\n",
      "Epoch 00028: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0574 - acc: 0.9816 - val_loss: 0.1539 - val_acc: 0.9590\n",
      "Epoch 29/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9904\n",
      "Epoch 00029: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0301 - acc: 0.9904 - val_loss: 0.1777 - val_acc: 0.9597\n",
      "Epoch 30/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9918\n",
      "Epoch 00030: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0256 - acc: 0.9918 - val_loss: 0.1482 - val_acc: 0.9676\n",
      "Epoch 31/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00031: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.1390 - val_acc: 0.9681\n",
      "Epoch 32/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9936\n",
      "Epoch 00032: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0214 - acc: 0.9936 - val_loss: 0.1711 - val_acc: 0.9634\n",
      "Epoch 33/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9940\n",
      "Epoch 00033: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.1522 - val_acc: 0.9653\n",
      "Epoch 34/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9950\n",
      "Epoch 00034: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0174 - acc: 0.9950 - val_loss: 0.1777 - val_acc: 0.9620\n",
      "Epoch 35/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9906\n",
      "Epoch 00035: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0277 - acc: 0.9906 - val_loss: 0.2595 - val_acc: 0.9446\n",
      "Epoch 36/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9902\n",
      "Epoch 00036: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0296 - acc: 0.9902 - val_loss: 0.1485 - val_acc: 0.9641\n",
      "Epoch 37/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9942\n",
      "Epoch 00037: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0188 - acc: 0.9942 - val_loss: 0.1595 - val_acc: 0.9637\n",
      "Epoch 38/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9902\n",
      "Epoch 00038: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0305 - acc: 0.9902 - val_loss: 0.1575 - val_acc: 0.9651\n",
      "Epoch 39/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9941\n",
      "Epoch 00039: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0190 - acc: 0.9941 - val_loss: 0.1363 - val_acc: 0.9679\n",
      "Epoch 40/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 00040: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0166 - acc: 0.9947 - val_loss: 0.1678 - val_acc: 0.9674\n",
      "Epoch 41/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9921\n",
      "Epoch 00041: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0268 - acc: 0.9921 - val_loss: 0.1665 - val_acc: 0.9653\n",
      "Epoch 42/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9954\n",
      "Epoch 00042: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0166 - acc: 0.9954 - val_loss: 0.1509 - val_acc: 0.9681\n",
      "Epoch 43/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00043: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.1581 - val_acc: 0.9660\n",
      "Epoch 44/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9965\n",
      "Epoch 00044: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0124 - acc: 0.9965 - val_loss: 0.5495 - val_acc: 0.9012\n",
      "Epoch 45/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9841\n",
      "Epoch 00045: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0518 - acc: 0.9841 - val_loss: 0.1725 - val_acc: 0.9592\n",
      "Epoch 46/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9896\n",
      "Epoch 00046: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0342 - acc: 0.9896 - val_loss: 0.1457 - val_acc: 0.9637\n",
      "Epoch 47/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9929\n",
      "Epoch 00047: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.1578 - val_acc: 0.9665\n",
      "Epoch 48/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9959\n",
      "Epoch 00048: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1498 - val_acc: 0.9660\n",
      "Epoch 49/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00049: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0116 - acc: 0.9965 - val_loss: 0.1315 - val_acc: 0.9679\n",
      "Epoch 50/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9952\n",
      "Epoch 00050: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0151 - acc: 0.9952 - val_loss: 0.1541 - val_acc: 0.9683\n",
      "Epoch 51/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00051: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.1455 - val_acc: 0.9674\n",
      "Epoch 52/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 00052: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1627 - val_acc: 0.9667\n",
      "Epoch 53/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9974\n",
      "Epoch 00053: val_loss did not improve from 0.12716\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0093 - acc: 0.9974 - val_loss: 0.1413 - val_acc: 0.9700\n",
      "Epoch 54/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 00054: val_loss improved from 0.12716 to 0.12541, saving model to model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/054-0.1254.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0083 - acc: 0.9976 - val_loss: 0.1254 - val_acc: 0.9744\n",
      "Epoch 55/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 00055: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0074 - acc: 0.9981 - val_loss: 0.1314 - val_acc: 0.9734\n",
      "Epoch 56/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9980\n",
      "Epoch 00056: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0072 - acc: 0.9980 - val_loss: 0.1446 - val_acc: 0.9688\n",
      "Epoch 57/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9980\n",
      "Epoch 00057: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1300 - val_acc: 0.9725\n",
      "Epoch 58/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 00058: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0059 - acc: 0.9985 - val_loss: 0.1394 - val_acc: 0.9690\n",
      "Epoch 59/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0078 - acc: 0.9978 - val_loss: 0.1567 - val_acc: 0.9660\n",
      "Epoch 60/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00060: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1315 - val_acc: 0.9734\n",
      "Epoch 61/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9975\n",
      "Epoch 00061: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0089 - acc: 0.9975 - val_loss: 0.4107 - val_acc: 0.9280\n",
      "Epoch 62/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9886\n",
      "Epoch 00062: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0362 - acc: 0.9886 - val_loss: 0.1394 - val_acc: 0.9695\n",
      "Epoch 63/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9965\n",
      "Epoch 00063: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0124 - acc: 0.9965 - val_loss: 0.2307 - val_acc: 0.9569\n",
      "Epoch 64/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 00064: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.2243 - val_acc: 0.9571\n",
      "Epoch 65/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9969\n",
      "Epoch 00065: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0103 - acc: 0.9969 - val_loss: 0.1454 - val_acc: 0.9690\n",
      "Epoch 66/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00066: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0084 - acc: 0.9976 - val_loss: 0.1740 - val_acc: 0.9674\n",
      "Epoch 67/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 00067: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0117 - acc: 0.9964 - val_loss: 0.1589 - val_acc: 0.9674\n",
      "Epoch 68/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9982\n",
      "Epoch 00068: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0068 - acc: 0.9982 - val_loss: 0.2769 - val_acc: 0.9476\n",
      "Epoch 69/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9932\n",
      "Epoch 00069: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0223 - acc: 0.9932 - val_loss: 0.1743 - val_acc: 0.9648\n",
      "Epoch 70/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971\n",
      "Epoch 00070: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0102 - acc: 0.9971 - val_loss: 0.1556 - val_acc: 0.9683\n",
      "Epoch 71/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 00071: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.1380 - val_acc: 0.9723\n",
      "Epoch 72/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00072: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0052 - acc: 0.9987 - val_loss: 0.1570 - val_acc: 0.9704\n",
      "Epoch 73/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 00073: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.1662 - val_acc: 0.9611\n",
      "Epoch 74/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 00074: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0171 - acc: 0.9947 - val_loss: 0.1864 - val_acc: 0.9630\n",
      "Epoch 75/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9986\n",
      "Epoch 00075: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.1634 - val_acc: 0.9674\n",
      "Epoch 76/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9943\n",
      "Epoch 00076: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.2021 - val_acc: 0.9571\n",
      "Epoch 77/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9981\n",
      "Epoch 00077: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0068 - acc: 0.9981 - val_loss: 0.1547 - val_acc: 0.9669\n",
      "Epoch 78/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9981\n",
      "Epoch 00078: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1445 - val_acc: 0.9727\n",
      "Epoch 79/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00079: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1578 - val_acc: 0.9686\n",
      "Epoch 80/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 00080: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.4111 - val_acc: 0.9343\n",
      "Epoch 81/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9946\n",
      "Epoch 00081: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0165 - acc: 0.9946 - val_loss: 0.2116 - val_acc: 0.9618\n",
      "Epoch 82/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 00082: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0076 - acc: 0.9977 - val_loss: 0.1581 - val_acc: 0.9695\n",
      "Epoch 83/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984\n",
      "Epoch 00083: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0055 - acc: 0.9984 - val_loss: 0.1637 - val_acc: 0.9690\n",
      "Epoch 84/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00084: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1794 - val_acc: 0.9651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9941\n",
      "Epoch 00085: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0206 - acc: 0.9941 - val_loss: 0.1624 - val_acc: 0.9665\n",
      "Epoch 86/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9969\n",
      "Epoch 00086: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0102 - acc: 0.9969 - val_loss: 0.2141 - val_acc: 0.9613\n",
      "Epoch 87/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
      "Epoch 00087: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.1552 - val_acc: 0.9686\n",
      "Epoch 88/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00088: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.1552 - val_acc: 0.9704\n",
      "Epoch 89/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00089: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.1820 - val_acc: 0.9648\n",
      "Epoch 90/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 00090: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0048 - acc: 0.9987 - val_loss: 0.1653 - val_acc: 0.9686\n",
      "Epoch 91/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 00091: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.2037 - val_acc: 0.9620\n",
      "Epoch 92/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 00092: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1468 - val_acc: 0.9727\n",
      "Epoch 93/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00093: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0033 - acc: 0.9993 - val_loss: 0.1491 - val_acc: 0.9713\n",
      "Epoch 94/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00094: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1921 - val_acc: 0.9658\n",
      "Epoch 95/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 00095: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1440 - val_acc: 0.9681\n",
      "Epoch 96/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00096: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1438 - val_acc: 0.9704\n",
      "Epoch 97/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00097: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1628 - val_acc: 0.9704\n",
      "Epoch 98/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993\n",
      "Epoch 00098: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1447 - val_acc: 0.9727\n",
      "Epoch 99/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00099: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1519 - val_acc: 0.9702\n",
      "Epoch 100/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00100: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1372 - val_acc: 0.9727\n",
      "Epoch 101/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9992\n",
      "Epoch 00101: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0027 - acc: 0.9992 - val_loss: 0.1712 - val_acc: 0.9676\n",
      "Epoch 102/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 00102: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1546 - val_acc: 0.9686\n",
      "Epoch 103/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00103: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1628 - val_acc: 0.9713\n",
      "Epoch 104/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00104: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1607 - val_acc: 0.9727\n",
      "Epoch 105/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00105: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1574 - val_acc: 0.9704\n",
      "Epoch 106/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1535 - val_acc: 0.9727\n",
      "Epoch 107/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00107: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1626 - val_acc: 0.9711\n",
      "Epoch 108/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00108: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0014 - acc: 0.9995 - val_loss: 0.1587 - val_acc: 0.9711\n",
      "Epoch 109/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00109: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.3090 - val_acc: 0.9471\n",
      "Epoch 110/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9905\n",
      "Epoch 00110: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0322 - acc: 0.9905 - val_loss: 0.1635 - val_acc: 0.9641\n",
      "Epoch 111/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 00111: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.1645 - val_acc: 0.9646\n",
      "Epoch 112/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00112: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1683 - val_acc: 0.9697\n",
      "Epoch 113/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00113: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1771 - val_acc: 0.9679\n",
      "Epoch 114/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 00114: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1493 - val_acc: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00115: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.1516 - val_acc: 0.9704\n",
      "Epoch 116/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00116: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1736 - val_acc: 0.9711\n",
      "Epoch 117/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00117: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.2243 - val_acc: 0.9595\n",
      "Epoch 118/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9933\n",
      "Epoch 00118: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0233 - acc: 0.9933 - val_loss: 0.2472 - val_acc: 0.9569\n",
      "Epoch 119/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9935\n",
      "Epoch 00119: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0238 - acc: 0.9935 - val_loss: 0.3357 - val_acc: 0.9455\n",
      "Epoch 120/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9928\n",
      "Epoch 00120: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0239 - acc: 0.9928 - val_loss: 0.2056 - val_acc: 0.9627\n",
      "Epoch 121/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9980\n",
      "Epoch 00121: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0076 - acc: 0.9980 - val_loss: 0.1678 - val_acc: 0.9679\n",
      "Epoch 122/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 00122: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.1646 - val_acc: 0.9695\n",
      "Epoch 123/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00123: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.1853 - val_acc: 0.9653\n",
      "Epoch 124/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00124: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0035 - acc: 0.9992 - val_loss: 0.6423 - val_acc: 0.9071\n",
      "Epoch 125/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9939\n",
      "Epoch 00125: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0208 - acc: 0.9939 - val_loss: 0.1616 - val_acc: 0.9669\n",
      "Epoch 126/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 00126: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1611 - val_acc: 0.9695\n",
      "Epoch 127/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 00127: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.8210 - val_acc: 0.8572\n",
      "Epoch 128/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9890\n",
      "Epoch 00128: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0362 - acc: 0.9890 - val_loss: 0.3655 - val_acc: 0.9229\n",
      "Epoch 129/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9864\n",
      "Epoch 00129: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0455 - acc: 0.9864 - val_loss: 0.1480 - val_acc: 0.9658\n",
      "Epoch 130/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9962\n",
      "Epoch 00130: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1678 - val_acc: 0.9651\n",
      "Epoch 131/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9969\n",
      "Epoch 00131: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0097 - acc: 0.9969 - val_loss: 0.1478 - val_acc: 0.9704\n",
      "Epoch 132/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 00132: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.1371 - val_acc: 0.9702\n",
      "Epoch 133/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9982\n",
      "Epoch 00133: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0054 - acc: 0.9982 - val_loss: 0.1315 - val_acc: 0.9713\n",
      "Epoch 134/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9990\n",
      "Epoch 00134: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.1523 - val_acc: 0.9718\n",
      "Epoch 135/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9991\n",
      "Epoch 00135: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.1426 - val_acc: 0.9716\n",
      "Epoch 136/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00136: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1358 - val_acc: 0.9720\n",
      "Epoch 137/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00137: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1514 - val_acc: 0.9702\n",
      "Epoch 138/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 00138: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.1433 - val_acc: 0.9704\n",
      "Epoch 139/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9972\n",
      "Epoch 00139: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.1410 - val_acc: 0.9695\n",
      "Epoch 140/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9983\n",
      "Epoch 00140: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 0.1619 - val_acc: 0.9688\n",
      "Epoch 141/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00141: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.1579 - val_acc: 0.9676\n",
      "Epoch 142/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 00142: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1479 - val_acc: 0.9695\n",
      "Epoch 143/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00143: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0039 - acc: 0.9992 - val_loss: 0.1515 - val_acc: 0.9704\n",
      "Epoch 144/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00144: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.1534 - val_acc: 0.9672\n",
      "Epoch 145/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00145: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1439 - val_acc: 0.9695\n",
      "Epoch 146/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 00146: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1517 - val_acc: 0.9697\n",
      "Epoch 147/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00147: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.1583 - val_acc: 0.9711\n",
      "Epoch 148/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00148: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0026 - acc: 0.9992 - val_loss: 0.1482 - val_acc: 0.9730\n",
      "Epoch 149/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00149: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1458 - val_acc: 0.9720\n",
      "Epoch 150/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00150: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1433 - val_acc: 0.9702\n",
      "Epoch 151/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00151: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1538 - val_acc: 0.9730\n",
      "Epoch 152/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00152: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 1.1828 - val_acc: 0.8300\n",
      "Epoch 153/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9799\n",
      "Epoch 00153: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0731 - acc: 0.9798 - val_loss: 0.2637 - val_acc: 0.9492\n",
      "Epoch 154/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9916\n",
      "Epoch 00154: val_loss did not improve from 0.12541\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0275 - acc: 0.9916 - val_loss: 0.1491 - val_acc: 0.9660\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=10000, \n",
    "                 validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                 callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "png_path = 'visualization/learning_curve/'\n",
    "filename = 'SampleCNN_8_conv_SGD'+'.png'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "fig.savefig(png_path+filename, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "4815/4815 [==============================] - 5s 976us/sample - loss: 0.1782 - acc: 0.9591\n",
      "Loss: 0.17821526111791205 Accuracy: 0.9590862\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_SGD_2_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "model = load_model(model_filename)\n",
    "[loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[371   0   5   2   2   0   0   2   1   2   0   0   0   0   0   0]\n",
      " [  0 347   0   0   4   4   0   0   0   6   1   0   2   0   0   0]\n",
      " [  3   0 373   2   1   0   0   1   0   0   0   0   1   2   0   1]\n",
      " [  0   0   4 359   1   0   3   2   2   0   0   0   0   0   0   6]\n",
      " [  0   1   0   1 358   3   0   0   0   1   0   0   2   0   2   0]\n",
      " [  0   1   0   0   2 396   0   1   1   2   1   4   0   0   0   0]\n",
      " [  0   0   1   1   0   0 368   0   2   0   1   1   0   0   0   0]\n",
      " [  1   0   1   0   0   2   1 367   1   0   1   1   1   0   0   0]\n",
      " [  0   0   3   4   1   1   0   1 363   1   0   0   1   1   0   0]\n",
      " [  0   3   0   0   1   5   0   0   0 363   2   2   1   0   0   0]\n",
      " [  2   0   1   0   0   0   1   0   3   1 166   7   2   0   0   0]\n",
      " [  0   1   0   3   1   0   0   0   1   2   4 138   1   2   0   0]\n",
      " [  0   0   1   0   0   0   0   0   1   0   0   0 165   0   1   0]\n",
      " [  0   0   4   0   0   0   1   1   1   0   1   1   1 182   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   1   0   0   5   0 159   0]\n",
      " [  1   0   2  13   0   0   0   0   4   0   0   0   0   0   0 143]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        zero       0.98      0.96      0.97       385\n",
      "         one       0.98      0.95      0.97       364\n",
      "         two       0.94      0.97      0.96       384\n",
      "       three       0.93      0.95      0.94       377\n",
      "        four       0.96      0.97      0.97       368\n",
      "        five       0.96      0.97      0.97       408\n",
      "         six       0.98      0.98      0.98       374\n",
      "       seven       0.98      0.98      0.98       376\n",
      "       eight       0.95      0.97      0.96       376\n",
      "        nine       0.96      0.96      0.96       377\n",
      "         bed       0.94      0.91      0.92       183\n",
      "        bird       0.90      0.90      0.90       153\n",
      "         cat       0.91      0.98      0.94       168\n",
      "         dog       0.97      0.95      0.96       192\n",
      "       house       0.98      0.95      0.97       167\n",
      "        tree       0.95      0.88      0.91       163\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      4815\n",
      "   macro avg       0.96      0.95      0.95      4815\n",
      "weighted avg       0.96      0.96      0.96      4815\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test_abs)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "y_real = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_real, y_pred, target_names=y_table.T[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
