{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, \\\n",
    "                                    BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wav):\n",
    "    wav = sklearn.preprocessing.maxabs_scale(wav)\n",
    "    wav_mfcc = librosa.feature.mfcc(y=wav, n_mfcc=13)\n",
    "    wav_mfcc_std = StandardScaler().fit_transform(wav_mfcc)\n",
    "    wav_mfcc_std_mean = wav_mfcc_std.mean(axis=1)\n",
    "\n",
    "    features = np.concatenate([wav_mfcc_std_mean])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 999,952\n",
      "Trainable params: 996,624\n",
      "Non-trainable params: 3,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv1D (kernel_size=3, filters=128, strides=3, padding='valid',\n",
    "                  kernel_initializer='he_uniform', input_shape=input_shape))                  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())c.NotebookApp.allow_remote_access = True\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 8\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 9\n",
    "# model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 10\n",
    "# model.add(Conv1D (kernel_size=3, filters=512, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 11\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 12\n",
    "model.add(Dense(output_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.6799\n",
      "Epoch 00001: val_loss improved from inf to 0.49429, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/001-0.4943.hdf5\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 1.0690 - acc: 0.6799 - val_loss: 0.4943 - val_acc: 0.8367\n",
      "Epoch 2/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8686\n",
      "Epoch 00002: val_loss improved from 0.49429 to 0.35811, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/002-0.3581.hdf5\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.4253 - acc: 0.8686 - val_loss: 0.3581 - val_acc: 0.8917\n",
      "Epoch 3/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9019\n",
      "Epoch 00003: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.3095 - acc: 0.9018 - val_loss: 0.5308 - val_acc: 0.8586\n",
      "Epoch 4/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9116\n",
      "Epoch 00004: val_loss improved from 0.35811 to 0.25441, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/004-0.2544.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.2714 - acc: 0.9116 - val_loss: 0.2544 - val_acc: 0.9229\n",
      "Epoch 5/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9315\n",
      "Epoch 00005: val_loss improved from 0.25441 to 0.21366, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/005-0.2137.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.2160 - acc: 0.9315 - val_loss: 0.2137 - val_acc: 0.9357\n",
      "Epoch 6/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9378\n",
      "Epoch 00006: val_loss improved from 0.21366 to 0.19275, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/006-0.1927.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1932 - acc: 0.9378 - val_loss: 0.1927 - val_acc: 0.9453\n",
      "Epoch 7/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9454\n",
      "Epoch 00007: val_loss improved from 0.19275 to 0.16467, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/007-0.1647.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1683 - acc: 0.9454 - val_loss: 0.1647 - val_acc: 0.9522\n",
      "Epoch 8/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9528\n",
      "Epoch 00008: val_loss improved from 0.16467 to 0.15081, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/008-0.1508.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1482 - acc: 0.9528 - val_loss: 0.1508 - val_acc: 0.9583\n",
      "Epoch 9/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9580\n",
      "Epoch 00009: val_loss did not improve from 0.15081\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1298 - acc: 0.9580 - val_loss: 0.1722 - val_acc: 0.9509\n",
      "Epoch 10/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9607\n",
      "Epoch 00010: val_loss did not improve from 0.15081\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1199 - acc: 0.9607 - val_loss: 0.2228 - val_acc: 0.9427\n",
      "Epoch 11/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9647\n",
      "Epoch 00011: val_loss improved from 0.15081 to 0.12964, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/011-0.1296.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1083 - acc: 0.9647 - val_loss: 0.1296 - val_acc: 0.9623\n",
      "Epoch 12/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9680\n",
      "Epoch 00012: val_loss did not improve from 0.12964\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0992 - acc: 0.9680 - val_loss: 0.1554 - val_acc: 0.9562\n",
      "Epoch 13/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9714\n",
      "Epoch 00013: val_loss did not improve from 0.12964\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0889 - acc: 0.9714 - val_loss: 0.1711 - val_acc: 0.9492\n",
      "Epoch 14/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9706\n",
      "Epoch 00014: val_loss did not improve from 0.12964\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0886 - acc: 0.9706 - val_loss: 0.3208 - val_acc: 0.9175\n",
      "Epoch 15/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9715\n",
      "Epoch 00015: val_loss did not improve from 0.12964\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0870 - acc: 0.9715 - val_loss: 0.1780 - val_acc: 0.9488\n",
      "Epoch 16/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9776\n",
      "Epoch 00016: val_loss improved from 0.12964 to 0.11913, saving model to model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/016-0.1191.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0699 - acc: 0.9776 - val_loss: 0.1191 - val_acc: 0.9690\n",
      "Epoch 17/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9795\n",
      "Epoch 00017: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0632 - acc: 0.9794 - val_loss: 0.1838 - val_acc: 0.9504\n",
      "Epoch 18/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9746\n",
      "Epoch 00018: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0782 - acc: 0.9746 - val_loss: 0.1210 - val_acc: 0.9676\n",
      "Epoch 19/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9795\n",
      "Epoch 00019: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0638 - acc: 0.9795 - val_loss: 0.1468 - val_acc: 0.9618\n",
      "Epoch 20/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9822\n",
      "Epoch 00020: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0557 - acc: 0.9822 - val_loss: 0.1410 - val_acc: 0.9639\n",
      "Epoch 21/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9845\n",
      "Epoch 00021: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0474 - acc: 0.9845 - val_loss: 0.1343 - val_acc: 0.9634\n",
      "Epoch 22/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9858\n",
      "Epoch 00022: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0451 - acc: 0.9857 - val_loss: 1.0060 - val_acc: 0.8039\n",
      "Epoch 23/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9622\n",
      "Epoch 00023: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1198 - acc: 0.9622 - val_loss: 0.1474 - val_acc: 0.9569\n",
      "Epoch 24/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9799\n",
      "Epoch 00024: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0604 - acc: 0.9799 - val_loss: 0.1551 - val_acc: 0.9592\n",
      "Epoch 25/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9840\n",
      "Epoch 00025: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0510 - acc: 0.9840 - val_loss: 0.2346 - val_acc: 0.9441\n",
      "Epoch 26/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9820\n",
      "Epoch 00026: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0540 - acc: 0.9820 - val_loss: 0.1306 - val_acc: 0.9660\n",
      "Epoch 27/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9875\n",
      "Epoch 00027: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0375 - acc: 0.9875 - val_loss: 0.1267 - val_acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9898\n",
      "Epoch 00028: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0336 - acc: 0.9898 - val_loss: 0.1288 - val_acc: 0.9672\n",
      "Epoch 29/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9887\n",
      "Epoch 00029: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0345 - acc: 0.9887 - val_loss: 0.1743 - val_acc: 0.9595\n",
      "Epoch 30/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9921\n",
      "Epoch 00030: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.1325 - val_acc: 0.9665\n",
      "Epoch 31/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 00031: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.1563 - val_acc: 0.9611\n",
      "Epoch 32/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9908\n",
      "Epoch 00032: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0297 - acc: 0.9907 - val_loss: 0.1927 - val_acc: 0.9578\n",
      "Epoch 33/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9895\n",
      "Epoch 00033: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0320 - acc: 0.9895 - val_loss: 0.1672 - val_acc: 0.9592\n",
      "Epoch 34/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9898\n",
      "Epoch 00034: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0318 - acc: 0.9898 - val_loss: 0.1272 - val_acc: 0.9674\n",
      "Epoch 35/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9913\n",
      "Epoch 00035: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0248 - acc: 0.9913 - val_loss: 0.1575 - val_acc: 0.9627\n",
      "Epoch 36/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 00036: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0291 - acc: 0.9905 - val_loss: 0.1420 - val_acc: 0.9613\n",
      "Epoch 37/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9930\n",
      "Epoch 00037: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0232 - acc: 0.9929 - val_loss: 0.2147 - val_acc: 0.9513\n",
      "Epoch 38/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9918\n",
      "Epoch 00038: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0243 - acc: 0.9918 - val_loss: 0.1620 - val_acc: 0.9665\n",
      "Epoch 39/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9946\n",
      "Epoch 00039: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.2231 - val_acc: 0.9441\n",
      "Epoch 40/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9933\n",
      "Epoch 00040: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0230 - acc: 0.9932 - val_loss: 1.3804 - val_acc: 0.7876\n",
      "Epoch 41/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9797\n",
      "Epoch 00041: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0656 - acc: 0.9797 - val_loss: 0.1853 - val_acc: 0.9536\n",
      "Epoch 42/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9911\n",
      "Epoch 00042: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0281 - acc: 0.9911 - val_loss: 0.1615 - val_acc: 0.9644\n",
      "Epoch 43/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9946\n",
      "Epoch 00043: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0186 - acc: 0.9946 - val_loss: 0.1325 - val_acc: 0.9681\n",
      "Epoch 44/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9949\n",
      "Epoch 00044: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.1818 - val_acc: 0.9576\n",
      "Epoch 45/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9919\n",
      "Epoch 00045: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.1461 - val_acc: 0.9679\n",
      "Epoch 46/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0166 - acc: 0.9951 - val_loss: 0.1584 - val_acc: 0.9665\n",
      "Epoch 47/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 00047: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0131 - acc: 0.9962 - val_loss: 0.1395 - val_acc: 0.9676\n",
      "Epoch 48/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9952\n",
      "Epoch 00048: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0161 - acc: 0.9952 - val_loss: 0.1516 - val_acc: 0.9681\n",
      "Epoch 49/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00049: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0105 - acc: 0.9974 - val_loss: 0.1495 - val_acc: 0.9672\n",
      "Epoch 50/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9967\n",
      "Epoch 00050: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0108 - acc: 0.9967 - val_loss: 0.1560 - val_acc: 0.9674\n",
      "Epoch 51/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9968\n",
      "Epoch 00051: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.1443 - val_acc: 0.9662\n",
      "Epoch 52/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9952\n",
      "Epoch 00052: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.4627 - val_acc: 0.9047\n",
      "Epoch 53/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9897\n",
      "Epoch 00053: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.1648 - val_acc: 0.9641\n",
      "Epoch 54/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9952\n",
      "Epoch 00054: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0154 - acc: 0.9952 - val_loss: 0.1519 - val_acc: 0.9672\n",
      "Epoch 55/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00055: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.1336 - val_acc: 0.9709\n",
      "Epoch 56/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 00056: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0089 - acc: 0.9977 - val_loss: 0.1298 - val_acc: 0.9713\n",
      "Epoch 57/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00057: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1431 - val_acc: 0.9716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 00058: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.1574 - val_acc: 0.9651\n",
      "Epoch 59/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0086 - acc: 0.9978 - val_loss: 0.1366 - val_acc: 0.9718\n",
      "Epoch 60/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9980\n",
      "Epoch 00060: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.3384 - val_acc: 0.9308\n",
      "Epoch 61/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9883\n",
      "Epoch 00061: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0383 - acc: 0.9883 - val_loss: 0.1824 - val_acc: 0.9595\n",
      "Epoch 62/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9936\n",
      "Epoch 00062: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0199 - acc: 0.9936 - val_loss: 0.2110 - val_acc: 0.9513\n",
      "Epoch 63/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9942\n",
      "Epoch 00063: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.1527 - val_acc: 0.9665\n",
      "Epoch 64/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00064: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1602 - val_acc: 0.9651\n",
      "Epoch 65/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9972\n",
      "Epoch 00065: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0093 - acc: 0.9972 - val_loss: 0.1351 - val_acc: 0.9704\n",
      "Epoch 66/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9981\n",
      "Epoch 00066: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 0.1421 - val_acc: 0.9683\n",
      "Epoch 67/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0113 - acc: 0.9962 - val_loss: 0.1406 - val_acc: 0.9709\n",
      "Epoch 68/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 00068: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.1465 - val_acc: 0.9688\n",
      "Epoch 69/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 00069: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0079 - acc: 0.9977 - val_loss: 0.3636 - val_acc: 0.9327\n",
      "Epoch 70/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9866\n",
      "Epoch 00070: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0429 - acc: 0.9866 - val_loss: 0.1801 - val_acc: 0.9609\n",
      "Epoch 71/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0123 - acc: 0.9967 - val_loss: 0.1574 - val_acc: 0.9630\n",
      "Epoch 72/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9967\n",
      "Epoch 00072: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.1543 - val_acc: 0.9660\n",
      "Epoch 73/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.1422 - val_acc: 0.9681\n",
      "Epoch 74/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9980\n",
      "Epoch 00074: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.4345 - val_acc: 0.9341\n",
      "Epoch 75/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9918\n",
      "Epoch 00075: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0288 - acc: 0.9918 - val_loss: 0.1934 - val_acc: 0.9590\n",
      "Epoch 76/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9948\n",
      "Epoch 00076: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0169 - acc: 0.9948 - val_loss: 0.1486 - val_acc: 0.9674\n",
      "Epoch 77/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9973\n",
      "Epoch 00077: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0094 - acc: 0.9972 - val_loss: 0.2982 - val_acc: 0.9462\n",
      "Epoch 78/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9902\n",
      "Epoch 00078: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0340 - acc: 0.9902 - val_loss: 0.1575 - val_acc: 0.9669\n",
      "Epoch 79/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9973\n",
      "Epoch 00079: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.1672 - val_acc: 0.9655\n",
      "Epoch 80/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 00080: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.1553 - val_acc: 0.9655\n",
      "Epoch 81/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9969\n",
      "Epoch 00081: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.1497 - val_acc: 0.9700\n",
      "Epoch 82/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 00082: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.1383 - val_acc: 0.9704\n",
      "Epoch 83/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9983\n",
      "Epoch 00083: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.1501 - val_acc: 0.9669\n",
      "Epoch 84/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 00084: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.1591 - val_acc: 0.9681\n",
      "Epoch 85/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00085: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1508 - val_acc: 0.9697\n",
      "Epoch 86/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9990\n",
      "Epoch 00086: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.1426 - val_acc: 0.9693\n",
      "Epoch 87/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00087: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1846 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00088: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1583 - val_acc: 0.9693\n",
      "Epoch 89/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00089: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.1526 - val_acc: 0.9681\n",
      "Epoch 90/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00090: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1611 - val_acc: 0.9679\n",
      "Epoch 91/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 00091: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.1597 - val_acc: 0.9676\n",
      "Epoch 92/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00092: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1426 - val_acc: 0.9716\n",
      "Epoch 93/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9990\n",
      "Epoch 00093: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.1624 - val_acc: 0.9672\n",
      "Epoch 94/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00094: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.2022 - val_acc: 0.9583\n",
      "Epoch 95/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9950\n",
      "Epoch 00095: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.1533 - val_acc: 0.9672\n",
      "Epoch 96/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.2311 - val_acc: 0.9527\n",
      "Epoch 97/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9938\n",
      "Epoch 00097: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0219 - acc: 0.9938 - val_loss: 0.1870 - val_acc: 0.9618\n",
      "Epoch 98/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9974\n",
      "Epoch 00098: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.1679 - val_acc: 0.9651\n",
      "Epoch 99/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 00099: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.1719 - val_acc: 0.9637\n",
      "Epoch 100/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00100: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1524 - val_acc: 0.9711\n",
      "Epoch 101/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00101: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.1545 - val_acc: 0.9674\n",
      "Epoch 102/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9991\n",
      "Epoch 00102: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.1568 - val_acc: 0.9700\n",
      "Epoch 103/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00103: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1638 - val_acc: 0.9672\n",
      "Epoch 104/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00104: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1498 - val_acc: 0.9679\n",
      "Epoch 105/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 00105: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1502 - val_acc: 0.9695\n",
      "Epoch 106/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9991\n",
      "Epoch 00106: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1771 - val_acc: 0.9662\n",
      "Epoch 107/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00107: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1572 - val_acc: 0.9681\n",
      "Epoch 108/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00108: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.9348 - val_acc: 0.8696\n",
      "Epoch 109/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9890\n",
      "Epoch 00109: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0363 - acc: 0.9890 - val_loss: 0.1746 - val_acc: 0.9660\n",
      "Epoch 110/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9972\n",
      "Epoch 00110: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.1683 - val_acc: 0.9662\n",
      "Epoch 111/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00111: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1601 - val_acc: 0.9653\n",
      "Epoch 112/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9964\n",
      "Epoch 00112: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.1593 - val_acc: 0.9704\n",
      "Epoch 113/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00113: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.1762 - val_acc: 0.9662\n",
      "Epoch 114/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
      "Epoch 00114: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1458 - val_acc: 0.9702\n",
      "Epoch 115/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00115: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1441 - val_acc: 0.9695\n",
      "Epoch 116/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00116: val_loss did not improve from 0.11913\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1747 - val_acc: 0.9674\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=10000, \n",
    "                 validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                 callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "png_path = 'visualization/learning_curve/'\n",
    "filename = 'SampleCNN_8_conv_SGD'+'.png'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "fig.savefig(png_path+filename, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/step\n",
      "Loss: 0.15826960811594565 Accuracy: 0.9511941848638026\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_SGD_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "model = load_model(model_filename)\n",
    "[loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 5 -r 5 model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
