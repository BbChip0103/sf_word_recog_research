{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, \\\n",
    "                                    BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Conv1D, MaxPool1D, BatchNormalization, \n",
    "                                     GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
    "                                     Dense, Dropout, Activation, Reshape, \n",
    "                                     Input, Concatenate, Add)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_fn(x, amplifying_ratio):\n",
    "    num_features = x.shape[-1].value\n",
    "    x = GlobalAvgPool1D()(x)\n",
    "    x = Reshape((1, num_features))(x)\n",
    "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
    "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def basic_block(x, num_features, weight_decay, _):\n",
    "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "               kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool1D(pool_size=3)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def se_block(x, num_features, weight_decay, amplifying_ratio):\n",
    "    x = basic_block(x, num_features, weight_decay, amplifying_ratio)\n",
    "    x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
    "    return x\n",
    "\n",
    "\n",
    "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
    "    if num_features != x.shape[-1].value:\n",
    "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
    "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "               kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "               kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if amplifying_ratio > 0:\n",
    "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
    "    x = Add()([shortcut, x])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool1D(pool_size=3)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resemul(input_shape, block_type='rese', multi=True, init_features=128, amplifying_ratio=16,\n",
    "                drop_rate=0.5, weight_decay=0., num_classes=16):\n",
    "    \"\"\"Build a SampleCNN model.\n",
    "    Args:\n",
    "        batch_shape: A tensor shape including batch size (e.g. [23, 59049])\n",
    "        block_type: A type of convolutional block among {se|rese|res|basic}\n",
    "        multi: Whether to use multi-level feature aggregation.\n",
    "        init_features: Number of feature maps of the first convolution.\n",
    "        amplifying_ratio: Amplifying ratio of SE (not used for res and basic).\n",
    "        weight_decay: L2 weight decay rate.\n",
    "        drop_rate: Dropout rate.\n",
    "        num_classes: Number of classes to predict.\n",
    "    Returns:\n",
    "        Keras Model.\n",
    "    \"\"\"\n",
    "    if block_type == 'se':\n",
    "        block = se_block\n",
    "    elif block_type == 'rese':\n",
    "        block = rese_block\n",
    "    elif block_type == 'res':\n",
    "        block = rese_block\n",
    "        amplifying_ratio = -1\n",
    "    elif block_type == 'basic':\n",
    "        block = basic_block\n",
    "    else:\n",
    "        raise Exception('Unknown block type: ' + block_type)\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
    "                 kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_features = init_features\n",
    "    layer_outputs = []\n",
    "    for i in range(7):\n",
    "        num_features *= 2 if (i == 2 or i == 6) else 1\n",
    "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
    "        layer_outputs.append(x)\n",
    "\n",
    "    if multi:\n",
    "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    else:\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    x = Dense(x.shape[-1].value, kernel_initializer='glorot_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if drop_rate > 0.:\n",
    "        x = Dropout(drop_rate)(x)\n",
    "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 5333, 128)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 5333, 128)    512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 5333, 128)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    49280       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 5333, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 5333, 128)    49280       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 5333, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 128)       0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 2048)      264192      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 128)       262272      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 5333, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 5333, 128)    0           activation[0][0]                 \n",
      "                                                                 multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5333, 128)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1777, 128)    0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1777, 128)    49280       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 1777, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1777, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1777, 128)    49280       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 1777, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 2048)      264192      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 128)       262272      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1777, 128)    0           batch_normalization_v1_4[0][0]   \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1777, 128)    0           max_pooling1d[0][0]              \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1777, 128)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 592, 128)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 256)     98560       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 256)     1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 592, 256)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 592, 256)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 592, 256)     196864      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 256)     1024        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 256)          0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 4096)      1052672     reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 592, 256)     33024       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 256)       1048832     dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 592, 256)     1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 592, 256)     0           batch_normalization_v1_7[0][0]   \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 592, 256)     0           batch_normalization_v1_5[0][0]   \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 256)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 197, 256)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 197, 256)     196864      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 197, 256)     1024        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 197, 256)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 197, 256)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 197, 256)     196864      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 197, 256)     1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 4096)      1052672     reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 256)       1048832     dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 197, 256)     0           batch_normalization_v1_9[0][0]   \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 197, 256)     0           max_pooling1d_2[0][0]            \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 197, 256)     0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65, 256)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 65, 256)      196864      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 65, 256)      1024        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 65, 256)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 65, 256)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 65, 256)      196864      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 65, 256)      1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 256)          0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 4096)      1052672     reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1, 256)       1048832     dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 65, 256)      0           batch_normalization_v1_11[0][0]  \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 65, 256)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 65, 256)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 21, 256)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 21, 256)      196864      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 21, 256)      1024        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 21, 256)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 21, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 21, 256)      196864      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 21, 256)      1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1, 4096)      1052672     reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1, 256)       1048832     dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 21, 256)      0           batch_normalization_v1_13[0][0]  \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 21, 256)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 21, 256)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 7, 256)       0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 7, 512)       393728      max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 7, 512)       2048        conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 7, 512)       0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 7, 512)       0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 7, 512)       786944      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 7, 512)       2048        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 512)          0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 512)       0           global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 8192)      4202496     reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 7, 512)       131584      max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 512)       4194816     dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 7, 512)       2048        conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 7, 512)       0           batch_normalization_v1_16[0][0]  \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 7, 512)       0           batch_normalization_v1_14[0][0]  \n",
      "                                                                 multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 7, 512)       0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 2, 512)       0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 512)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1024)         1049600     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1024)         4096        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 1024)         0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 16)           16400       dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,963,792\n",
      "Trainable params: 21,952,784\n",
      "Non-trainable params: 11,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resemul(input_shape=input_shape, num_classes=output_size)\n",
    "model.summary()\n",
    "model = multi_gpu_model(model, gpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_RESE_SGD_checkpoint/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1545 - acc: 0.6462\n",
      "Epoch 00001: val_loss improved from inf to 0.41933, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/001-0.4193.hdf5\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 1.1545 - acc: 0.6461 - val_loss: 0.4193 - val_acc: 0.8670\n",
      "Epoch 2/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8682\n",
      "Epoch 00002: val_loss improved from 0.41933 to 0.24594, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/002-0.2459.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.4203 - acc: 0.8681 - val_loss: 0.2459 - val_acc: 0.9215\n",
      "Epoch 3/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9021\n",
      "Epoch 00003: val_loss improved from 0.24594 to 0.15076, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/003-0.1508.hdf5\n",
      "36805/36805 [==============================] - 124s 3ms/sample - loss: 0.3096 - acc: 0.9020 - val_loss: 0.1508 - val_acc: 0.9520\n",
      "Epoch 4/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9217\n",
      "Epoch 00004: val_loss improved from 0.15076 to 0.14494, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/004-0.1449.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.2471 - acc: 0.9217 - val_loss: 0.1449 - val_acc: 0.9562\n",
      "Epoch 5/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9314\n",
      "Epoch 00005: val_loss did not improve from 0.14494\n",
      "36805/36805 [==============================] - 125s 3ms/sample - loss: 0.2115 - acc: 0.9313 - val_loss: 0.1536 - val_acc: 0.9497\n",
      "Epoch 6/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9386\n",
      "Epoch 00006: val_loss improved from 0.14494 to 0.11729, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/006-0.1173.hdf5\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.1883 - acc: 0.9385 - val_loss: 0.1173 - val_acc: 0.9623\n",
      "Epoch 7/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9446\n",
      "Epoch 00007: val_loss did not improve from 0.11729\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.1690 - acc: 0.9445 - val_loss: 0.1511 - val_acc: 0.9529\n",
      "Epoch 8/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9472\n",
      "Epoch 00008: val_loss did not improve from 0.11729\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.1619 - acc: 0.9471 - val_loss: 0.1174 - val_acc: 0.9639\n",
      "Epoch 9/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9524\n",
      "Epoch 00009: val_loss did not improve from 0.11729\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.1470 - acc: 0.9524 - val_loss: 0.1174 - val_acc: 0.9644\n",
      "Epoch 10/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9566\n",
      "Epoch 00010: val_loss improved from 0.11729 to 0.11349, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/010-0.1135.hdf5\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.1318 - acc: 0.9566 - val_loss: 0.1135 - val_acc: 0.9630\n",
      "Epoch 11/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9581\n",
      "Epoch 00011: val_loss improved from 0.11349 to 0.10028, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/011-0.1003.hdf5\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.1269 - acc: 0.9580 - val_loss: 0.1003 - val_acc: 0.9704\n",
      "Epoch 12/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9621\n",
      "Epoch 00012: val_loss did not improve from 0.10028\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.1116 - acc: 0.9621 - val_loss: 0.1035 - val_acc: 0.9695\n",
      "Epoch 13/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9681\n",
      "Epoch 00013: val_loss improved from 0.10028 to 0.09870, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/013-0.0987.hdf5\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0989 - acc: 0.9680 - val_loss: 0.0987 - val_acc: 0.9723\n",
      "Epoch 14/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 00014: val_loss improved from 0.09870 to 0.09304, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/014-0.0930.hdf5\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0975 - acc: 0.9673 - val_loss: 0.0930 - val_acc: 0.9718\n",
      "Epoch 15/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9703\n",
      "Epoch 00015: val_loss improved from 0.09304 to 0.09110, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/015-0.0911.hdf5\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0876 - acc: 0.9702 - val_loss: 0.0911 - val_acc: 0.9732\n",
      "Epoch 16/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9722\n",
      "Epoch 00016: val_loss did not improve from 0.09110\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0831 - acc: 0.9722 - val_loss: 0.0912 - val_acc: 0.9732\n",
      "Epoch 17/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9741\n",
      "Epoch 00017: val_loss did not improve from 0.09110\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0782 - acc: 0.9741 - val_loss: 0.1022 - val_acc: 0.9706\n",
      "Epoch 18/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9738\n",
      "Epoch 00018: val_loss did not improve from 0.09110\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0750 - acc: 0.9738 - val_loss: 0.0958 - val_acc: 0.9734\n",
      "Epoch 19/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9755\n",
      "Epoch 00019: val_loss did not improve from 0.09110\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0720 - acc: 0.9754 - val_loss: 0.1042 - val_acc: 0.9704\n",
      "Epoch 20/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9752\n",
      "Epoch 00020: val_loss did not improve from 0.09110\n",
      "36805/36805 [==============================] - 126s 3ms/sample - loss: 0.0750 - acc: 0.9751 - val_loss: 0.0979 - val_acc: 0.9734\n",
      "Epoch 21/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9776\n",
      "Epoch 00021: val_loss improved from 0.09110 to 0.08958, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/021-0.0896.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0659 - acc: 0.9775 - val_loss: 0.0896 - val_acc: 0.9741\n",
      "Epoch 22/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9756\n",
      "Epoch 00022: val_loss improved from 0.08958 to 0.08603, saving model to model/checkpoint/SampleCNN_RESE_SGD_checkpoint/022-0.0860.hdf5\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0722 - acc: 0.9756 - val_loss: 0.0860 - val_acc: 0.9767\n",
      "Epoch 23/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9805\n",
      "Epoch 00023: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0581 - acc: 0.9804 - val_loss: 0.0951 - val_acc: 0.9739\n",
      "Epoch 24/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9806\n",
      "Epoch 00024: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0587 - acc: 0.9805 - val_loss: 0.0912 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9831\n",
      "Epoch 00025: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0508 - acc: 0.9830 - val_loss: 0.0957 - val_acc: 0.9741\n",
      "Epoch 26/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9835\n",
      "Epoch 00026: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0475 - acc: 0.9834 - val_loss: 0.0991 - val_acc: 0.9741\n",
      "Epoch 27/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9853\n",
      "Epoch 00027: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0450 - acc: 0.9852 - val_loss: 0.0912 - val_acc: 0.9755\n",
      "Epoch 28/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9853\n",
      "Epoch 00028: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0428 - acc: 0.9852 - val_loss: 0.0906 - val_acc: 0.9755\n",
      "Epoch 29/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9863\n",
      "Epoch 00029: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0423 - acc: 0.9863 - val_loss: 0.1008 - val_acc: 0.9755\n",
      "Epoch 30/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9865\n",
      "Epoch 00030: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0397 - acc: 0.9864 - val_loss: 0.0930 - val_acc: 0.9755\n",
      "Epoch 31/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9859\n",
      "Epoch 00031: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0418 - acc: 0.9858 - val_loss: 0.1049 - val_acc: 0.9730\n",
      "Epoch 32/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9869\n",
      "Epoch 00032: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0378 - acc: 0.9868 - val_loss: 0.0947 - val_acc: 0.9748\n",
      "Epoch 33/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9880\n",
      "Epoch 00033: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0367 - acc: 0.9879 - val_loss: 0.0927 - val_acc: 0.9772\n",
      "Epoch 34/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9879\n",
      "Epoch 00034: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0354 - acc: 0.9877 - val_loss: 0.2139 - val_acc: 0.9502\n",
      "Epoch 35/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9726\n",
      "Epoch 00035: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0846 - acc: 0.9726 - val_loss: 0.1126 - val_acc: 0.9718\n",
      "Epoch 36/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9824\n",
      "Epoch 00036: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0544 - acc: 0.9823 - val_loss: 0.0980 - val_acc: 0.9753\n",
      "Epoch 37/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9855\n",
      "Epoch 00037: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0440 - acc: 0.9853 - val_loss: 0.3180 - val_acc: 0.9241\n",
      "Epoch 38/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9625\n",
      "Epoch 00038: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.1208 - acc: 0.9624 - val_loss: 0.1039 - val_acc: 0.9713\n",
      "Epoch 39/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9795\n",
      "Epoch 00039: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0626 - acc: 0.9794 - val_loss: 0.3704 - val_acc: 0.9136\n",
      "Epoch 40/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9605\n",
      "Epoch 00040: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.1210 - acc: 0.9604 - val_loss: 0.1749 - val_acc: 0.9527\n",
      "Epoch 41/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9635\n",
      "Epoch 00041: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.1143 - acc: 0.9634 - val_loss: 0.1037 - val_acc: 0.9693\n",
      "Epoch 42/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9779\n",
      "Epoch 00042: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0651 - acc: 0.9778 - val_loss: 0.0936 - val_acc: 0.9744\n",
      "Epoch 43/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9816\n",
      "Epoch 00043: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0553 - acc: 0.9816 - val_loss: 0.0947 - val_acc: 0.9737\n",
      "Epoch 44/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9831\n",
      "Epoch 00044: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0493 - acc: 0.9830 - val_loss: 0.0963 - val_acc: 0.9744\n",
      "Epoch 45/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9861\n",
      "Epoch 00045: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0411 - acc: 0.9860 - val_loss: 0.1177 - val_acc: 0.9700\n",
      "Epoch 46/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9769\n",
      "Epoch 00046: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0684 - acc: 0.9768 - val_loss: 0.0933 - val_acc: 0.9741\n",
      "Epoch 47/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9862\n",
      "Epoch 00047: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0415 - acc: 0.9861 - val_loss: 0.1026 - val_acc: 0.9739\n",
      "Epoch 48/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9831\n",
      "Epoch 00048: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0499 - acc: 0.9830 - val_loss: 0.1172 - val_acc: 0.9709\n",
      "Epoch 49/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9839\n",
      "Epoch 00049: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0465 - acc: 0.9838 - val_loss: 0.1023 - val_acc: 0.9744\n",
      "Epoch 50/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9883\n",
      "Epoch 00050: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0359 - acc: 0.9882 - val_loss: 0.1055 - val_acc: 0.9730\n",
      "Epoch 51/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9897\n",
      "Epoch 00051: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0306 - acc: 0.9896 - val_loss: 0.1168 - val_acc: 0.9723\n",
      "Epoch 52/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9855\n",
      "Epoch 00052: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0446 - acc: 0.9854 - val_loss: 0.0992 - val_acc: 0.9746\n",
      "Epoch 53/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9898\n",
      "Epoch 00053: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0302 - acc: 0.9896 - val_loss: 0.0900 - val_acc: 0.9748\n",
      "Epoch 54/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9905\n",
      "Epoch 00054: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0300 - acc: 0.9904 - val_loss: 0.1122 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9805\n",
      "Epoch 00055: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0589 - acc: 0.9805 - val_loss: 0.1168 - val_acc: 0.9713\n",
      "Epoch 56/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9885\n",
      "Epoch 00056: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0342 - acc: 0.9884 - val_loss: 0.0975 - val_acc: 0.9753\n",
      "Epoch 57/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9874\n",
      "Epoch 00057: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0380 - acc: 0.9874 - val_loss: 0.0938 - val_acc: 0.9779\n",
      "Epoch 58/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9907\n",
      "Epoch 00058: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 125s 3ms/sample - loss: 0.0278 - acc: 0.9906 - val_loss: 0.0938 - val_acc: 0.9769\n",
      "Epoch 59/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9920\n",
      "Epoch 00059: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0924 - val_acc: 0.9793\n",
      "Epoch 60/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9925\n",
      "Epoch 00060: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0218 - acc: 0.9924 - val_loss: 0.1106 - val_acc: 0.9739\n",
      "Epoch 61/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 00061: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0395 - acc: 0.9869 - val_loss: 0.0959 - val_acc: 0.9755\n",
      "Epoch 62/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9914\n",
      "Epoch 00062: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0258 - acc: 0.9914 - val_loss: 0.0997 - val_acc: 0.9753\n",
      "Epoch 63/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9926\n",
      "Epoch 00063: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0998 - val_acc: 0.9776\n",
      "Epoch 64/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9934\n",
      "Epoch 00064: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0967 - val_acc: 0.9769\n",
      "Epoch 65/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9930\n",
      "Epoch 00065: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0206 - acc: 0.9929 - val_loss: 0.1017 - val_acc: 0.9767\n",
      "Epoch 66/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9939\n",
      "Epoch 00066: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0193 - acc: 0.9938 - val_loss: 0.1555 - val_acc: 0.9676\n",
      "Epoch 67/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 00067: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0973 - val_acc: 0.9765\n",
      "Epoch 68/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 00068: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0186 - acc: 0.9942 - val_loss: 0.1354 - val_acc: 0.9693\n",
      "Epoch 69/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9901\n",
      "Epoch 00069: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0311 - acc: 0.9900 - val_loss: 0.1022 - val_acc: 0.9772\n",
      "Epoch 70/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9944\n",
      "Epoch 00070: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0182 - acc: 0.9943 - val_loss: 0.1711 - val_acc: 0.9613\n",
      "Epoch 71/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9807\n",
      "Epoch 00071: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0590 - acc: 0.9805 - val_loss: 0.1420 - val_acc: 0.9658\n",
      "Epoch 72/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9803\n",
      "Epoch 00072: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0587 - acc: 0.9802 - val_loss: 0.1325 - val_acc: 0.9676\n",
      "Epoch 73/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9810\n",
      "Epoch 00073: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0583 - acc: 0.9809 - val_loss: 0.1024 - val_acc: 0.9732\n",
      "Epoch 74/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9904\n",
      "Epoch 00074: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0298 - acc: 0.9903 - val_loss: 0.1006 - val_acc: 0.9767\n",
      "Epoch 75/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9919\n",
      "Epoch 00075: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0267 - acc: 0.9918 - val_loss: 0.0995 - val_acc: 0.9755\n",
      "Epoch 76/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9927\n",
      "Epoch 00076: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0225 - acc: 0.9927 - val_loss: 0.1114 - val_acc: 0.9727\n",
      "Epoch 77/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 00077: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 130s 4ms/sample - loss: 0.0192 - acc: 0.9935 - val_loss: 0.0965 - val_acc: 0.9772\n",
      "Epoch 78/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9942\n",
      "Epoch 00078: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0175 - acc: 0.9941 - val_loss: 0.0978 - val_acc: 0.9779\n",
      "Epoch 79/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9943\n",
      "Epoch 00079: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 123s 3ms/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 0.0955 - val_acc: 0.9793\n",
      "Epoch 80/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9946\n",
      "Epoch 00080: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 126s 3ms/sample - loss: 0.0153 - acc: 0.9945 - val_loss: 0.0954 - val_acc: 0.9779\n",
      "Epoch 81/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9943\n",
      "Epoch 00081: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.1307 - val_acc: 0.9702\n",
      "Epoch 82/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9885\n",
      "Epoch 00082: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0330 - acc: 0.9884 - val_loss: 0.1068 - val_acc: 0.9758\n",
      "Epoch 83/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915\n",
      "Epoch 00083: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0255 - acc: 0.9914 - val_loss: 0.1045 - val_acc: 0.9769\n",
      "Epoch 84/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9949\n",
      "Epoch 00084: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0165 - acc: 0.9948 - val_loss: 0.1074 - val_acc: 0.9748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9904\n",
      "Epoch 00085: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0301 - acc: 0.9903 - val_loss: 0.1022 - val_acc: 0.9767\n",
      "Epoch 86/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9938\n",
      "Epoch 00086: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0184 - acc: 0.9937 - val_loss: 0.1387 - val_acc: 0.9706\n",
      "Epoch 87/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9885\n",
      "Epoch 00087: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0353 - acc: 0.9883 - val_loss: 0.1047 - val_acc: 0.9755\n",
      "Epoch 88/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9920\n",
      "Epoch 00088: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0237 - acc: 0.9918 - val_loss: 0.0880 - val_acc: 0.9781\n",
      "Epoch 89/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9920\n",
      "Epoch 00089: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.1257 - val_acc: 0.9720\n",
      "Epoch 90/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9889\n",
      "Epoch 00090: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0336 - acc: 0.9888 - val_loss: 0.1067 - val_acc: 0.9737\n",
      "Epoch 91/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9938\n",
      "Epoch 00091: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0176 - acc: 0.9937 - val_loss: 0.1082 - val_acc: 0.9737\n",
      "Epoch 92/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9944\n",
      "Epoch 00092: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0162 - acc: 0.9943 - val_loss: 0.0993 - val_acc: 0.9746\n",
      "Epoch 93/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9902\n",
      "Epoch 00093: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0289 - acc: 0.9901 - val_loss: 0.0952 - val_acc: 0.9767\n",
      "Epoch 94/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9944\n",
      "Epoch 00094: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0935 - val_acc: 0.9760\n",
      "Epoch 95/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9946\n",
      "Epoch 00095: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0153 - acc: 0.9945 - val_loss: 0.0947 - val_acc: 0.9772\n",
      "Epoch 96/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9955\n",
      "Epoch 00096: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0135 - acc: 0.9954 - val_loss: 0.1054 - val_acc: 0.9755\n",
      "Epoch 97/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9937\n",
      "Epoch 00097: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0997 - val_acc: 0.9776\n",
      "Epoch 98/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9952\n",
      "Epoch 00098: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0136 - acc: 0.9952 - val_loss: 0.1003 - val_acc: 0.9788\n",
      "Epoch 99/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9955\n",
      "Epoch 00099: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0128 - acc: 0.9954 - val_loss: 0.0957 - val_acc: 0.9786\n",
      "Epoch 100/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 00100: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0988 - val_acc: 0.9762\n",
      "Epoch 101/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9962\n",
      "Epoch 00101: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0113 - acc: 0.9961 - val_loss: 0.0979 - val_acc: 0.9769\n",
      "Epoch 102/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9973\n",
      "Epoch 00102: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0954 - val_acc: 0.9800\n",
      "Epoch 103/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9974\n",
      "Epoch 00103: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 126s 3ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1006 - val_acc: 0.9779\n",
      "Epoch 104/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
      "Epoch 00104: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0089 - acc: 0.9972 - val_loss: 0.1126 - val_acc: 0.9741\n",
      "Epoch 105/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9925\n",
      "Epoch 00105: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0234 - acc: 0.9924 - val_loss: 0.0991 - val_acc: 0.9772\n",
      "Epoch 106/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9952\n",
      "Epoch 00106: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.0138 - acc: 0.9951 - val_loss: 0.1091 - val_acc: 0.9774\n",
      "Epoch 107/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9950\n",
      "Epoch 00107: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0152 - acc: 0.9949 - val_loss: 0.1088 - val_acc: 0.9746\n",
      "Epoch 108/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9961\n",
      "Epoch 00108: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0123 - acc: 0.9960 - val_loss: 0.1079 - val_acc: 0.9786\n",
      "Epoch 109/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00109: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0103 - acc: 0.9968 - val_loss: 0.1109 - val_acc: 0.9760\n",
      "Epoch 110/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 00110: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 129s 3ms/sample - loss: 0.0095 - acc: 0.9973 - val_loss: 0.1497 - val_acc: 0.9720\n",
      "Epoch 111/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9870\n",
      "Epoch 00111: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0433 - acc: 0.9869 - val_loss: 0.1081 - val_acc: 0.9762\n",
      "Epoch 112/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 00112: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 0.1052 - val_acc: 0.9769\n",
      "Epoch 113/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 00113: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0136 - acc: 0.9955 - val_loss: 0.1066 - val_acc: 0.9753\n",
      "Epoch 114/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9966\n",
      "Epoch 00114: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0109 - acc: 0.9965 - val_loss: 0.1023 - val_acc: 0.9769\n",
      "Epoch 115/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9966\n",
      "Epoch 00115: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 0.2372 - val_acc: 0.9550\n",
      "Epoch 116/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9872\n",
      "Epoch 00116: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0403 - acc: 0.9871 - val_loss: 0.1027 - val_acc: 0.9767\n",
      "Epoch 117/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9935\n",
      "Epoch 00117: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0186 - acc: 0.9934 - val_loss: 0.1116 - val_acc: 0.9765\n",
      "Epoch 118/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9940\n",
      "Epoch 00118: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0167 - acc: 0.9939 - val_loss: 0.0994 - val_acc: 0.9767\n",
      "Epoch 119/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9960\n",
      "Epoch 00119: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 0.0136 - acc: 0.9959 - val_loss: 0.1114 - val_acc: 0.9744\n",
      "Epoch 120/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9964\n",
      "Epoch 00120: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0115 - acc: 0.9963 - val_loss: 0.1030 - val_acc: 0.9760\n",
      "Epoch 121/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9970\n",
      "Epoch 00121: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.1098 - val_acc: 0.9762\n",
      "Epoch 122/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 00122: val_loss did not improve from 0.08603\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0964 - val_acc: 0.9779\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=10000, \n",
    "                 validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                 callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "png_path = 'visualization/learning_curve/'\n",
    "filename = 'SampleCNN_RESE_SGD'+'.png'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "fig.savefig(png_path+filename, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1271 - acc: 0.9674\n",
      "Loss: 0.12708916977688145 Accuracy: 0.9673936\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_RESE_SGD_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "model = load_model(model_filename)\n",
    "[loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[373   0   4   0   3   0   0   3   0   2   0   0   0   0   0   0]\n",
      " [  1 348   0   0   4   2   0   0   0   4   1   4   0   0   0   0]\n",
      " [  3   0 379   1   1   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   3 358   2   0   1   0   1   0   0   2   0   0   0   9]\n",
      " [  0   1   0   0 363   1   0   0   0   1   0   1   0   0   1   0]\n",
      " [  0   1   0   2   5 396   0   0   2   1   1   0   0   0   0   0]\n",
      " [  0   0   1   3   1   1 365   1   1   0   0   1   0   0   0   0]\n",
      " [  1   0   1   1   2   0   0 366   1   0   3   1   0   0   0   0]\n",
      " [  0   0   4   3   0   0   0   0 359   4   2   3   0   0   0   1]\n",
      " [  0   2   0   0   0   4   0   0   0 369   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2 176   3   0   1   0   1]\n",
      " [  0   0   0   0   1   0   0   0   1   1   3 145   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0 165   1   0   1]\n",
      " [  0   0   4   0   0   0   0   2   0   0   1   1   0 184   0   0]\n",
      " [  0   0   0   0   0   1   0   1   1   0   0   0   2   0 162   0]\n",
      " [  0   0   2  11   0   0   0   0   0   0   0   0   0   0   0 150]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        zero       0.98      0.97      0.98       385\n",
      "         one       0.99      0.96      0.97       364\n",
      "         two       0.95      0.99      0.97       384\n",
      "       three       0.94      0.95      0.95       377\n",
      "        four       0.95      0.99      0.97       368\n",
      "        five       0.98      0.97      0.97       408\n",
      "         six       1.00      0.98      0.99       374\n",
      "       seven       0.98      0.97      0.98       376\n",
      "       eight       0.98      0.95      0.97       376\n",
      "        nine       0.96      0.98      0.97       377\n",
      "         bed       0.94      0.96      0.95       183\n",
      "        bird       0.89      0.95      0.92       153\n",
      "         cat       0.99      0.98      0.99       168\n",
      "         dog       0.98      0.96      0.97       192\n",
      "       house       0.99      0.97      0.98       167\n",
      "        tree       0.93      0.92      0.92       163\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4815\n",
      "   macro avg       0.96      0.97      0.96      4815\n",
      "weighted avg       0.97      0.97      0.97      4815\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test_abs)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "y_real = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_real, y_pred, target_names=y_table.T[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
