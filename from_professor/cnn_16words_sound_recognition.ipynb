{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n",
    "\n",
    "Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n",
    "\n",
    "Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n",
    "\n",
    "Prepared by JH Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Classifying Images with Deep Convolutional Neural Networks\n",
    "\n",
    "\n",
    "In this chapter, we'll learn about __Convolutional Neural Networks (CNNs)__ and how to implement CNNs in TensorFlow (TF).\n",
    "\n",
    "* Understanding convolution operations in one and two dimensions\n",
    "* Learning about the building blocks of CNN architectures\n",
    "* Implementing deep CNNs in TF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Building blocks of convolutional neural networks](#Building-blocks-of-convolutional-neural-networks)\n",
    "  - [Understanding CNNs and learning feature hierarchies](#Understanding-CNNs-and-learning-feature-hierarchies)\n",
    "  - [Performing discrete convolutions](#Performing-discrete-convolutions)\n",
    "    - [Performing a discrete convolution in one dimension](#Performing-a-discrete-convolution-in-one-dimension)\n",
    "    - [The effect of zero-padding in convolution](#The-effect-of-zero-padding-in-convolution)\n",
    "    - [Determining the size of the convolution output](#Determining-the-size-of-the-convolution-output)\n",
    "    - [Performing a discrete convolution in 2D](#Performing-a-discrete-convolution-in-2D)\n",
    "    - [Sub-sampling](#Sub-sampling)\n",
    "  - [Putting everything together to build a CNN](#Putting-everything-together-to-build-a-CNN)\n",
    "    - [The multilayer CNN architecture](#The-multilayer-CNN-architecture)\n",
    "    - [Loading and preprocessing the data](#Loading-and-preprocessing-the-data)\n",
    "    - [Implementing a CNN in TensorFlow low-level API](#Implementing-a-CNN-in-TensorFlow-low-level-API)\n",
    "    - [Implementing a CNN in the TensorFlow layers API](#Implementing-a-CNN-in-the-TensorFlow-layers-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-578a4987e602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pil'"
     ]
    }
   ],
   "source": [
    "from pil import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Python 2.7 compatibility\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of convolutional neural networks \n",
    "\n",
    "\n",
    "Convolutional neural networks, or (CNNs) are a family of models that were inspired by how the visual cortex of human brain works when recognizing objects.\n",
    "* Yann LeCun and his colleagues propoesd a novel neural network architecture for classifying handwritten digits from images (Handwritten Digit Recognition with a Back-Propagation Network, Y LeCun, and others, 1989, published at Neural Information Processing Systems.(NIPS) conference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CNNs and learning feature hierarchies\n",
    "\n",
    "Successfully extracting __salient (relevant) features__ is key to the performance of any machine learning algorithm.\n",
    "* traditional machine learning models rely on input features that may come from a domain expert, or are based on computational feature extraction techniques\n",
    "* neural networks are able to automatically learn the features from raw data that are most useful for a particular task\n",
    "    * for this reason, it's common to consider a neural network as a feature extraction engine: the early layers (those right after the input layer) extract __low-level features__ (e.g., edges and blobs) \n",
    "    * high-level features (e.g., object shapes like a building, a car, or a dog)\n",
    "* multilayer neural networks and deep CNNs construct a so-called __feature hierarchy__ by combining the low-level features in a layer-wise fashion to form high-level features\n",
    "\n",
    "A CNN computes __feature maps__ from an input image, where each element comes from a local patch of pixels in the input image:\n",
    "\n",
    "<img src='images/15_01.png' width=700> \n",
    "\n",
    "This local patch of pixels is referred to as the __local receptive field__. \n",
    "\n",
    "CNNs will usually perform very well for image-related tasks largely due to two important ideas:\n",
    "* __Sparse-connectivity__: a single element in the feature map is connected to only a small patch of pixels (cf. connecting to the whole input image)\n",
    "* __Parmeter-sharing__: the same weights are used for different patches of the input image\n",
    "\n",
    "The number of weights (parameters) in the network decreases dramatically and ability to capture __salient__ feaures is improved. \n",
    "\n",
    "Typically, CNNs are composed of several __Convolutional (conv)__ layers and subsampling (a.k.a. __Pooling (P)__) layers that are followed by one or more __Fully Connected (FC)__ layers at the end.\n",
    "* there is no weights or bias in the pooling layers, however, both convolution and fully connected layers have weights and biases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing discrete convolutions  \n",
    "\n",
    "A __discrete convolution__ (or simply __convolution__) is a fundamental operation in a CNN.\n",
    "* in this section, the mathematical definition and discuss some of the __naive__ algorithms to compute convolutions of two one-dimensional vectors or two two-dimensional matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Performing a discrete convolution in one dimension\n",
    "\n",
    "A discrete convolution is defined as follows:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x} * \\mathbf{w} \\longrightarrow y[i] = \\sum^{+\\infty}_{k = -\\infty} x[i-k] w[k] $$\n",
    "\n",
    "In practical situations, the bounded feature vector $\\mathbf{x}$ is padded with a finite number of zeros, which is called a __zero-padding__ or simply __padding__.\n",
    "\n",
    "The number of zeros padded on each side is denoted by $p$. \n",
    "e.g.,\n",
    "<img src='images/15_02.png' width=700>\n",
    "\n",
    "Let's assume that the original input $\\mathbf{x}$ and filter $\\mathbf{w}$ have $n$ and $m$ elements, respectively, where $m \\le n$. Thus, the padded vector $\\mathbf{x}^p$ has size $n+2p$. \n",
    "\n",
    "The practical formula for computing a discrete convolution will change to the following:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x} * \\mathbf{w} \\longrightarrow y[i] = \\sum^{k=m-1}_{k=0} x^p [i+m-k] w[k] $$\n",
    "\n",
    "Let's assume we flip the filter $\\mathbf{w}$ to get the rotated filter $\\mathbf{w}^r$. Then, the dot product $\\mathbf{x}[i:i+m] \\cdot \\mathbf{w}^r$ is computed to get one element $\\mathbf{y}[i]$, where $\\mathbf{x} [i:i+m]$ is a patch of $\\mathbf{x}$ with size $m$.\n",
    "\n",
    "This operation is repeated like in a sliding window approach to get all the output elements, as exemplified below:\n",
    "\n",
    "<img src='images/15_03.png' width=700> \n",
    "* padding size is zero ($p=0$)\n",
    "* __shift__ of the rotated filter $\\mathbf{w}^r$ is another hyperparameter of a convolution, the __stride__ $s$ (e.g., $s = 2$ in this example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of zero-padding in a convolution\n",
    "\n",
    "Depending on the choice of $p$, boundary cells may be treated differently than the cells located in the middle of $\\mathbf{x}$.\n",
    "e.g., an example where $n = 5, m = 3$. Then, $p = 0$, $\\mathbf{x}[0]$ is only used in computing one output element (i.e., y[0]), whereas $\\mathbf{x}[1]$ is used in the computation of two output elements (i.e., y[0] and y[1]).\n",
    "* put more emphsis on x[2] than x[1] \n",
    "\n",
    "The size of the output $\\mathbf{y}$ also depends on the choice of the padding strategy we use. \n",
    "\n",
    "Three modes of padding that are commonly used in practice: __full__, __same__, and __valid__\n",
    "* in the __full__ mode, $p = m - 1$. increased the dimension of the output, and thus rarely used in CNN architecture\n",
    "* in the __same__ padding, the size of the output is the same as the size of the input vector $\\mathbf{x}$\n",
    "* in the __valid__ mode, $p = 0$ (no padding)\n",
    "\n",
    "In the following figure, the three different padding modes were examplified for a simple $5 \\times 5$ pixel input with a kernel size of $3 \\times 3$ and a stride of 1:\n",
    "\n",
    "<img src='images/15_11.png' width=700> \n",
    "* __same__ padding is commonly used in CNN since the height and width of the input images or tensors are preserved and designing a network architecture more convenient\n",
    "* __full__ padding is usually used in signal processing applications where it is important to minimize boundary effects, however, in deep learning context, bounday effect is not usually an issue, so full padding is rarely seen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the size of the convolution output\n",
    "\n",
    "The output of a convolution is determined by the total number of times that we shift the filter $\\mathbf{w}$ along the input vector. The size of the output resulting from $\\mathbf{x} * \\mathbf{w}$ with padding $p$ and stride $s$ is determined as follows:\n",
    "\n",
    "$$ o = \\left\\lfloor \\frac{n + 2p - m}{s} + 1 \\right\\rfloor, $$\n",
    "where $\\lfloor \\cdot \\rfloor$ denotes the floor operation.\n",
    "\n",
    "\n",
    "Finally, a naive implementation is shown in the following code block, and the results are compared with the `numpy.convolve` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation:  [  5.  14.  16.  26.  24.  34.  19.  22.]\n",
      "Numpy Results:          [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s),s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "print('Conv1d Implementation: ', \n",
    "      conv1d(x, w, p=2, s=1))\n",
    "print('Numpy Results:         ', \n",
    "      np.convolve(x, w, mode='same'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a discrete convolution in 2D\n",
    "\n",
    "The concepts in the previous sections are easily extendible to two dimensions.\n",
    "\n",
    "When we deal with 2D input, such as a matrix $\\mathbf{X}_{n_1 \\times n_2}$ and the filter matrix $\\mathbf{W}_{m_1 \\times m_2}$, where $m_1 \\le n_1$ and $m_2 \\le n_2$, then the matrix $\\mathbf{Y} = \\mathbf{X} * \\mathbf{W}$ is the result of 2D convolution of $\\mathbf{X}$ with $\\mathbf{W}$ defined as follows:\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{X} * \\mathbf{W} \\longrightarrow \\mathbf{Y}[i,j] = \\sum^{+\\infty}_{k_1=-\\infty} \\sum^{+\\infty}_{k_2=-\\infty} \\mathbf{X}[i-k_1, j-k_2] \\mathbf{W}[k_1, k_2] $$\n",
    "\n",
    "The following example illustrates the computation of a 2D convolution between an input matrix $\\mathbf{X}_{3\\times3}$, a kernel matrix $\\mathbf{W}_{3\\times3}$, a padding $p=(1,1)$, and stride $s=(2,2)$.\n",
    "\n",
    "First, the zero padded matrix $\\mathbf{X}^{padded}_{5\\times5}$:\n",
    "\n",
    "<img src='images/15_04.png' width=500> \n",
    "\n",
    "The rotated filter will be:\n",
    "\n",
    "$$ \\mathbf{W}^r = \\begin{bmatrix} 0.5 & 1 & 0.5 \\\\ 0.1 & 0.4 & 0.3 \\\\ 0.4 & 0.7 & 0.5 \\end{bmatrix} $$\n",
    "\n",
    "Next, shifting the rotated filter matrix along the padded input matrix $\\mathbf{X}^{padded}$ like a sliding window and compute the sum of the element-wise product ($\\odot$):\n",
    "\n",
    "<img src='images/15_05.png' width=700> \n",
    "\n",
    "Let's implement 2D convolution according to the naive algorithm described. \n",
    "\n",
    "The `scipy.signal` package provides a way to compute 2D convolution via the `scipy.signal.convolve2d` function:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation: \n",
      " [[ 11.  25.  32.  13.]\n",
      " [ 19.  25.  24.  13.]\n",
      " [ 13.  28.  25.  17.]\n",
      " [ 11.  17.  14.   9.]]\n",
      "Scipy Results:         \n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def conv2d(X, W, p=(0,0), s=(1,1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1,n2))\n",
    "    X_padded[p[0]:p[0] + X_orig.shape[0], \n",
    "             p[1]:p[1] + X_orig.shape[1]] = X_orig\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - \n",
    "                           W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - \n",
    "                               W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return(np.array(res))\n",
    "    \n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1 , 2,0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "print('Conv2d Implementation: \\n', \n",
    "      conv2d(X, W, p=(1,1), s=(1,1)))\n",
    "\n",
    "print('Scipy Results:         \\n', \n",
    "      scipy.signal.convolve2d(X, W, mode='same'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Subsampling is typically applied in two forms of pooling operations in CNNs: __max-pooling__ and __mean-pooling__ (a.k.a. __average-pooling__).\n",
    "\n",
    "The pooling layer is usually denoted by $\\mathbf{P}_{n_1 \\times n_2}$, where the subscript determines the size of the neighborhood that the max or mean operation is performed (i.e., __pooling size__).\n",
    "\n",
    "The operation is described in the following figure:\n",
    "\n",
    "<img src='images/15_06.png' width=500> \n",
    "\n",
    "The advantage of pooling is twofold:\n",
    "* introduces some sort of local invariance\n",
    "    * small changes in a local neighborhood do not change the result of max-pooling\n",
    "    * helps to generate features more robust to noise in the input data\n",
    "    <img src='images/15_X1X2.png' width=500>\n",
    "* decrease the size of features\n",
    "    * results in higher computational efficiency\n",
    "    * reduce the number of features and may reduce the degree of overfitting as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together to build a CNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with multiple input or color channels\n",
    "\n",
    "\n",
    "An input sample to a convolutional layer may contain one or more 2D arrays or matrices with dimensions $N_1 \\times N_2$ (e.g., the image height and width in pixels).\n",
    "\n",
    "These $N_1 \\times N_2$ matrices are called __channels__. Therefore, rank-3 tensors or 3D array, $\\mathbf{X}_{N_1 \\times N_2 \\times C_{in}}$, where $C_{in}$ is the number of input channels, is required as input to convolutional layer (e.g., $C_{in} = 3$ for color image, $C_{in} = 1$ for grayscale image).\n",
    "\n",
    "The convolution operation is performed for each channel separately and then add the results together using the matrix summation.\n",
    "\n",
    "The total pre-activation result is computed in the following formula:\n",
    "\n",
    "<img src='images/15_FM_Cin.png' width=500>\n",
    "* the final result, $\\mathbf{h}$ is called a __feature map__\n",
    "\n",
    "If we use multiple feature maps, the kernel tensor becomes 4D: $width \\times height \\times C_{in} \\times C_{out}$.\n",
    "\n",
    "Now, let's include the number of output feature maps:\n",
    "\n",
    "<img src='images/15_FM_CinCout.png' width=500>\n",
    "\n",
    "To conclude our discussion of computing convolutions in the context of neural networks, let's look at the example in the following figure that shows a convolutional layer followed by a pooling layer:\n",
    "\n",
    "<img src='images/15_07.png' width=800> \n",
    "\n",
    "* three input channels\n",
    "* kernel tensor is 4D ($m_1 \\times m_2$ for each kernel), and there are three of them, one for each input channel\n",
    "* five such kernels, accounting for five output feature maps\n",
    "* finally, a polling layer for subsampling the feature maps\n",
    "\n",
    "Q: how many trainable parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing a neural network with dropout\n",
    "\n",
    "The __capacity__ of a network refer to the level of complexity of the function that can be learned.\n",
    "* small networks with a relatively small number of parameters, have a low capacity and are therefore likely to be __underfit__\n",
    "* very large networks may more easily result in __overfitting__\n",
    "    * memorize the training data and do extremely well on the training set while achieving poor performance on the held-out test set\n",
    "    \n",
    "One way to address this problem is to build a network with a relatively large capacity (in practice, that is slightly larger than necessary) to do well on the training set\n",
    "* then, one or multiple regularization schemes to achieve good generalization performance on new data such as the held-out test set (e.g., L2 regularization) \n",
    "\n",
    "In recent years, another popular regularization technique called __dropout__ has emerged.\n",
    "* intuitively, dropout can be considered as the consensus (averaging) of an ensemble of models \n",
    "* usually applied to the hidden units of higher layers\n",
    "* during the training phase of a neural network, a fraction of the hidden units is __randomly__ dropped at every iteration with probability $p_{drop}$ (or the keep probability $p_{keep} = 1 - p_{drop}$)\n",
    "    * when dropping a certain fraction of input neurons, the weights associated with the remaining neurons are rescaled to account for the missing (dropped) neurons\n",
    "    * the network cannot rely on an activation of any set of hidden units since they may be turned off at any time during trianing and is forced to learn more general and robust patterns from the data\n",
    " \n",
    "<img src='images/15_08.png' width=800> \n",
    "* to ensure that the overall activations are on the same scale during training and prediction, the activations of the active neurons have to be scaled appropriately (e.g., by halving the activation if the dropout probability was set to 0.5)\n",
    "    * in practice, TF and other tools scale the activations during training (e.g., by doubling the activations if the dropout probability was set to p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep convolutional neural network using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multilayer CNN architecture \n",
    "\n",
    "The architecture of the network to implement is shown in the following figure:\n",
    "\n",
    "<img src='images/15_09.png' width=800> \n",
    "* the input is $28 \\times 28$ grayscale image\n",
    "    * input tensor's dimension will be $batchsize \\times 28 \\times 28 \\times 1$\n",
    "* a kernel size is $5\\times5$ and 32 output feature maps from the first convolution layer and 64 output feature maps from the second convolution layer\n",
    "* a subsampling layer in the form of a max-pooloing operation is following\n",
    "* then, a fully-connected layer passes the output to a second fully-connected layer (which is a _softmax_ output layer)\n",
    "\n",
    "The dimensions of the tensors in each layer are as follows:\n",
    "* __Input__: [$batchsize \\times 28 \\times 28 \\times 1$]\n",
    "* __Conv_1__: [$batchsize \\times 24 \\times 24 \\times 32$]\n",
    "* __Pooling_1__: [$batchsize \\times 12 \\times 12 \\times 32$]\n",
    "* __Conv_2__: [$batchsize \\times 8 \\times 8 \\times 64$]\n",
    "* __Pooling_2__: [$batchsize \\times 4 \\times 4 \\times 64$]\n",
    "* __FC_1__: [$batchsize \\times 1024$]\n",
    "* __FC_2 and softmax layer__: [$batchsize \\times 10$]\n",
    "\n",
    "This network will be implemented using two APIs:\n",
    "* the low-level TF API\n",
    "* the TF Layers API\n",
    "\n",
    "First, let's define some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "The `load_mnist` function to read the MNIST handwritten digit dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzips mnist\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "if (sys.version_info > (3, 0)):\n",
    "    writemode = 'wb'\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir('./')\n",
    "                if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode='rb') as decompressed, open(z[:-3], writemode) as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n",
      "Training:    (50000, 784) (50000,)\n",
      "Validation:  (10000, 784) (10000,)\n",
      "Test Set:    (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(Path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                                % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_data, y_data = load_mnist('./', kind='train')\n",
    "print('Rows: %d,  Columns: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train, y_train = X_data[:50000,:], y_data[:50000]\n",
    "X_valid, y_valid = X_data[50000:,:], y_data[50000:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Set:   ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path \n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/data/01_experiment_data/img_snd/'\n",
    "sound_path = '/users/jhlee/data/img_snd/'\n",
    "\n",
    "train_list = os.path.join(root_path, 'train_16words_png.txt')\n",
    "validation_list = os.path.join(root_path, 'validation_16words_png.txt')\n",
    "test_list = os.path.join(root_path, 'test_16words_png.txt')\n",
    "\n",
    "img_dim = (99, 257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'bed', 'bird', 'cat', 'dog', 'house', 'tree']\n"
     ]
    }
   ],
   "source": [
    "list_words = ['zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "              'bed','bird','cat','dog','house','tree']\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n"
     ]
    }
   ],
   "source": [
    "X_train = np.empty((0,np.prod(img_dim)))\n",
    "y_train = np.empty(0, dtype=int)\n",
    "with open(train_list, 'rt') as filelist:\n",
    "    idx = 0\n",
    "    for fname in filelist.read().splitlines():\n",
    "        idx += 1\n",
    "        img = np.array(Image.open(os.path.join(sound_path,fname)))\n",
    "        if img.shape == img_dim:\n",
    "            X_train = np.append(X_train, img.reshape(1,-1), axis=0)  \n",
    "            y_train = np.append(y_train, \\\n",
    "               [i for i in range(len(list_words)) if os.path.split(fname)[0] == list_words[i]])\n",
    "        if idx % 100 == 0:    \n",
    "            print(idx)\n",
    "            \n",
    "X_validation = np.empty((0,np.prod(img_dim)))\n",
    "y_validation = np.empty(0, dtype=int)\n",
    "with open(validation_list, 'rt') as filelist:\n",
    "    idx = 0\n",
    "    for fname in filelist.read().splitlines():\n",
    "        idx += 1\n",
    "        img = np.array(Image.open(os.path.join(sound_path,fname)))\n",
    "        if img.shape == img_dim:\n",
    "            X_validation = np.append(X_validation, img.reshape(1,-1), axis=0)  \n",
    "            y_validation = np.append(y_validation, \\\n",
    "               [i for i in range(len(list_words)) if os.path.split(fname)[0] == list_words[i]])\n",
    "        if idx % 100 == 0:    \n",
    "            print(idx)\n",
    "            \n",
    "X_test = np.empty((0,np.prod(img_dim)))\n",
    "y_test = np.empty(0, dtype=int)\n",
    "with open(test_list, 'rt') as filelist:\n",
    "    idx = 0\n",
    "    for fname in filelist.read().splitlines():\n",
    "        idx += 1\n",
    "        img = np.array(Image.open(os.path.join(sound_path,fname)))\n",
    "        if img.shape == img_dim:\n",
    "            X_test = np.append(X_test, img.reshape(1,-1), axis=0)  \n",
    "            y_test = np.append(y_test, \\\n",
    "               [i for i in range(len(list_words)) if os.path.split(fname)[0] == list_words[i]])\n",
    "        if idx % 100 == 0:    \n",
    "            print(idx)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_list, 'rt') as filelist:\n",
    "    X_train = np.array([np.array(Image.open(os.path.join(sound_path,fname))) for fname in filelist.read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(validation_list, 'rt') as filelist:\n",
    "    X_validation = np.array([np.array(Image.open(os.path.join(sound_path,fname))) for fname in filelist.read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_list, 'rt') as filelist:\n",
    "    X_test = np.array([np.array(Image.open(os.path.join(sound_path,fname))) for fname in filelist.read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7a48246eb8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X_validation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = os.path.join(root_path, 'tmp_test_16words_png.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = np.empty((0,np.prod(img_dim)))\n",
    "y_test = np.empty(0, dtype=int)\n",
    "with open(tmp_list, 'rt') as filelist:\n",
    "    #filelist = open(test_list, 'rt') \n",
    "    #X_test = np.array([np.array(Image.open(os.path.join(sound_path,fname))) for fname in filelist.read().splitlines()])\n",
    "    #X_test = [np.array(Image.open(os.path.join(sound_path,fname))).reshape(np.prod(img_dim), 1) \n",
    "    idx = 0\n",
    "    for fname in filelist.read().splitlines():\n",
    "        idx += 1\n",
    "        img = np.array(Image.open(os.path.join(sound_path,fname)))\n",
    "        if img.shape == img_dim:\n",
    "            X_test = np.append(X_test, img.reshape(1,-1), axis=0)  \n",
    "            \n",
    "            y_test = np.append(y_test, \\\n",
    "               [i for i in range(len(list_words)) if os.path.split(fname)[0] == list_words[i]])\n",
    "            \n",
    "        if idx % 100 == 0:    \n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'bed', 'bird', 'cat', 'dog', 'house', 'tree']\n"
     ]
    }
   ],
   "source": [
    "list_words = ['zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "              'bed','bird','cat','dog','house','tree']\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(fname)[0] == list_words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(list_words)) if os.path.split(fname)[0] == list_words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25443)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-50971ec3e595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimg_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4526\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_val = np.empty((len(X_validation), np.prod(img_dim)))\n",
    "cnt = 0\n",
    "for idx in range(len(X_validation)):\n",
    "    img = X_validation[idx]\n",
    "    if img.shape == img_dim:\n",
    "        X_val = np.append(X_val, img.reshape(1,-1), axis=0)\n",
    "        cnt += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "11\n",
      "14\n",
      "18\n",
      "20\n",
      "24\n",
      "32\n",
      "35\n",
      "52\n",
      "59\n",
      "70\n",
      "80\n",
      "89\n",
      "92\n",
      "106\n",
      "108\n",
      "109\n",
      "119\n",
      "122\n",
      "123\n",
      "131\n",
      "144\n",
      "153\n",
      "157\n",
      "158\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "187\n",
      "208\n",
      "212\n",
      "213\n",
      "228\n",
      "229\n",
      "231\n",
      "244\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "252\n",
      "273\n",
      "274\n",
      "276\n",
      "277\n",
      "278\n",
      "283\n",
      "284\n",
      "295\n",
      "311\n",
      "314\n",
      "316\n",
      "318\n",
      "330\n",
      "331\n",
      "350\n",
      "354\n",
      "402\n",
      "403\n",
      "406\n",
      "413\n",
      "415\n",
      "418\n",
      "419\n",
      "420\n",
      "431\n",
      "432\n",
      "433\n",
      "455\n",
      "456\n",
      "460\n",
      "461\n",
      "462\n",
      "469\n",
      "471\n",
      "478\n",
      "479\n",
      "484\n",
      "485\n",
      "511\n",
      "515\n",
      "528\n",
      "534\n",
      "535\n",
      "560\n",
      "561\n",
      "580\n",
      "584\n",
      "590\n",
      "596\n",
      "597\n",
      "606\n",
      "611\n",
      "612\n",
      "613\n",
      "615\n",
      "616\n",
      "638\n",
      "655\n",
      "665\n",
      "676\n",
      "689\n",
      "692\n",
      "727\n",
      "743\n",
      "744\n",
      "751\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "782\n",
      "786\n",
      "791\n",
      "792\n",
      "793\n",
      "795\n",
      "806\n",
      "807\n",
      "809\n",
      "810\n",
      "825\n",
      "831\n",
      "832\n",
      "833\n",
      "837\n",
      "838\n",
      "852\n",
      "853\n",
      "854\n",
      "862\n",
      "872\n",
      "875\n",
      "876\n",
      "879\n",
      "882\n",
      "883\n",
      "884\n",
      "891\n",
      "892\n",
      "893\n",
      "897\n",
      "899\n",
      "907\n",
      "915\n",
      "921\n",
      "931\n",
      "932\n",
      "938\n",
      "939\n",
      "940\n",
      "951\n",
      "952\n",
      "953\n",
      "963\n",
      "973\n",
      "974\n",
      "997\n",
      "1011\n",
      "1012\n",
      "1020\n",
      "1023\n",
      "1034\n",
      "1046\n",
      "1056\n",
      "1062\n",
      "1066\n",
      "1072\n",
      "1084\n",
      "1091\n",
      "1094\n",
      "1099\n",
      "1103\n",
      "1104\n",
      "1141\n",
      "1142\n",
      "1148\n",
      "1154\n",
      "1163\n",
      "1175\n",
      "1176\n",
      "1179\n",
      "1212\n",
      "1215\n",
      "1216\n",
      "1222\n",
      "1223\n",
      "1227\n",
      "1228\n",
      "1232\n",
      "1236\n",
      "1239\n",
      "1240\n",
      "1242\n",
      "1253\n",
      "1255\n",
      "1265\n",
      "1274\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1286\n",
      "1287\n",
      "1290\n",
      "1291\n",
      "1297\n",
      "1304\n",
      "1306\n",
      "1316\n",
      "1318\n",
      "1319\n",
      "1324\n",
      "1325\n",
      "1327\n",
      "1331\n",
      "1336\n",
      "1338\n",
      "1354\n",
      "1356\n",
      "1359\n",
      "1361\n",
      "1366\n",
      "1367\n",
      "1375\n",
      "1385\n",
      "1392\n",
      "1395\n",
      "1402\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1417\n",
      "1423\n",
      "1428\n",
      "1429\n",
      "1433\n",
      "1434\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1441\n",
      "1444\n",
      "1456\n",
      "1468\n",
      "1470\n",
      "1475\n",
      "1480\n",
      "1485\n",
      "1498\n",
      "1503\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1518\n",
      "1525\n",
      "1533\n",
      "1546\n",
      "1547\n",
      "1553\n",
      "1554\n",
      "1565\n",
      "1578\n",
      "1579\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1588\n",
      "1590\n",
      "1594\n",
      "1600\n",
      "1601\n",
      "1621\n",
      "1623\n",
      "1633\n",
      "1636\n",
      "1639\n",
      "1640\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1654\n",
      "1655\n",
      "1661\n",
      "1685\n",
      "1698\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1707\n",
      "1709\n",
      "1717\n",
      "1720\n",
      "1724\n",
      "1725\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1738\n",
      "1739\n",
      "1749\n",
      "1751\n",
      "1752\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1788\n",
      "1805\n",
      "1820\n",
      "1821\n",
      "1823\n",
      "1836\n",
      "1838\n",
      "1839\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1846\n",
      "1851\n",
      "1854\n",
      "1862\n",
      "1866\n",
      "1868\n",
      "1873\n",
      "1878\n",
      "1886\n",
      "1898\n",
      "1903\n",
      "1904\n",
      "1906\n",
      "1907\n",
      "1909\n",
      "1913\n",
      "1914\n",
      "1916\n",
      "1918\n",
      "1929\n",
      "1951\n",
      "1952\n",
      "1961\n",
      "1966\n",
      "1977\n",
      "1991\n",
      "2016\n",
      "2019\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2028\n",
      "2036\n",
      "2039\n",
      "2045\n",
      "2046\n",
      "2050\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2075\n",
      "2084\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2092\n",
      "2102\n",
      "2105\n",
      "2127\n",
      "2129\n",
      "2135\n",
      "2136\n",
      "2141\n",
      "2142\n",
      "2155\n",
      "2165\n",
      "2180\n",
      "2187\n",
      "2189\n",
      "2190\n",
      "2217\n",
      "2222\n",
      "2224\n",
      "2226\n",
      "2227\n",
      "2241\n",
      "2242\n",
      "2248\n",
      "2275\n",
      "2292\n",
      "2304\n",
      "2305\n",
      "2319\n",
      "2322\n",
      "2356\n",
      "2358\n",
      "2359\n",
      "2374\n",
      "2381\n",
      "2382\n",
      "2386\n",
      "2398\n",
      "2450\n",
      "2455\n",
      "2463\n",
      "2464\n",
      "2469\n",
      "2474\n",
      "2475\n",
      "2480\n",
      "2486\n",
      "2488\n",
      "2491\n",
      "2499\n",
      "2509\n",
      "2521\n",
      "2525\n",
      "2526\n",
      "2535\n",
      "2546\n",
      "2557\n",
      "2563\n",
      "2571\n",
      "2572\n",
      "2577\n",
      "2578\n",
      "2580\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2603\n",
      "2614\n",
      "2616\n",
      "2617\n",
      "2642\n",
      "2654\n",
      "2677\n",
      "2679\n",
      "2681\n",
      "2697\n",
      "2698\n",
      "2706\n",
      "2707\n",
      "2718\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2726\n",
      "2731\n",
      "2736\n",
      "2760\n",
      "2773\n",
      "2781\n",
      "2782\n",
      "2793\n",
      "2809\n",
      "2815\n",
      "2821\n",
      "2836\n",
      "2845\n",
      "2846\n",
      "2856\n",
      "2877\n",
      "2893\n",
      "2895\n",
      "2897\n",
      "2904\n",
      "2911\n",
      "2919\n",
      "2923\n",
      "2943\n",
      "2949\n",
      "2966\n",
      "2968\n",
      "2969\n",
      "2979\n",
      "2987\n",
      "2995\n",
      "2996\n",
      "3013\n",
      "3016\n",
      "3020\n",
      "3028\n",
      "3031\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3041\n",
      "3042\n",
      "3058\n",
      "3065\n",
      "3079\n",
      "3088\n",
      "3092\n",
      "3095\n",
      "3098\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3133\n",
      "3144\n",
      "3157\n",
      "3160\n",
      "3162\n",
      "3165\n",
      "3170\n",
      "3179\n",
      "3234\n",
      "3263\n",
      "3284\n",
      "3287\n",
      "3288\n",
      "3292\n",
      "3293\n",
      "3297\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3313\n",
      "3328\n",
      "3338\n",
      "3340\n",
      "3341\n",
      "3345\n",
      "3348\n",
      "3350\n",
      "3375\n",
      "3376\n",
      "3385\n",
      "3405\n",
      "3414\n",
      "3415\n",
      "3428\n",
      "3432\n",
      "3434\n",
      "3439\n",
      "3440\n",
      "3442\n",
      "3456\n",
      "3457\n",
      "3459\n",
      "3470\n",
      "3485\n",
      "3502\n",
      "3505\n",
      "3506\n",
      "3523\n",
      "3524\n",
      "3527\n",
      "3529\n",
      "3530\n",
      "3555\n",
      "3564\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3585\n",
      "3592\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3604\n",
      "3605\n",
      "3624\n",
      "3632\n",
      "3633\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3649\n",
      "3660\n",
      "3682\n",
      "3690\n",
      "3715\n",
      "3733\n",
      "3736\n",
      "3743\n",
      "3744\n",
      "3746\n",
      "3751\n",
      "3761\n",
      "3769\n",
      "3778\n",
      "3781\n",
      "3787\n",
      "3810\n",
      "3816\n",
      "3817\n",
      "3826\n",
      "3828\n",
      "3851\n",
      "3853\n",
      "3861\n",
      "3862\n",
      "3867\n",
      "3868\n",
      "3882\n",
      "3887\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3898\n",
      "3902\n",
      "3903\n",
      "3910\n",
      "3916\n",
      "3922\n",
      "3940\n",
      "3943\n",
      "3962\n",
      "3965\n",
      "3981\n",
      "4018\n",
      "4027\n",
      "4028\n",
      "4052\n",
      "4089\n",
      "4093\n",
      "4094\n",
      "4106\n",
      "4107\n",
      "4111\n",
      "4113\n",
      "4119\n",
      "4120\n",
      "4124\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4140\n",
      "4141\n",
      "4153\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4162\n",
      "4168\n",
      "4182\n",
      "4192\n",
      "4194\n",
      "4201\n",
      "4202\n",
      "4218\n",
      "4219\n",
      "4223\n",
      "4227\n",
      "4243\n",
      "4250\n",
      "4270\n",
      "4271\n",
      "4273\n",
      "4274\n",
      "4282\n",
      "4284\n",
      "4288\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4312\n",
      "4333\n",
      "4334\n",
      "4336\n",
      "4337\n",
      "4341\n",
      "4342\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4375\n",
      "4384\n",
      "4385\n",
      "4388\n",
      "4389\n",
      "4407\n",
      "4418\n",
      "4429\n",
      "4430\n",
      "4432\n",
      "4450\n",
      "4456\n",
      "4457\n",
      "4459\n",
      "4491\n",
      "4492\n",
      "4494\n",
      "4499\n",
      "4505\n",
      "4512\n",
      "4519\n",
      "4522\n",
      "4531\n",
      "4550\n",
      "4551\n",
      "4560\n",
      "4561\n",
      "4570\n",
      "4577\n",
      "4588\n",
      "4590\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4602\n",
      "4613\n",
      "4616\n",
      "4623\n",
      "4624\n",
      "4626\n",
      "4628\n",
      "4633\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4651\n",
      "4654\n",
      "4660\n",
      "4661\n",
      "4668\n",
      "4678\n",
      "4685\n",
      "4692\n",
      "4693\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4715\n",
      "4716\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4736\n",
      "4741\n",
      "4742\n",
      "4745\n",
      "4746\n",
      "4748\n",
      "4759\n",
      "4760\n",
      "4767\n",
      "4768\n",
      "4773\n",
      "4780\n",
      "4781\n",
      "4788\n",
      "4801\n",
      "4802\n",
      "4806\n",
      "4813\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4837\n",
      "4849\n",
      "4865\n",
      "4866\n",
      "4881\n",
      "4882\n",
      "4891\n",
      "4909\n",
      "4928\n",
      "4929\n",
      "4934\n",
      "4936\n",
      "4967\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "5025\n",
      "5033\n",
      "5061\n",
      "5076\n",
      "5077\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5091\n",
      "5092\n",
      "5103\n",
      "5106\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5121\n",
      "5123\n",
      "5127\n",
      "5128\n",
      "5135\n",
      "5136\n",
      "5141\n",
      "5149\n",
      "5166\n",
      "5177\n",
      "5178\n",
      "5195\n",
      "5197\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5226\n",
      "5251\n",
      "5255\n",
      "5260\n",
      "5265\n",
      "5274\n",
      "5285\n",
      "5287\n",
      "5298\n",
      "5325\n",
      "5331\n",
      "5343\n",
      "5352\n",
      "5359\n",
      "5396\n",
      "5398\n",
      "5399\n",
      "5402\n",
      "5403\n",
      "5408\n",
      "5409\n",
      "5416\n",
      "5418\n",
      "5431\n",
      "5436\n",
      "5438\n",
      "5442\n",
      "5445\n",
      "5453\n",
      "5456\n",
      "5461\n",
      "5467\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5493\n",
      "5494\n",
      "5506\n",
      "5527\n",
      "5530\n",
      "5574\n",
      "5578\n",
      "5579\n",
      "5583\n",
      "5585\n",
      "5586\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5597\n",
      "5600\n",
      "5616\n",
      "5617\n",
      "5627\n",
      "5637\n",
      "5649\n",
      "5650\n",
      "5657\n",
      "5658\n",
      "5662\n",
      "5674\n",
      "5675\n",
      "5677\n",
      "5680\n",
      "5698\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5723\n",
      "5727\n",
      "5737\n",
      "5738\n",
      "5782\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5802\n",
      "5806\n",
      "5807\n",
      "5817\n",
      "5822\n",
      "5826\n",
      "5831\n",
      "5832\n",
      "5836\n",
      "5837\n",
      "5839\n",
      "5841\n",
      "5851\n",
      "5852\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5868\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5905\n",
      "5906\n",
      "5909\n",
      "5912\n",
      "5923\n",
      "5932\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5946\n",
      "5948\n",
      "5956\n",
      "5958\n",
      "5980\n",
      "5981\n",
      "6023\n",
      "6026\n",
      "6056\n",
      "6058\n",
      "6065\n",
      "6066\n",
      "6079\n",
      "6080\n",
      "6084\n",
      "6087\n",
      "6091\n",
      "6092\n",
      "6103\n",
      "6114\n",
      "6116\n",
      "6121\n",
      "6174\n",
      "6181\n",
      "6186\n",
      "6190\n",
      "6197\n",
      "6198\n",
      "6201\n",
      "6203\n",
      "6213\n",
      "6230\n",
      "6236\n",
      "6240\n",
      "6241\n",
      "6246\n",
      "6250\n",
      "6251\n",
      "6254\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6269\n",
      "6287\n",
      "6290\n",
      "6291\n",
      "6313\n",
      "6314\n",
      "6316\n",
      "6317\n",
      "6322\n",
      "6332\n",
      "6333\n",
      "6337\n",
      "6342\n",
      "6344\n",
      "6349\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6368\n",
      "6375\n",
      "6377\n",
      "6392\n",
      "6395\n",
      "6397\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6406\n",
      "6422\n",
      "6427\n",
      "6429\n",
      "6437\n",
      "6445\n",
      "6454\n",
      "6462\n",
      "6473\n",
      "6477\n",
      "6485\n",
      "6486\n",
      "6489\n",
      "6493\n",
      "6496\n",
      "6508\n",
      "6509\n",
      "6511\n",
      "6512\n",
      "6522\n",
      "6528\n",
      "6541\n",
      "6556\n",
      "6574\n",
      "6577\n",
      "6627\n",
      "6635\n",
      "6636\n",
      "6648\n",
      "6649\n",
      "6651\n",
      "6653\n",
      "6660\n",
      "6676\n",
      "6677\n",
      "6685\n",
      "6702\n",
      "6703\n",
      "6731\n",
      "6734\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6799\n",
      "6804\n",
      "6865\n",
      "6867\n",
      "6872\n",
      "6883\n",
      "6891\n",
      "6916\n",
      "6921\n",
      "6936\n",
      "6945\n",
      "6954\n",
      "6956\n",
      "6973\n",
      "6986\n",
      "6998\n",
      "6999\n",
      "7017\n",
      "7018\n",
      "7023\n",
      "7033\n",
      "7043\n",
      "7057\n",
      "7064\n",
      "7065\n",
      "7078\n",
      "7115\n",
      "7124\n",
      "7142\n",
      "7144\n",
      "7186\n",
      "7191\n",
      "7196\n",
      "7205\n",
      "7244\n",
      "7245\n",
      "7247\n",
      "7251\n",
      "7252\n",
      "7257\n",
      "7258\n",
      "7277\n",
      "7309\n",
      "7310\n",
      "7311\n",
      "7346\n",
      "7364\n",
      "7377\n",
      "7416\n",
      "7445\n",
      "7446\n",
      "7477\n",
      "7484\n",
      "7489\n",
      "7490\n",
      "7493\n",
      "7494\n",
      "7499\n",
      "7506\n",
      "7507\n",
      "7511\n",
      "7512\n",
      "7524\n",
      "7525\n",
      "7528\n",
      "7529\n",
      "7531\n",
      "7532\n",
      "7552\n",
      "7553\n",
      "7556\n",
      "7558\n",
      "7562\n",
      "7589\n",
      "7592\n",
      "7607\n",
      "7608\n",
      "7652\n",
      "7661\n",
      "7662\n",
      "7663\n",
      "7664\n",
      "7672\n",
      "7674\n",
      "7675\n",
      "7678\n",
      "7697\n",
      "7700\n",
      "7704\n",
      "7705\n",
      "7706\n",
      "7753\n",
      "7772\n",
      "7818\n",
      "7820\n",
      "7827\n",
      "7833\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7848\n",
      "7849\n",
      "7868\n",
      "7871\n",
      "7898\n",
      "7916\n",
      "7946\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7969\n",
      "7987\n",
      "8023\n",
      "8024\n",
      "8064\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8081\n",
      "8097\n",
      "8118\n",
      "8119\n",
      "8149\n",
      "8155\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8191\n",
      "8200\n",
      "8201\n",
      "8235\n",
      "8236\n",
      "8240\n",
      "8242\n",
      "8266\n",
      "8275\n",
      "8281\n",
      "8297\n",
      "8300\n",
      "8301\n",
      "8324\n",
      "8342\n",
      "8343\n",
      "8353\n",
      "8362\n",
      "8368\n",
      "8391\n",
      "8404\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8422\n",
      "8423\n",
      "8428\n",
      "8430\n",
      "8431\n",
      "8447\n",
      "8466\n",
      "8471\n",
      "8487\n",
      "8536\n",
      "8543\n",
      "8544\n",
      "8556\n",
      "8568\n",
      "8569\n",
      "8570\n",
      "8571\n",
      "8605\n",
      "8614\n",
      "8615\n",
      "8616\n",
      "8638\n",
      "8641\n",
      "8655\n",
      "8656\n",
      "8673\n",
      "8694\n",
      "8702\n",
      "8727\n",
      "8740\n",
      "8746\n",
      "8776\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8807\n",
      "8819\n",
      "8854\n",
      "8918\n",
      "8919\n",
      "8923\n",
      "8928\n",
      "8938\n",
      "8965\n",
      "8985\n",
      "8996\n",
      "9009\n",
      "9010\n",
      "9011\n",
      "9012\n",
      "9013\n",
      "9020\n",
      "9021\n",
      "9025\n",
      "9026\n",
      "9027\n",
      "9028\n",
      "9033\n",
      "9054\n",
      "9057\n",
      "9058\n",
      "9059\n",
      "9110\n",
      "9127\n",
      "9149\n",
      "9150\n",
      "9176\n",
      "9177\n",
      "9195\n",
      "9197\n",
      "9198\n",
      "9199\n",
      "9200\n",
      "9205\n",
      "9207\n",
      "9212\n",
      "9223\n",
      "9231\n",
      "9232\n",
      "9233\n",
      "9234\n",
      "9235\n",
      "9246\n",
      "9254\n",
      "9255\n",
      "9256\n",
      "9258\n",
      "9260\n",
      "9263\n",
      "9313\n",
      "9314\n",
      "9321\n",
      "9322\n",
      "9323\n",
      "9324\n",
      "9334\n",
      "9335\n",
      "9336\n",
      "9351\n",
      "9362\n",
      "9364\n",
      "9365\n",
      "9386\n",
      "9387\n",
      "9388\n",
      "9403\n",
      "9404\n",
      "9405\n",
      "9406\n",
      "9407\n",
      "9408\n",
      "9410\n",
      "9427\n",
      "9446\n",
      "9450\n",
      "9456\n",
      "9457\n",
      "9474\n",
      "9476\n",
      "9500\n",
      "9512\n",
      "9552\n",
      "9553\n",
      "9554\n",
      "9611\n",
      "9681\n",
      "9682\n",
      "9684\n",
      "9694\n",
      "9696\n",
      "9708\n",
      "9709\n",
      "9710\n",
      "9722\n",
      "9723\n",
      "9725\n",
      "9754\n",
      "9755\n",
      "9763\n",
      "9810\n",
      "9811\n",
      "9813\n",
      "9831\n",
      "9841\n",
      "9842\n",
      "9870\n",
      "9892\n",
      "9894\n",
      "9902\n",
      "9903\n",
      "9923\n",
      "9924\n",
      "9925\n",
      "9936\n",
      "9958\n",
      "9959\n",
      "9977\n",
      "9994\n",
      "10001\n",
      "10037\n",
      "10074\n",
      "10075\n",
      "10076\n",
      "10082\n",
      "10120\n",
      "10121\n",
      "10122\n",
      "10123\n",
      "10136\n",
      "10140\n",
      "10183\n",
      "10184\n",
      "10185\n",
      "10187\n",
      "10188\n",
      "10189\n",
      "10270\n",
      "10271\n",
      "10283\n",
      "10284\n",
      "10294\n",
      "10295\n",
      "10296\n",
      "10297\n",
      "10300\n",
      "10306\n",
      "10332\n",
      "10333\n",
      "10334\n",
      "10358\n",
      "10359\n",
      "10401\n",
      "10459\n",
      "10465\n",
      "10522\n",
      "10550\n",
      "10555\n",
      "10556\n",
      "10573\n",
      "10576\n",
      "10577\n",
      "10578\n",
      "10594\n",
      "10599\n",
      "10602\n",
      "10606\n",
      "10607\n",
      "10609\n",
      "10610\n",
      "10632\n",
      "10633\n",
      "10657\n",
      "10659\n",
      "10660\n",
      "10665\n",
      "10666\n",
      "10667\n",
      "10668\n",
      "10676\n",
      "10695\n",
      "10696\n",
      "10697\n",
      "10713\n",
      "10758\n",
      "10779\n",
      "10780\n",
      "10781\n",
      "10792\n",
      "10813\n",
      "10819\n",
      "10820\n",
      "10826\n",
      "10855\n",
      "10856\n",
      "10867\n",
      "10951\n",
      "10952\n",
      "10953\n",
      "10959\n",
      "10974\n",
      "10975\n",
      "10983\n",
      "11007\n",
      "11078\n",
      "11100\n",
      "11111\n",
      "11116\n",
      "11136\n",
      "11137\n",
      "11138\n",
      "11139\n",
      "11211\n",
      "11266\n",
      "11274\n",
      "11289\n",
      "11342\n",
      "11343\n",
      "11352\n",
      "11359\n",
      "11373\n",
      "11392\n",
      "11399\n",
      "11409\n",
      "11416\n",
      "11417\n",
      "11431\n",
      "11457\n",
      "11490\n",
      "11496\n",
      "11520\n",
      "11525\n",
      "11528\n",
      "11530\n",
      "11568\n",
      "11583\n",
      "11597\n",
      "11598\n",
      "11599\n",
      "11600\n",
      "11603\n",
      "11639\n",
      "11647\n",
      "11651\n",
      "11652\n",
      "11674\n",
      "11714\n",
      "11740\n",
      "11742\n",
      "11755\n",
      "11759\n",
      "11771\n",
      "11789\n",
      "11823\n",
      "11824\n",
      "11839\n",
      "11855\n",
      "11856\n",
      "11857\n",
      "11861\n",
      "11862\n",
      "11863\n",
      "11864\n",
      "11901\n",
      "11937\n",
      "11943\n",
      "12076\n",
      "12083\n",
      "12089\n",
      "12100\n",
      "12124\n",
      "12126\n",
      "12148\n",
      "12208\n",
      "12216\n",
      "12217\n",
      "12224\n",
      "12226\n",
      "12236\n",
      "12237\n",
      "12238\n",
      "12241\n",
      "12266\n",
      "12267\n",
      "12273\n",
      "12318\n",
      "12319\n",
      "12320\n",
      "12334\n",
      "12359\n",
      "12360\n",
      "12393\n",
      "12394\n",
      "12395\n",
      "12404\n",
      "12405\n",
      "12413\n",
      "12414\n",
      "12433\n",
      "12442\n",
      "12443\n",
      "12444\n",
      "12445\n",
      "12451\n",
      "12463\n",
      "12468\n",
      "12469\n",
      "12472\n",
      "12473\n",
      "12474\n",
      "12476\n",
      "12507\n",
      "12520\n",
      "12536\n",
      "12543\n",
      "12575\n",
      "12576\n",
      "12596\n",
      "12598\n",
      "12599\n",
      "12610\n",
      "12611\n",
      "12612\n",
      "12638\n",
      "12644\n",
      "12645\n",
      "12652\n",
      "12658\n",
      "12659\n",
      "12660\n",
      "12665\n",
      "12666\n",
      "12674\n",
      "12675\n",
      "12697\n",
      "12725\n",
      "12726\n",
      "12738\n",
      "12767\n",
      "12806\n",
      "12866\n",
      "12868\n",
      "12895\n",
      "12898\n",
      "12915\n",
      "12916\n",
      "12919\n",
      "12932\n",
      "12933\n",
      "12936\n",
      "12945\n",
      "12981\n",
      "13013\n",
      "13027\n",
      "13028\n",
      "13029\n",
      "13031\n",
      "13043\n",
      "13057\n",
      "13072\n",
      "13120\n",
      "13130\n",
      "13131\n",
      "13141\n",
      "13150\n",
      "13166\n",
      "13174\n",
      "13175\n",
      "13188\n",
      "13193\n",
      "13218\n",
      "13235\n",
      "13259\n",
      "13261\n",
      "13263\n",
      "13264\n",
      "13285\n",
      "13296\n",
      "13310\n",
      "13341\n",
      "13344\n",
      "13404\n",
      "13407\n",
      "13408\n",
      "13431\n",
      "13432\n",
      "13433\n",
      "13473\n",
      "13474\n",
      "13475\n",
      "13481\n",
      "13488\n",
      "13489\n",
      "13490\n",
      "13500\n",
      "13531\n",
      "13533\n",
      "13534\n",
      "13538\n",
      "13574\n",
      "13587\n",
      "13590\n",
      "13591\n",
      "13598\n",
      "13632\n",
      "13633\n",
      "13634\n",
      "13664\n",
      "13688\n",
      "13724\n",
      "13745\n",
      "13751\n",
      "13752\n",
      "13763\n",
      "13764\n",
      "13770\n",
      "13771\n",
      "13774\n",
      "13775\n",
      "13776\n",
      "13777\n",
      "13779\n",
      "13790\n",
      "13791\n",
      "13795\n",
      "13805\n",
      "13807\n",
      "13814\n",
      "13815\n",
      "13840\n",
      "13864\n",
      "13874\n",
      "13883\n",
      "13931\n",
      "13932\n",
      "13933\n",
      "13946\n",
      "13948\n",
      "13949\n",
      "13950\n",
      "13975\n",
      "13976\n",
      "13979\n",
      "14001\n",
      "14013\n",
      "14014\n",
      "14031\n",
      "14032\n",
      "14052\n",
      "14090\n",
      "14097\n",
      "14100\n",
      "14101\n",
      "14111\n",
      "14113\n",
      "14114\n",
      "14126\n",
      "14130\n",
      "14131\n",
      "14136\n",
      "14143\n",
      "14148\n",
      "14153\n",
      "14177\n",
      "14215\n",
      "14216\n",
      "14233\n",
      "14240\n",
      "14244\n",
      "14260\n",
      "14261\n",
      "14262\n",
      "14264\n",
      "14265\n",
      "14346\n",
      "14369\n",
      "14377\n",
      "14385\n",
      "14419\n",
      "14453\n",
      "14461\n",
      "14467\n",
      "14468\n",
      "14469\n",
      "14500\n",
      "14501\n",
      "14502\n",
      "14506\n",
      "14508\n",
      "14509\n",
      "14510\n",
      "14528\n",
      "14539\n",
      "14553\n",
      "14554\n",
      "14575\n",
      "14576\n",
      "14577\n",
      "14599\n",
      "14606\n",
      "14616\n",
      "14635\n",
      "14642\n",
      "14646\n",
      "14647\n",
      "14652\n",
      "14679\n",
      "14686\n",
      "14691\n",
      "14708\n",
      "14711\n",
      "14717\n",
      "14719\n",
      "14735\n",
      "14750\n",
      "14751\n",
      "14764\n",
      "14769\n",
      "14791\n",
      "14802\n",
      "14817\n",
      "14828\n",
      "14837\n",
      "14838\n",
      "14847\n",
      "14881\n",
      "14901\n",
      "14927\n",
      "14928\n",
      "14944\n",
      "14945\n",
      "14956\n",
      "14982\n",
      "14983\n",
      "14985\n",
      "14990\n",
      "15021\n",
      "15042\n",
      "15043\n",
      "15113\n",
      "15114\n",
      "15140\n",
      "15141\n",
      "15157\n",
      "15165\n",
      "15232\n",
      "15243\n",
      "15250\n",
      "15252\n",
      "15255\n",
      "15278\n",
      "15279\n",
      "15282\n",
      "15283\n",
      "15304\n",
      "15305\n",
      "15306\n",
      "15307\n",
      "15320\n",
      "15332\n",
      "15354\n",
      "15369\n",
      "15393\n",
      "15394\n",
      "15406\n",
      "15407\n",
      "15413\n",
      "15428\n",
      "15445\n",
      "15455\n",
      "15456\n",
      "15464\n",
      "15470\n",
      "15472\n",
      "15473\n",
      "15474\n",
      "15475\n",
      "15476\n",
      "15477\n",
      "15507\n",
      "15538\n",
      "15539\n",
      "15540\n",
      "15547\n",
      "15548\n",
      "15558\n",
      "15567\n",
      "15568\n",
      "15582\n",
      "15584\n",
      "15606\n",
      "15607\n",
      "15618\n",
      "15623\n",
      "15624\n",
      "15644\n",
      "15645\n",
      "15648\n",
      "15653\n",
      "15661\n",
      "15672\n",
      "15678\n",
      "15682\n",
      "15695\n",
      "15734\n",
      "15760\n",
      "15764\n",
      "15765\n",
      "15768\n",
      "15770\n",
      "15796\n",
      "15859\n",
      "15860\n",
      "15871\n",
      "15887\n",
      "15891\n",
      "15895\n",
      "15900\n",
      "15904\n",
      "15911\n",
      "15912\n",
      "15919\n",
      "15946\n",
      "15950\n",
      "15952\n",
      "15964\n",
      "15975\n",
      "15976\n",
      "15998\n",
      "16002\n",
      "16022\n",
      "16025\n",
      "16036\n",
      "16037\n",
      "16038\n",
      "16041\n",
      "16047\n",
      "16053\n",
      "16081\n",
      "16082\n",
      "16084\n",
      "16094\n",
      "16130\n",
      "16131\n",
      "16132\n",
      "16136\n",
      "16137\n",
      "16141\n",
      "16142\n",
      "16156\n",
      "16157\n",
      "16160\n",
      "16164\n",
      "16170\n",
      "16182\n",
      "16183\n",
      "16184\n",
      "16192\n",
      "16206\n",
      "16220\n",
      "16223\n",
      "16235\n",
      "16237\n",
      "16243\n",
      "16247\n",
      "16249\n",
      "16250\n",
      "16265\n",
      "16266\n",
      "16271\n",
      "16272\n",
      "16293\n",
      "16294\n",
      "16311\n",
      "16354\n",
      "16367\n",
      "16372\n",
      "16374\n",
      "16375\n",
      "16379\n",
      "16384\n",
      "16387\n",
      "16394\n",
      "16401\n",
      "16410\n",
      "16425\n",
      "16429\n",
      "16432\n",
      "16437\n",
      "16451\n",
      "16466\n",
      "16499\n",
      "16504\n",
      "16505\n",
      "16515\n",
      "16524\n",
      "16527\n",
      "16540\n",
      "16542\n",
      "16546\n",
      "16548\n",
      "16553\n",
      "16558\n",
      "16561\n",
      "16562\n",
      "16566\n",
      "16569\n",
      "16582\n",
      "16583\n",
      "16584\n",
      "16587\n",
      "16602\n",
      "16603\n",
      "16628\n",
      "16637\n",
      "16644\n",
      "16684\n",
      "16702\n",
      "16703\n",
      "16704\n",
      "16705\n",
      "16715\n",
      "16721\n",
      "16737\n",
      "16738\n",
      "16739\n",
      "16766\n",
      "16768\n",
      "16769\n",
      "16799\n",
      "16804\n",
      "16807\n",
      "16821\n",
      "16822\n",
      "16823\n",
      "16827\n",
      "16832\n",
      "16834\n",
      "16835\n",
      "16847\n",
      "16853\n",
      "16854\n",
      "16859\n",
      "16860\n",
      "16861\n",
      "16886\n",
      "16896\n",
      "16905\n",
      "16915\n",
      "16917\n",
      "16918\n",
      "16923\n",
      "16924\n",
      "16926\n",
      "16935\n",
      "16941\n",
      "16965\n",
      "16998\n",
      "17003\n",
      "17004\n",
      "17005\n",
      "17008\n",
      "17013\n",
      "17019\n",
      "17023\n",
      "17030\n",
      "17047\n",
      "17053\n",
      "17056\n",
      "17078\n",
      "17079\n",
      "17080\n",
      "17087\n",
      "17088\n",
      "17100\n",
      "17101\n",
      "17103\n",
      "17104\n",
      "17105\n",
      "17133\n",
      "17145\n",
      "17149\n",
      "17155\n",
      "17156\n",
      "17157\n",
      "17165\n",
      "17189\n",
      "17200\n",
      "17201\n",
      "17206\n",
      "17224\n",
      "17225\n",
      "17240\n",
      "17249\n",
      "17250\n",
      "17254\n",
      "17259\n",
      "17263\n",
      "17264\n",
      "17277\n",
      "17287\n",
      "17289\n",
      "17291\n",
      "17292\n",
      "17293\n",
      "17294\n",
      "17314\n",
      "17318\n",
      "17319\n",
      "17321\n",
      "17327\n",
      "17328\n",
      "17347\n",
      "17355\n",
      "17361\n",
      "17362\n",
      "17369\n",
      "17370\n",
      "17371\n",
      "17378\n",
      "17381\n",
      "17384\n",
      "17403\n",
      "17404\n",
      "17405\n",
      "17406\n",
      "17408\n",
      "17418\n",
      "17423\n",
      "17430\n",
      "17431\n",
      "17435\n",
      "17436\n",
      "17449\n",
      "17457\n",
      "17480\n",
      "17484\n",
      "17488\n",
      "17491\n",
      "17512\n",
      "17520\n",
      "17550\n",
      "17565\n",
      "17566\n",
      "17600\n",
      "17612\n",
      "17625\n",
      "17626\n",
      "17627\n",
      "17629\n",
      "17642\n",
      "17659\n",
      "17685\n",
      "17724\n",
      "17726\n",
      "17738\n",
      "17739\n",
      "17743\n",
      "17745\n",
      "17746\n",
      "17772\n",
      "17773\n",
      "17799\n",
      "17844\n",
      "17850\n",
      "17851\n",
      "17862\n",
      "17872\n",
      "17889\n",
      "17890\n",
      "17899\n",
      "17900\n",
      "17915\n",
      "17920\n",
      "17926\n",
      "17936\n",
      "17938\n",
      "17951\n",
      "17953\n",
      "17965\n",
      "17985\n",
      "17986\n",
      "17992\n",
      "18001\n",
      "18020\n",
      "18059\n",
      "18065\n",
      "18071\n",
      "18091\n",
      "18094\n",
      "18095\n",
      "18096\n",
      "18102\n",
      "18109\n",
      "18141\n",
      "18143\n",
      "18144\n",
      "18145\n",
      "18212\n",
      "18217\n",
      "18218\n",
      "18224\n",
      "18225\n",
      "18226\n",
      "18240\n",
      "18241\n",
      "18246\n",
      "18247\n",
      "18288\n",
      "18291\n",
      "18292\n",
      "18376\n",
      "18398\n",
      "18412\n",
      "18476\n",
      "18478\n",
      "18498\n",
      "18499\n",
      "18509\n",
      "18510\n",
      "18516\n",
      "18518\n",
      "18520\n",
      "18521\n",
      "18522\n",
      "18546\n",
      "18549\n",
      "18578\n",
      "18587\n",
      "18607\n",
      "18619\n",
      "18620\n",
      "18653\n",
      "18689\n",
      "18698\n",
      "18699\n",
      "18717\n",
      "18726\n",
      "18742\n",
      "18751\n",
      "18752\n",
      "18845\n",
      "18846\n",
      "18851\n",
      "18852\n",
      "18856\n",
      "18857\n",
      "18859\n",
      "18860\n",
      "18869\n",
      "18870\n",
      "18873\n",
      "18874\n",
      "18875\n",
      "18876\n",
      "18886\n",
      "18906\n",
      "18932\n",
      "18945\n",
      "18946\n",
      "18978\n",
      "19000\n",
      "19025\n",
      "19028\n",
      "19039\n",
      "19076\n",
      "19104\n",
      "19128\n",
      "19129\n",
      "19158\n",
      "19168\n",
      "19189\n",
      "19224\n",
      "19226\n",
      "19265\n",
      "19266\n",
      "19267\n",
      "19268\n",
      "19278\n",
      "19279\n",
      "19282\n",
      "19284\n",
      "19313\n",
      "19314\n",
      "19336\n",
      "19348\n",
      "19381\n",
      "19386\n",
      "19389\n",
      "19398\n",
      "19409\n",
      "19413\n",
      "19461\n",
      "19462\n",
      "19464\n",
      "19466\n",
      "19480\n",
      "19510\n",
      "19532\n",
      "19548\n",
      "19615\n",
      "19618\n",
      "19625\n",
      "19626\n",
      "19640\n",
      "19641\n",
      "19642\n",
      "19643\n",
      "19652\n",
      "19659\n",
      "19678\n",
      "19712\n",
      "19717\n",
      "19730\n",
      "19745\n",
      "19749\n",
      "19751\n",
      "19752\n",
      "19756\n",
      "19772\n",
      "19786\n",
      "19788\n",
      "19789\n",
      "19794\n",
      "19809\n",
      "19821\n",
      "19838\n",
      "19951\n",
      "19958\n",
      "20001\n",
      "20002\n",
      "20009\n",
      "20010\n",
      "20011\n",
      "20049\n",
      "20072\n",
      "20075\n",
      "20097\n",
      "20103\n",
      "20117\n",
      "20118\n",
      "20123\n",
      "20144\n",
      "20145\n",
      "20173\n",
      "20207\n",
      "20208\n",
      "20209\n",
      "20232\n",
      "20233\n",
      "20236\n",
      "20267\n",
      "20278\n",
      "20281\n",
      "20284\n",
      "20286\n",
      "20287\n",
      "20288\n",
      "20289\n",
      "20310\n",
      "20311\n",
      "20320\n",
      "20327\n",
      "20330\n",
      "20339\n",
      "20344\n",
      "20345\n",
      "20351\n",
      "20398\n",
      "20404\n",
      "20409\n",
      "20441\n",
      "20475\n",
      "20480\n",
      "20481\n",
      "20482\n",
      "20483\n",
      "20484\n",
      "20502\n",
      "20503\n",
      "20507\n",
      "20509\n",
      "20510\n",
      "20515\n",
      "20531\n",
      "20535\n",
      "20542\n",
      "20562\n",
      "20587\n",
      "20588\n",
      "20604\n",
      "20636\n",
      "20640\n",
      "20727\n",
      "20747\n",
      "20761\n",
      "20762\n",
      "20773\n",
      "20806\n",
      "20812\n",
      "20821\n",
      "20822\n",
      "20823\n",
      "20831\n",
      "20836\n",
      "20848\n",
      "20886\n",
      "20892\n",
      "20893\n",
      "20904\n",
      "20906\n",
      "20930\n",
      "20931\n",
      "20940\n",
      "20961\n",
      "20987\n",
      "20988\n",
      "20989\n",
      "21008\n",
      "21012\n",
      "21023\n",
      "21027\n",
      "21048\n",
      "21049\n",
      "21086\n",
      "21087\n",
      "21089\n",
      "21090\n",
      "21091\n",
      "21099\n",
      "21116\n",
      "21117\n",
      "21122\n",
      "21142\n",
      "21143\n",
      "21163\n",
      "21164\n",
      "21202\n",
      "21208\n",
      "21209\n",
      "21210\n",
      "21211\n",
      "21228\n",
      "21229\n",
      "21231\n",
      "21237\n",
      "21241\n",
      "21276\n",
      "21277\n",
      "21300\n",
      "21302\n",
      "21351\n",
      "21352\n",
      "21360\n",
      "21361\n",
      "21362\n",
      "21363\n",
      "21368\n",
      "21369\n",
      "21370\n",
      "21371\n",
      "21372\n",
      "21373\n",
      "21381\n",
      "21401\n",
      "21402\n",
      "21421\n",
      "21452\n",
      "21470\n",
      "21475\n",
      "21477\n",
      "21508\n",
      "21514\n",
      "21526\n",
      "21538\n",
      "21582\n",
      "21583\n",
      "21598\n",
      "21600\n",
      "21629\n",
      "21653\n",
      "21664\n",
      "21665\n",
      "21666\n",
      "21667\n",
      "21668\n",
      "21673\n",
      "21675\n",
      "21677\n",
      "21701\n",
      "21702\n",
      "21719\n",
      "21725\n",
      "21728\n",
      "21729\n",
      "21735\n",
      "21736\n",
      "21743\n",
      "21744\n",
      "21760\n",
      "21761\n",
      "21789\n",
      "21790\n",
      "21817\n",
      "21826\n",
      "21827\n",
      "21828\n",
      "21829\n",
      "21830\n",
      "21839\n",
      "21840\n",
      "21863\n",
      "21864\n",
      "21866\n",
      "21867\n",
      "21890\n",
      "21902\n",
      "21905\n",
      "21909\n",
      "21990\n",
      "21992\n",
      "22002\n",
      "22004\n",
      "22005\n",
      "22006\n",
      "22041\n",
      "22043\n",
      "22081\n",
      "22089\n",
      "22106\n",
      "22107\n",
      "22136\n",
      "22138\n",
      "22145\n",
      "22156\n",
      "22159\n",
      "22165\n",
      "22211\n",
      "22220\n",
      "22233\n",
      "22246\n",
      "22250\n",
      "22251\n",
      "22252\n",
      "22253\n",
      "22259\n",
      "22267\n",
      "22272\n",
      "22285\n",
      "22286\n",
      "22346\n",
      "22347\n",
      "22356\n",
      "22363\n",
      "22382\n",
      "22383\n",
      "22385\n",
      "22392\n",
      "22396\n",
      "22399\n",
      "22424\n",
      "22427\n",
      "22428\n",
      "22435\n",
      "22440\n",
      "22441\n",
      "22444\n",
      "22448\n",
      "22461\n",
      "22462\n",
      "22466\n",
      "22495\n",
      "22502\n",
      "22529\n",
      "22530\n",
      "22572\n",
      "22580\n",
      "22581\n",
      "22582\n",
      "22598\n",
      "22599\n",
      "22606\n",
      "22607\n",
      "22609\n",
      "22627\n",
      "22628\n",
      "22630\n",
      "22649\n",
      "22651\n",
      "22670\n",
      "22701\n",
      "22704\n",
      "22729\n",
      "22747\n",
      "22756\n",
      "22770\n",
      "22777\n",
      "22778\n",
      "22779\n",
      "22780\n",
      "22782\n",
      "22783\n",
      "22784\n",
      "22785\n",
      "22799\n",
      "22800\n",
      "22801\n",
      "22825\n",
      "22826\n",
      "22830\n",
      "22831\n",
      "22832\n",
      "22834\n",
      "22835\n",
      "22840\n",
      "22859\n",
      "22899\n",
      "22905\n",
      "22931\n",
      "22940\n",
      "22944\n",
      "23030\n",
      "23078\n",
      "23079\n",
      "23086\n",
      "23104\n",
      "23165\n",
      "23170\n",
      "23189\n",
      "23196\n",
      "23197\n",
      "23203\n",
      "23207\n",
      "23208\n",
      "23219\n",
      "23224\n",
      "23225\n",
      "23226\n",
      "23249\n",
      "23250\n",
      "23252\n",
      "23254\n",
      "23256\n",
      "23257\n",
      "23271\n",
      "23289\n",
      "23290\n",
      "23317\n",
      "23344\n",
      "23345\n",
      "23346\n",
      "23350\n",
      "23351\n",
      "23354\n",
      "23377\n",
      "23385\n",
      "23386\n",
      "23388\n",
      "23396\n",
      "23400\n",
      "23401\n",
      "23402\n",
      "23409\n",
      "23413\n",
      "23430\n",
      "23434\n",
      "23435\n",
      "23441\n",
      "23442\n",
      "23449\n",
      "23455\n",
      "23471\n",
      "23472\n",
      "23473\n",
      "23474\n",
      "23479\n",
      "23510\n",
      "23524\n",
      "23525\n",
      "23526\n",
      "23527\n",
      "23533\n",
      "23535\n",
      "23541\n",
      "23553\n",
      "23554\n",
      "23555\n",
      "23559\n",
      "23571\n",
      "23579\n",
      "23583\n",
      "23585\n",
      "23586\n",
      "23604\n",
      "23605\n",
      "23617\n",
      "23618\n",
      "23619\n",
      "23620\n",
      "23621\n",
      "23625\n",
      "23646\n",
      "23654\n",
      "23663\n",
      "23664\n",
      "23673\n",
      "23674\n",
      "23681\n",
      "23682\n",
      "23699\n",
      "23717\n",
      "23730\n",
      "23731\n",
      "23743\n",
      "23744\n",
      "23765\n",
      "23825\n",
      "23832\n",
      "23847\n",
      "23856\n",
      "23863\n",
      "23908\n",
      "23909\n",
      "23912\n",
      "23917\n",
      "23924\n",
      "23926\n",
      "23927\n",
      "23928\n",
      "23929\n",
      "23940\n",
      "23951\n",
      "23967\n",
      "23968\n",
      "23976\n",
      "23989\n",
      "23990\n",
      "23992\n",
      "23998\n",
      "24017\n",
      "24026\n",
      "24036\n",
      "24038\n",
      "24053\n",
      "24114\n",
      "24142\n",
      "24151\n",
      "24175\n",
      "24176\n",
      "24183\n",
      "24198\n",
      "24213\n",
      "24228\n",
      "24229\n",
      "24230\n",
      "24305\n",
      "24306\n",
      "24310\n",
      "24311\n",
      "24323\n",
      "24339\n",
      "24344\n",
      "24345\n",
      "24352\n",
      "24354\n",
      "24355\n",
      "24367\n",
      "24405\n",
      "24406\n",
      "24435\n",
      "24485\n",
      "24496\n",
      "24501\n",
      "24502\n",
      "24511\n",
      "24549\n",
      "24550\n",
      "24558\n",
      "24598\n",
      "24607\n",
      "24610\n",
      "24612\n",
      "24648\n",
      "24652\n",
      "24653\n",
      "24665\n",
      "24676\n",
      "24677\n",
      "24764\n",
      "24765\n",
      "24809\n",
      "24811\n",
      "24814\n",
      "24815\n",
      "24816\n",
      "24818\n",
      "24823\n",
      "24831\n",
      "24835\n",
      "24836\n",
      "24847\n",
      "24851\n",
      "24853\n",
      "24855\n",
      "24857\n",
      "24858\n",
      "24882\n",
      "24883\n",
      "24892\n",
      "24893\n",
      "24911\n",
      "24912\n",
      "24920\n",
      "24921\n",
      "24938\n",
      "24951\n",
      "24952\n",
      "24970\n",
      "24990\n",
      "25003\n",
      "25004\n",
      "25025\n",
      "25031\n",
      "25034\n",
      "25079\n",
      "25080\n",
      "25081\n",
      "25092\n",
      "25093\n",
      "25094\n",
      "25096\n",
      "25116\n",
      "25148\n",
      "25171\n",
      "25172\n",
      "25184\n",
      "25211\n",
      "25283\n",
      "25311\n",
      "25333\n",
      "25375\n",
      "25441\n",
      "25447\n",
      "25453\n",
      "25455\n",
      "25465\n",
      "25506\n",
      "25514\n",
      "25536\n",
      "25551\n",
      "25577\n",
      "25586\n",
      "25587\n",
      "25616\n",
      "25617\n",
      "25633\n",
      "25651\n",
      "25652\n",
      "25655\n",
      "25658\n",
      "25659\n",
      "25703\n",
      "25704\n",
      "25719\n",
      "25725\n",
      "25731\n",
      "25767\n",
      "25771\n",
      "25778\n",
      "25779\n",
      "25781\n",
      "25782\n",
      "25800\n",
      "25801\n",
      "25803\n",
      "25804\n",
      "25812\n",
      "25815\n",
      "25816\n",
      "25837\n",
      "25838\n",
      "25858\n",
      "25859\n",
      "25860\n",
      "25861\n",
      "25877\n",
      "25887\n",
      "25911\n",
      "25963\n",
      "25971\n",
      "25976\n",
      "25977\n",
      "25978\n",
      "26016\n",
      "26039\n",
      "26040\n",
      "26053\n",
      "26066\n",
      "26072\n",
      "26073\n",
      "26074\n",
      "26084\n",
      "26113\n",
      "26114\n",
      "26130\n",
      "26147\n",
      "26169\n",
      "26182\n",
      "26207\n",
      "26281\n",
      "26322\n",
      "26325\n",
      "26340\n",
      "26416\n",
      "26417\n",
      "26419\n",
      "26424\n",
      "26435\n",
      "26445\n",
      "26446\n",
      "26448\n",
      "26449\n",
      "26475\n",
      "26478\n",
      "26479\n",
      "26501\n",
      "26513\n",
      "26526\n",
      "26527\n",
      "26543\n",
      "26570\n",
      "26571\n",
      "26574\n",
      "26599\n",
      "26612\n",
      "26619\n",
      "26620\n",
      "26624\n",
      "26625\n",
      "26637\n",
      "26638\n",
      "26639\n",
      "26653\n",
      "26654\n",
      "26671\n",
      "26678\n",
      "26681\n",
      "26682\n",
      "26683\n",
      "26684\n",
      "26686\n",
      "26688\n",
      "26718\n",
      "26727\n",
      "26734\n",
      "26735\n",
      "26736\n",
      "26737\n",
      "26740\n",
      "26748\n",
      "26749\n",
      "26762\n",
      "26763\n",
      "26764\n",
      "26778\n",
      "26803\n",
      "26826\n",
      "26839\n",
      "26840\n",
      "26841\n",
      "26842\n",
      "26879\n",
      "26891\n",
      "26905\n",
      "26906\n",
      "26938\n",
      "26951\n",
      "26952\n",
      "26953\n",
      "26976\n",
      "26977\n",
      "26978\n",
      "27008\n",
      "27019\n",
      "27035\n",
      "27058\n",
      "27066\n",
      "27096\n",
      "27112\n",
      "27129\n",
      "27142\n",
      "27157\n",
      "27172\n",
      "27174\n",
      "27180\n",
      "27182\n",
      "27195\n",
      "27249\n",
      "27250\n",
      "27255\n",
      "27257\n",
      "27258\n",
      "27259\n",
      "27260\n",
      "27274\n",
      "27284\n",
      "27337\n",
      "27367\n",
      "27382\n",
      "27401\n",
      "27410\n",
      "27420\n",
      "27421\n",
      "27431\n",
      "27464\n",
      "27469\n",
      "27470\n",
      "27477\n",
      "27486\n",
      "27487\n",
      "27488\n",
      "27510\n",
      "27511\n",
      "27556\n",
      "27558\n",
      "27559\n",
      "27578\n",
      "27609\n",
      "27610\n",
      "27611\n",
      "27612\n",
      "27695\n",
      "27696\n",
      "27697\n",
      "27698\n",
      "27699\n",
      "27701\n",
      "27730\n",
      "27732\n",
      "27747\n",
      "27751\n",
      "27791\n",
      "27800\n",
      "27803\n",
      "27835\n",
      "27838\n",
      "27839\n",
      "27850\n",
      "27855\n",
      "27870\n",
      "27928\n",
      "27929\n",
      "27957\n",
      "27960\n",
      "27966\n",
      "27979\n",
      "27992\n",
      "27993\n",
      "28006\n",
      "28011\n",
      "28012\n",
      "28045\n",
      "28054\n",
      "28056\n",
      "28069\n",
      "28070\n",
      "28082\n",
      "28085\n",
      "28099\n",
      "28110\n",
      "28146\n",
      "28147\n",
      "28159\n",
      "28209\n",
      "28220\n",
      "28221\n",
      "28222\n",
      "28235\n",
      "28310\n",
      "28312\n",
      "28325\n",
      "28328\n",
      "28350\n",
      "28355\n",
      "28397\n",
      "28458\n",
      "28459\n",
      "28461\n",
      "28534\n",
      "28542\n",
      "28606\n",
      "28607\n",
      "28649\n",
      "28661\n",
      "28662\n",
      "28664\n",
      "28693\n",
      "28694\n",
      "28695\n",
      "28696\n",
      "28699\n",
      "28700\n",
      "28706\n",
      "28708\n",
      "28725\n",
      "28731\n",
      "28748\n",
      "28764\n",
      "28765\n",
      "28770\n",
      "28771\n",
      "28810\n",
      "28811\n",
      "28824\n",
      "28845\n",
      "28847\n",
      "28848\n",
      "28885\n",
      "28888\n",
      "28889\n",
      "28895\n",
      "28927\n",
      "28965\n",
      "28981\n",
      "29054\n",
      "29067\n",
      "29070\n",
      "29071\n",
      "29072\n",
      "29076\n",
      "29085\n",
      "29156\n",
      "29160\n",
      "29161\n",
      "29170\n",
      "29173\n",
      "29185\n",
      "29199\n",
      "29200\n",
      "29230\n",
      "29236\n",
      "29263\n",
      "29271\n",
      "29274\n",
      "29400\n",
      "29401\n",
      "29404\n",
      "29482\n",
      "29502\n",
      "29503\n",
      "29504\n",
      "29511\n",
      "29520\n",
      "29521\n",
      "29537\n",
      "29560\n",
      "29561\n",
      "29566\n",
      "29567\n",
      "29568\n",
      "29600\n",
      "29615\n",
      "29616\n",
      "29617\n",
      "29688\n",
      "29689\n",
      "29690\n",
      "29691\n",
      "29698\n",
      "29701\n",
      "29702\n",
      "29703\n",
      "29709\n",
      "29736\n",
      "29737\n",
      "29738\n",
      "29750\n",
      "29762\n",
      "29813\n",
      "29817\n",
      "29818\n",
      "29820\n",
      "29824\n",
      "29901\n",
      "29902\n",
      "29903\n",
      "29904\n",
      "29933\n",
      "29950\n",
      "29964\n",
      "29965\n",
      "29966\n",
      "29968\n",
      "29984\n",
      "30011\n",
      "30024\n",
      "30025\n",
      "30026\n",
      "30057\n",
      "30059\n",
      "30102\n",
      "30217\n",
      "30221\n",
      "30231\n",
      "30245\n",
      "30255\n",
      "30256\n",
      "30257\n",
      "30260\n",
      "30271\n",
      "30272\n",
      "30279\n",
      "30309\n",
      "30314\n",
      "30338\n",
      "30340\n",
      "30348\n",
      "30349\n",
      "30359\n",
      "30397\n",
      "30408\n",
      "30421\n",
      "30424\n",
      "30436\n",
      "30452\n",
      "30453\n",
      "30454\n",
      "30475\n",
      "30482\n",
      "30504\n",
      "30531\n",
      "30551\n",
      "30561\n",
      "30563\n",
      "30584\n",
      "30599\n",
      "30609\n",
      "30624\n",
      "30625\n",
      "30627\n",
      "30628\n",
      "30629\n",
      "30644\n",
      "30645\n",
      "30680\n",
      "30689\n",
      "30690\n",
      "30691\n",
      "30692\n",
      "30693\n",
      "30696\n",
      "30698\n",
      "30723\n",
      "30768\n",
      "30774\n",
      "30784\n",
      "30785\n",
      "30786\n",
      "30787\n",
      "30789\n",
      "30815\n",
      "30836\n",
      "30837\n",
      "30842\n",
      "30857\n",
      "30880\n",
      "30881\n",
      "30887\n",
      "30888\n",
      "30889\n",
      "30919\n",
      "30920\n",
      "30922\n",
      "30926\n",
      "30927\n",
      "30942\n",
      "30943\n",
      "30945\n",
      "31022\n",
      "31023\n",
      "31047\n",
      "31051\n",
      "31057\n",
      "31064\n",
      "31074\n",
      "31089\n",
      "31090\n",
      "31091\n",
      "31127\n",
      "31132\n",
      "31134\n",
      "31144\n",
      "31145\n",
      "31160\n",
      "31161\n",
      "31167\n",
      "31176\n",
      "31188\n",
      "31194\n",
      "31203\n",
      "31216\n",
      "31217\n",
      "31227\n",
      "31228\n",
      "31229\n",
      "31230\n",
      "31235\n",
      "31236\n",
      "31237\n",
      "31238\n",
      "31239\n",
      "31269\n",
      "31297\n",
      "31309\n",
      "31329\n",
      "31374\n",
      "31377\n",
      "31378\n",
      "31386\n",
      "31413\n",
      "31473\n",
      "31494\n",
      "31500\n",
      "31528\n",
      "31557\n",
      "31582\n",
      "31619\n",
      "31630\n",
      "31663\n",
      "31675\n",
      "31680\n",
      "31682\n",
      "31683\n",
      "31711\n",
      "31712\n",
      "31742\n",
      "31743\n",
      "31744\n",
      "31745\n",
      "31754\n",
      "31764\n",
      "31765\n",
      "31766\n",
      "31780\n",
      "31781\n",
      "31803\n",
      "31804\n",
      "31824\n",
      "31852\n",
      "31853\n",
      "31858\n",
      "31868\n",
      "31872\n",
      "31873\n",
      "31909\n",
      "31914\n",
      "31922\n",
      "31923\n",
      "31929\n",
      "31930\n",
      "31931\n",
      "31945\n",
      "31946\n",
      "31952\n",
      "31953\n",
      "31989\n",
      "31990\n",
      "31997\n",
      "32006\n",
      "32079\n",
      "32091\n",
      "32100\n",
      "32125\n",
      "32165\n",
      "32166\n",
      "32167\n",
      "32176\n",
      "32177\n",
      "32178\n",
      "32179\n",
      "32181\n",
      "32201\n",
      "32214\n",
      "32215\n",
      "32236\n",
      "32249\n",
      "32255\n",
      "32256\n",
      "32278\n",
      "32288\n",
      "32361\n",
      "32388\n",
      "32415\n",
      "32416\n",
      "32417\n",
      "32422\n",
      "32424\n",
      "32425\n",
      "32436\n",
      "32439\n",
      "32493\n",
      "32504\n",
      "32510\n",
      "32511\n",
      "32520\n",
      "32521\n",
      "32533\n",
      "32534\n",
      "32535\n",
      "32557\n",
      "32593\n",
      "32619\n",
      "32635\n",
      "32654\n",
      "32692\n",
      "32693\n",
      "32697\n",
      "32713\n",
      "32725\n",
      "32727\n",
      "32733\n",
      "32734\n",
      "32735\n",
      "32736\n",
      "32744\n",
      "32748\n",
      "32758\n",
      "32760\n",
      "32762\n",
      "32787\n",
      "32797\n",
      "32810\n",
      "32811\n",
      "32812\n",
      "32813\n",
      "32822\n",
      "32823\n",
      "32824\n",
      "32831\n",
      "32832\n",
      "32841\n",
      "32872\n",
      "32873\n",
      "32874\n",
      "32878\n",
      "32893\n",
      "32894\n",
      "32895\n",
      "32896\n",
      "32897\n",
      "32917\n",
      "32925\n",
      "32942\n",
      "32950\n",
      "32955\n",
      "32969\n",
      "32998\n",
      "33014\n",
      "33015\n",
      "33035\n",
      "33043\n",
      "33127\n",
      "33156\n",
      "33187\n",
      "33192\n",
      "33198\n",
      "33199\n",
      "33200\n",
      "33201\n",
      "33202\n",
      "33208\n",
      "33211\n",
      "33213\n",
      "33217\n",
      "33229\n",
      "33230\n",
      "33232\n",
      "33235\n",
      "33238\n",
      "33266\n",
      "33271\n",
      "33280\n",
      "33281\n",
      "33283\n",
      "33287\n",
      "33294\n",
      "33314\n",
      "33317\n",
      "33321\n",
      "33322\n",
      "33333\n",
      "33343\n",
      "33350\n",
      "33357\n",
      "33361\n",
      "33362\n",
      "33367\n",
      "33377\n",
      "33378\n",
      "33385\n",
      "33390\n",
      "33391\n",
      "33395\n",
      "33405\n",
      "33407\n",
      "33409\n",
      "33430\n",
      "33431\n",
      "33432\n",
      "33433\n",
      "33445\n",
      "33454\n",
      "33457\n",
      "33458\n",
      "33460\n",
      "33461\n",
      "33464\n",
      "33473\n",
      "33474\n",
      "33502\n",
      "33516\n",
      "33517\n",
      "33518\n",
      "33531\n",
      "33537\n",
      "33543\n",
      "33566\n",
      "33567\n",
      "33571\n",
      "33577\n",
      "33587\n",
      "33594\n",
      "33595\n",
      "33609\n",
      "33610\n",
      "33611\n",
      "33612\n",
      "33613\n",
      "33614\n",
      "33626\n",
      "33627\n",
      "33637\n",
      "33643\n",
      "33649\n",
      "33659\n",
      "33675\n",
      "33676\n",
      "33694\n",
      "33699\n",
      "33700\n",
      "33701\n",
      "33705\n",
      "33720\n",
      "33721\n",
      "33727\n",
      "33728\n",
      "33731\n",
      "33732\n",
      "33733\n",
      "33740\n",
      "33755\n",
      "33759\n",
      "33773\n",
      "33792\n",
      "33804\n",
      "33808\n",
      "33819\n",
      "33820\n",
      "33821\n",
      "33822\n",
      "33836\n",
      "33837\n",
      "33856\n",
      "33875\n",
      "33877\n",
      "33906\n",
      "33913\n",
      "33920\n",
      "33921\n",
      "33922\n",
      "33924\n",
      "33926\n",
      "33929\n",
      "33937\n",
      "33938\n",
      "33939\n",
      "33945\n",
      "33946\n",
      "33954\n",
      "33955\n",
      "33961\n",
      "33967\n",
      "33968\n",
      "33980\n",
      "33997\n",
      "33998\n",
      "33999\n",
      "34015\n",
      "34016\n",
      "34017\n",
      "34018\n",
      "34020\n",
      "34024\n",
      "34025\n",
      "34032\n",
      "34040\n",
      "34042\n",
      "34059\n",
      "34060\n",
      "34070\n",
      "34087\n",
      "34096\n",
      "34097\n",
      "34098\n",
      "34100\n",
      "34102\n",
      "34116\n",
      "34132\n",
      "34135\n",
      "34136\n",
      "34140\n",
      "34146\n",
      "34159\n",
      "34162\n",
      "34168\n",
      "34172\n",
      "34173\n",
      "34188\n",
      "34193\n",
      "34219\n",
      "34222\n",
      "34223\n",
      "34227\n",
      "34229\n",
      "34230\n",
      "34231\n",
      "34247\n",
      "34254\n",
      "34255\n",
      "34258\n",
      "34262\n",
      "34268\n",
      "34273\n",
      "34276\n",
      "34286\n",
      "34295\n",
      "34305\n",
      "34306\n",
      "34313\n",
      "34327\n",
      "34332\n",
      "34344\n",
      "34350\n",
      "34363\n",
      "34371\n",
      "34376\n",
      "34378\n",
      "34380\n",
      "34381\n",
      "34382\n",
      "34392\n",
      "34396\n",
      "34397\n",
      "34402\n",
      "34403\n",
      "34408\n",
      "34422\n",
      "34423\n",
      "34424\n",
      "34425\n",
      "34426\n",
      "34429\n",
      "34441\n",
      "34449\n",
      "34450\n",
      "34452\n",
      "34457\n",
      "34459\n",
      "34473\n",
      "34476\n",
      "34482\n",
      "34494\n",
      "34512\n",
      "34513\n",
      "34543\n",
      "34546\n",
      "34560\n",
      "34565\n",
      "34578\n",
      "34583\n",
      "34602\n",
      "34609\n",
      "34617\n",
      "34633\n",
      "34634\n",
      "34635\n",
      "34636\n",
      "34639\n",
      "34640\n",
      "34643\n",
      "34644\n",
      "34645\n",
      "34646\n",
      "34704\n",
      "34716\n",
      "34743\n",
      "34779\n",
      "34799\n",
      "34823\n",
      "34824\n",
      "34827\n",
      "34828\n",
      "34829\n",
      "34840\n",
      "34843\n",
      "34846\n",
      "34865\n",
      "34867\n",
      "34877\n",
      "34888\n",
      "34900\n",
      "34901\n",
      "34912\n",
      "34913\n",
      "34914\n",
      "34945\n",
      "34951\n",
      "34952\n",
      "34958\n",
      "34967\n",
      "34987\n",
      "34988\n",
      "34993\n",
      "34994\n",
      "35031\n",
      "35032\n",
      "35033\n",
      "35050\n",
      "35051\n",
      "35058\n",
      "35064\n",
      "35073\n",
      "35083\n",
      "35090\n",
      "35091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35119\n",
      "35172\n",
      "35173\n",
      "35175\n",
      "35176\n",
      "35182\n",
      "35183\n",
      "35184\n",
      "35185\n",
      "35186\n",
      "35187\n",
      "35194\n",
      "35195\n",
      "35219\n",
      "35220\n",
      "35221\n",
      "35238\n",
      "35243\n",
      "35244\n",
      "35245\n",
      "35248\n",
      "35289\n",
      "35341\n",
      "35354\n",
      "35411\n",
      "35413\n",
      "35414\n",
      "35434\n",
      "35448\n",
      "35449\n",
      "35466\n",
      "35472\n",
      "35475\n",
      "35476\n",
      "35478\n",
      "35488\n",
      "35490\n",
      "35502\n",
      "35503\n",
      "35510\n",
      "35535\n",
      "35543\n",
      "35544\n",
      "35545\n",
      "35546\n",
      "35547\n",
      "35561\n",
      "35562\n",
      "35573\n",
      "35574\n",
      "35584\n",
      "35586\n",
      "35587\n",
      "35592\n",
      "35608\n",
      "35629\n",
      "35630\n",
      "35649\n",
      "35650\n",
      "35651\n",
      "35652\n",
      "35653\n",
      "35683\n",
      "35684\n",
      "35687\n",
      "35716\n",
      "35717\n",
      "35732\n",
      "35775\n",
      "35805\n",
      "35821\n",
      "35823\n",
      "35824\n",
      "35825\n",
      "35826\n",
      "35829\n",
      "35838\n",
      "35839\n",
      "35840\n",
      "35843\n",
      "35844\n",
      "35850\n",
      "35852\n",
      "35856\n",
      "35868\n",
      "35873\n",
      "35916\n",
      "35951\n",
      "36042\n",
      "36043\n",
      "36051\n",
      "36060\n",
      "36080\n",
      "36081\n",
      "36093\n",
      "36098\n",
      "36100\n",
      "36101\n",
      "36127\n",
      "36157\n",
      "36185\n",
      "36186\n",
      "36197\n",
      "36210\n",
      "36220\n",
      "36223\n",
      "36226\n",
      "36255\n",
      "36256\n",
      "36257\n",
      "36273\n",
      "36274\n",
      "36285\n",
      "36289\n",
      "36292\n",
      "36321\n",
      "36322\n",
      "36323\n",
      "36347\n",
      "36363\n",
      "36400\n",
      "36406\n",
      "36409\n",
      "36425\n",
      "36426\n",
      "36433\n",
      "36434\n",
      "36436\n",
      "36500\n",
      "36501\n",
      "36536\n",
      "36561\n",
      "36566\n",
      "36575\n",
      "36576\n",
      "36578\n",
      "36579\n",
      "36592\n",
      "36612\n",
      "36614\n",
      "36620\n",
      "36622\n",
      "36623\n",
      "36633\n",
      "36638\n",
      "36639\n",
      "36660\n",
      "36669\n",
      "36679\n",
      "36681\n",
      "36687\n",
      "36688\n",
      "36689\n",
      "36691\n",
      "36711\n",
      "36712\n",
      "36723\n",
      "36724\n",
      "36725\n",
      "36726\n",
      "36733\n",
      "36750\n",
      "36762\n",
      "36768\n",
      "36788\n",
      "36792\n",
      "36926\n",
      "36927\n",
      "36928\n",
      "36938\n",
      "36944\n",
      "36945\n",
      "37001\n",
      "37028\n",
      "37036\n",
      "37042\n",
      "37046\n",
      "37068\n",
      "37078\n",
      "37132\n",
      "37133\n",
      "37167\n",
      "37168\n",
      "37169\n",
      "37173\n",
      "37207\n",
      "37212\n",
      "37224\n",
      "37236\n",
      "37237\n",
      "37257\n",
      "37258\n",
      "37265\n",
      "37268\n",
      "37269\n",
      "37272\n",
      "37286\n",
      "37287\n",
      "37289\n",
      "37296\n",
      "37320\n",
      "37325\n",
      "37339\n",
      "37340\n",
      "37341\n",
      "37342\n",
      "37343\n",
      "37372\n",
      "37373\n",
      "37395\n",
      "37396\n",
      "37397\n",
      "37411\n",
      "37412\n",
      "37414\n",
      "37432\n",
      "37438\n",
      "37439\n",
      "37452\n",
      "37459\n",
      "37467\n",
      "37468\n",
      "37533\n",
      "37562\n",
      "37565\n",
      "37596\n",
      "37599\n",
      "37613\n",
      "37646\n",
      "37677\n",
      "37695\n",
      "37702\n",
      "37714\n",
      "37715\n",
      "37717\n",
      "37733\n",
      "37743\n",
      "37748\n",
      "37749\n",
      "37763\n",
      "37764\n",
      "37771\n",
      "37803\n",
      "37804\n",
      "37805\n",
      "37817\n",
      "37845\n",
      "37846\n",
      "37870\n",
      "37925\n",
      "37934\n",
      "37946\n",
      "37952\n",
      "37968\n",
      "37971\n",
      "37980\n",
      "37990\n",
      "37991\n",
      "37998\n",
      "38052\n",
      "38060\n",
      "38061\n",
      "38080\n",
      "38111\n",
      "38112\n",
      "38140\n",
      "38148\n",
      "38150\n",
      "38169\n",
      "38170\n",
      "38171\n",
      "38177\n",
      "38207\n",
      "38209\n",
      "38217\n",
      "38218\n",
      "38219\n",
      "38256\n",
      "38300\n",
      "38321\n",
      "38322\n",
      "38366\n",
      "38392\n",
      "38417\n",
      "38418\n",
      "38419\n",
      "38458\n",
      "38462\n",
      "38575\n",
      "38603\n",
      "38619\n",
      "38620\n",
      "38633\n",
      "38635\n",
      "38649\n",
      "38678\n",
      "38684\n",
      "38685\n",
      "38686\n",
      "38698\n",
      "38709\n",
      "38712\n",
      "38720\n",
      "38721\n",
      "38722\n",
      "38761\n",
      "38762\n",
      "38768\n",
      "38783\n",
      "38784\n",
      "38785\n",
      "38786\n",
      "38808\n",
      "38820\n",
      "38821\n",
      "38877\n",
      "38880\n",
      "38902\n",
      "38932\n",
      "38945\n",
      "38952\n",
      "38957\n",
      "38961\n",
      "38962\n",
      "38963\n",
      "38972\n",
      "38973\n",
      "38976\n",
      "38977\n",
      "38978\n",
      "38985\n",
      "38992\n",
      "39011\n",
      "39014\n",
      "39023\n",
      "39080\n",
      "39105\n",
      "39131\n",
      "39142\n",
      "39187\n",
      "39224\n",
      "39225\n",
      "39240\n",
      "39251\n",
      "39265\n",
      "39342\n",
      "39350\n",
      "39351\n",
      "39352\n",
      "39368\n",
      "39389\n",
      "39407\n",
      "39408\n",
      "39417\n",
      "39419\n",
      "39420\n",
      "39427\n",
      "39428\n",
      "39433\n",
      "39439\n",
      "39446\n",
      "39447\n",
      "39451\n",
      "39480\n",
      "39500\n",
      "39501\n",
      "39517\n",
      "39519\n",
      "39520\n",
      "39565\n",
      "39573\n",
      "39595\n",
      "39596\n",
      "39606\n",
      "39643\n",
      "39644\n",
      "39667\n",
      "39672\n",
      "39674\n",
      "39744\n",
      "39745\n",
      "39762\n",
      "39763\n",
      "39764\n",
      "39765\n",
      "39766\n",
      "39779\n",
      "39800\n",
      "39831\n",
      "39835\n",
      "39865\n",
      "39866\n",
      "39869\n",
      "39870\n",
      "39876\n",
      "39877\n",
      "39878\n",
      "39904\n",
      "39907\n",
      "39910\n",
      "39911\n",
      "39924\n",
      "39925\n",
      "39943\n",
      "39959\n",
      "39960\n",
      "39989\n",
      "40160\n",
      "40215\n",
      "40220\n",
      "40244\n",
      "40255\n",
      "40268\n",
      "40297\n",
      "40298\n",
      "40347\n",
      "40359\n",
      "40371\n",
      "40380\n",
      "40381\n",
      "40421\n",
      "40444\n",
      "40447\n",
      "40448\n",
      "40475\n",
      "40485\n",
      "40494\n",
      "40495\n",
      "40505\n",
      "40510\n",
      "40511\n",
      "40516\n",
      "40517\n",
      "40564\n",
      "40567\n",
      "40569\n",
      "40570\n",
      "40571\n",
      "40573\n",
      "40576\n",
      "40577\n",
      "40578\n",
      "40586\n",
      "40589\n",
      "40633\n",
      "40634\n",
      "40638\n",
      "40647\n",
      "40653\n",
      "40654\n",
      "40655\n",
      "40656\n",
      "40670\n",
      "40688\n",
      "40707\n",
      "40708\n",
      "40769\n",
      "40827\n",
      "40858\n",
      "40916\n",
      "40936\n",
      "40940\n",
      "cnt=4146\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for idx in range(len(X_train)):\n",
    "    if X_train[idx].shape != img_dim: \n",
    "        cnt += 1\n",
    "        print(idx) \n",
    "print(\"cnt={}\".format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.15688125353974"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "514640/5297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27225"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(img_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-034f3ed36142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-52e7cffdbb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}, {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "img = X_validation[0]\n",
    "print('{}, {}'.format(img.shape, X_validation.shape))\n",
    "X = []\n",
    "X = X.append(img.flatten())\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=257x99 at 0x7F7A479872B0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image.open('/data/01_experiment_data/img_snd/four/f34e6f44_nohash_0.png')\n",
    "Image.open('/data/01_experiment_data/img_snd/dog/471a0925_nohash_0.png')\n",
    "#Image.open('/data/01_experiment_data/img_snd/cat/30276d03_nohash_0.png')\n",
    "#Image.open('/data/01_experiment_data/img_snd/cat/00f0204f_nohash_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a fuNction for iterating through mini-batches of data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=64, \n",
    "                    shuffle=False, random_seed=None):\n",
    "    \n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this function returns a generator with a tuple for a match of samples\n",
    "    * data $X$ and lables $y$\n",
    "\n",
    "Then, we need to normalize the data (mean centering and division by the standard deviation) for better training performance and convergence.\n",
    "* compute standard deviation from the `X_train` array using `np.std` without specifying an `axis` argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_valid_centered = (X_valid - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_data, y_data, X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in TensorFlow low-level API\n",
    "\n",
    "To implement a CNN in a TF, first we define two wrapper functions to make the process of building the network simpler.\n",
    "* a wrapper function for convolutional layer\n",
    "* a wrapper function for a fully connected layer\n",
    "\n",
    "First, a convolution layer is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/net_pre-activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## wrapper functions \n",
    "\n",
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels, \n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ##   input tensor shape: \n",
    "        ##   [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1] \n",
    "\n",
    "        weights_shape = (list(kernel_size) + \n",
    "                         [n_input_channels, n_output_channels])\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_channels]))\n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input=input_tensor, \n",
    "                            filter=weights,\n",
    "                            strides=strides, \n",
    "                            padding=padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases, \n",
    "                              name='net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name='activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "\n",
    "## testing\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name='convtest', kernel_size=(3, 3), n_output_channels=32)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* defining the weights, biases, initializing them, and the convolution operation using the `tf.nn.conv2d` function with the following required arguments\n",
    "    * `input_tensor`: the tensor given as input to the convolutional layer\n",
    "    * `name`: the name of the layer, which is used as the scope name\n",
    "    * `kernel_size`: the dimensions of the kernel tensor provided as a tuple or list\n",
    "    * `n_output_channels`: the number of output feature maps\n",
    "* this function was tested with a simple input by defining a placeholder\n",
    "\n",
    "<br>\n",
    "\n",
    "The next wrapper function is for defining the fully connected layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def fc_layer(input_tensor, name, \n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor, \n",
    "                                      shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                              name='net_pre-activation')\n",
    "        print(layer)\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        \n",
    "        layer = activation_fn(layer, name='activation')\n",
    "        print(layer)\n",
    "        return layer\n",
    "\n",
    "    \n",
    "## testing:\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, \n",
    "                       shape=[None, 28, 28, 1])\n",
    "    fc_layer(x, name='fctest', n_output_units=32, \n",
    "             activation_fn=tf.nn.relu)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fc1/_weights:0' shape=(784, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc1/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc1/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc1/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc1/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "<tf.Variable 'fc2/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc2/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc2/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc2/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc2/activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    fc_layer1 = fc_layer(x, name='fc1', n_output_units=1024, activation_fn=tf.nn.relu)\n",
    "    fc_layer2 = fc_layer(fc_layer1, name='fc2', n_output_units=10, activation_fn=tf.nn.sigmoid)\n",
    "    \n",
    "del g2, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* builds the weights and biases, initializes them similar to the `conv_layer` function, and then perform a matrix multiplication using the `tf.matmul` function\n",
    "* the requied arguments are\n",
    "    * `input_tensor`: the input tensor\n",
    "    * `name`: the name of the layer, a scope name\n",
    "    * `n_output_units`: the number of output units\n",
    "* the `fc_layer` function was tested for a simple input tensor \n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we can utilize these wrapper functions to build the whole convolutional network.\n",
    "* define a function called `build_cnn` to handle the building of the CNN model as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, 784],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    h1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5), \n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling\n",
    "    h1_pool = tf.nn.max_pool(h1, \n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], \n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    h2 = conv_layer(h1_pool, name='conv_2', \n",
    "                    kernel_size=(5,5), \n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    h2_pool = tf.nn.max_pool(h2, \n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], \n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Fully Connected\n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    h3 = fc_layer(h2_pool, name='fc_3',\n",
    "                  n_output_units=1024, \n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, \n",
    "                            name='dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    h4 = fc_layer(h3_drop, name='fc_4',\n",
    "                  n_output_units=10, \n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(h4, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(h4, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "    \n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=h4, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'], \n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n",
    "\n",
    "    \n",
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' % path)\n",
    "    saver.save(sess, os.path.join(path,'cnn-model.ckpt'),\n",
    "               global_step=epoch)\n",
    "\n",
    "    \n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "            path, 'cnn-model.ckpt-%d' % epoch))\n",
    "\n",
    "    \n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=0.5, random_seed=None):\n",
    "\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "\n",
    "    ## initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed) # for shuflling in batch_generator\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(\n",
    "                        X_data, y_data, \n",
    "                        shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0': batch_x, \n",
    "                    'tf_y:0': batch_y, \n",
    "                    'fc_keep_prob:0': dropout}\n",
    "            loss, _ = sess.run(\n",
    "                    ['cross_entropy_loss:0', 'train_op'],\n",
    "                    feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "\n",
    "        training_loss.append(avg_loss / (i+1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f' % (\n",
    "            epoch, avg_loss), end=' ')\n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0': validation_set[0],\n",
    "                    'tf_y:0': validation_set[1],\n",
    "                    'fc_keep_prob:0':1.0}\n",
    "            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "            \n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_x:0': X_test, \n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the following figure shows the TF graph related to our multilayer CNN as visualized by TensorBoard:\n",
    "<img src='images/15_10.png' width=800> \n",
    "* also, defined four other functions\n",
    "    * `save`, `load`, `train` for training the model using `training_set`, and `predict` to get prediction probabilities or prediction labels of the test data\n",
    "\n",
    "Now, we can create a TF graph object, set the graph-level random seed, and build the CNN model in that graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer: \n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "## create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to train our CNN model.\n",
    "* need to create a TF session to launch the graph\n",
    "    * then, call the `train` function\n",
    "    * to train the model for the first time, need to initialize all the variables in the network\n",
    "        * `initialize` argument to `True` will execute `tf.global_variables_initializer` through `session.run`\n",
    "            * this initialization step should be avoided in case you want to train additional epochs (e.g., restoring an already trained model and train further for additional epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training Avg. Loss: 272.307  Validation Acc:   0.972\n",
      "Epoch 02 Training Avg. Loss:  75.711  Validation Acc:   0.982\n",
      "Epoch 03 Training Avg. Loss:  51.187  Validation Acc:   0.985\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "## crearte a TF session \n",
    "## and train the CNN model\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    train(sess, \n",
    "          training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid), \n",
    "          initialize=True,\n",
    "          random_seed=123, epochs=3)\n",
    "    save(saver, sess, epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* after the 20 epochs, the trained model was saved for future use\n",
    "* the following code shows how to restore a saved model\n",
    "    * delte the graph `g`, then create a new graph `g2`, and reload the trained model to do prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer: \n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-3\n",
      "Test Accuracy: 98.630%\n"
     ]
    }
   ],
   "source": [
    "### Calculate prediction accuracy\n",
    "### on test set\n",
    "### restoring the saved model\n",
    "\n",
    "del g\n",
    "\n",
    "## create a new graph \n",
    "## and build the model\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=3, path='./model/')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, \n",
    "                    return_proba=False)\n",
    "\n",
    "    print('Test Accuracy: %.3f%%' % (100*\n",
    "                np.sum(preds == y_test)/len(y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the prediction accuracy on the test set is already better than what we obtained using multilayer perceptron in Chapter 13\n",
    "* `X_test_centered` as test data not `X_test` \n",
    "* now, let's look at the predicted labels as well as their probabilities on the first 10 test samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[[ 0.    0.    0.    0.    0.    0.    0.    1.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 1.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.99  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.99]\n",
      " [ 0.    0.    0.    0.    0.    0.99  0.01  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "## run the prediction on \n",
    "##  some test samples\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=20, path='./model/')\n",
    "        \n",
    "    print(predict(sess, X_test_centered[:10], \n",
    "              return_proba=False))\n",
    "    \n",
    "    print(predict(sess, X_test_centered[:10], \n",
    "                  return_proba=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how we can train the model further to reach a total of 40 epochs.\n",
    "* restoring the already trained model and continue training for 20 additional epochs\n",
    "    * call the `train` function again with `initialize = False` to avoid the initialization step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-3\n",
      "Epoch 01 Training Avg. Loss:  40.560  Validation Acc:   0.986\n",
      "Epoch 02 Training Avg. Loss:  32.581  Validation Acc:   0.988\n",
      "Epoch 03 Training Avg. Loss:  27.416  Validation Acc:   0.988\n",
      "Saving model in ./model/\n",
      "Test Accuracy: 99.100%\n"
     ]
    }
   ],
   "source": [
    "## continue training for 20 more epochs\n",
    "## without re-initializing :: initialize=False\n",
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=3, path='./model/')\n",
    "    \n",
    "    train(sess,\n",
    "          training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid),\n",
    "          initialize=False,\n",
    "          epochs=3,\n",
    "          random_seed=123)\n",
    "        \n",
    "    save(saver, sess, epoch=6, path='./model/')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, \n",
    "                    return_proba=False)\n",
    "    \n",
    "    print('Test Accuracy: %.3f%%' % (100*\n",
    "                np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the result shows that training for 20 additional epochs slightly improved the performance to get 99.37% prediction accuracy on the test set\n",
    "\n",
    "In the next section, we'll now implement the CNN using the TF Layers API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in the TensorFlow Layers API\n",
    "\n",
    "We can implement the model in a new class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConvNN(object):\n",
    "    def __init__(self, batchsize=64,\n",
    "                 epochs=20, learning_rate=1e-4, \n",
    "                 dropout_rate=0.5,\n",
    "                 shuffle=True, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "                \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            ## set random-seed:\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            ## build the network:\n",
    "            self.build()\n",
    "\n",
    "            ## initializer\n",
    "            self.init_op = \\\n",
    "                tf.global_variables_initializer()\n",
    "\n",
    "            ## saver\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "        ## create a session\n",
    "        self.sess = tf.Session(graph=g)\n",
    "                \n",
    "    def build(self):\n",
    "        \n",
    "        ## Placeholders for X and y:\n",
    "        tf_x = tf.placeholder(tf.float32, \n",
    "                              shape=[None, 784],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[None],\n",
    "                              name='tf_y')\n",
    "        is_train = tf.placeholder(tf.bool, \n",
    "                              shape=(),\n",
    "                              name='is_train')\n",
    "\n",
    "        ## reshape x to a 4D tensor: \n",
    "        ##  [batchsize, width, height, 1]\n",
    "        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                              name='input_x_2dimages')\n",
    "        ## One-hot encoding:\n",
    "        tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                              dtype=tf.float32,\n",
    "                              name='input_y_onehot')\n",
    "\n",
    "        ## 1st layer: Conv_1\n",
    "        h1 = tf.layers.conv2d(tf_x_image, \n",
    "                              kernel_size=(5, 5), \n",
    "                              filters=32, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling\n",
    "        h1_pool = tf.layers.max_pooling2d(h1, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "        ## 2n layer: Conv_2\n",
    "        h2 = tf.layers.conv2d(h1_pool, kernel_size=(5,5), \n",
    "                              filters=64, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling \n",
    "        h2_pool = tf.layers.max_pooling2d(h2, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "\n",
    "        ## 3rd layer: Fully Connected\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool, \n",
    "                              shape=[-1, n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat, 1024, \n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "        ## Dropout\n",
    "        h3_drop = tf.layers.dropout(h3, \n",
    "                              rate=self.dropout_rate,\n",
    "                              training=is_train)\n",
    "        \n",
    "        ## 4th layer: Fully Connected (linear activation)\n",
    "        h4 = tf.layers.dense(h3_drop, 10, \n",
    "                              activation=None)\n",
    "\n",
    "        ## Prediction\n",
    "        predictions = {\n",
    "            'probabilities': tf.nn.softmax(h4, \n",
    "                              name='probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(h4, axis=1), \n",
    "                              tf.int32, name='labels')}\n",
    "        \n",
    "        ## Loss Function and Optimization\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=h4, labels=tf_y_onehot),\n",
    "            name='cross_entropy_loss')\n",
    "        \n",
    "        ## Optimizer:\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                              name='train_op')\n",
    "\n",
    "        ## Finding accuracy\n",
    "        correct_predictions = tf.equal(\n",
    "            predictions['labels'], \n",
    "            tf_y, name='correct_preds')\n",
    "        \n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "\n",
    "    def save(self, epoch, path='./tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s' % path)\n",
    "        self.saver.save(self.sess, \n",
    "                        os.path.join(path, 'model.ckpt'),\n",
    "                        global_step=epoch)\n",
    "        \n",
    "    def load(self, epoch, path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess, \n",
    "             os.path.join(path, 'model.ckpt-%d' % epoch))\n",
    "        \n",
    "    def train(self, training_set, \n",
    "              validation_set=None,\n",
    "              initialize=True):\n",
    "        ## initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            batch_gen = \\\n",
    "                batch_generator(X_data, y_data, \n",
    "                                 shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x,batch_y) in \\\n",
    "                enumerate(batch_gen):\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': True} ## for dropout\n",
    "                loss, _ = self.sess.run(\n",
    "                        ['cross_entropy_loss:0', 'train_op'], \n",
    "                        feed_dict=feed)\n",
    "                avg_loss += loss\n",
    "                \n",
    "            print('Epoch %02d: Training Avg. Loss: '\n",
    "                  '%7.3f' % (epoch, avg_loss), end=' ')\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': False} ## for dropout\n",
    "                valid_acc = self.sess.run('accuracy:0',\n",
    "                                          feed_dict=feed)\n",
    "                print('Validation Acc: %7.3f' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "                    \n",
    "    def predict(self, X_test, return_proba = False):\n",
    "        feed = {'tf_x:0': X_test,\n",
    "                'is_train:0': False} ## for dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',\n",
    "                                 feed_dict=feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',\n",
    "                                 feed_dict=feed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the structure of this class is very similar to the previous section with the low-level TF API\n",
    "    * the class has a constructor that sets the training parameters, creates a graph `g`, and builds the model\n",
    "    * besides the constructor, there are five major methods \n",
    "        * `.build`: builds the model\n",
    "        * `.save`: to save a trained model\n",
    "        * `.load`: to restore a saved model\n",
    "        * `.train`: trains the model\n",
    "        * `.predict`: to do prediction on a test set\n",
    "* a dropout layer after the first fully connected layer (`tf.nn.dropout` in low-level API and `tf.layers.dropout` in the high-level Layers API)\n",
    "    * `tf.nn.dropout`: :an argument, `keep_prob` indicates the probability of keeping the units, whereas `tf.layers.dropout` has a `rate` parameter, which is the rate of dropping units (i.e., `rate = 1 - keep_prob`)\n",
    "    * in the `tf.nn.dropout` function, we fed the `keep_prob` parameter using a placeholder\n",
    "        * during the training, `keep_prob = 0.5`\n",
    "        * during the inference (or prediction), `keep_prob = 1`\n",
    "    * in the `tf.layers.dropout`, the value `rate` is provided upon the creation of the dropout layer in the graph\n",
    "        * we cannot change it during the training or the inference modes\n",
    "            * instead, a Boolean argument called `training` to determine whether we need to apply dropout or not (`tf.bool=True` during the training mode and `tf.bool=False` during the inference mode)\n",
    "            \n",
    "\n",
    "We can create an instance of the `ConvNN` class, train it for 20 epochs, and save the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss: 269.172 Validation Acc:   1.000\n",
      "Epoch 02: Training Avg. Loss:  74.298 Validation Acc:   0.938\n",
      "Epoch 03: Training Avg. Loss:  50.655 Validation Acc:   1.000\n",
      "Saving model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNN(random_seed=123, epochs=3)\n",
    "cnn.train(training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid))\n",
    "\n",
    "#cnn.save(epoch=20)\n",
    "cnn.save(epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* after the training is finished, the model can be used to do prediction on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./tflayers-model/\n",
      "INFO:tensorflow:Restoring parameters from ./tflayers-model/model.ckpt-3\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "del cnn\n",
    "\n",
    "cnn2 = ConvNN(random_seed=123)\n",
    "\n",
    "#cnn2.load(epoch=20, path='./tflayers-model/')\n",
    "cnn2.load(epoch=3, path='./tflayers-model/')\n",
    "\n",
    "print(cnn2.predict(X_test_centered[:10,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can measure the accuracy of the test dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.76%\n"
     ]
    }
   ],
   "source": [
    "preds = cnn2.predict(X_test_centered)\n",
    "\n",
    "print('Test Accuracy: %.2f%%' % (100*\n",
    "      np.sum(y_test == preds)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the obtained accuracy is 98.76%, which means there are 124 misclassified test samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "In this chapter, we learned about CNNs and explored the building blocks that form different CNN architectures.\n",
    "* started by defining the convolution operation\n",
    "* covered subsampling by discussing two forms of pooling operations\n",
    "    * max-pooling \n",
    "    * average-pooling\n",
    "\n",
    "Then, putting all these blocks together, a deep CNN was built using the TF core API as well as TF __Layers__ API for image classification.\n",
    "\n",
    "In the next chapter, we'll move on to __Recurrent Neural Networks (RNN)__.\n",
    "* used for learning the structure of sequence data \n",
    "* applications including language translation and image captioning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
