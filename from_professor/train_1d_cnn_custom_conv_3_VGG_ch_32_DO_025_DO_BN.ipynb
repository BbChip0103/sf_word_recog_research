{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,504\n",
      "Trainable params: 8,195,376\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,740,464\n",
      "Trainable params: 2,740,208\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 926,256\n",
      "Trainable params: 925,872\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1709 - acc: 0.3524\n",
      "Epoch 00001: val_loss improved from inf to 2.12355, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv_checkpoint/001-2.1235.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.1707 - acc: 0.3525 - val_loss: 2.1235 - val_acc: 0.3778\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4838 - acc: 0.5350\n",
      "Epoch 00002: val_loss improved from 2.12355 to 1.28673, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv_checkpoint/002-1.2867.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.4838 - acc: 0.5350 - val_loss: 1.2867 - val_acc: 0.6021\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2601 - acc: 0.6050\n",
      "Epoch 00003: val_loss improved from 1.28673 to 1.25715, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv_checkpoint/003-1.2572.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.2602 - acc: 0.6050 - val_loss: 1.2572 - val_acc: 0.6038\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1193 - acc: 0.6512\n",
      "Epoch 00004: val_loss did not improve from 1.25715\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.1194 - acc: 0.6511 - val_loss: 1.3315 - val_acc: 0.5882\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9995 - acc: 0.6873\n",
      "Epoch 00005: val_loss did not improve from 1.25715\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9996 - acc: 0.6872 - val_loss: 1.2925 - val_acc: 0.5982\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9056 - acc: 0.7195\n",
      "Epoch 00006: val_loss did not improve from 1.25715\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9056 - acc: 0.7195 - val_loss: 1.2735 - val_acc: 0.6066\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.7464\n",
      "Epoch 00007: val_loss improved from 1.25715 to 1.19617, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv_checkpoint/007-1.1962.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.8213 - acc: 0.7464 - val_loss: 1.1962 - val_acc: 0.6429\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7605 - acc: 0.7646\n",
      "Epoch 00008: val_loss improved from 1.19617 to 1.15976, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv_checkpoint/008-1.1598.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7605 - acc: 0.7646 - val_loss: 1.1598 - val_acc: 0.6520\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6867 - acc: 0.7862\n",
      "Epoch 00009: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6867 - acc: 0.7863 - val_loss: 1.2464 - val_acc: 0.6136\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.8050\n",
      "Epoch 00010: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6305 - acc: 0.8050 - val_loss: 1.2286 - val_acc: 0.6364\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8240\n",
      "Epoch 00011: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5786 - acc: 0.8240 - val_loss: 1.1634 - val_acc: 0.6494\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8376\n",
      "Epoch 00012: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5351 - acc: 0.8376 - val_loss: 1.2405 - val_acc: 0.6324\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8489\n",
      "Epoch 00013: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4952 - acc: 0.8489 - val_loss: 1.3168 - val_acc: 0.6310\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8608\n",
      "Epoch 00014: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4586 - acc: 0.8608 - val_loss: 1.2186 - val_acc: 0.6490\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8702\n",
      "Epoch 00015: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4316 - acc: 0.8702 - val_loss: 1.1663 - val_acc: 0.6720\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8852\n",
      "Epoch 00016: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3931 - acc: 0.8852 - val_loss: 1.1889 - val_acc: 0.6692\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8898\n",
      "Epoch 00017: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3692 - acc: 0.8898 - val_loss: 1.3350 - val_acc: 0.6341\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.9008\n",
      "Epoch 00018: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3409 - acc: 0.9008 - val_loss: 1.3217 - val_acc: 0.6450\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9054\n",
      "Epoch 00019: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3241 - acc: 0.9053 - val_loss: 1.2709 - val_acc: 0.6515\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9073\n",
      "Epoch 00020: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3132 - acc: 0.9073 - val_loss: 1.2977 - val_acc: 0.6476\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9160\n",
      "Epoch 00021: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2898 - acc: 0.9159 - val_loss: 1.3609 - val_acc: 0.6445\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9194\n",
      "Epoch 00022: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2760 - acc: 0.9194 - val_loss: 1.3991 - val_acc: 0.6375\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9263\n",
      "Epoch 00023: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2572 - acc: 0.9263 - val_loss: 1.2948 - val_acc: 0.6660\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9311\n",
      "Epoch 00024: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2410 - acc: 0.9311 - val_loss: 1.4870 - val_acc: 0.6215\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9357\n",
      "Epoch 00025: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2279 - acc: 0.9357 - val_loss: 1.4124 - val_acc: 0.6483\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9360\n",
      "Epoch 00026: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2244 - acc: 0.9359 - val_loss: 1.4501 - val_acc: 0.6464\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9379\n",
      "Epoch 00027: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2192 - acc: 0.9379 - val_loss: 1.3804 - val_acc: 0.6532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9423\n",
      "Epoch 00028: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2030 - acc: 0.9423 - val_loss: 1.3931 - val_acc: 0.6548\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9460\n",
      "Epoch 00029: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1921 - acc: 0.9460 - val_loss: 1.4802 - val_acc: 0.6541\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9479\n",
      "Epoch 00030: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1854 - acc: 0.9479 - val_loss: 1.3770 - val_acc: 0.6634\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9490\n",
      "Epoch 00031: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1825 - acc: 0.9491 - val_loss: 1.3856 - val_acc: 0.6720\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9516\n",
      "Epoch 00032: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1727 - acc: 0.9516 - val_loss: 1.4263 - val_acc: 0.6709\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9543\n",
      "Epoch 00033: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1651 - acc: 0.9543 - val_loss: 1.4786 - val_acc: 0.6583\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9558\n",
      "Epoch 00034: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.1581 - acc: 0.9558 - val_loss: 1.3936 - val_acc: 0.6723\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9580\n",
      "Epoch 00035: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.1542 - acc: 0.9579 - val_loss: 1.4447 - val_acc: 0.6578\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9558\n",
      "Epoch 00036: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.1565 - acc: 0.9558 - val_loss: 1.5363 - val_acc: 0.6587\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9620\n",
      "Epoch 00037: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1422 - acc: 0.9620 - val_loss: 1.4747 - val_acc: 0.6678\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9619\n",
      "Epoch 00038: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1416 - acc: 0.9618 - val_loss: 1.5292 - val_acc: 0.6560\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9621\n",
      "Epoch 00039: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.1357 - acc: 0.9621 - val_loss: 1.5447 - val_acc: 0.6529\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9615\n",
      "Epoch 00040: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1368 - acc: 0.9614 - val_loss: 1.5458 - val_acc: 0.6543\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9644\n",
      "Epoch 00041: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.1281 - acc: 0.9644 - val_loss: 1.5674 - val_acc: 0.6611\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9666\n",
      "Epoch 00042: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1192 - acc: 0.9666 - val_loss: 1.4697 - val_acc: 0.6702\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9653\n",
      "Epoch 00043: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1276 - acc: 0.9652 - val_loss: 1.5835 - val_acc: 0.6634\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9667\n",
      "Epoch 00044: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1211 - acc: 0.9667 - val_loss: 1.4710 - val_acc: 0.6720\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9697\n",
      "Epoch 00045: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1152 - acc: 0.9696 - val_loss: 1.4961 - val_acc: 0.6792\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9689\n",
      "Epoch 00046: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1170 - acc: 0.9689 - val_loss: 1.5431 - val_acc: 0.6697\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9698\n",
      "Epoch 00047: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1139 - acc: 0.9698 - val_loss: 1.5370 - val_acc: 0.6674\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9697\n",
      "Epoch 00048: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1097 - acc: 0.9697 - val_loss: 1.5134 - val_acc: 0.6783\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9713\n",
      "Epoch 00049: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1062 - acc: 0.9713 - val_loss: 1.8513 - val_acc: 0.6261\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9723\n",
      "Epoch 00050: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1013 - acc: 0.9723 - val_loss: 1.7109 - val_acc: 0.6550\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9726\n",
      "Epoch 00051: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1032 - acc: 0.9726 - val_loss: 1.5467 - val_acc: 0.6778\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9736\n",
      "Epoch 00052: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.0998 - acc: 0.9736 - val_loss: 1.6359 - val_acc: 0.6695\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9743\n",
      "Epoch 00053: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.0963 - acc: 0.9743 - val_loss: 1.6034 - val_acc: 0.6758\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9749\n",
      "Epoch 00054: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.0978 - acc: 0.9749 - val_loss: 1.5733 - val_acc: 0.6799\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9729\n",
      "Epoch 00055: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.0991 - acc: 0.9729 - val_loss: 1.5992 - val_acc: 0.6769\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9745\n",
      "Epoch 00056: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.0949 - acc: 0.9745 - val_loss: 1.8702 - val_acc: 0.6389\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9767\n",
      "Epoch 00057: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.0903 - acc: 0.9766 - val_loss: 1.5896 - val_acc: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9732\n",
      "Epoch 00058: val_loss did not improve from 1.15976\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.1016 - acc: 0.9732 - val_loss: 1.6869 - val_acc: 0.6615\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 793us/sample - loss: 1.3146 - acc: 0.6035\n",
      "Loss: 1.3145651748494815 Accuracy: 0.60353065\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0506 - acc: 0.3770\n",
      "Epoch 00001: val_loss improved from inf to 2.78989, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/001-2.7899.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 2.0506 - acc: 0.3770 - val_loss: 2.7899 - val_acc: 0.2457\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4128 - acc: 0.5626\n",
      "Epoch 00002: val_loss improved from 2.78989 to 1.26553, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/002-1.2655.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.4128 - acc: 0.5626 - val_loss: 1.2655 - val_acc: 0.6042\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2137 - acc: 0.6229\n",
      "Epoch 00003: val_loss did not improve from 1.26553\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.2139 - acc: 0.6229 - val_loss: 1.2894 - val_acc: 0.5968\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0846 - acc: 0.6582\n",
      "Epoch 00004: val_loss did not improve from 1.26553\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.0846 - acc: 0.6581 - val_loss: 1.4041 - val_acc: 0.5679\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9860 - acc: 0.6927\n",
      "Epoch 00005: val_loss improved from 1.26553 to 1.12228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/005-1.1223.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.9860 - acc: 0.6927 - val_loss: 1.1223 - val_acc: 0.6483\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7212\n",
      "Epoch 00006: val_loss did not improve from 1.12228\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.9012 - acc: 0.7213 - val_loss: 1.3473 - val_acc: 0.5928\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8225 - acc: 0.7438\n",
      "Epoch 00007: val_loss improved from 1.12228 to 1.04739, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/007-1.0474.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.8227 - acc: 0.7438 - val_loss: 1.0474 - val_acc: 0.6832\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7591 - acc: 0.7646\n",
      "Epoch 00008: val_loss did not improve from 1.04739\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.7590 - acc: 0.7647 - val_loss: 1.4010 - val_acc: 0.5772\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6970 - acc: 0.7851\n",
      "Epoch 00009: val_loss improved from 1.04739 to 1.01775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/009-1.0178.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6969 - acc: 0.7851 - val_loss: 1.0178 - val_acc: 0.6921\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.7988\n",
      "Epoch 00010: val_loss did not improve from 1.01775\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6484 - acc: 0.7988 - val_loss: 1.1060 - val_acc: 0.6753\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8179\n",
      "Epoch 00011: val_loss improved from 1.01775 to 0.98452, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv_checkpoint/011-0.9845.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5978 - acc: 0.8179 - val_loss: 0.9845 - val_acc: 0.7014\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.8290\n",
      "Epoch 00012: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5619 - acc: 0.8290 - val_loss: 1.0008 - val_acc: 0.6993\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.8414\n",
      "Epoch 00013: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5213 - acc: 0.8414 - val_loss: 1.0148 - val_acc: 0.7053\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.8511\n",
      "Epoch 00014: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4914 - acc: 0.8511 - val_loss: 1.0489 - val_acc: 0.6965\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8623\n",
      "Epoch 00015: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4563 - acc: 0.8623 - val_loss: 1.0404 - val_acc: 0.7000\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8686\n",
      "Epoch 00016: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4350 - acc: 0.8685 - val_loss: 0.9849 - val_acc: 0.7205\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8799\n",
      "Epoch 00017: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4051 - acc: 0.8799 - val_loss: 1.0868 - val_acc: 0.7007\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8822\n",
      "Epoch 00018: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3887 - acc: 0.8822 - val_loss: 1.0769 - val_acc: 0.6930\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8953\n",
      "Epoch 00019: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3586 - acc: 0.8953 - val_loss: 1.0103 - val_acc: 0.7221\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8974\n",
      "Epoch 00020: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3422 - acc: 0.8975 - val_loss: 1.0463 - val_acc: 0.7070\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9058\n",
      "Epoch 00021: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3235 - acc: 0.9059 - val_loss: 1.1791 - val_acc: 0.6790\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9087\n",
      "Epoch 00022: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3063 - acc: 0.9087 - val_loss: 1.1445 - val_acc: 0.6967\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9099\n",
      "Epoch 00023: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2967 - acc: 0.9098 - val_loss: 1.0464 - val_acc: 0.7165\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9174\n",
      "Epoch 00024: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.2796 - acc: 0.9174 - val_loss: 1.4956 - val_acc: 0.6287\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9212\n",
      "Epoch 00025: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2671 - acc: 0.9213 - val_loss: 1.3311 - val_acc: 0.6606\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9235\n",
      "Epoch 00026: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2597 - acc: 0.9235 - val_loss: 1.1099 - val_acc: 0.7053\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9263\n",
      "Epoch 00027: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2469 - acc: 0.9263 - val_loss: 1.1479 - val_acc: 0.6983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9321\n",
      "Epoch 00028: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2363 - acc: 0.9321 - val_loss: 1.1474 - val_acc: 0.7042\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9305\n",
      "Epoch 00029: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2360 - acc: 0.9305 - val_loss: 1.1574 - val_acc: 0.7023\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9355\n",
      "Epoch 00030: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2205 - acc: 0.9355 - val_loss: 1.1820 - val_acc: 0.7002\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9406\n",
      "Epoch 00031: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2030 - acc: 0.9406 - val_loss: 1.1243 - val_acc: 0.7174\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9417\n",
      "Epoch 00032: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2016 - acc: 0.9417 - val_loss: 1.1773 - val_acc: 0.7065\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9428\n",
      "Epoch 00033: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1951 - acc: 0.9428 - val_loss: 1.3912 - val_acc: 0.6699\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9439\n",
      "Epoch 00034: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1920 - acc: 0.9439 - val_loss: 1.1321 - val_acc: 0.7186\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9470\n",
      "Epoch 00035: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1844 - acc: 0.9469 - val_loss: 1.2439 - val_acc: 0.6986\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9484\n",
      "Epoch 00036: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1789 - acc: 0.9483 - val_loss: 1.2353 - val_acc: 0.7053\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9514\n",
      "Epoch 00037: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1693 - acc: 0.9514 - val_loss: 1.0912 - val_acc: 0.7289\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9504\n",
      "Epoch 00038: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1666 - acc: 0.9504 - val_loss: 1.1360 - val_acc: 0.7186\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9522\n",
      "Epoch 00039: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1646 - acc: 0.9522 - val_loss: 1.1415 - val_acc: 0.7221\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9532\n",
      "Epoch 00040: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1627 - acc: 0.9531 - val_loss: 1.5333 - val_acc: 0.6576\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9556\n",
      "Epoch 00041: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1542 - acc: 0.9555 - val_loss: 1.1843 - val_acc: 0.7195\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9544\n",
      "Epoch 00042: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1609 - acc: 0.9544 - val_loss: 1.2519 - val_acc: 0.7086\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9595\n",
      "Epoch 00043: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1431 - acc: 0.9595 - val_loss: 1.1702 - val_acc: 0.7191\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9594\n",
      "Epoch 00044: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1419 - acc: 0.9594 - val_loss: 1.1978 - val_acc: 0.7119\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9602\n",
      "Epoch 00045: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1374 - acc: 0.9602 - val_loss: 1.2059 - val_acc: 0.7177\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9625\n",
      "Epoch 00046: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1319 - acc: 0.9625 - val_loss: 1.2446 - val_acc: 0.7079\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9608\n",
      "Epoch 00047: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1384 - acc: 0.9608 - val_loss: 1.2809 - val_acc: 0.7112\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9620\n",
      "Epoch 00048: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1281 - acc: 0.9620 - val_loss: 1.1999 - val_acc: 0.7230\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9644\n",
      "Epoch 00049: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1266 - acc: 0.9644 - val_loss: 1.3091 - val_acc: 0.7032\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9650\n",
      "Epoch 00050: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1231 - acc: 0.9650 - val_loss: 1.1867 - val_acc: 0.7307\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9617\n",
      "Epoch 00051: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1285 - acc: 0.9617 - val_loss: 1.3217 - val_acc: 0.7077\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9687\n",
      "Epoch 00052: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1119 - acc: 0.9687 - val_loss: 1.2427 - val_acc: 0.7207\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9670\n",
      "Epoch 00053: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1179 - acc: 0.9670 - val_loss: 1.2122 - val_acc: 0.7296\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9674\n",
      "Epoch 00054: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1132 - acc: 0.9675 - val_loss: 1.3580 - val_acc: 0.7018\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9681\n",
      "Epoch 00055: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1131 - acc: 0.9681 - val_loss: 1.2600 - val_acc: 0.7160\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9708\n",
      "Epoch 00056: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1056 - acc: 0.9707 - val_loss: 1.2383 - val_acc: 0.7265\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9681\n",
      "Epoch 00057: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1123 - acc: 0.9681 - val_loss: 1.2488 - val_acc: 0.7214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9688\n",
      "Epoch 00058: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1114 - acc: 0.9688 - val_loss: 1.2667 - val_acc: 0.7251\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9707\n",
      "Epoch 00059: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1034 - acc: 0.9707 - val_loss: 1.2899 - val_acc: 0.7174\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9716\n",
      "Epoch 00060: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1014 - acc: 0.9716 - val_loss: 1.2581 - val_acc: 0.7258\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9737\n",
      "Epoch 00061: val_loss did not improve from 0.98452\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0944 - acc: 0.9737 - val_loss: 1.2288 - val_acc: 0.7305\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 886us/sample - loss: 1.0661 - acc: 0.6673\n",
      "Loss: 1.0661346805058536 Accuracy: 0.66728973\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1525 - acc: 0.3417\n",
      "Epoch 00001: val_loss improved from inf to 2.05860, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/001-2.0586.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 2.1526 - acc: 0.3417 - val_loss: 2.0586 - val_acc: 0.3748\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4399 - acc: 0.5455\n",
      "Epoch 00002: val_loss improved from 2.05860 to 1.18356, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/002-1.1836.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.4399 - acc: 0.5455 - val_loss: 1.1836 - val_acc: 0.6219\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2421 - acc: 0.6099\n",
      "Epoch 00003: val_loss improved from 1.18356 to 1.08063, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/003-1.0806.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.2420 - acc: 0.6099 - val_loss: 1.0806 - val_acc: 0.6634\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1331 - acc: 0.6493\n",
      "Epoch 00004: val_loss did not improve from 1.08063\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.1332 - acc: 0.6493 - val_loss: 1.1129 - val_acc: 0.6611\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0563 - acc: 0.6717\n",
      "Epoch 00005: val_loss improved from 1.08063 to 1.05193, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/005-1.0519.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.0562 - acc: 0.6717 - val_loss: 1.0519 - val_acc: 0.6848\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9871 - acc: 0.6945\n",
      "Epoch 00006: val_loss did not improve from 1.05193\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9872 - acc: 0.6945 - val_loss: 1.3827 - val_acc: 0.5816\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9388 - acc: 0.7103\n",
      "Epoch 00007: val_loss did not improve from 1.05193\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9390 - acc: 0.7103 - val_loss: 1.1512 - val_acc: 0.6525\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9032 - acc: 0.7214\n",
      "Epoch 00008: val_loss did not improve from 1.05193\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9032 - acc: 0.7214 - val_loss: 1.0640 - val_acc: 0.6669\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8575 - acc: 0.7360\n",
      "Epoch 00009: val_loss did not improve from 1.05193\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8575 - acc: 0.7360 - val_loss: 1.1812 - val_acc: 0.6310\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7494\n",
      "Epoch 00010: val_loss did not improve from 1.05193\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8167 - acc: 0.7494 - val_loss: 1.3542 - val_acc: 0.5984\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7571\n",
      "Epoch 00011: val_loss improved from 1.05193 to 0.98468, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/011-0.9847.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7910 - acc: 0.7570 - val_loss: 0.9847 - val_acc: 0.6918\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7652 - acc: 0.7645\n",
      "Epoch 00012: val_loss improved from 0.98468 to 0.84104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/012-0.8410.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7651 - acc: 0.7645 - val_loss: 0.8410 - val_acc: 0.7447\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7283 - acc: 0.7761\n",
      "Epoch 00013: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7283 - acc: 0.7761 - val_loss: 0.9129 - val_acc: 0.7216\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7014 - acc: 0.7850\n",
      "Epoch 00014: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7014 - acc: 0.7850 - val_loss: 1.1309 - val_acc: 0.6515\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.7905\n",
      "Epoch 00015: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6798 - acc: 0.7905 - val_loss: 0.8801 - val_acc: 0.7396\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.8002\n",
      "Epoch 00016: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6561 - acc: 0.8003 - val_loss: 1.3478 - val_acc: 0.6070\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.8040\n",
      "Epoch 00017: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6403 - acc: 0.8040 - val_loss: 0.9642 - val_acc: 0.7112\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.8120\n",
      "Epoch 00018: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6147 - acc: 0.8120 - val_loss: 1.0400 - val_acc: 0.6839\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.8157\n",
      "Epoch 00019: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5999 - acc: 0.8157 - val_loss: 0.8671 - val_acc: 0.7417\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8183\n",
      "Epoch 00020: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5855 - acc: 0.8183 - val_loss: 1.1782 - val_acc: 0.6604\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.8301\n",
      "Epoch 00021: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5599 - acc: 0.8301 - val_loss: 0.8594 - val_acc: 0.7447\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8338\n",
      "Epoch 00022: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5428 - acc: 0.8338 - val_loss: 1.0912 - val_acc: 0.6881\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8369\n",
      "Epoch 00023: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5329 - acc: 0.8368 - val_loss: 0.9480 - val_acc: 0.7200\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8406\n",
      "Epoch 00024: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5194 - acc: 0.8406 - val_loss: 1.1501 - val_acc: 0.6681\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.8486\n",
      "Epoch 00025: val_loss did not improve from 0.84104\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4976 - acc: 0.8485 - val_loss: 0.8603 - val_acc: 0.7533\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8504\n",
      "Epoch 00026: val_loss improved from 0.84104 to 0.82501, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv_checkpoint/026-0.8250.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4852 - acc: 0.8504 - val_loss: 0.8250 - val_acc: 0.7545\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8544\n",
      "Epoch 00027: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4735 - acc: 0.8544 - val_loss: 1.1384 - val_acc: 0.6746\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.8598\n",
      "Epoch 00028: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4613 - acc: 0.8597 - val_loss: 0.9327 - val_acc: 0.7314\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8610\n",
      "Epoch 00029: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4494 - acc: 0.8609 - val_loss: 1.0113 - val_acc: 0.7023\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8627\n",
      "Epoch 00030: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4404 - acc: 0.8627 - val_loss: 0.8777 - val_acc: 0.7391\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8690\n",
      "Epoch 00031: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4252 - acc: 0.8690 - val_loss: 0.9502 - val_acc: 0.7279\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8682\n",
      "Epoch 00032: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4222 - acc: 0.8681 - val_loss: 1.0101 - val_acc: 0.7058\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8724\n",
      "Epoch 00033: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4092 - acc: 0.8724 - val_loss: 0.9986 - val_acc: 0.7223\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8762\n",
      "Epoch 00034: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3961 - acc: 0.8762 - val_loss: 0.9108 - val_acc: 0.7445\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8782\n",
      "Epoch 00035: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3915 - acc: 0.8782 - val_loss: 0.8762 - val_acc: 0.7503\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8823\n",
      "Epoch 00036: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3802 - acc: 0.8823 - val_loss: 0.8718 - val_acc: 0.7524\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8849\n",
      "Epoch 00037: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3696 - acc: 0.8850 - val_loss: 0.8704 - val_acc: 0.7561\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8904\n",
      "Epoch 00038: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3561 - acc: 0.8904 - val_loss: 1.0053 - val_acc: 0.7282\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8896\n",
      "Epoch 00039: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3527 - acc: 0.8896 - val_loss: 0.9197 - val_acc: 0.7468\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8921\n",
      "Epoch 00040: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3482 - acc: 0.8921 - val_loss: 0.9459 - val_acc: 0.7391\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8957\n",
      "Epoch 00041: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3379 - acc: 0.8956 - val_loss: 0.9208 - val_acc: 0.7515\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8947\n",
      "Epoch 00042: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3399 - acc: 0.8947 - val_loss: 1.1340 - val_acc: 0.7039\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8987\n",
      "Epoch 00043: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3242 - acc: 0.8987 - val_loss: 0.9860 - val_acc: 0.7335\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9012\n",
      "Epoch 00044: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3186 - acc: 0.9011 - val_loss: 0.9551 - val_acc: 0.7440\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9033\n",
      "Epoch 00045: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3161 - acc: 0.9033 - val_loss: 0.9103 - val_acc: 0.7456\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9058\n",
      "Epoch 00046: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3021 - acc: 0.9058 - val_loss: 1.0590 - val_acc: 0.7223\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9070\n",
      "Epoch 00047: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3014 - acc: 0.9069 - val_loss: 0.9409 - val_acc: 0.7515\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9073\n",
      "Epoch 00048: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2959 - acc: 0.9072 - val_loss: 0.9263 - val_acc: 0.7463\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9059\n",
      "Epoch 00049: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2942 - acc: 0.9059 - val_loss: 0.8855 - val_acc: 0.7650\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9134\n",
      "Epoch 00050: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2807 - acc: 0.9134 - val_loss: 0.8720 - val_acc: 0.7701\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9132\n",
      "Epoch 00051: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2755 - acc: 0.9131 - val_loss: 0.9984 - val_acc: 0.7333\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9114\n",
      "Epoch 00052: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2794 - acc: 0.9114 - val_loss: 0.9548 - val_acc: 0.7475\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9172\n",
      "Epoch 00053: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2683 - acc: 0.9172 - val_loss: 1.1378 - val_acc: 0.7067\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9151\n",
      "Epoch 00054: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2683 - acc: 0.9151 - val_loss: 1.0003 - val_acc: 0.7412\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9182\n",
      "Epoch 00055: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2618 - acc: 0.9182 - val_loss: 0.9041 - val_acc: 0.7715\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9168\n",
      "Epoch 00056: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2607 - acc: 0.9168 - val_loss: 1.0157 - val_acc: 0.7410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9203\n",
      "Epoch 00057: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2527 - acc: 0.9203 - val_loss: 0.9857 - val_acc: 0.7552\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9221\n",
      "Epoch 00058: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2483 - acc: 0.9221 - val_loss: 0.8922 - val_acc: 0.7605\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9250\n",
      "Epoch 00059: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2392 - acc: 0.9250 - val_loss: 1.0091 - val_acc: 0.7347\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9231\n",
      "Epoch 00060: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2415 - acc: 0.9231 - val_loss: 1.1165 - val_acc: 0.7174\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9255\n",
      "Epoch 00061: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2380 - acc: 0.9255 - val_loss: 0.9460 - val_acc: 0.7531\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9260\n",
      "Epoch 00062: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2300 - acc: 0.9260 - val_loss: 1.0306 - val_acc: 0.7365\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9271\n",
      "Epoch 00063: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2327 - acc: 0.9271 - val_loss: 0.9578 - val_acc: 0.7517\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9290\n",
      "Epoch 00064: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2252 - acc: 0.9289 - val_loss: 1.0090 - val_acc: 0.7431\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9314\n",
      "Epoch 00065: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2191 - acc: 0.9314 - val_loss: 0.9989 - val_acc: 0.7484\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9321\n",
      "Epoch 00066: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2180 - acc: 0.9321 - val_loss: 0.9615 - val_acc: 0.7540\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9313\n",
      "Epoch 00067: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2143 - acc: 0.9313 - val_loss: 1.0590 - val_acc: 0.7282\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9329\n",
      "Epoch 00068: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2140 - acc: 0.9329 - val_loss: 1.1240 - val_acc: 0.7305\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9333\n",
      "Epoch 00069: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2106 - acc: 0.9334 - val_loss: 1.0033 - val_acc: 0.7386\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9363\n",
      "Epoch 00070: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2023 - acc: 0.9363 - val_loss: 0.9917 - val_acc: 0.7612\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9340\n",
      "Epoch 00071: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2076 - acc: 0.9341 - val_loss: 0.9814 - val_acc: 0.7533\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9377\n",
      "Epoch 00072: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1991 - acc: 0.9377 - val_loss: 1.0677 - val_acc: 0.7365\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9390\n",
      "Epoch 00073: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1944 - acc: 0.9390 - val_loss: 1.2351 - val_acc: 0.7158\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9388\n",
      "Epoch 00074: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1952 - acc: 0.9388 - val_loss: 1.2954 - val_acc: 0.6939\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9409\n",
      "Epoch 00075: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1923 - acc: 0.9409 - val_loss: 1.0262 - val_acc: 0.7456\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9416\n",
      "Epoch 00076: val_loss did not improve from 0.82501\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1903 - acc: 0.9416 - val_loss: 1.2182 - val_acc: 0.7195\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 932us/sample - loss: 0.9156 - acc: 0.7252\n",
      "Loss: 0.9155668600573594 Accuracy: 0.7252337\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2482 - acc: 0.3161\n",
      "Epoch 00001: val_loss improved from inf to 2.16457, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/001-2.1646.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 2.2481 - acc: 0.3162 - val_loss: 2.1646 - val_acc: 0.2884\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5325 - acc: 0.5106\n",
      "Epoch 00002: val_loss improved from 2.16457 to 1.34750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/002-1.3475.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.5324 - acc: 0.5106 - val_loss: 1.3475 - val_acc: 0.5686\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3164 - acc: 0.5830\n",
      "Epoch 00003: val_loss improved from 1.34750 to 1.13260, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/003-1.1326.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.3163 - acc: 0.5830 - val_loss: 1.1326 - val_acc: 0.6506\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1889 - acc: 0.6261\n",
      "Epoch 00004: val_loss did not improve from 1.13260\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.1890 - acc: 0.6261 - val_loss: 1.1524 - val_acc: 0.6438\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1059 - acc: 0.6529\n",
      "Epoch 00005: val_loss improved from 1.13260 to 1.03860, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/005-1.0386.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.1060 - acc: 0.6528 - val_loss: 1.0386 - val_acc: 0.6802\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0297 - acc: 0.6779\n",
      "Epoch 00006: val_loss improved from 1.03860 to 1.02765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/006-1.0276.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.0296 - acc: 0.6779 - val_loss: 1.0276 - val_acc: 0.6709\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9722 - acc: 0.7002\n",
      "Epoch 00007: val_loss did not improve from 1.02765\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9721 - acc: 0.7003 - val_loss: 1.1611 - val_acc: 0.6331\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9245 - acc: 0.7129\n",
      "Epoch 00008: val_loss improved from 1.02765 to 0.92905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/008-0.9290.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9248 - acc: 0.7129 - val_loss: 0.9290 - val_acc: 0.7212\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8857 - acc: 0.7281\n",
      "Epoch 00009: val_loss improved from 0.92905 to 0.88705, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/009-0.8871.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8857 - acc: 0.7281 - val_loss: 0.8871 - val_acc: 0.7312\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8506 - acc: 0.7392\n",
      "Epoch 00010: val_loss improved from 0.88705 to 0.78878, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/010-0.7888.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8508 - acc: 0.7391 - val_loss: 0.7888 - val_acc: 0.7678\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7509\n",
      "Epoch 00011: val_loss did not improve from 0.78878\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8195 - acc: 0.7508 - val_loss: 0.8803 - val_acc: 0.7368\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7873 - acc: 0.7591\n",
      "Epoch 00012: val_loss did not improve from 0.78878\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7872 - acc: 0.7592 - val_loss: 0.8559 - val_acc: 0.7400\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7588 - acc: 0.7696\n",
      "Epoch 00013: val_loss did not improve from 0.78878\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7588 - acc: 0.7696 - val_loss: 0.8606 - val_acc: 0.7421\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7355 - acc: 0.7770\n",
      "Epoch 00014: val_loss improved from 0.78878 to 0.76136, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/014-0.7614.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7355 - acc: 0.7770 - val_loss: 0.7614 - val_acc: 0.7792\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7086 - acc: 0.7880\n",
      "Epoch 00015: val_loss did not improve from 0.76136\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7087 - acc: 0.7880 - val_loss: 0.7956 - val_acc: 0.7694\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.7921\n",
      "Epoch 00016: val_loss did not improve from 0.76136\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6871 - acc: 0.7921 - val_loss: 0.7877 - val_acc: 0.7566\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.7990\n",
      "Epoch 00017: val_loss did not improve from 0.76136\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6657 - acc: 0.7990 - val_loss: 0.9726 - val_acc: 0.6962\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.8021\n",
      "Epoch 00018: val_loss improved from 0.76136 to 0.74389, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/018-0.7439.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6515 - acc: 0.8021 - val_loss: 0.7439 - val_acc: 0.7859\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.8126\n",
      "Epoch 00019: val_loss improved from 0.74389 to 0.70331, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/019-0.7033.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6245 - acc: 0.8126 - val_loss: 0.7033 - val_acc: 0.7934\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.8138\n",
      "Epoch 00020: val_loss did not improve from 0.70331\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6124 - acc: 0.8137 - val_loss: 0.7468 - val_acc: 0.7883\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8183\n",
      "Epoch 00021: val_loss did not improve from 0.70331\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5971 - acc: 0.8183 - val_loss: 0.7768 - val_acc: 0.7775\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.8239\n",
      "Epoch 00022: val_loss did not improve from 0.70331\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5801 - acc: 0.8238 - val_loss: 0.7249 - val_acc: 0.7810\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8271\n",
      "Epoch 00023: val_loss did not improve from 0.70331\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5703 - acc: 0.8271 - val_loss: 0.7188 - val_acc: 0.7894\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.8340\n",
      "Epoch 00024: val_loss improved from 0.70331 to 0.64859, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/024-0.6486.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5554 - acc: 0.8340 - val_loss: 0.6486 - val_acc: 0.8109\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.8346\n",
      "Epoch 00025: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5423 - acc: 0.8346 - val_loss: 0.6829 - val_acc: 0.8060\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8398\n",
      "Epoch 00026: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5243 - acc: 0.8398 - val_loss: 0.6781 - val_acc: 0.8099\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8441\n",
      "Epoch 00027: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5151 - acc: 0.8441 - val_loss: 0.6916 - val_acc: 0.7927\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8502\n",
      "Epoch 00028: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5036 - acc: 0.8502 - val_loss: 0.6885 - val_acc: 0.7952\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.8494\n",
      "Epoch 00029: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4956 - acc: 0.8494 - val_loss: 0.6811 - val_acc: 0.8025\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8554\n",
      "Epoch 00030: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4828 - acc: 0.8554 - val_loss: 0.6714 - val_acc: 0.8027\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8560\n",
      "Epoch 00031: val_loss improved from 0.64859 to 0.64106, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/031-0.6411.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4733 - acc: 0.8559 - val_loss: 0.6411 - val_acc: 0.8150\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4575 - acc: 0.8605\n",
      "Epoch 00032: val_loss did not improve from 0.64106\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4575 - acc: 0.8605 - val_loss: 0.6496 - val_acc: 0.8174\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8647\n",
      "Epoch 00033: val_loss improved from 0.64106 to 0.61764, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/033-0.6176.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4480 - acc: 0.8647 - val_loss: 0.6176 - val_acc: 0.8206\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8650\n",
      "Epoch 00034: val_loss did not improve from 0.61764\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4471 - acc: 0.8650 - val_loss: 0.6536 - val_acc: 0.8164\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8662\n",
      "Epoch 00035: val_loss improved from 0.61764 to 0.61761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/035-0.6176.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4406 - acc: 0.8662 - val_loss: 0.6176 - val_acc: 0.8192\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8717\n",
      "Epoch 00036: val_loss improved from 0.61761 to 0.61442, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/036-0.6144.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4222 - acc: 0.8717 - val_loss: 0.6144 - val_acc: 0.8202\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8751\n",
      "Epoch 00037: val_loss improved from 0.61442 to 0.61061, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/037-0.6106.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4122 - acc: 0.8751 - val_loss: 0.6106 - val_acc: 0.8174\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8746\n",
      "Epoch 00038: val_loss improved from 0.61061 to 0.60910, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/038-0.6091.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4089 - acc: 0.8746 - val_loss: 0.6091 - val_acc: 0.8274\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8801\n",
      "Epoch 00039: val_loss did not improve from 0.60910\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3973 - acc: 0.8801 - val_loss: 0.6220 - val_acc: 0.8255\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8795\n",
      "Epoch 00040: val_loss did not improve from 0.60910\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3930 - acc: 0.8794 - val_loss: 0.6544 - val_acc: 0.8160\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8838\n",
      "Epoch 00041: val_loss did not improve from 0.60910\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3807 - acc: 0.8838 - val_loss: 0.6384 - val_acc: 0.8118\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8851\n",
      "Epoch 00042: val_loss improved from 0.60910 to 0.58940, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/042-0.5894.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3755 - acc: 0.8851 - val_loss: 0.5894 - val_acc: 0.8290\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8871\n",
      "Epoch 00043: val_loss did not improve from 0.58940\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3643 - acc: 0.8871 - val_loss: 0.6126 - val_acc: 0.8223\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8865\n",
      "Epoch 00044: val_loss did not improve from 0.58940\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3651 - acc: 0.8865 - val_loss: 0.6382 - val_acc: 0.8253\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8916\n",
      "Epoch 00045: val_loss improved from 0.58940 to 0.58386, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv_checkpoint/045-0.5839.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3615 - acc: 0.8916 - val_loss: 0.5839 - val_acc: 0.8402\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8937\n",
      "Epoch 00046: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3464 - acc: 0.8937 - val_loss: 0.6701 - val_acc: 0.8155\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8940\n",
      "Epoch 00047: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3404 - acc: 0.8939 - val_loss: 0.6666 - val_acc: 0.8220\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8979\n",
      "Epoch 00048: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3366 - acc: 0.8979 - val_loss: 0.6368 - val_acc: 0.8295\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8996\n",
      "Epoch 00049: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3290 - acc: 0.8996 - val_loss: 0.6491 - val_acc: 0.8171\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.8992\n",
      "Epoch 00050: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3303 - acc: 0.8992 - val_loss: 0.6548 - val_acc: 0.8220\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8995\n",
      "Epoch 00051: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3233 - acc: 0.8994 - val_loss: 0.6606 - val_acc: 0.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9029\n",
      "Epoch 00052: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3141 - acc: 0.9029 - val_loss: 0.5993 - val_acc: 0.8323\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9054\n",
      "Epoch 00053: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3044 - acc: 0.9054 - val_loss: 0.6097 - val_acc: 0.8328\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9065\n",
      "Epoch 00054: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2998 - acc: 0.9066 - val_loss: 0.5926 - val_acc: 0.8386\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9097\n",
      "Epoch 00055: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2948 - acc: 0.9097 - val_loss: 0.7392 - val_acc: 0.7976\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9073\n",
      "Epoch 00056: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2967 - acc: 0.9072 - val_loss: 0.6345 - val_acc: 0.8276\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9115\n",
      "Epoch 00057: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2856 - acc: 0.9115 - val_loss: 0.6746 - val_acc: 0.8134\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9129\n",
      "Epoch 00058: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2821 - acc: 0.9128 - val_loss: 0.6620 - val_acc: 0.8244\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9112\n",
      "Epoch 00059: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2847 - acc: 0.9112 - val_loss: 0.6334 - val_acc: 0.8251\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9158\n",
      "Epoch 00060: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2707 - acc: 0.9158 - val_loss: 0.6479 - val_acc: 0.8281\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9170\n",
      "Epoch 00061: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2689 - acc: 0.9170 - val_loss: 0.6871 - val_acc: 0.8244\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9186\n",
      "Epoch 00062: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2585 - acc: 0.9186 - val_loss: 0.5987 - val_acc: 0.8353\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9193\n",
      "Epoch 00063: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2577 - acc: 0.9193 - val_loss: 0.6525 - val_acc: 0.8230\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9206\n",
      "Epoch 00064: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2554 - acc: 0.9206 - val_loss: 0.6522 - val_acc: 0.8365\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9194\n",
      "Epoch 00065: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2563 - acc: 0.9194 - val_loss: 0.6564 - val_acc: 0.8293\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9213\n",
      "Epoch 00066: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2498 - acc: 0.9213 - val_loss: 0.6391 - val_acc: 0.8321\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9253\n",
      "Epoch 00067: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2421 - acc: 0.9253 - val_loss: 0.7061 - val_acc: 0.8074\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9246\n",
      "Epoch 00068: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2379 - acc: 0.9246 - val_loss: 0.6250 - val_acc: 0.8316\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9286\n",
      "Epoch 00069: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2303 - acc: 0.9286 - val_loss: 0.6641 - val_acc: 0.8164\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9268\n",
      "Epoch 00070: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2338 - acc: 0.9268 - val_loss: 0.7131 - val_acc: 0.8174\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9286\n",
      "Epoch 00071: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2266 - acc: 0.9285 - val_loss: 0.7050 - val_acc: 0.8143\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9280\n",
      "Epoch 00072: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2263 - acc: 0.9279 - val_loss: 0.6640 - val_acc: 0.8269\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9298\n",
      "Epoch 00073: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2221 - acc: 0.9297 - val_loss: 0.6765 - val_acc: 0.8227\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9310\n",
      "Epoch 00074: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2225 - acc: 0.9309 - val_loss: 0.6877 - val_acc: 0.8255\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9310\n",
      "Epoch 00075: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2164 - acc: 0.9310 - val_loss: 0.6265 - val_acc: 0.8374\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9311\n",
      "Epoch 00076: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2155 - acc: 0.9311 - val_loss: 0.6552 - val_acc: 0.8332\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9361\n",
      "Epoch 00077: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2057 - acc: 0.9361 - val_loss: 0.6743 - val_acc: 0.8223\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9349\n",
      "Epoch 00078: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2081 - acc: 0.9348 - val_loss: 0.7284 - val_acc: 0.8178\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9343\n",
      "Epoch 00079: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2075 - acc: 0.9343 - val_loss: 0.6392 - val_acc: 0.8332\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9362\n",
      "Epoch 00080: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2026 - acc: 0.9361 - val_loss: 0.7047 - val_acc: 0.8188\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9373\n",
      "Epoch 00081: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1961 - acc: 0.9372 - val_loss: 0.7422 - val_acc: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9336\n",
      "Epoch 00082: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2095 - acc: 0.9336 - val_loss: 0.8058 - val_acc: 0.7939\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9394\n",
      "Epoch 00083: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1918 - acc: 0.9394 - val_loss: 0.6646 - val_acc: 0.8262\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9379\n",
      "Epoch 00084: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1937 - acc: 0.9379 - val_loss: 0.6926 - val_acc: 0.8341\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9402\n",
      "Epoch 00085: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1859 - acc: 0.9403 - val_loss: 0.7668 - val_acc: 0.8132\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9418\n",
      "Epoch 00086: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1851 - acc: 0.9417 - val_loss: 0.7658 - val_acc: 0.8113\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9415\n",
      "Epoch 00087: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1848 - acc: 0.9415 - val_loss: 0.6723 - val_acc: 0.8302\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9420\n",
      "Epoch 00088: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1828 - acc: 0.9420 - val_loss: 0.7240 - val_acc: 0.8223\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9423\n",
      "Epoch 00089: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1851 - acc: 0.9422 - val_loss: 0.7932 - val_acc: 0.8088\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9409\n",
      "Epoch 00090: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1883 - acc: 0.9409 - val_loss: 0.6947 - val_acc: 0.8230\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9485\n",
      "Epoch 00091: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1657 - acc: 0.9485 - val_loss: 0.6841 - val_acc: 0.8369\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9449\n",
      "Epoch 00092: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1714 - acc: 0.9449 - val_loss: 0.7050 - val_acc: 0.8216\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9464\n",
      "Epoch 00093: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1707 - acc: 0.9464 - val_loss: 0.7599 - val_acc: 0.8206\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9439\n",
      "Epoch 00094: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1747 - acc: 0.9439 - val_loss: 0.7198 - val_acc: 0.8213\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9489\n",
      "Epoch 00095: val_loss did not improve from 0.58386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1635 - acc: 0.9489 - val_loss: 0.8101 - val_acc: 0.8109\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 948us/sample - loss: 0.6809 - acc: 0.8023\n",
      "Loss: 0.680887314104712 Accuracy: 0.80228454\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2827 - acc: 0.2970\n",
      "Epoch 00001: val_loss improved from inf to 1.78018, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/001-1.7802.hdf5\n",
      "36805/36805 [==============================] - 121s 3ms/sample - loss: 2.2826 - acc: 0.2971 - val_loss: 1.7802 - val_acc: 0.4952\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5425 - acc: 0.5106\n",
      "Epoch 00002: val_loss improved from 1.78018 to 1.14806, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/002-1.1481.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.5426 - acc: 0.5106 - val_loss: 1.1481 - val_acc: 0.6543\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2410 - acc: 0.6129\n",
      "Epoch 00003: val_loss improved from 1.14806 to 0.96175, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/003-0.9617.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2409 - acc: 0.6129 - val_loss: 0.9617 - val_acc: 0.7142\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0579 - acc: 0.6746\n",
      "Epoch 00004: val_loss improved from 0.96175 to 0.83794, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/004-0.8379.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0582 - acc: 0.6745 - val_loss: 0.8379 - val_acc: 0.7526\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.7115\n",
      "Epoch 00005: val_loss improved from 0.83794 to 0.75831, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/005-0.7583.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9471 - acc: 0.7115 - val_loss: 0.7583 - val_acc: 0.7773\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8472 - acc: 0.7437\n",
      "Epoch 00006: val_loss improved from 0.75831 to 0.67462, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/006-0.6746.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8473 - acc: 0.7436 - val_loss: 0.6746 - val_acc: 0.8116\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7679 - acc: 0.7699\n",
      "Epoch 00007: val_loss improved from 0.67462 to 0.63650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/007-0.6365.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7681 - acc: 0.7699 - val_loss: 0.6365 - val_acc: 0.8178\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7924\n",
      "Epoch 00008: val_loss improved from 0.63650 to 0.57515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/008-0.5752.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6984 - acc: 0.7924 - val_loss: 0.5752 - val_acc: 0.8383\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.8093\n",
      "Epoch 00009: val_loss improved from 0.57515 to 0.54899, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/009-0.5490.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6377 - acc: 0.8092 - val_loss: 0.5490 - val_acc: 0.8486\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8212\n",
      "Epoch 00010: val_loss improved from 0.54899 to 0.50699, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/010-0.5070.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5964 - acc: 0.8212 - val_loss: 0.5070 - val_acc: 0.8588\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.8373\n",
      "Epoch 00011: val_loss improved from 0.50699 to 0.47250, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/011-0.4725.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5460 - acc: 0.8373 - val_loss: 0.4725 - val_acc: 0.8682\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8484\n",
      "Epoch 00012: val_loss improved from 0.47250 to 0.46048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/012-0.4605.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5116 - acc: 0.8484 - val_loss: 0.4605 - val_acc: 0.8719\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8582\n",
      "Epoch 00013: val_loss improved from 0.46048 to 0.42187, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/013-0.4219.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4786 - acc: 0.8582 - val_loss: 0.4219 - val_acc: 0.8775\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8674\n",
      "Epoch 00014: val_loss improved from 0.42187 to 0.41231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/014-0.4123.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4518 - acc: 0.8674 - val_loss: 0.4123 - val_acc: 0.8826\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8743\n",
      "Epoch 00015: val_loss improved from 0.41231 to 0.41194, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/015-0.4119.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4268 - acc: 0.8743 - val_loss: 0.4119 - val_acc: 0.8810\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8806\n",
      "Epoch 00016: val_loss improved from 0.41194 to 0.38545, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/016-0.3854.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3992 - acc: 0.8806 - val_loss: 0.3854 - val_acc: 0.8889\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8851\n",
      "Epoch 00017: val_loss improved from 0.38545 to 0.37043, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/017-0.3704.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3881 - acc: 0.8851 - val_loss: 0.3704 - val_acc: 0.8926\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8910\n",
      "Epoch 00018: val_loss improved from 0.37043 to 0.33229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/018-0.3323.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3633 - acc: 0.8910 - val_loss: 0.3323 - val_acc: 0.9068\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8955\n",
      "Epoch 00019: val_loss did not improve from 0.33229\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3472 - acc: 0.8955 - val_loss: 0.3474 - val_acc: 0.9031\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8990\n",
      "Epoch 00020: val_loss did not improve from 0.33229\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3349 - acc: 0.8990 - val_loss: 0.3360 - val_acc: 0.8973\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9039\n",
      "Epoch 00021: val_loss did not improve from 0.33229\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3221 - acc: 0.9038 - val_loss: 0.3529 - val_acc: 0.8947\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9051\n",
      "Epoch 00022: val_loss did not improve from 0.33229\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3114 - acc: 0.9051 - val_loss: 0.3655 - val_acc: 0.9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9087\n",
      "Epoch 00023: val_loss did not improve from 0.33229\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2994 - acc: 0.9087 - val_loss: 0.3326 - val_acc: 0.9029\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9149\n",
      "Epoch 00024: val_loss improved from 0.33229 to 0.29551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/024-0.2955.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2834 - acc: 0.9149 - val_loss: 0.2955 - val_acc: 0.9140\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9153\n",
      "Epoch 00025: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2821 - acc: 0.9153 - val_loss: 0.3478 - val_acc: 0.9038\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9189\n",
      "Epoch 00026: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2672 - acc: 0.9189 - val_loss: 0.3192 - val_acc: 0.9115\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9207\n",
      "Epoch 00027: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2615 - acc: 0.9207 - val_loss: 0.3272 - val_acc: 0.9071\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9227\n",
      "Epoch 00028: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2520 - acc: 0.9227 - val_loss: 0.3092 - val_acc: 0.9129\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9241\n",
      "Epoch 00029: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2480 - acc: 0.9241 - val_loss: 0.3248 - val_acc: 0.9075\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9282\n",
      "Epoch 00030: val_loss did not improve from 0.29551\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2337 - acc: 0.9282 - val_loss: 0.3438 - val_acc: 0.9073\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9297\n",
      "Epoch 00031: val_loss improved from 0.29551 to 0.28983, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/031-0.2898.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2276 - acc: 0.9297 - val_loss: 0.2898 - val_acc: 0.9152\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9326\n",
      "Epoch 00032: val_loss did not improve from 0.28983\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2209 - acc: 0.9326 - val_loss: 0.3451 - val_acc: 0.8994\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9330\n",
      "Epoch 00033: val_loss improved from 0.28983 to 0.28054, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/033-0.2805.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2145 - acc: 0.9330 - val_loss: 0.2805 - val_acc: 0.9173\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9374\n",
      "Epoch 00034: val_loss did not improve from 0.28054\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2036 - acc: 0.9373 - val_loss: 0.3238 - val_acc: 0.9140\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9348\n",
      "Epoch 00035: val_loss improved from 0.28054 to 0.27241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv_checkpoint/035-0.2724.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2089 - acc: 0.9348 - val_loss: 0.2724 - val_acc: 0.9189\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9408\n",
      "Epoch 00036: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1951 - acc: 0.9408 - val_loss: 0.2792 - val_acc: 0.9210\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9400\n",
      "Epoch 00037: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1943 - acc: 0.9400 - val_loss: 0.2978 - val_acc: 0.9203\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9436\n",
      "Epoch 00038: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1806 - acc: 0.9436 - val_loss: 0.3532 - val_acc: 0.9047\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9411\n",
      "Epoch 00039: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1892 - acc: 0.9410 - val_loss: 0.3328 - val_acc: 0.9085\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9417\n",
      "Epoch 00040: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1892 - acc: 0.9417 - val_loss: 0.2730 - val_acc: 0.9231\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9474\n",
      "Epoch 00041: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1691 - acc: 0.9474 - val_loss: 0.2909 - val_acc: 0.9199\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9504\n",
      "Epoch 00042: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1626 - acc: 0.9504 - val_loss: 0.3084 - val_acc: 0.9161\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9508\n",
      "Epoch 00043: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1584 - acc: 0.9508 - val_loss: 0.2843 - val_acc: 0.9201\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9516\n",
      "Epoch 00044: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1554 - acc: 0.9516 - val_loss: 0.3446 - val_acc: 0.9101\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9506\n",
      "Epoch 00045: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1579 - acc: 0.9506 - val_loss: 0.3449 - val_acc: 0.9085\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9524\n",
      "Epoch 00046: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1484 - acc: 0.9524 - val_loss: 0.3186 - val_acc: 0.9173\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9563\n",
      "Epoch 00047: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1421 - acc: 0.9563 - val_loss: 0.3032 - val_acc: 0.9171\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9563\n",
      "Epoch 00048: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1396 - acc: 0.9563 - val_loss: 0.3126 - val_acc: 0.9166\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9576\n",
      "Epoch 00049: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1364 - acc: 0.9576 - val_loss: 0.3291 - val_acc: 0.9080\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9564\n",
      "Epoch 00050: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1361 - acc: 0.9564 - val_loss: 0.3101 - val_acc: 0.9168\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9563\n",
      "Epoch 00051: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1373 - acc: 0.9563 - val_loss: 0.3081 - val_acc: 0.9159\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9590\n",
      "Epoch 00052: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1288 - acc: 0.9590 - val_loss: 0.3186 - val_acc: 0.9126\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9630\n",
      "Epoch 00053: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1205 - acc: 0.9630 - val_loss: 0.3423 - val_acc: 0.9064\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9604\n",
      "Epoch 00054: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1240 - acc: 0.9604 - val_loss: 0.3375 - val_acc: 0.9075\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9640\n",
      "Epoch 00055: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1147 - acc: 0.9640 - val_loss: 0.3079 - val_acc: 0.9192\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9644\n",
      "Epoch 00056: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1157 - acc: 0.9644 - val_loss: 0.3255 - val_acc: 0.9171\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9647\n",
      "Epoch 00057: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1140 - acc: 0.9647 - val_loss: 0.3114 - val_acc: 0.9192\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9649\n",
      "Epoch 00058: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1102 - acc: 0.9649 - val_loss: 0.2944 - val_acc: 0.9206\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9660\n",
      "Epoch 00059: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1078 - acc: 0.9660 - val_loss: 0.3108 - val_acc: 0.9175\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9679\n",
      "Epoch 00060: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1045 - acc: 0.9679 - val_loss: 0.3116 - val_acc: 0.9180\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9663\n",
      "Epoch 00061: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1065 - acc: 0.9662 - val_loss: 0.3744 - val_acc: 0.9094\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9652\n",
      "Epoch 00062: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1094 - acc: 0.9652 - val_loss: 0.2972 - val_acc: 0.9243\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9696\n",
      "Epoch 00063: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0972 - acc: 0.9696 - val_loss: 0.3311 - val_acc: 0.9178\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9698\n",
      "Epoch 00064: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0984 - acc: 0.9697 - val_loss: 0.3223 - val_acc: 0.9180\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9692\n",
      "Epoch 00065: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0974 - acc: 0.9692 - val_loss: 0.3410 - val_acc: 0.9101\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9723\n",
      "Epoch 00066: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0890 - acc: 0.9723 - val_loss: 0.3405 - val_acc: 0.9185\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9707\n",
      "Epoch 00067: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0952 - acc: 0.9707 - val_loss: 0.3433 - val_acc: 0.9131\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9699\n",
      "Epoch 00068: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0947 - acc: 0.9699 - val_loss: 0.3469 - val_acc: 0.9087\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9740\n",
      "Epoch 00069: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0846 - acc: 0.9740 - val_loss: 0.3741 - val_acc: 0.9106\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9706\n",
      "Epoch 00070: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0923 - acc: 0.9706 - val_loss: 0.3485 - val_acc: 0.9180\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9735\n",
      "Epoch 00071: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0826 - acc: 0.9735 - val_loss: 0.3311 - val_acc: 0.9180\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9726\n",
      "Epoch 00072: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0879 - acc: 0.9726 - val_loss: 0.3438 - val_acc: 0.9168\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9710\n",
      "Epoch 00073: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0884 - acc: 0.9710 - val_loss: 0.3220 - val_acc: 0.9213\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9752\n",
      "Epoch 00074: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0769 - acc: 0.9752 - val_loss: 0.3754 - val_acc: 0.9117\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9742\n",
      "Epoch 00075: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0843 - acc: 0.9741 - val_loss: 0.3556 - val_acc: 0.9138\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9763\n",
      "Epoch 00076: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0767 - acc: 0.9762 - val_loss: 0.3737 - val_acc: 0.9099\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9724\n",
      "Epoch 00077: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0901 - acc: 0.9724 - val_loss: 0.3518 - val_acc: 0.9140\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9800\n",
      "Epoch 00078: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0665 - acc: 0.9800 - val_loss: 0.3226 - val_acc: 0.9208\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9780\n",
      "Epoch 00079: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0686 - acc: 0.9780 - val_loss: 0.3628 - val_acc: 0.9182\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9785\n",
      "Epoch 00080: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0691 - acc: 0.9785 - val_loss: 0.3739 - val_acc: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9764\n",
      "Epoch 00081: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0704 - acc: 0.9763 - val_loss: 0.3901 - val_acc: 0.9103\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9726\n",
      "Epoch 00082: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0842 - acc: 0.9726 - val_loss: 0.3327 - val_acc: 0.9178\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9749\n",
      "Epoch 00083: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0788 - acc: 0.9749 - val_loss: 0.3504 - val_acc: 0.9108\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9802\n",
      "Epoch 00084: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0638 - acc: 0.9802 - val_loss: 0.3769 - val_acc: 0.9143\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9807\n",
      "Epoch 00085: val_loss did not improve from 0.27241\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0620 - acc: 0.9807 - val_loss: 0.3768 - val_acc: 0.9103\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3279 - acc: 0.9059\n",
      "Loss: 0.3278949049650571 Accuracy: 0.905919\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8089 - acc: 0.4463\n",
      "Epoch 00001: val_loss improved from inf to 1.48522, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/001-1.4852.hdf5\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 1.8087 - acc: 0.4464 - val_loss: 1.4852 - val_acc: 0.5211\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0239 - acc: 0.6821\n",
      "Epoch 00002: val_loss improved from 1.48522 to 0.66852, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/002-0.6685.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0241 - acc: 0.6820 - val_loss: 0.6685 - val_acc: 0.8025\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7751\n",
      "Epoch 00003: val_loss improved from 0.66852 to 0.51569, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/003-0.5157.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7399 - acc: 0.7751 - val_loss: 0.5157 - val_acc: 0.8630\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8237\n",
      "Epoch 00004: val_loss improved from 0.51569 to 0.42007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/004-0.4201.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5791 - acc: 0.8237 - val_loss: 0.4201 - val_acc: 0.8779\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8564\n",
      "Epoch 00005: val_loss improved from 0.42007 to 0.37193, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/005-0.3719.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4766 - acc: 0.8564 - val_loss: 0.3719 - val_acc: 0.8989\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8758\n",
      "Epoch 00006: val_loss improved from 0.37193 to 0.34743, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/006-0.3474.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4129 - acc: 0.8758 - val_loss: 0.3474 - val_acc: 0.9012\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8894\n",
      "Epoch 00007: val_loss improved from 0.34743 to 0.27837, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/007-0.2784.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3648 - acc: 0.8894 - val_loss: 0.2784 - val_acc: 0.9215\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.9021\n",
      "Epoch 00008: val_loss improved from 0.27837 to 0.26938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/008-0.2694.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3281 - acc: 0.9020 - val_loss: 0.2694 - val_acc: 0.9201\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9097\n",
      "Epoch 00009: val_loss improved from 0.26938 to 0.25303, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/009-0.2530.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2986 - acc: 0.9097 - val_loss: 0.2530 - val_acc: 0.9255\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9183\n",
      "Epoch 00010: val_loss improved from 0.25303 to 0.23566, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/010-0.2357.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2751 - acc: 0.9183 - val_loss: 0.2357 - val_acc: 0.9322\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9228\n",
      "Epoch 00011: val_loss did not improve from 0.23566\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2528 - acc: 0.9228 - val_loss: 0.2373 - val_acc: 0.9280\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9300\n",
      "Epoch 00012: val_loss improved from 0.23566 to 0.20833, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/012-0.2083.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2334 - acc: 0.9300 - val_loss: 0.2083 - val_acc: 0.9366\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9319\n",
      "Epoch 00013: val_loss did not improve from 0.20833\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2244 - acc: 0.9319 - val_loss: 0.2331 - val_acc: 0.9308\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9359\n",
      "Epoch 00014: val_loss did not improve from 0.20833\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2084 - acc: 0.9359 - val_loss: 0.2158 - val_acc: 0.9369\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9424\n",
      "Epoch 00015: val_loss improved from 0.20833 to 0.18968, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/015-0.1897.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1923 - acc: 0.9423 - val_loss: 0.1897 - val_acc: 0.9450\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9413\n",
      "Epoch 00016: val_loss did not improve from 0.18968\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1918 - acc: 0.9413 - val_loss: 0.2135 - val_acc: 0.9404\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9471\n",
      "Epoch 00017: val_loss did not improve from 0.18968\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1723 - acc: 0.9470 - val_loss: 0.2053 - val_acc: 0.9406\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9490\n",
      "Epoch 00018: val_loss improved from 0.18968 to 0.18563, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/018-0.1856.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1661 - acc: 0.9490 - val_loss: 0.1856 - val_acc: 0.9420\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9515\n",
      "Epoch 00019: val_loss did not improve from 0.18563\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1574 - acc: 0.9515 - val_loss: 0.2035 - val_acc: 0.9378\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9548\n",
      "Epoch 00020: val_loss improved from 0.18563 to 0.18135, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/020-0.1814.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1473 - acc: 0.9548 - val_loss: 0.1814 - val_acc: 0.9495\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9578\n",
      "Epoch 00021: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1382 - acc: 0.9578 - val_loss: 0.1941 - val_acc: 0.9408\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9588\n",
      "Epoch 00022: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1323 - acc: 0.9588 - val_loss: 0.1875 - val_acc: 0.9460\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9625\n",
      "Epoch 00023: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1265 - acc: 0.9625 - val_loss: 0.2044 - val_acc: 0.9425\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9619\n",
      "Epoch 00024: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1231 - acc: 0.9619 - val_loss: 0.1988 - val_acc: 0.9399\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9651\n",
      "Epoch 00025: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1141 - acc: 0.9651 - val_loss: 0.2035 - val_acc: 0.9383\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9664\n",
      "Epoch 00026: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1078 - acc: 0.9663 - val_loss: 0.1877 - val_acc: 0.9418\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9636\n",
      "Epoch 00027: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1153 - acc: 0.9636 - val_loss: 0.2211 - val_acc: 0.9348\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9704\n",
      "Epoch 00028: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0956 - acc: 0.9703 - val_loss: 0.1814 - val_acc: 0.9457\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9694\n",
      "Epoch 00029: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0972 - acc: 0.9694 - val_loss: 0.1941 - val_acc: 0.9439\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9675\n",
      "Epoch 00030: val_loss improved from 0.18135 to 0.17454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/030-0.1745.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1015 - acc: 0.9675 - val_loss: 0.1745 - val_acc: 0.9511\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9730\n",
      "Epoch 00031: val_loss did not improve from 0.17454\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0868 - acc: 0.9730 - val_loss: 0.1911 - val_acc: 0.9446\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9739\n",
      "Epoch 00032: val_loss did not improve from 0.17454\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0837 - acc: 0.9739 - val_loss: 0.1807 - val_acc: 0.9460\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9763\n",
      "Epoch 00033: val_loss did not improve from 0.17454\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0783 - acc: 0.9763 - val_loss: 0.2226 - val_acc: 0.9366\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9735\n",
      "Epoch 00034: val_loss improved from 0.17454 to 0.17132, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv_checkpoint/034-0.1713.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0867 - acc: 0.9735 - val_loss: 0.1713 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9790\n",
      "Epoch 00035: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0703 - acc: 0.9791 - val_loss: 0.1749 - val_acc: 0.9471\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9795\n",
      "Epoch 00036: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0681 - acc: 0.9795 - val_loss: 0.1827 - val_acc: 0.9478\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9802\n",
      "Epoch 00037: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0674 - acc: 0.9802 - val_loss: 0.1976 - val_acc: 0.9455\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9797\n",
      "Epoch 00038: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0662 - acc: 0.9797 - val_loss: 0.1883 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9801\n",
      "Epoch 00039: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0660 - acc: 0.9801 - val_loss: 0.2322 - val_acc: 0.9369\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9825\n",
      "Epoch 00040: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0584 - acc: 0.9825 - val_loss: 0.2099 - val_acc: 0.9441\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9840\n",
      "Epoch 00041: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0542 - acc: 0.9840 - val_loss: 0.2049 - val_acc: 0.9448\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9822\n",
      "Epoch 00042: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0576 - acc: 0.9822 - val_loss: 0.2201 - val_acc: 0.9429\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9837\n",
      "Epoch 00043: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0541 - acc: 0.9837 - val_loss: 0.2399 - val_acc: 0.9364\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9841\n",
      "Epoch 00044: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0524 - acc: 0.9841 - val_loss: 0.2286 - val_acc: 0.9383\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9837\n",
      "Epoch 00045: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0531 - acc: 0.9838 - val_loss: 0.2457 - val_acc: 0.9336\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9837\n",
      "Epoch 00046: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0516 - acc: 0.9838 - val_loss: 0.2301 - val_acc: 0.9425\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9858\n",
      "Epoch 00047: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0462 - acc: 0.9858 - val_loss: 0.2195 - val_acc: 0.9432\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9871\n",
      "Epoch 00048: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0429 - acc: 0.9871 - val_loss: 0.2153 - val_acc: 0.9411\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9860\n",
      "Epoch 00049: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0467 - acc: 0.9860 - val_loss: 0.2237 - val_acc: 0.9446\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9881\n",
      "Epoch 00050: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0410 - acc: 0.9881 - val_loss: 0.2217 - val_acc: 0.9383\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9883\n",
      "Epoch 00051: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0401 - acc: 0.9883 - val_loss: 0.2709 - val_acc: 0.9362\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9882\n",
      "Epoch 00052: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0390 - acc: 0.9882 - val_loss: 0.2154 - val_acc: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9896\n",
      "Epoch 00053: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0363 - acc: 0.9896 - val_loss: 0.2130 - val_acc: 0.9436\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9889\n",
      "Epoch 00054: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0373 - acc: 0.9889 - val_loss: 0.2262 - val_acc: 0.9425\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9872\n",
      "Epoch 00055: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0414 - acc: 0.9872 - val_loss: 0.2308 - val_acc: 0.9436\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9886\n",
      "Epoch 00056: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 0.2169 - val_acc: 0.9446\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9911\n",
      "Epoch 00057: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0319 - acc: 0.9911 - val_loss: 0.2488 - val_acc: 0.9397\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9896\n",
      "Epoch 00058: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0337 - acc: 0.9896 - val_loss: 0.2419 - val_acc: 0.9441\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9890\n",
      "Epoch 00059: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0357 - acc: 0.9891 - val_loss: 0.2407 - val_acc: 0.9397\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9907\n",
      "Epoch 00060: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0308 - acc: 0.9907 - val_loss: 0.2507 - val_acc: 0.9378\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9892\n",
      "Epoch 00061: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.2365 - val_acc: 0.9422\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9897\n",
      "Epoch 00062: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0341 - acc: 0.9897 - val_loss: 0.2452 - val_acc: 0.9415\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9892\n",
      "Epoch 00063: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0358 - acc: 0.9892 - val_loss: 0.2471 - val_acc: 0.9448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9891\n",
      "Epoch 00064: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0345 - acc: 0.9891 - val_loss: 0.2442 - val_acc: 0.9408\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9924\n",
      "Epoch 00065: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0269 - acc: 0.9924 - val_loss: 0.2364 - val_acc: 0.9446\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9917\n",
      "Epoch 00066: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0276 - acc: 0.9917 - val_loss: 0.2331 - val_acc: 0.9427\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9933\n",
      "Epoch 00067: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0240 - acc: 0.9933 - val_loss: 0.2306 - val_acc: 0.9434\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9880\n",
      "Epoch 00068: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0381 - acc: 0.9880 - val_loss: 0.2403 - val_acc: 0.9401\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9932\n",
      "Epoch 00069: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.2294 - val_acc: 0.9464\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9919\n",
      "Epoch 00070: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0270 - acc: 0.9919 - val_loss: 0.2718 - val_acc: 0.9408\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9929\n",
      "Epoch 00071: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0250 - acc: 0.9929 - val_loss: 0.2293 - val_acc: 0.9469\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 00072: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0252 - acc: 0.9923 - val_loss: 0.2736 - val_acc: 0.9362\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9911\n",
      "Epoch 00073: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0293 - acc: 0.9911 - val_loss: 0.2771 - val_acc: 0.9394\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9901\n",
      "Epoch 00074: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0320 - acc: 0.9901 - val_loss: 0.2510 - val_acc: 0.9448\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9941\n",
      "Epoch 00075: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0217 - acc: 0.9941 - val_loss: 0.2671 - val_acc: 0.9406\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9896\n",
      "Epoch 00076: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0319 - acc: 0.9896 - val_loss: 0.2579 - val_acc: 0.9415\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9944\n",
      "Epoch 00077: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0211 - acc: 0.9944 - val_loss: 0.2647 - val_acc: 0.9432\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9943\n",
      "Epoch 00078: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 0.2636 - val_acc: 0.9422\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 00079: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0242 - acc: 0.9929 - val_loss: 0.2350 - val_acc: 0.9471\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9922\n",
      "Epoch 00080: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0257 - acc: 0.9922 - val_loss: 0.2641 - val_acc: 0.9425\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9940\n",
      "Epoch 00081: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0203 - acc: 0.9940 - val_loss: 0.2636 - val_acc: 0.9443\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9942\n",
      "Epoch 00082: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0205 - acc: 0.9942 - val_loss: 0.2779 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9934\n",
      "Epoch 00083: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0218 - acc: 0.9934 - val_loss: 0.2661 - val_acc: 0.9422\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9944\n",
      "Epoch 00084: val_loss did not improve from 0.17132\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0198 - acc: 0.9944 - val_loss: 0.2360 - val_acc: 0.9471\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2432 - acc: 0.9356\n",
      "Loss: 0.243172841066512 Accuracy: 0.93561786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 922us/sample - loss: 1.3146 - acc: 0.6035\n",
      "Loss: 1.3145651748494815 Accuracy: 0.60353065\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 820us/sample - loss: 1.0661 - acc: 0.6673\n",
      "Loss: 1.0661346805058536 Accuracy: 0.66728973\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 847us/sample - loss: 0.9156 - acc: 0.7252\n",
      "Loss: 0.9155668600573594 Accuracy: 0.7252337\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 897us/sample - loss: 0.6809 - acc: 0.8023\n",
      "Loss: 0.680887314104712 Accuracy: 0.80228454\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 956us/sample - loss: 0.3279 - acc: 0.9059\n",
      "Loss: 0.3278949049650571 Accuracy: 0.905919\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 939us/sample - loss: 0.2432 - acc: 0.9356\n",
      "Loss: 0.243172841066512 Accuracy: 0.93561786\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 924us/sample - loss: 1.9682 - acc: 0.6093\n",
      "Loss: 1.9682023558661201 Accuracy: 0.6093458\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 968us/sample - loss: 1.3715 - acc: 0.6872\n",
      "Loss: 1.3715272848975002 Accuracy: 0.6872274\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 982us/sample - loss: 1.3471 - acc: 0.6687\n",
      "Loss: 1.3471461690970052 Accuracy: 0.6687435\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8915 - acc: 0.7871\n",
      "Loss: 0.8914513988782065 Accuracy: 0.78712356\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4469 - acc: 0.8926\n",
      "Loss: 0.44691954959466323 Accuracy: 0.8926272\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2922 - acc: 0.9342\n",
      "Loss: 0.2922333494115037 Accuracy: 0.93416405\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
