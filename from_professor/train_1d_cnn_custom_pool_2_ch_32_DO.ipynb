{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,192,208\n",
      "Trainable params: 8,192,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,101,360\n",
      "Trainable params: 4,101,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,058,512\n",
      "Trainable params: 2,058,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,039,664\n",
      "Trainable params: 1,039,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,049,968\n",
      "Trainable params: 1,049,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 558,512\n",
      "Trainable params: 558,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 323,056\n",
      "Trainable params: 323,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 215,600\n",
      "Trainable params: 215,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 255,664\n",
      "Trainable params: 255,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 274,224\n",
      "Trainable params: 274,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 323,504\n",
      "Trainable params: 323,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 389,168\n",
      "Trainable params: 389,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_78 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_90 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 551,216\n",
      "Trainable params: 551,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3125 - acc: 0.2724\n",
      "Epoch 00001: val_loss improved from inf to 1.96813, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_3_conv_checkpoint/001-1.9681.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 2.3121 - acc: 0.2726 - val_loss: 1.9681 - val_acc: 0.4074\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7869 - acc: 0.4601\n",
      "Epoch 00002: val_loss improved from 1.96813 to 1.75116, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_3_conv_checkpoint/002-1.7512.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.7871 - acc: 0.4601 - val_loss: 1.7512 - val_acc: 0.4621\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5564 - acc: 0.5336\n",
      "Epoch 00003: val_loss improved from 1.75116 to 1.65352, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_3_conv_checkpoint/003-1.6535.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.5564 - acc: 0.5336 - val_loss: 1.6535 - val_acc: 0.4962\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3864 - acc: 0.5851\n",
      "Epoch 00004: val_loss did not improve from 1.65352\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.3865 - acc: 0.5850 - val_loss: 1.6639 - val_acc: 0.4771\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2432 - acc: 0.6271\n",
      "Epoch 00005: val_loss improved from 1.65352 to 1.63493, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_3_conv_checkpoint/005-1.6349.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 1.2432 - acc: 0.6270 - val_loss: 1.6349 - val_acc: 0.4964\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1129 - acc: 0.6653\n",
      "Epoch 00006: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 1.1130 - acc: 0.6653 - val_loss: 1.6768 - val_acc: 0.4864\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0037 - acc: 0.6976\n",
      "Epoch 00007: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 1.0036 - acc: 0.6976 - val_loss: 1.6581 - val_acc: 0.4959\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.7264\n",
      "Epoch 00008: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.9059 - acc: 0.7265 - val_loss: 1.7060 - val_acc: 0.4847\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8124 - acc: 0.7569\n",
      "Epoch 00009: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.8121 - acc: 0.7570 - val_loss: 1.7658 - val_acc: 0.4801\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7719\n",
      "Epoch 00010: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.7436 - acc: 0.7719 - val_loss: 1.7971 - val_acc: 0.4908\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6682 - acc: 0.8012\n",
      "Epoch 00011: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.6681 - acc: 0.8012 - val_loss: 1.8415 - val_acc: 0.4927\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.8155\n",
      "Epoch 00012: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.6085 - acc: 0.8156 - val_loss: 1.8516 - val_acc: 0.4948\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.8371\n",
      "Epoch 00013: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5455 - acc: 0.8371 - val_loss: 1.9199 - val_acc: 0.4915\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8482\n",
      "Epoch 00014: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4979 - acc: 0.8482 - val_loss: 1.9966 - val_acc: 0.4915\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8622\n",
      "Epoch 00015: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4578 - acc: 0.8622 - val_loss: 2.0609 - val_acc: 0.4875\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8761\n",
      "Epoch 00016: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4114 - acc: 0.8761 - val_loss: 2.1409 - val_acc: 0.4906\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8851\n",
      "Epoch 00017: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3836 - acc: 0.8851 - val_loss: 2.1699 - val_acc: 0.4843\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8954\n",
      "Epoch 00018: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3462 - acc: 0.8954 - val_loss: 2.2203 - val_acc: 0.4959\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9039\n",
      "Epoch 00019: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3178 - acc: 0.9039 - val_loss: 2.2888 - val_acc: 0.4906\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9079\n",
      "Epoch 00020: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3032 - acc: 0.9079 - val_loss: 2.3065 - val_acc: 0.4999\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9183\n",
      "Epoch 00021: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2777 - acc: 0.9182 - val_loss: 2.3922 - val_acc: 0.5034\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9237\n",
      "Epoch 00022: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2590 - acc: 0.9237 - val_loss: 2.4265 - val_acc: 0.4983\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9292\n",
      "Epoch 00023: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2414 - acc: 0.9291 - val_loss: 2.4742 - val_acc: 0.4990\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9342\n",
      "Epoch 00024: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2230 - acc: 0.9342 - val_loss: 2.5795 - val_acc: 0.4913\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9391\n",
      "Epoch 00025: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2107 - acc: 0.9391 - val_loss: 2.5800 - val_acc: 0.4896\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9408\n",
      "Epoch 00026: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2004 - acc: 0.9408 - val_loss: 2.5858 - val_acc: 0.4983\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9450\n",
      "Epoch 00027: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1863 - acc: 0.9449 - val_loss: 2.6615 - val_acc: 0.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9461\n",
      "Epoch 00028: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1828 - acc: 0.9461 - val_loss: 2.6942 - val_acc: 0.4878\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9494\n",
      "Epoch 00029: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1700 - acc: 0.9494 - val_loss: 2.7029 - val_acc: 0.5059\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9519\n",
      "Epoch 00030: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1667 - acc: 0.9519 - val_loss: 2.7966 - val_acc: 0.4922\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9558\n",
      "Epoch 00031: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1579 - acc: 0.9558 - val_loss: 2.8000 - val_acc: 0.5010\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9568\n",
      "Epoch 00032: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1498 - acc: 0.9568 - val_loss: 2.8282 - val_acc: 0.5038\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9593\n",
      "Epoch 00033: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1452 - acc: 0.9593 - val_loss: 2.8313 - val_acc: 0.5083\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9613\n",
      "Epoch 00034: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1408 - acc: 0.9613 - val_loss: 2.8975 - val_acc: 0.5010\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9625\n",
      "Epoch 00035: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1348 - acc: 0.9625 - val_loss: 2.9202 - val_acc: 0.5059\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9645\n",
      "Epoch 00036: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1252 - acc: 0.9645 - val_loss: 2.9162 - val_acc: 0.5097\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9651\n",
      "Epoch 00037: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1256 - acc: 0.9651 - val_loss: 2.9821 - val_acc: 0.5048\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9647\n",
      "Epoch 00038: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1236 - acc: 0.9647 - val_loss: 2.9757 - val_acc: 0.4983\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9668\n",
      "Epoch 00039: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1180 - acc: 0.9668 - val_loss: 2.9705 - val_acc: 0.5104\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9680\n",
      "Epoch 00040: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1151 - acc: 0.9680 - val_loss: 3.0170 - val_acc: 0.5078\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9704\n",
      "Epoch 00041: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1089 - acc: 0.9704 - val_loss: 3.0241 - val_acc: 0.5041\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9710\n",
      "Epoch 00042: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1040 - acc: 0.9710 - val_loss: 3.0442 - val_acc: 0.5104\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9715\n",
      "Epoch 00043: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1042 - acc: 0.9715 - val_loss: 3.0153 - val_acc: 0.5076\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9718\n",
      "Epoch 00044: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1027 - acc: 0.9718 - val_loss: 3.0570 - val_acc: 0.5129\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9732\n",
      "Epoch 00045: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0989 - acc: 0.9732 - val_loss: 3.0607 - val_acc: 0.5150\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9740\n",
      "Epoch 00046: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0970 - acc: 0.9740 - val_loss: 3.1508 - val_acc: 0.5092\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9754\n",
      "Epoch 00047: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.0919 - acc: 0.9754 - val_loss: 3.0973 - val_acc: 0.5155\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9755\n",
      "Epoch 00048: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0927 - acc: 0.9755 - val_loss: 3.1802 - val_acc: 0.5127\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9745\n",
      "Epoch 00049: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0946 - acc: 0.9745 - val_loss: 3.1752 - val_acc: 0.5090\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9756\n",
      "Epoch 00050: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0934 - acc: 0.9756 - val_loss: 3.2146 - val_acc: 0.5052\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9780\n",
      "Epoch 00051: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0853 - acc: 0.9780 - val_loss: 3.2430 - val_acc: 0.5094\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9771\n",
      "Epoch 00052: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0856 - acc: 0.9771 - val_loss: 3.2183 - val_acc: 0.5164\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9780\n",
      "Epoch 00053: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0854 - acc: 0.9780 - val_loss: 3.1707 - val_acc: 0.5218\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9795\n",
      "Epoch 00054: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0818 - acc: 0.9795 - val_loss: 3.3065 - val_acc: 0.5167\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9794\n",
      "Epoch 00055: val_loss did not improve from 1.63493\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0809 - acc: 0.9794 - val_loss: 3.2053 - val_acc: 0.5215\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 359us/sample - loss: 1.7100 - acc: 0.4746\n",
      "Loss: 1.7099558357385336 Accuracy: 0.47455868\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.2720\n",
      "Epoch 00001: val_loss improved from inf to 1.91819, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/001-1.9182.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 2.3104 - acc: 0.2720 - val_loss: 1.9182 - val_acc: 0.4249\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7257 - acc: 0.4714\n",
      "Epoch 00002: val_loss improved from 1.91819 to 1.56205, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/002-1.5621.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.7257 - acc: 0.4714 - val_loss: 1.5621 - val_acc: 0.5236\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4334 - acc: 0.5618\n",
      "Epoch 00003: val_loss improved from 1.56205 to 1.44878, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/003-1.4488.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.4337 - acc: 0.5618 - val_loss: 1.4488 - val_acc: 0.5518\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2664 - acc: 0.6124\n",
      "Epoch 00004: val_loss improved from 1.44878 to 1.38805, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/004-1.3880.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.2664 - acc: 0.6124 - val_loss: 1.3880 - val_acc: 0.5691\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1434 - acc: 0.6543\n",
      "Epoch 00005: val_loss improved from 1.38805 to 1.34701, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/005-1.3470.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1434 - acc: 0.6543 - val_loss: 1.3470 - val_acc: 0.5749\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0513 - acc: 0.6822\n",
      "Epoch 00006: val_loss improved from 1.34701 to 1.33626, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_4_conv_checkpoint/006-1.3363.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0513 - acc: 0.6822 - val_loss: 1.3363 - val_acc: 0.5830\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.7013\n",
      "Epoch 00007: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9756 - acc: 0.7013 - val_loss: 1.3471 - val_acc: 0.5844\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9018 - acc: 0.7229\n",
      "Epoch 00008: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9014 - acc: 0.7230 - val_loss: 1.3422 - val_acc: 0.5893\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8356 - acc: 0.7433\n",
      "Epoch 00009: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8356 - acc: 0.7433 - val_loss: 1.4132 - val_acc: 0.5784\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7585\n",
      "Epoch 00010: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7785 - acc: 0.7583 - val_loss: 1.3845 - val_acc: 0.5900\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7248 - acc: 0.7774\n",
      "Epoch 00011: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.7248 - acc: 0.7774 - val_loss: 1.3904 - val_acc: 0.5945\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6841 - acc: 0.7880\n",
      "Epoch 00012: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.6842 - acc: 0.7879 - val_loss: 1.4016 - val_acc: 0.5956\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6365 - acc: 0.7990\n",
      "Epoch 00013: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.6367 - acc: 0.7990 - val_loss: 1.4415 - val_acc: 0.5882\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.8163\n",
      "Epoch 00014: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.5895 - acc: 0.8162 - val_loss: 1.4903 - val_acc: 0.5907\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8286\n",
      "Epoch 00015: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5512 - acc: 0.8286 - val_loss: 1.4935 - val_acc: 0.5980\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8350\n",
      "Epoch 00016: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.5223 - acc: 0.8350 - val_loss: 1.5082 - val_acc: 0.5996\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8470\n",
      "Epoch 00017: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4857 - acc: 0.8470 - val_loss: 1.5439 - val_acc: 0.5970\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8533\n",
      "Epoch 00018: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4595 - acc: 0.8532 - val_loss: 1.5882 - val_acc: 0.5921\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8630\n",
      "Epoch 00019: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.4381 - acc: 0.8630 - val_loss: 1.6129 - val_acc: 0.5998\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8701\n",
      "Epoch 00020: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4108 - acc: 0.8702 - val_loss: 1.6286 - val_acc: 0.5970\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8793\n",
      "Epoch 00021: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.3831 - acc: 0.8794 - val_loss: 1.6647 - val_acc: 0.5966\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8837\n",
      "Epoch 00022: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.3678 - acc: 0.8837 - val_loss: 1.6558 - val_acc: 0.6105\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8882\n",
      "Epoch 00023: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3499 - acc: 0.8882 - val_loss: 1.7143 - val_acc: 0.6047\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8940\n",
      "Epoch 00024: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3325 - acc: 0.8940 - val_loss: 1.7473 - val_acc: 0.5975\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9026\n",
      "Epoch 00025: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3101 - acc: 0.9026 - val_loss: 1.7845 - val_acc: 0.5977\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9041\n",
      "Epoch 00026: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.2969 - acc: 0.9041 - val_loss: 1.7859 - val_acc: 0.5989\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9097\n",
      "Epoch 00027: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.2827 - acc: 0.9097 - val_loss: 1.8341 - val_acc: 0.6021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9153\n",
      "Epoch 00028: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.2663 - acc: 0.9152 - val_loss: 1.8045 - val_acc: 0.6091\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9188\n",
      "Epoch 00029: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2551 - acc: 0.9189 - val_loss: 1.8500 - val_acc: 0.6084\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9206\n",
      "Epoch 00030: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2494 - acc: 0.9206 - val_loss: 1.8553 - val_acc: 0.6070\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9225\n",
      "Epoch 00031: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.2432 - acc: 0.9225 - val_loss: 1.8884 - val_acc: 0.6038\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9263\n",
      "Epoch 00032: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.2307 - acc: 0.9263 - val_loss: 1.8881 - val_acc: 0.6112\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9297\n",
      "Epoch 00033: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.2223 - acc: 0.9297 - val_loss: 1.9316 - val_acc: 0.6028\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9328\n",
      "Epoch 00034: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.2117 - acc: 0.9329 - val_loss: 1.9265 - val_acc: 0.6115\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9370\n",
      "Epoch 00035: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2038 - acc: 0.9370 - val_loss: 1.9610 - val_acc: 0.6175\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9382\n",
      "Epoch 00036: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.1979 - acc: 0.9382 - val_loss: 1.9493 - val_acc: 0.6196\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9413\n",
      "Epoch 00037: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.1901 - acc: 0.9413 - val_loss: 1.9448 - val_acc: 0.6210\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9430\n",
      "Epoch 00038: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1854 - acc: 0.9430 - val_loss: 1.9625 - val_acc: 0.6222\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9450\n",
      "Epoch 00039: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1806 - acc: 0.9450 - val_loss: 1.9646 - val_acc: 0.6226\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9452\n",
      "Epoch 00040: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1775 - acc: 0.9451 - val_loss: 1.9537 - val_acc: 0.6245\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9486\n",
      "Epoch 00041: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.1675 - acc: 0.9486 - val_loss: 1.9755 - val_acc: 0.6219\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9512\n",
      "Epoch 00042: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1604 - acc: 0.9512 - val_loss: 2.0253 - val_acc: 0.6245\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9504\n",
      "Epoch 00043: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1596 - acc: 0.9504 - val_loss: 2.0504 - val_acc: 0.6212\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9520\n",
      "Epoch 00044: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.1577 - acc: 0.9520 - val_loss: 2.0416 - val_acc: 0.6285\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9523\n",
      "Epoch 00045: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1560 - acc: 0.9522 - val_loss: 2.0434 - val_acc: 0.6252\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9542\n",
      "Epoch 00046: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1507 - acc: 0.9542 - val_loss: 2.0610 - val_acc: 0.6259\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9573\n",
      "Epoch 00047: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1439 - acc: 0.9573 - val_loss: 2.0859 - val_acc: 0.6317\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9560\n",
      "Epoch 00048: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1437 - acc: 0.9560 - val_loss: 2.0260 - val_acc: 0.6315\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9573\n",
      "Epoch 00049: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1418 - acc: 0.9573 - val_loss: 2.1136 - val_acc: 0.6250\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9585\n",
      "Epoch 00050: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1400 - acc: 0.9585 - val_loss: 2.0880 - val_acc: 0.6292\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9616\n",
      "Epoch 00051: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1276 - acc: 0.9616 - val_loss: 2.1446 - val_acc: 0.6252\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9615\n",
      "Epoch 00052: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.1278 - acc: 0.9615 - val_loss: 2.0707 - val_acc: 0.6327\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9619\n",
      "Epoch 00053: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1278 - acc: 0.9618 - val_loss: 2.1058 - val_acc: 0.6357\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9639\n",
      "Epoch 00054: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.1227 - acc: 0.9639 - val_loss: 2.1150 - val_acc: 0.6364\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9644\n",
      "Epoch 00055: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.1204 - acc: 0.9644 - val_loss: 2.1704 - val_acc: 0.6303\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9647\n",
      "Epoch 00056: val_loss did not improve from 1.33626\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1192 - acc: 0.9647 - val_loss: 2.1213 - val_acc: 0.6348\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 369us/sample - loss: 1.4001 - acc: 0.5664\n",
      "Loss: 1.4001398931287408 Accuracy: 0.56635517\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2341 - acc: 0.2849\n",
      "Epoch 00001: val_loss improved from inf to 1.68214, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/001-1.6821.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 2.2340 - acc: 0.2849 - val_loss: 1.6821 - val_acc: 0.4701\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5087 - acc: 0.5274\n",
      "Epoch 00002: val_loss improved from 1.68214 to 1.36156, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/002-1.3616.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.5087 - acc: 0.5274 - val_loss: 1.3616 - val_acc: 0.5851\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2658 - acc: 0.6083\n",
      "Epoch 00003: val_loss improved from 1.36156 to 1.30909, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/003-1.3091.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.2660 - acc: 0.6083 - val_loss: 1.3091 - val_acc: 0.5795\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1344 - acc: 0.6518\n",
      "Epoch 00004: val_loss improved from 1.30909 to 1.23087, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/004-1.2309.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.1344 - acc: 0.6519 - val_loss: 1.2309 - val_acc: 0.6184\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0230 - acc: 0.6859\n",
      "Epoch 00005: val_loss improved from 1.23087 to 1.17595, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/005-1.1759.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.0231 - acc: 0.6859 - val_loss: 1.1759 - val_acc: 0.6366\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.7170\n",
      "Epoch 00006: val_loss did not improve from 1.17595\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9278 - acc: 0.7170 - val_loss: 1.1915 - val_acc: 0.6320\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8463 - acc: 0.7406\n",
      "Epoch 00007: val_loss improved from 1.17595 to 1.16856, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_5_conv_checkpoint/007-1.1686.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8462 - acc: 0.7407 - val_loss: 1.1686 - val_acc: 0.6371\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7624\n",
      "Epoch 00008: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7710 - acc: 0.7625 - val_loss: 1.1740 - val_acc: 0.6403\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6966 - acc: 0.7864\n",
      "Epoch 00009: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.6966 - acc: 0.7864 - val_loss: 1.2015 - val_acc: 0.6352\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.8054\n",
      "Epoch 00010: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.6297 - acc: 0.8054 - val_loss: 1.2683 - val_acc: 0.6306\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8274\n",
      "Epoch 00011: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5655 - acc: 0.8274 - val_loss: 1.2044 - val_acc: 0.6485\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8416\n",
      "Epoch 00012: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5153 - acc: 0.8416 - val_loss: 1.2275 - val_acc: 0.6501\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8564\n",
      "Epoch 00013: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.4671 - acc: 0.8563 - val_loss: 1.2696 - val_acc: 0.6529\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8676\n",
      "Epoch 00014: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4243 - acc: 0.8676 - val_loss: 1.3123 - val_acc: 0.6548\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8761\n",
      "Epoch 00015: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.3892 - acc: 0.8761 - val_loss: 1.3541 - val_acc: 0.6513\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8890\n",
      "Epoch 00016: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.3552 - acc: 0.8890 - val_loss: 1.3785 - val_acc: 0.6504\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9026\n",
      "Epoch 00017: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3163 - acc: 0.9025 - val_loss: 1.4077 - val_acc: 0.6555\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9075\n",
      "Epoch 00018: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2984 - acc: 0.9075 - val_loss: 1.3807 - val_acc: 0.6632\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9172\n",
      "Epoch 00019: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2693 - acc: 0.9172 - val_loss: 1.4301 - val_acc: 0.6615\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9208\n",
      "Epoch 00020: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2562 - acc: 0.9208 - val_loss: 1.4419 - val_acc: 0.6669\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9265\n",
      "Epoch 00021: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2342 - acc: 0.9266 - val_loss: 1.4824 - val_acc: 0.6653\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9328\n",
      "Epoch 00022: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.2166 - acc: 0.9328 - val_loss: 1.4811 - val_acc: 0.6601\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9349\n",
      "Epoch 00023: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2059 - acc: 0.9349 - val_loss: 1.5284 - val_acc: 0.6678\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9413\n",
      "Epoch 00024: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1891 - acc: 0.9413 - val_loss: 1.5482 - val_acc: 0.6606\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9427\n",
      "Epoch 00025: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1813 - acc: 0.9428 - val_loss: 1.5571 - val_acc: 0.6783\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9482\n",
      "Epoch 00026: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1665 - acc: 0.9482 - val_loss: 1.5795 - val_acc: 0.6732\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9511\n",
      "Epoch 00027: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1569 - acc: 0.9511 - val_loss: 1.5980 - val_acc: 0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9541\n",
      "Epoch 00028: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1533 - acc: 0.9541 - val_loss: 1.5522 - val_acc: 0.6785\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9562\n",
      "Epoch 00029: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1418 - acc: 0.9562 - val_loss: 1.5944 - val_acc: 0.6811\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9573\n",
      "Epoch 00030: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1358 - acc: 0.9573 - val_loss: 1.6240 - val_acc: 0.6853\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9602\n",
      "Epoch 00031: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1312 - acc: 0.9602 - val_loss: 1.7283 - val_acc: 0.6709\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9580\n",
      "Epoch 00032: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1367 - acc: 0.9580 - val_loss: 1.6335 - val_acc: 0.6869\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9632\n",
      "Epoch 00033: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1242 - acc: 0.9632 - val_loss: 1.6879 - val_acc: 0.6844\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9637\n",
      "Epoch 00034: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1198 - acc: 0.9637 - val_loss: 1.6843 - val_acc: 0.6846\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9679\n",
      "Epoch 00035: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1075 - acc: 0.9679 - val_loss: 1.7046 - val_acc: 0.6748\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9686\n",
      "Epoch 00036: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1078 - acc: 0.9686 - val_loss: 1.6786 - val_acc: 0.6960\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9680\n",
      "Epoch 00037: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1038 - acc: 0.9680 - val_loss: 1.7097 - val_acc: 0.6825\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9715\n",
      "Epoch 00038: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0961 - acc: 0.9715 - val_loss: 1.7428 - val_acc: 0.6874\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9711\n",
      "Epoch 00039: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0990 - acc: 0.9711 - val_loss: 1.7553 - val_acc: 0.6837\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9718\n",
      "Epoch 00040: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0952 - acc: 0.9718 - val_loss: 1.7102 - val_acc: 0.6937\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9718\n",
      "Epoch 00041: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0967 - acc: 0.9718 - val_loss: 1.6986 - val_acc: 0.6900\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9710\n",
      "Epoch 00042: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0971 - acc: 0.9710 - val_loss: 1.7606 - val_acc: 0.6837\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9733\n",
      "Epoch 00043: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0901 - acc: 0.9733 - val_loss: 1.7722 - val_acc: 0.6860\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9746\n",
      "Epoch 00044: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0851 - acc: 0.9747 - val_loss: 1.7498 - val_acc: 0.6962\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9749\n",
      "Epoch 00045: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0858 - acc: 0.9749 - val_loss: 1.7303 - val_acc: 0.6909\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9771\n",
      "Epoch 00046: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0789 - acc: 0.9771 - val_loss: 1.7978 - val_acc: 0.6869\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9757\n",
      "Epoch 00047: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0810 - acc: 0.9757 - val_loss: 1.7705 - val_acc: 0.6853\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9780\n",
      "Epoch 00048: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0778 - acc: 0.9780 - val_loss: 1.7347 - val_acc: 0.6939\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9785\n",
      "Epoch 00049: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0752 - acc: 0.9785 - val_loss: 1.8724 - val_acc: 0.6804\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9795\n",
      "Epoch 00050: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0720 - acc: 0.9795 - val_loss: 1.7594 - val_acc: 0.7060\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9784\n",
      "Epoch 00051: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0739 - acc: 0.9784 - val_loss: 1.7844 - val_acc: 0.6960\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9795\n",
      "Epoch 00052: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0713 - acc: 0.9795 - val_loss: 1.7590 - val_acc: 0.6990\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9809\n",
      "Epoch 00053: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0666 - acc: 0.9809 - val_loss: 1.8045 - val_acc: 0.7018\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9795\n",
      "Epoch 00054: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0716 - acc: 0.9795 - val_loss: 1.7821 - val_acc: 0.7002\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9810\n",
      "Epoch 00055: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0679 - acc: 0.9810 - val_loss: 1.8096 - val_acc: 0.6990\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9795\n",
      "Epoch 00056: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0704 - acc: 0.9795 - val_loss: 1.7956 - val_acc: 0.6983\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9816\n",
      "Epoch 00057: val_loss did not improve from 1.16856\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0636 - acc: 0.9816 - val_loss: 1.7621 - val_acc: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmS2TfSMQCEuCIhCWBAJIRUClWrfiinsrWrWLtbW2VLRqtdqfW6t1V1ppQa1LUetaqVQgKqJsYd+XQCJkn6yTZJbz++NkIRBIgEwmybyf5znPnczcufe9Idz33rNdpbVGCCGEALAEOwAhhBBdhyQFIYQQTSQpCCGEaCJJQQghRBNJCkIIIZpIUhBCCNFEkoIQQogmkhSEEEI0kaQghBCiiS3YARyrXr166dTU1GCHIYQQ3cqqVauKtdZJba3X7ZJCamoqK1euDHYYQgjRrSilctuznlQfCSGEaCJJQQghRBNJCkIIIZp0uzaF1ng8HvLy8qitrQ12KN2W0+mkf//+2O32YIcihAiiHpEU8vLyiI6OJjU1FaVUsMPpdrTWlJSUkJeXR1paWrDDEUIEUY+oPqqtrSUxMVESwnFSSpGYmCh3WkKInpEUAEkIJ0h+f0II6EFJoS0+n5u6unz8fm+wQxFCiC4rZJKC319Lff1+tK7v8G27XC6ef/754/ru+eefj8vlavf6999/P3/605+Oa19CCNGWkEkKSpk2da09Hb7toyUFr/fodyYff/wxcXFxHR6TEEIcjxBKCqarpdYdX300e/Zsdu7cSWZmJrNmzWLJkiVMnjyZ6dOnk56eDsDFF19MVlYWI0aMYM6cOU3fTU1Npbi4mD179jB8+HBuvvlmRowYwTnnnIPb7T7qfnNycpg4cSKjR4/mkksuoaysDICnn36a9PR0Ro8ezVVXXQXA0qVLyczMJDMzkzFjxlBZWdnhvwchRPfXI7qkHmz79tupqspp5RONz1eFxRKGUo5j2mZUVCZDhvzliJ8/8sgjbNiwgZwcs98lS5awevVqNmzY0NTFc+7cuSQkJOB2uxk/fjyXXXYZiYmJh8S+nddff52//vWvXHHFFbz99ttcd911R9zvD3/4Q5555hmmTp3KfffdxwMPPMBf/vIXHnnkEXbv3k1YWFhT1dSf/vQnnnvuOSZNmkRVVRVOp/OYfgdCiNAQMncK0Ni7RnfK3iZMmNCiz//TTz9NRkYGEydOZN++fWzfvv2w76SlpZGZmQlAVlYWe/bsOeL2y8vLcblcTJ06FYDrr7+e7OxsAEaPHs21117Lq6++is1m8v6kSZO44447ePrpp3G5XE3vCyHEwXrcmeFoV/RVVWux2WJxOlMDHkdkZGTT6yVLlrBo0SK++uorIiIiOOOMM1odExAWFtb02mq1tll9dCQfffQR2dnZfPDBB/zxj39k/fr1zJ49mwsuuICPP/6YSZMmsXDhQoYNG3Zc2xdC9FwhdKdgGpsD0SU1Ojr6qHX05eXlxMfHExERwZYtW1i+fPkJ7zM2Npb4+Hg+//xzAF555RWmTp2K3+9n3759nHnmmTz66KOUl5dTVVXFzp07GTVqFHfeeSfjx49ny5YtJxyDEKLn6XF3CkejlC0gDc2JiYlMmjSJkSNHct5553HBBRe0+Pzcc8/lxRdfZPjw4QwdOpSJEyd2yH7nzZvHT37yE2pqahg8eDB///vf8fl8XHfddZSXl6O15he/+AVxcXHce++9LF68GIvFwogRIzjvvPM6JAYhRM+itO6cOvaOMm7cOH3oQ3Y2b97M8OHD2/yu270Tn6+GqKhRgQqvW2vv71EI0f0opVZprce1tV6IVR/ZA3KnIIQQPUWIJQUb4ENrf7BDEUKILikEk0JgBrAJIURPIElBCCFEkxBLCoGb6kIIIXqCEEsKcqcghBBHE6JJoeNnSj1WUVFRx/S+EEJ0hhBNCnKnIIQQrQmxpKACMqp59uzZPPfcc00/Nz4Ip6qqimnTpjF27FhGjRrFe++91+5taq2ZNWsWI0eOZNSoUbz55psA7N+/nylTppCZmcnIkSP5/PPP8fl8zJw5s2ndJ598skOPTwgROgI2zYVSagAwH+iDmZp0jtb6qUPWUcBTwPlADTBTa736hHZ8++2Q09rU2Ua4rxqUBSzh7d9mZib85cgT7V155ZXcfvvt3HrrrQC89dZbLFy4EKfTybvvvktMTAzFxcVMnDiR6dOnt+t5yO+88w45OTmsXbuW4uJixo8fz5QpU/jnP//J9773PX73u9/h8/moqakhJyeH/Px8NmzYAHBMT3ITQoiDBXLuIy/wa631aqVUNLBKKfWp1nrTQeucBwxpKKcCLzQsA0cpOnr67DFjxlBYWMi3335LUVER8fHxDBgwAI/Hw9133012djYWi4X8/HwKCgpITk5uc5tffPEFV199NVarlT59+jB16lRWrFjB+PHjufHGG/F4PFx88cVkZmYyePBgdu3axW233cYFF1zAOeec06HHJ4QIHQFLClrr/cD+hteVSqnNQApwcFK4CJivzQRMy5VScUqpvg3fPT5HuaIHqHfvxO93Exk58rh30ZoZM2awYMECDhw4wJVXXgnAa6+9RlFREatWrcJut5OamtrqlNnHYsqUKWRnZ/PRRx8xc+ZM7rjjDn74wx+ydu1aFi5cyIsvvshbb73F3LlzO+KwhBAhplPaFJRSqcAY4OtDPkoB9h30c17DewGMJTDTZ1955ZW88cYbLFiwgBkzZgBmyuzevXtjt9tZvHgxubm57d7e5MmTefPNN/H5fBQVFZGdnc2ECRPIzc2lT58+3Hzzzdx0002sXr2a4uJi/H4/l112GQ899BCrV59YDZwQInQFfOpspVQU8DZwu9a64ji3cQtwC8DAgQNPMB4b4EVr3a66/fYaMWIElZWVpKSk0LdvXwCuvfZavv/97zNq1CjGjRt3TA+1ueSSS/jqq6/IyMhAKcVjjz1GcnIy8+bN4/HHH8dutxMVFcX8+fPJz8/nhhtuwO83czo9/PDDHXZcQojQEtCps5UZQvwhsFBr/UQrn78ELNFav97w81bgjKNVH53I1NkA9fUF1NXtIzIyA4vF3v6DCQEydbYQPVfQp85u6Fn0MrC5tYTQ4H3gh8qYCJSfUHtCu+KSqS6EEOJIAll9NAn4AbBeKdXYR/RuYCCA1vpF4GNMd9QdmC6pNwQwHkAGsAkhxNEEsvfRF8BRK+0beh3dGqgYWtOVproQQoiuJqRGNINUHwkhxNGEYFKwApIUhBCiNSGYFCyAVZKCEEK0IuSSApgqpI5sU3C5XDz//PPH9d3zzz9f5ioSQnQZIZoUOnam1KMlBa/36Pv5+OOPiYuL67BYhBDiRIRkUrBYOjYpzJ49m507d5KZmcmsWbNYsmQJkydPZvr06aSnpwNw8cUXk5WVxYgRI5gzZ07Td1NTUykuLmbPnj0MHz6cm2++mREjRnDOOefgdrsP29cHH3zAqaeeypgxY/jud79LQUEBAFVVVdxwww2MGjWK0aNH8/bbbwPwySefMHbsWDIyMpg2bVqHHbMQ3d6ePfDUUxDAAbzdUcCnuehsbcycDYDf3x+tvVit7dtmGzNn88gjj7BhwwZyGna8ZMkSVq9ezYYNG0hLSwNg7ty5JCQk4Ha7GT9+PJdddhmJiYkttrN9+3Zef/11/vrXv3LFFVfw9ttvc91117VY5/TTT2f58uUopfjb3/7GY489xp///GcefPBBYmNjWb9+PQBlZWUUFRVx8803k52dTVpaGqWlpe07YCFCwezZ8OabMHAgXHJJsKPpMnpcUmifxumzNW0MpThuEyZMaEoIAE8//TTvvvsuAPv27WP79u2HJYW0tDQyMzMByMrKYs+ePYdtNy8vjyuvvJL9+/dTX1/ftI9FixbxxhtvNK0XHx/PBx98wJQpU5rWSUhI6NBjFKLbysuDBQvM69/9DqZPp91XiT1cj0sKbcycDUB9vath/qNMLJbA/AoiIyObXi9ZsoRFixbx1VdfERERwRlnnNHqFNphYWFNr61Wa6vVR7fddht33HEH06dPZ8mSJdx///0BiV+IHu3550210eOPw6xZ8MorMHNmsKPqEkKyTaGjRzVHR0dTWVl5xM/Ly8uJj48nIiKCLVu2sHz58uPeV3l5OSkpZnbxefPmNb1/9tlnt3gkaFlZGRMnTiQ7O5vdu3cDSPWREABuN8yZAxddBL/+NWRlwe9/D3V1wY6sSwjxpNAxjc2JiYlMmjSJkSNHMmvWrMM+P/fcc/F6vQwfPpzZs2czceLE497X/fffz4wZM8jKyqJXr15N799zzz2UlZUxcuRIMjIyWLx4MUlJScyZM4dLL72UjIyMpof/CBHSXnsNSkrgl780T2J8+GHYuxdeeinYkXUJAZ06OxBOdOpsAJ+vhpqaTTidJ2G3x3d0iN2WTJ0tejytYfRo036wZo1JClrDtGmwYQPs2gVRUSe2j23bYP9+OP30I7dTaA3//a/p/VRfb9Y9/XSYOPHE938EQZ86uyuTmVKF6Oa0hvvvh1tugY0b2/+9xYvNyb/xLgGa7xaKitrXKHm0mF54AUaNgjPOgLQ0uOce2L69eR2Px9ypZGbCuefC2rVQWgoPPghnnw1xcTBuHPz2t3DgwPHHcgJCPCnITKlCdEuPPAIPPABz58LIkXDBBbBkSdtjDp56CpKS4OqrW75/6qlw8cWm4bmkpOVn9fXw1lsmYRQWtr7dqiq49lr42c/MXcdrr5m4Hn4YTjkFJk82CeLkk+G668DrhX/8A3bvhtWroawMFi6Eu++GmBh48kkYMgT++EfTBtKZtNbdqmRlZelDbdq06bD32lJRsUq73bnH/L2e7Hh+j0Ics8pKrb/4QuuXXtJ61Sqt/f5j+/78+VqD1ldfrXVhodZ/+IPWSUnmvXHjtH7zTa19vsO/t2OH1kppfc89rW93wwbz+W9+Y37Oz9f6vvu0Tk422watHQ6tf/ADrb/5pvl7GzdqPXy41haL1g891HLf+flaP/qo1sOGme9PmaL1Bx+0Ht/Btm3T+pJLzHcGDND6lVfa/k4bgJW6HefYoJ/kj7V0VFKorFyna2p2HvP3ejJJCiIgCgu1fvxxcxIfOtSceBtPsqD1qFFaP/GE1gUFbW/r00+1ttm0PvNMrWtrm9+vqTFJ5pRTzDa/8x2tV69u+d3bbzffzc8/8vavv15rp1Pryy4z6yql9Xnnaf3RR+bkf+utWkdFmX2ceqrW996rdUSE1r17a/3ZZ0fert+vdVFR28d3qCVLtB47tjnhff75sW+jgSSFNlRVbdLV1VuP+Xs9mSQFobXWuq5O65UrtX7+ea1vuMGctK+9VuuysmPfVkFB81XywIFaX3yx1g88YK6Wt23T+oUXzMkVzEl4+nStFyzQurr68G3l5GgdHa31yJFHjsXn0/rvfzd3DhaLOYmXlmpdUaF1TIzW11xz9Hh37zZJIT5e61//2txdHKq8XOunn25OQFOmHD3RnCifz9wdpaSYu6LjJEmhDdXV23RV1YZj/l5PJkkhhPn9Wr/7rtannaZ1WFjzVXyvXlpPm2ZO2GlpJlm0V0mJ1qNHax0ervXixUdfd+NGrX/72+aqmogIrS+/XOs33jAn9Nxcrfv1MyfGffva3ndZmdY//7lJDElJWs+YYba7fHnb383Laz0pHcrn03rrVq09nrbX7QjV1eaO6DhJUmhDTc1uXVmZc8zf6yiRkZFB2/eRSFLoJHV1Wr/3ntb19cGOxPj6a61PP92cDk45xdSpv/mmuWpurO9ftkzr/v1Nnfqzz7bdDuBymeqOsDCt//vf9sfi8Wj9v/9p/dOfat2nj4kpLMy8jonRet26Yzu2NWtMomus7glhkhTa4Hbv0xUVK7X/WBu5OogkhRB2773mv95DDwU3jt27tb7qKhNL795av/ji0a96i4pM/TpofcUVphqlNZWV5kRss5lqouPl9Wqdna31L36hdWZm23cbR+LzmbugraFdXSxJoQ11dft1RcUK7fef+K3fnXfeqZ999tmmn3//+9/rxx9/XFdWVuqzzjpLjxkzRo8cOVL/+9//blrnSEnhoosu0mPHjtXp6en6pZdeanr/P//5jx4zZowePXq0Puuss7TWWldWVuqZM2fqkSNH6lGjRukFCxac0HFIUugEubmmzjoszJRt2zpu299+a+rd27rQKSzU+o47zFV/eLjpjVNR0b59+HxaP/yw1laraSO45RaTTL75Rmu321RvnHWWqbb5179O/JhEh2lvUuhxI5pv/+R2cg60MXc2ZoyC31+LxRLZ8IjOI8tMzuQv5x55UMuaNWu4/fbbWbp0KQDp6eksXLiQvn37UlNTQ0xMDMXFxUycOJHt27ejlCIqKoqqqqrDtlVaWtpiiu2lS5fi9/sZO3ZsiymwExISuPPOO6mrq+MvDQNuysrKiI8//hHaMqK5E1x9Nbz3nhlEdc45ZqDSokXNA6mOVWEhvP22mQI6O9u0BGRlmTnkr7gCHI7mdcvL4c9/Nn3ga2rghz80g6b69z/2/X7+ufnuypWmjz2Y0bu9epmY5s83/fFFl9HeEc09bpbUI9IafD6wNR5y43/CE0+KY8aMobCwkG+//ZaioiLi4+MZMGAAHo+Hu+++m+zsbCwWC/n5+RQUFJCcnHzEbbU2xXZRUVGrU2C3Nl226MKWLYM33oD77jODpR55xAx2euUVc4JuzSefwBNPQFiYGe0aG2tKRIQZrPXZZ+D3w/DhZlK3Xr3g2WfhBz8ws3/+7Gfm9ZtvwqOPmhP4jBlm4NeJXABMnmymadAacnPNAKzVq81o4SuvPHxwmOg2elxSOOIVfUmJGT2Yng4REfh81dTUbMbpPBm7/cQfhzljxgwWLFjAgQMHmiaee+211ygqKmLVqlXY7XZSU1NbnTK7UXun2BbdkN9vplZISTFTGAD8+Mfw6qtwxx1w/vnmhH6wV16BG24wV/Lx8WY6h/JycLnM9k46Ce66y5yER45svtv46U/h00/N6N377jMFzD4eegjGjOm441IKUlNNufTSjtuuCJrQmeYiIsIsa2qAjp/q4sorr+SNN95gwYIFzJgxAzDTXPfu3Ru73c7ixYvJzc096jaONMX2kabAbm26bNFFvfKKqWp55BFofNaGxWJm5iwvN1M4H+zPfzZ3D1Onwrp1ZvK2XbvMxY3Xa6ZV2L7dnORHjWpZ/WSxwPe+Bx9/DJs3m2qeL76Ajz7q2IQgeqTQSQpOp/mP0zCPSEdPijdixAgqKytJSUmhb9++AFx77bWsXLmSUaNGMX/+fIYNG3bUbRxpiu0jTYHd2nTZoguqqjJX9BMmwDXXtPxs5Ehz5zB/Pvzvf6Y65re/hd/8Bi6/3JzYY2Jafkcpk1ja0w4xbJiZc2fSpI47HtGj9biG5qPavNlcRQ0dCkBl5Wrs9iSczgGBCLXbkYbmALnnHjOx2bJl8J3vHP65222mcwZz8p43z1QBPfOMPCJSdBhpaG5NeLipj9UalEIpW4dVHwnRqtxc+NOfzB1CawkBzN/liy/Cd78LO3aYKaHvu+/4eyQJcQJCKylEREBxsZnT3OFoSAryTAURALrhISqzZ5u700ceOfr606aZrqKJiaa3kBBB0mOSgtYa1daV1cGNzQ4HStnlTqFBd6tG7LK0Nt1IH3gAvv4aBgwwjcwD2lFFefvtgY9PiDb0iIZmp9NJSUlJ2ye28HCzPKixWe4UTEIoKSnB6XQGO5Suw+drOUCrPT7+2Iw/OP988zjGF180PYQuuyywsQrRgXrEnUL//v3Jy8ujqKio7ZVdLnOn4HLh8ZTh81XidNoDH2QX53Q66X88I1t7kpoaU+Xz3nvw4YemqhFMYnjiCbj11tbr+V0uM0js9ddNf/2//tV0Jz14NLEQ3USPSAp2u71ptG+b7r3XPBd1+3b27n2UXbtmM2pUJTZbYB6WLbo4n89MM/H3v8P775u7yNhY83jHiy4yV/633gq33QZffmlO+Ac/WH3pUpMA8vPhD38wbQh2ucgQ3VePSArHJCMD3nkHKiux23sD4PEUSVIINdu2mWfkzp9vTugJCTBzpqnqmTKl5Yn9/ffNFBH33AM5OWauoZNPNtNKPPqoGVm8bJkZhyBENxd6SSEz0zQGrl+P/ZQkwCSF8PB23mmI7qWszCSAxrJ9uxmvsm6d6RV03nnmgezf/76ZX6g1FosZfDZxIlx1FYwfD4MHm3l+brrJ9BqKkosK0TOEZlIAyMnBMTILMElB9CC5ufCvf5nyzTfN71utkJYGQ4bAtdearp8No8/b5cwzzXQTV10FmzaZO85LLun4+IUIooAlBaXUXOBCoFBrPbKVz88A3gN2N7z1jtb6D4GKp0nj5GJr12K/4VwA6usLA75bEWD798Nrr7VMBFlZZm6gjAyTCNLSTrzxt18/047QMNZFiJ4mkHcK/wCeBeYfZZ3PtdYXBjCGwyll7hZycrDbm6uPRDf2ySdmqmaXyySCRx4x00MPHhyY/SklCUH0WAFLClrrbKVUaqC2f0IyM+HFF7ESjlJhkhS6K63h4YdNA/CoUaZ3UHp6sKMSolsL9uC17yil1iql/qOUGtFpe83IALcbtWMHYWH9cbt3dtquRQeprDQ9hX73O1PHv2yZJAQhOkAwk8JqYJDWOgN4Bvj3kVZUSt2ilFqplFrZrgFqbTmosTk2dhLl5Z/LNA9dkdZmQJnHY1432rrVdP98/30zqOy115qfUSCEOCFBSwpa6wqtdVXD648Bu1Kq1xHWnaO1Hqe1HpeUlHTiOx8+3PRDz8khLm4qHk8xNTWbTny7ouNs2wann25O9g6H6RbqcJiunyNHmofNfPop/OpXMpuoEB0oaF1SlVLJQIHWWiulJmASVEmn7NzhMFUNa9cSF3czAC7XUiIjO68GSxyB32+eMTx7tnkw0n33mX+v+npzx1Bfb56z/bOfwcCBwY5WiB4nkF1SXwfOAHoppfKA3wN2AK31i8DlwE+VUl7ADVylO7MOJzMTFi7E6RyMw5GCy7WUlJSfddruRSv27IEbb4TFi82kcn/9q+kCKoToNIHsfXR1G58/i+myGhyZmTBvHqqwkLi4qZSV/a9902+LjuX3m2cP/+c/cPfdpiro5ZfNA+vl30KIThd6I5obZWSY5dq1xI2cSmHhP3G7txERMTS4cfV0lZWmgXjVKli92owQrqgwn511FsydC4MGBTdGIUKYJIWcHOJOvwgAlytbkkIgbd8O06fDli2mvSAjw0w3kZUFY8eauze5OxAiqEI3KSQkmIbKtWsJD5+F3d4Hl2sp/frdHOzIup/6eti5E4YNO/JJ/X//M6OMlTIPozn7bNNgLIToUoI9eC24MjIgJwelFHFxUykvXyrjFY5FbS0895yZRjo93Vzpv/KKSRIHe/55+N73zORzK1aYmUklIQjRJYV2UsjMNFUZbjdxcVOoq8ujtnZ3298LdTU1ZrrpwYPh5z83d1yPPw5er3ngzODB5ufiYtN19NZbTSL46qvAzUckhOgQkhT8fti4kdjYqYAZryCOoLAQ/u//zGyjv/oVDB0Kn31mnmX8m9+Y5wt8/LF5/7e/hT594IUXzOt//xtiYoJ9BEKINoR2UjiosTkyMh2bLVGSwqG0huxsMwtp//5mrqHMTJMIFi82zxhobEdQytwR/O9/pmfRjTfCq6+ap5NZrcE9DiFEu4R2xW5amrl6nT8fddVVxMVNobxckgJg2gXmzDFX+ps2mecW/+xn8JOfmAbltowZYwafCSG6ldC+U7BY4KmnzAybU6eSUD+G2to91NbuDXZkwbVpk3n05G23QUSEGUz27bemHaE9CUEI0W2FdlIA87D299+HLVtIvuQFInJDuF3B74dnnjHjBvbtg3ffNb2FbrzRJAchRI8nSQHMPDtLl6JqfYy5TVG36M1gR9T5vv3WtAf84hdmZPH69XDxxcGOSgjRySQpNBo3DrV8Ob7ECAbc+BG89VawI+o8b79tnlz2+edmTMGHH0JycrCjEkIEgSSFg6WlUfLeb6kYBvrqq03vmp6sqgpuugkuv9yMH1izBn76U5lqQogQJknhENGp57P+UfCd1Nd0w9y/P9ghBcaqVWa+oblz4a67TGP7UJn3SYhQJ0nhEFFRmRAVTd5fJpkZPa+6yozU7Sn8fnjsMfjOd8zI5M8+MwPS7PZgRyaE6AIkKRzCYrERGzuJwl7rTT/97Gy4555gh2Xk5preQR7P8X0/L89MRHfnnWa20nXr4IwzOjREIUT3JkmhFXFx06ip2Uz1xWPhxz82I3I/+CC4QRUWwrRppnfQjTeaK/5jsWABjB4Ny5ebQWX/+peZKVYIIQ4iSaEVycnXY7E4yct7wgzYGjvWTPS2O0iT5VVVwQUXmG6jN91kpo745S/NFBRtqagwTzGbMcPMZpqTY7YhjclCiFZIUmiFw5FEcvJMDhyYT51ymatqrc2Jta6uc4PxeEzvoDVrTDfZOXPM5HPPPgu///3Rv7tsmZmnaP58uPde+PJLGDKkc+IWQnRLkhSOoH//X6G1h2+/fc5015w3z/TYufJK8xyBzqC1uapfuBBefBEuvNBc4T/2GPzoR/Dgg/DEE4d/58svTRXT5Mnmvexs+MMfpDFZCNE2rXWbBfglEAMo4GVgNXBOe77b0SUrK0t3lvXrL9aff56gvd4q88Yzz2gNWp9xhtbl5YEP4K67zP4eeODwz7xerS+/3Hz+8staHzig9WOPaT1smHkvKkrrW2/tnDiFEF0esFK353zfrpVgbcPye8A7wAhgdXu+29GlM5OCy/WFXrwYnZf3bPObr72mtc2m9ZgxWhcUBGbHdXVaP/yw+ee55Rat/f7W16ut1fqcc7S2WExMoPWkSVrPnat1ZWVgYhNCdEvtTQrtnTq7sVXyfOAVrfVGpXp+S2VMzGnExExk374n6NfvJyhlhWuugfh4uOwyOP10+O9/ITW1+UsVFWa6iJUrzednnmlmY22PykrTM+jJJ0330UsuMY+7PNKvOiwM3nnHTGndp4+pMpJZTIUQJ6C9SWGVUuq/QBpwl1IqGjjGPpHdj1KKAQN+w8ZaUnO9AAAgAElEQVSNl1Nc/G+Ski4zH5x3HixaZHoETZpkTuLr1pmHy6xYAT5f80ZSU03vn5kzzWMrW3PggBl/8Pzz4HKZsQNz5sC557bdSygy0rR3CCFEB1C6Hd0alVIWIBPYpbV2KaUSgP5a63WBDvBQ48aN0ytXruy0/Wnt4+uvh2K392Ls2K9ocYO0YQOcc46ZCsNqhfHjzViCs84yD5n55BMzjcSiRebk/t3vwrhxZsxBQYFJBgUFpqup3w+XXgqzZsGpp3ba8QkhQoNSapXWelyb67UzKUwCcrTW1Uqp64CxwFNa69wTD/XYdHZSAMjPf57t229lzJgviI2d1PLDwkJzlzBhwpGfQbxnD/zjH/D3v5tqod69zSykffqY5YABZhyEdBcVQgRIRyeFdUAGMBr4B/A34Aqt9dQTjPOYBSMp+Hw1fPXVQOLiJjNy5LvHvyHTFNz+NgYhhOgg7U0K7T07eRtary8CntVaPwdEn0iA3YnVGkFKys8oLn6Pmpptx78hpSQhCCG6tPaeoSqVUncBPwA+amhjCKmRUCkpt2KxhJGb+8dghyKEEAHT3qRwJVAH3Ki1PgD0Bx4PWFRdkMPRh5SU2ygoeIWqqvXBDkcIIQKiXUmhIRG8BsQqpS4EarXW8wMaWRc0cOBsbLZYdu26K9ihCCFEQLQrKSilrgC+AWYAVwBfK6UuD2RgXZHdnsDAgbMpLf0Il+vzYIcjhBAdrr3VR78Dxmutr9da/xCYANwbuLC6rpSU23A4+rFr1520p+eWEEJ0J+1NChatdeFBP5ccw3d7FKs1gtTU+6mo+IqSkveDHY4QQnSo9p7YP1FKLVRKzVRKzQQ+Aj4OXFhdW3LyDYSHD2XXrrvw+3vQ85uFECGvvQ3Ns4A5mMFro4E5Wus7AxlYV2ax2Bg8+I/U1GymoCDk2tuFED1YeyfEQ2v9NvB2AGPpVnr1upTo6Ans2fN7eve+Gqs1PNghCSHECTvqnYJSqlIpVdFKqVRKVbTx3blKqUKl1IYjfK6UUk8rpXYopdYppcaeyIF0NqUUgwc/Sl1dHvn5zwY7HCGE6BBHTQpa62itdUwrJVprfYTZ35r8Azj3KJ+fBwxpKLcALxxL4F1BfPwZJCScS27uH6mrOxDscIQQ4oQFrAeR1jobKD3KKhcB8xseCrQciFNK9Q1UPIFy0klP4ve72bnzjmCHIoQQJyyY3UpTgH0H/ZzX8F63Ehk5jIED76Kw8HVKSj4JdjhCCHFC2t3QHExKqVswVUwMPNLTy4Jo0CCTFLZv/xlxcRuwWiOCHZIQPZ7HY2ait9uP/IBCjwdqakzxeMxDEf1+U3w+U7xeUzye5tcWi9luY3E4zD48ntZLfX1zaYzL4WhZbLbmdevqmtdXyjxZt3G9sDDzzK7aWhO3291csrLMU34DKZhJIR8YcNDP/RveO4zWeg6mSyzjxo3rcsOILZYwTjnlJdauPZPc3AcZPPjhYIckQojPZ57iWlpqHvPt9Taf7BpPfBaLOdE0Lq1W83l5ectSWWm2abE0z/RusRx+Em08ubamcR82W/NSa3NSazzJNZ6oDz6ZNhavt/nRI43F7295kqypMes1stubT6w2W/O63h42jGjWrJ6dFN4Hfq6UegM4FSjXWu8PYjwnJD7+DJKTb2Dfvj/Ru/c1REWNCnZIohN4PM1XcbW1zVeKB195ut3mZFtVZZaVlea9sDAID29Z3G7zML/GJ7YWFkJZWcsr3MYTdGWlSQQuV8cdj1KmHOmE35hQGhNMa1fofn/LhNTIboeIiOYSHt7yCjkqyiyt1uY4GovFAk7n4d+3WFpeedfVmX2Hhx++rsPRMjk2vrbbTSKx2cxrq9Ucw6F3A35/y7uHxvUbj6HxjsLhaP7bOPQO4uA7h7Aw8x2tm9dpPA6v1xzvoX8fUVEd9299JAFLCkqp14EzgF5KqTzg9zQ8g0Fr/SJmRPT5wA6gBrghULF0lpNOepySkg/Ytu0Wxoz5EvPYCdEVaQ0VFc2PyS4ogKIic+JuPHkf/PrQUl1tTuAHn/Q6ks1mntrauzckJJifG09kjSfjmBjz2cElOrr5BHfwCVzr5pN0Y7HZIDa2ZYmIaD7RN16lN95pHCkJHE3jVb7WZn+i6wvYP5PW+uo2PtfArYHafzDY7YmcdNITbNnyQ7799iVSUn4a7JB6JK3NVfnBda1uN5SUQH6+eQx2fr4pBQXNV/AHX02Wlppla5QyV2TR0RAZaZbR0ZCSYpZRUaYcehXndDZfMTZeRTZeSTZuo7GEh5tYDq4zrqkx7/fuDfHxx34C7mgHX6WfyDas1o6LSQSe5O4O1qfPdRw4MI9du2bTq9dFhIX1C3ZI3Ybfb07s+/ebcuAAfPtt8wm+sRw4cOTqjUYxMeYknpxsroAbb9cbb93j481nffo0L5OSzPfCwzvnhGy3m6QjRFciSaGDKaU45ZQXWLkygy1brmf06IUhX43k88Hu3bB5M2zaBHv2mHrw8vLmZVmZqb5prWEwLs6c4FNSYORI6Nu3+Wr74BIf37xedMg8QVyIjiVJIQAiIoZw8slPsW3bLezd+xiDBs0OdkgBVVoK69Y118sXFjYvd+yArVtbVtX06mVO4LGx5oTfr5953aePOeEnJ5tlY4mQHr5CdBpJCgHSt+9NlJUtYvfue4iLO4PY2InBDqlDaA3btsGXX8KyZaZs3txyHaUgMdFUxwweDOecA+npMHy4KbGxwYldCNE2SQoBopRi6NA5VFZ+w+bNV5OVtQa7PS7YYR2z0lL45hv4+mtYvty8Lm2YvCQ+Hk47Da67DsaNM9U2SUkmIUjjohDdkySFALLZYhk+/HXWrDmdbdtuIT39TVSwu5S0wu+HvXtNVc/27c3LLVvMEszV/4gRcOmlMHEiTJoEp5xyYj1ThBBdjySFAIuNnUha2kPs3n0X+/efTb9+Nwc1HrcbNmyAnBxT1qwx7QHV1c3rhIfDySfDqFFwww1w6qnmTiCmrXlxhRDdniSFTjBw4G9xuf7Hjh2/JDZ2EpGR6Z2278pKU/+/dKkpK1Y09/CJiYHMTPjRj0wCGDLEJIN+/YLfR14IERySFDqBUhaGDXuFlSsz2LhxBmPHfo3NFpjx6iUl8MUX8PnnkJ0Nq1c3j14dPx5+/WuzHDMGUlOl+kcI0ZIkhU4SFpZMevrrrF17Nlu33kB6+lsd0r5QUwMffgiffWYSwaZN5n2Hw1T73HUXTJ0K3/mODJQSQrRNkkInio8/i8GDH2XXrlns2/cYAwfeeVzb8fvN3cC8efCvf5kqopgY0/h73XUwebJpA3A6O/gAhBA9niSFTjZgwK+prFzJrl13ExU1loSEs9v1Pa1h40Z46y145RUzKjgqCmbMgB/8AKZMkW6gQogTJ0mhkymlGDbsZWpqNrJp01VkZa0kPDyt1XX9ftMw/M478O67pnuoxQLf/S788Y9w8cUy2lcI0bGkmTEIrNZIRox4F/CzceOl+Hw1LT7ftAluvx0GDDBjAp54AtLS4IUXzAygCxfCNddIQhBCdDy5UwiSiIiTGT78Ndavv5CtW28hLe0V3n5b8dJLpr3AbocLLjCDxS680IweFkKIQJOkEESJiefjdr/A739fzaJFtbhc4QwZAo8/Dtdfb6aMEEKIziRJIQj274d//hPmz4d1636Mzebh9NPf5fbbhzB9+hgZOCaECBpJCp2krs40Fv/jH/Dpp6YR+dRT4dln4bLLatm79348niLq6lbidA4KdrhCiBAlDc0Btnkz3HGHmUH06qvNJHN3322Wy5fDrbdCcnI0I0f+G7/fw4YNlxzW8CyEEJ1FkkIAeDzw6qtmEFl6urkbOOssc4ewaxc8+CAMHdryOxERp5Ce/hpVVTls3XoL5hHWQgjRuSQpdCC/37QVpKebAWUFBfDYY6Yb6VtvmfEFR5trKDHxAlJT/0Bh4Wvk5f2l8wIXQgDg8/v4bPdnLNu3DFetK9jhAFDnrWNn6U4W717MluItAd+ftCl0AK3h/ffh3nth/XoYPdr8fOGFxz7b6KBBd1NVtZqdO2cRFTWa+PhpgQlahISCqgIW7lxI/5j+TEiZQJSjYyZiLKouYm/5XqLDoolzxhEbFkuYLey4t1frrWVL8RY2FG7AVeuid2Rv+kT2oU9UH/pE9iHOGXfcc4VV1FWws3QniRGJDIgZ0Op2ytxlzF0zl2dXPMse156m91OiU0hPSmdE0gj6Rfejqr6KyvpKKusqzbK+kqr6Kqrqq6iur2567fV7CbOFEWYNa7G0W+zYrfYWS5vFhtVixaqsTUuP30NeRR57y/dyoOpAUzyzTpvFY2c/dly/h/ZS3a2aYty4cXrlypXBDqPJkiVw553miWRDhsAf/gBXXHFis496vZWsXv0d6uryGDPmc6KiRnVYvKLjaa2p99VTVV9FWW0Zpe7SFiU2LJbJgyYzKHZQqyekOm8dX+77ks92f0aZu4xIRySR9simpdNmJrHSaLTWaMz/2cYT1oDYAVhU8x9cVX0V725+l9fWv8aiXYvwaR8AFmUho08Gpw04jdMGnEb/mP4UVRdRVFPUtHTVuohyRBHnjGsqsWGxlLpL2Vi00ZTCjRTVFB12HE6bkzhnHH0i+5ASk0K/qH6kxKSQEp1CQngCbq+bGk8NNZ4a3B43lfWVbCvZxobCDWwv3Y5f+4/4O3ZYHcQ744kPj2+xjHJEHXbi1Vqzy7WL7SXb2V66ncLqwqbtpESnNB3/aQNOw2lz8sKKF5i/bj41nhqmDprKreNvJdwezsbCjU3HvLloM26vG4BIeyTRYdFEO6KbllGOqBbFqqzU+eqo89aZZcNrj9+Dx+dpsfT6vfj8Pnza17S0Kiv9Y/ozMHZgizKs1zD6Rfc7jr9SUEqt0lqPa3M9SQrHZ+dOmDXL9Cjq3x/uv9+MLbB10L1Xbe1eVq/+DqAYO/YrnM4BHbPhEKW1ZlvJNj7c9iH/2fEf3F43vSN70zuit7kqjepDtCOaGk9N09VeU/GYq8BqT3WryxpPTdOJ92gGxAxgyqApTBk0hRFJI1iet5xPd31Kdm42bq8bq7IS64ylxlNDrbe23ccWaY9keNJw0pPSqfPW8f7W93F73aTGpXLNyGu4LP0yiqqLWLZvGV/u+5Kv87+mqr7qsO3EhsUS54yjxlNDWW0ZXr+3xecxYTFNV80jkkaQFp9GjacGV60LV62L8tpyymrLOFB1gPzKfPIr8imoLjhi3BZl4aT4kxjZe2SLkhieSGF1IQXVBRRUFVBQXUBhdSGl7lLKastw1booc5dRVltGVX1V84nXW9eUMPtG9WVI4hCGJJhyUsJJHKg6wLJ9y1i2bxm55blNcThtTq4ddS23TbiNjOSMVmP1az9V9VVE2iOxWrrnJGOSFAKkvNzMO/TUU2bU8V13md5F4eEdv6+qqnWsWTOZsLD+jBnzBXZ71x/WXOutJdeVy27XbnJduXj8HhxWR4urOb/2H/afvqimCIUizBbWYv1oR7Q5eR9U4pxxfFv5LXtce9hdtpvdLlO01gyKG8Sg2IYSN4gIewSf7vyUD7d/yI7SHQCM7D2S3pG9KawupLC6kOKa4lavUhuv1qMcUS2u3JuWh7wX5Ygi3hlPQnhCU4kPj6egqoDs3Gyy92azdM/SFifKYb2Gcfbgszl78NlMTZ1KTJh5vJ3P76PGU0O1pxq3x41SCoVqWmo0ua5cNhVtMqXYLD0+D5enX861o67ltAGntXpn4vV72VC4geKaYpIikkiKTKJXRC8cVkfTOlpr3F530wk4JiyG/jH9j7kKp95Xz/7K/bhqXUTYI1oUh9XRoY+n1Vqbq27ta7q7OpL8iny+yvuKouoiZoyYQa+IXh0WR1clSaGDaQ0vv2y6kxYVwcyZJjn0O747uXYrK/uMdevOJSZmIqNH/xer9fA/9oq6CkrdpS2umOp8dbg9bqo91Ydd+bo9bmq9tab4zFKhiA2LJdYZ27SMCYvBYXVgs9halOr66qYTamF1IYU1heyv3M9u126+rfz2mI4vMTyRPlF96B3ZG+CwY6ioqzjiSRsg3BZOWnwaaXFmUsHc8lxyXblU1lc2rRNmDWPa4GlcOORCLjjlAgbGDmyxDZ/fR6m7lIq6CpMAHJFE2CNaVMl0FK0120u3s6loE1l9sxgQK3eAonNIUuhAxcVw443wwQdw+unwl79AVlbz537tb9cJpM5bR1ltGRV1FS2K2+MmMSKx6Ur40Ku2goI32Lz5apKSLic9/Q3AwtaSrXyw9QM+2PYBX+778qj1sYdy2pyHFZ/fR3ldOeW15dT56tq9rYTwhKZGwdS4VNLi0ppO0mnxaYRZw1rWrXrrUErRO7I3SRFJ2K32Nvfh135K3aVNSajMXUZyVDKD4wfTO7L3YVebWmtctS5yy3Nx1boY3288kQ55wpAIbZIUOsiSJXDttSYxPPYY/OIX4NNevtz7Je9vfZ/3t73PztKdJIQn0CuiF70iepEUmUScM47y2vIWV9TldeXt3m+cM46kiKSmbYbrAyj3CsIiMlhWXN1UFZLRJ4MLT7mQk+JPOqy3g9PmJDqsZSNYe66A67x1lNeVU1FXgcfnwad9eP1evH4vHp+HcHs4fSL70CuiV7tO6kKI4JOkcIK8XtOT6KGHzMPsX39dkx/1Af/a9C8+2vYRZbVlOKwOpqVNY0zyGMpqyyiuKW4qZbVlxIbFtqgL7xPZh/jweGLDTNVMY3HanC2uhBvr2xu3VeIuobimmMKq/Wjt47R+Q7gi43YuPOXCw6pChBCiNe1NCjJOoRX795tupV98YXoU3fl/e7lj8Y/5ZMcnJIYnMn3odKYPnc7Zg88mOiy60+Ly+31s2nw9xUWvMaSfJkUSghCig0lSOITHA5ddBmvXwrz5fqqGvciEeXeiteaZ857hp+N+GrQuaRaLlfThf2ejv5Lt23+OzRZHnz7XBiUWIUTPFPJJYV3BOvZX7ic5KpnkqGSe/L9efPWVlSfmbedl301kf5zN2YPPZs7355AalxrscLFY7KSnv8n69eexefP1WK2x9Op1YbDDEkL0ECHdplBYXcjgpwZT7aluftNvIVz3xu8oI9wezhPnPMHMzJkd2p+6I3i9FeTknEVNzUZGj15IXNyUYIckhOjCpE2hHR754hHcXjfvXPEOJWV+fnXvAcJ7FXDhVQeICHPwu8m/o29032CH2SqbLYbRo/9DTs4U1q//PhkZi4iJGR/ssIQQ3VzIJoW8ijyeX/E812dcz0VDL+F73wPfV7B4BYwYEezo2sfhSGL06P+SkzOFnJypDB/+KklJlwY7LCFENxayU2c/lP0Qfu3nvqn38cgjsGgRPP1090kIjZzOAYwdu5yoqEw2bryM3Nz/k2cxCCGOW0gmhV1lu3h5zcvcknUL+RtTue8+uOoq+NGPgh3Z8XE4+pCR8Rm9e1/D7t2/Y8uW6/H72z8qWQghGgU0KSilzlVKbVVK7VBKzW7l85lKqSKlVE5DuSmQ8TR6YOkD2Cw27p58Nz/6EQwaBC+9dOzPPuhKrFYnw4e/SmrqgxQUvEJOzjTq6w+f3lgIIY4mYElBKWUFngPOA9KBq5VS6a2s+qbWOrOh/C1Q8TTaXLSZV9e9ys/H/5z6kn5s3Qq33w4xMYHec+AppUhNvYf09LeoqlrFqlXjqapaG+ywhBDdSCDvFCYAO7TWu7TW9cAbwEUB3F+73LfkPiLtkdx5+p0sXWremzo1uDF1tN69Z5CZ+Tlae1m9+jQKC98KdkhCiG4ikEkhBdh30M95De8d6jKl1Dql1AKlVKvzCCulblFKrVRKrSwqOv4qkTX717Bg0wJ+NfFX9IroxdKlkJAAI0ce9ya7rJiYcWRlrSQqagybNl3Jrl13o9vxIBghRGgLdkPzB0Cq1no08Ckwr7WVtNZztNbjtNbjkpKSjntn9yy+h3hnPHd85w4Ali6FyZNP7NGZXVlYWDKZmZ/Rt+8t7N37MOvXfx+Pp2s8jFwI0TUF8nSYDxx85d+/4b0mWusSrXVjN5m/AVkEyLJ9y/h4+8f8dtJviXXGkpcHu3b1vKqjQ1ksDoYOfYlTTnmRsrJPWb16AlVV64MdlhCiiwpkUlgBDFFKpSmlHMBVwPsHr6CUOni48HRgc6CCsVlsXDDkAm6bcBtAj21POJJ+/X5MRsZivN4KVq0aT17eszKeQQhxmIAlBa21F/g5sBBzsn9La71RKfUHpdT0htV+oZTaqJRaC/wCmBmoeCakTODDaz5segLX0qUQGwsZrT+nu0eKizud8ePXER8/jR07bmPDhunSbVUI0ULITog3dCgMGQIfftgBQXUzWmvy859l585Z2O3xDBs2j4SEc4IdlhAigNo7IV4PbWI9uv37Ydu20Kk6OpRSiv79byMr6xtstgTWrfse27f/Eq+3/Y8LFUL0TCGZFLKzzTJUk0KjqKjRZGWtJCXl5+TnP8PXXw9l//5/oLU/2KEJIYIkJJPC0qUQFQVjxwY7kuCzWsMZMuQZsrJWEB4+mK1bb2D16tOoqFgR7NCEEEEQsklh0iSwhezE4YeLjs5izJgvGDZsPnV1uaxefSpbttxEfX1BsEMTQnSikEsKRUWwaZNUHbVGKQvJyT9gwoStDBjwawoK5vH110PYu/dRfL7aYIcnhOgEIZcUpD2hbTZbDCed9Djjx28kLu5Mdu2azYoV6RQWLpCxDUL0cCGXFJYuhfBwGNdmxywREXEKo0a9R0bGIqzWKDZtmkFOzlQqK3OCHZoQIkBCMimcdho4HMGOpPuIj5/GuHFrOOWUl6ip2cKqVePYtetuqVISogcKqaRQWgrr10vV0fFQykq/frcwYcJWkpN/yN69D7Nq1RjKy5cFOzQhRAcKqaTw+eegtSSFE2FGQM9l9OiF+Hxu1qw5vWHgW1WwQxNCdICQSgpLl0JYGEyYEOxIur+EhHMYP349/fr9jPz8p1mxYjg7dvwGl2spfr832OEJIY5TSM19lJUF0dGwZEnHxhTqXK7Pyc19EJdrCVp7sNniSEg4j8TEC0lMnI7NFhXsEIUIee2d+yhkhm+Vl0NODtxzT7Aj6Xni4iYTF/dfvN5Kyso+paTkA0pKPqKw8HVstgT69/8V/fvfhs0WG+xQhRBtCJnqoy++AL9f2hMCyWaLJinpUoYN+zunnXaAzMxsYmNPY8+ee/nqq0Hs3n0fHk9psMMUQhxFyNwpDBoEd9wBEycGO5LQoJSl4Q5iMpWVa8jNfYjc3AfJy3uS5OQfER9/JtHRpxIWlhzsUIUQBwmpNgURXFVVG9i7948UFb2N1h4AnM5UYmImEhMzkcTE6YSHpwU5SiF6pva2KUhSEJ3O53NTVbWGiorlTaWubh8AsbGTSU6+nqSky6UNQogOJElBdCtu9x4KC//JgQPzcLu3YbE46dXrEnr3vob4+GlYreHBDlGIbk2SguiWtNZUVn7DgQPzKSx8Ha+3DIslnPj4aSQkXEBi4gU4nQOCHaYQ3Y4kBdHt+f11uFxLKSn5kJKSD6mt3Q1AZOQooqMnEBU1msjI0URFjcJuTwxytEJ0bZIURI+itaamZgslJR9RVraQqqocPJ7ips8djhRiYk4lIeFcEhK+h9M5MIjRCtH1yOA10aMopYiMHE5k5HAGDvwNWmvq6wuorl5HVdU6qqvX4nItpbj4HQAiIoaTkHAu8fHfJSJiGGFhA7BY7EE+CiG6PkkKoltSShEWlkxYWDIJCecAzXcTpaWfUFr6Cfn5z5OX92TDN6w4nQNxOtMIDx9MVNQY4uKmEhGRjlIqeAciRBcj1Ueix/L5aqisXIHbvRO3exe1tbuord2N272jqerJbu9FbOxU4uKmEht7OhERw7FanUGOXIiOJ9VHIuRZrRHExZkT/sG01tTW7sblWoLLtRSXawnFxW83fGrB6UwjMnI4ERGmOBzJ2Gyx2GyxWK2Nyyi5wxA9kiQFEXKUUoSHDyY8fDB9+94ImHESFRXLqanZ3FRKS/+L1vWtbsNqjSIyMoPo6LFERY0hOnosERHp0m4huj1JCkIA4eGphIentnjP7/dSW7sHj6cYr9eFz1eO12tKXd1eKivXsH//XPz+agCUshMWloLD0ReHox9hYWbpcPTBbu/VolitMXi9JdTXHzioFGCzJRATM6Ehwch/T9H55K9OiCOwWGxERJwMnHzEdbT24XbvoLJyDdXVa6mry6Oubj81NZsoK1uEz1d+nPuOIDp6bMN4jMyGxJKIzZaI3Z4o1VciYCQpCHEClLISETGUiIihwFWHfe7zVVNfX4THU9yi+HwV2O2JOBzJTcVu7019/QEqK7+homIFlZXfkJ//HFrXtbJfBxERQ4mOHt9QxhEVNRqLxdEJRy16Mul9JEQX5vd7qK3ddVBCKWkohVRXb6CiYgVebwlgEoXT2TjLrAb8mP/fGpstBpstHpstHrs9AZstHoejD05nWlOx2+MO27/WGr/fjVJWLJawTjtu0fGk95EQPYDFYm+4Cxna6uemJ1UulZUrqKxc2TAViEIpC6Awz9HS+HyVeL1l1NRswestw+stw++vbbEtmy2OsLD++P31+HxV+HyV+HxVmAQDFosTmy2uocRjtUahtRe/vx6t6/H769C6Hq19aO0H/IBGaz8Wi4OwsAGEhQ3A6RxIWNhAwsIGNNwlJWG3J8ldThchSUGIbsz0pDKN5L17zzim73o8LmprdzcVt3sXdXX5WCxOrNYobLZorNaohpO/D6/X1aL4fJUoZcdicWKxxKCUA4vFgVI2wNLQ5mFBKQs+n5u6ujxcrsXU1eVjEkZLVmssDkcSVmt0Q2LxAY1LcDiSWyQVp3MgFkskfr+7qfh8brT2NMQdjc0Wc9AyCoslEqs1UnqJHYUkBSFClN0eh90+hujoMZ26X7/fS339furq9lJfX4DHU0R9fWHT0vTmsqJUczHTmq3JM0cAAAe3SURBVOynouIrioreQmvvCcWglAOrNfKQ5NGYQKLR2o/fX9uigL8pqTSXqIbG/5a9y5RSDW1Jzcfl9boa1o9rUczvpAafrxqfrxq/vwatfTgc/XA6BzTcUfVBKeuJ//LbQZKCEKJTWSw2nM4Bxz0FutY+6usLqK3di9/vxmIJx2oNx2IxRSk7fn81Xm8FPl8FXm8lPl9500nXlCr8frM0n1c0dDXOw+erAKwNd0DNRSkbXm8pdXV5Dd9t3I67nccdgd9fc1zHrJQNhyOF/v1vY8CAXx/XNtpLkoIQoltRykpYWD/CwvoFOxTAPEmwuSNAER5PEVprHI7e2O29W7SZmGq4yhbVcEDDXUdE050IKOrr86mt3Udd3T7q6vZSW7sPh6NvwI9HkoIQQpwAqzUcq7V9dz5KWRuq7Q7v6XUoh6MXUVEZHRHiMbEEcuNKqXOVUluVUjuUUrNb+TxMKfVmw+dfK6VSAxmPEEKIowtYUlCmVeQ54DwgHbhaKZV+yGo/Asq01icDTwKPBioeIYQQbQvkncIEYIfWepc2s4q9AVx0yDoXAfMaXi8ApikZuy+EEEETyKSQAuw76Oe8hvdaXUebPmblgDxsVwghgiSgbQodRSl1i1JqpVJqZVFRUbDDEUKIHiuQSSEfOLg5vn/De62uo8wwyFig5NANaa3naK3Haa3HJSUlBShcIYQQgUwKK4AhSqk0pZQDM4Xk+4es8z5wfcPry4HPdHeboU8IIXqQgI1T0Fp7lVI/BxYCVmCu1nqjUuoPwEqt9fvAy8ArSqkdQCmtzT0shBCi03S7qbOVUkVA7nF+vRdQ3IHhdCU99djkuLqfnnps3f24Bmmt26x/73ZJ4UQopVa2Zz7x7qinHpscV/fTU4+tpx7XobpF7yMhhBCdQ5KCEEKIJqGWFOYEO4AA6qnHJsfV/fTUY+upx9VCSLUpCCGEOLpQu1MQQghxFCGTFNqaxrs7UUrNVUoVKqU2HPReglLqU6XU9oZlfDBjPB5KqQFKqcVKqU1KqY1KqV82vN+tj00p5VRKfaOUWttwXA80vJ/WMGX8joYp5Lvlk+uVUlal1Bql1IcNP/eU49qjlFqvlMpRSq1seK9b/y22R0gkhXZO492d/AM495D3ZgP/01oPAf7X8HN34wV+rbVOByYCtzb8O3X3Y6sDztJaZwCZwLlKqYmYqeKfbJg6vgwzlXx39Etg80E/95TjAjhTa515UFfU7v632KaQSAq0bxrvbkNrnY0ZAX6wg6chnwdc3KlBdQCt9X6t9eqG15WYE00K3fzYtFHV8KO9oWjgLMyU8dANjwtAKdUfuAD4W8PPih5wXEfRrf8W2yNUkkJ7pvHu7vporfc3vD4A9AlmMCeq4Sl8Y/6/vbsHkasKwzj+f/xAYlZcDBFE0RAtFCGsCAFNhEXRQoJY+AEmQWzS2KSQSEQRAmn9KARTWERcxahZbY0xLKbwK7qoaBqDRVJkG6NEUGTzWJwz13FX2WFkd/bOPL9m9p47XM4L5+5777lz3wN8xhDEVqdYZoE54AjwI3CuloyH9o7Jl4A9wIW6vY7hiAtK4v5Q0glJu2pb68fiUrJG8xCybUmt/VmZpDHgPWC37V+7111qa2y254EJSePANHDzgLv0v0naBszZPiFpctD9WQZbbZ+RdDVwRNLJ7p1tHYtLGZU7hV7KeLfdWUnXANTPuQH3py+SLqUkhCnbh2vzUMQGYPsccAy4AxivJeOhnWNyC/CApJ8oU7J3Ay/T/rgAsH2mfs5REvlmhmgs/pdRSQq9lPFuu+4y5I8DHwywL32p89GvAT/YfqFrV6tjk7S+3iEgaQ1wL+V5yTFKyXhoYVy299q+zvYGyjn1se3ttDwuAElrJV3R+Ru4D/iOlo/FXozMy2uS7qfMf3bKeO8fcJf6JuktYJJStfEs8DzwPnAIuJ5SRfYR2wsfRq9qkrYCnwDf8vcc9TOU5wqtjU3SJspDyYspF2KHbO+TtJFyhX0V8DWww/Yfg+tp/+r00VO2tw1DXDWG6bp5CfCm7f2S1tHisdiLkUkKERGxtFGZPoqIiB4kKURERCNJISIiGkkKERHRSFKIiIhGkkLECpI02akmGrEaJSlEREQjSSHiX0jaUddAmJV0oBa0Oy/pxbomwlFJ6+t3JyR9KukbSdOdGvuSbpL0UV1H4StJN9bDj0l6V9JJSVPqLu4UMWBJChELSLoFeBTYYnsCmAe2A2uBL23fCsxQ3iQHeB142vYmytvYnfYp4JW6jsKdQKe65m3AbsraHhspNYQiVoVUSY1Y7B7gduCLehG/hlL47ALwdv3OG8BhSVcC47ZnavtB4J1aN+da29MAtn8HqMf73Pbpuj0LbACOL39YEUtLUohYTMBB23v/0Sg9t+B7/daI6a4DNE/Ow1hFMn0UsdhR4KFaR7+zLu8NlPOlU/3zMeC47V+AnyXdVdt3AjN15bjTkh6sx7hM0uUrGkVEH3KFErGA7e8lPUtZdesi4E/gSeA3YHPdN0d57gClhPKr9Z/+KeCJ2r4TOCBpXz3GwysYRkRfUiU1okeSztseG3Q/IpZTpo8iIqKRO4WIiGjkTiEiIhpJChER0UhSiIiIRpJCREQ0khQiIqKRpBAREY2/ABZUcznJM/otAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 405us/sample - loss: 1.2685 - acc: 0.6147\n",
      "Loss: 1.2684546218359087 Accuracy: 0.61474556\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1048 - acc: 0.3253\n",
      "Epoch 00001: val_loss improved from inf to 1.58550, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/001-1.5855.hdf5\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 2.1048 - acc: 0.3253 - val_loss: 1.5855 - val_acc: 0.5043\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5341 - acc: 0.5070\n",
      "Epoch 00002: val_loss improved from 1.58550 to 1.40019, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/002-1.4002.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 1.5341 - acc: 0.5070 - val_loss: 1.4002 - val_acc: 0.5563\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3622 - acc: 0.5757\n",
      "Epoch 00003: val_loss improved from 1.40019 to 1.27362, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/003-1.2736.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 1.3622 - acc: 0.5757 - val_loss: 1.2736 - val_acc: 0.6124\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2339 - acc: 0.6194\n",
      "Epoch 00004: val_loss improved from 1.27362 to 1.19305, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/004-1.1930.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 1.2338 - acc: 0.6194 - val_loss: 1.1930 - val_acc: 0.6366\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1280 - acc: 0.6567\n",
      "Epoch 00005: val_loss did not improve from 1.19305\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 1.1280 - acc: 0.6567 - val_loss: 1.2151 - val_acc: 0.6310\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0378 - acc: 0.6860\n",
      "Epoch 00006: val_loss improved from 1.19305 to 1.05215, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/006-1.0522.hdf5\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 1.0377 - acc: 0.6860 - val_loss: 1.0522 - val_acc: 0.6846\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9573 - acc: 0.7113\n",
      "Epoch 00007: val_loss did not improve from 1.05215\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.9573 - acc: 0.7113 - val_loss: 1.0528 - val_acc: 0.6839\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7315\n",
      "Epoch 00008: val_loss improved from 1.05215 to 0.99992, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/008-0.9999.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.8880 - acc: 0.7315 - val_loss: 0.9999 - val_acc: 0.7011\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8250 - acc: 0.7508\n",
      "Epoch 00009: val_loss did not improve from 0.99992\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.8250 - acc: 0.7508 - val_loss: 1.0035 - val_acc: 0.6914\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7717 - acc: 0.7645\n",
      "Epoch 00010: val_loss did not improve from 0.99992\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.7720 - acc: 0.7644 - val_loss: 1.0328 - val_acc: 0.6820\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7184 - acc: 0.7801\n",
      "Epoch 00011: val_loss improved from 0.99992 to 0.95229, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/011-0.9523.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.7184 - acc: 0.7801 - val_loss: 0.9523 - val_acc: 0.7170\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.7947\n",
      "Epoch 00012: val_loss did not improve from 0.95229\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.6669 - acc: 0.7947 - val_loss: 0.9806 - val_acc: 0.7056\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6269 - acc: 0.8078\n",
      "Epoch 00013: val_loss improved from 0.95229 to 0.95104, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/013-0.9510.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.6268 - acc: 0.8079 - val_loss: 0.9510 - val_acc: 0.7163\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.8188\n",
      "Epoch 00014: val_loss did not improve from 0.95104\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.5825 - acc: 0.8188 - val_loss: 0.9715 - val_acc: 0.7165\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.8323\n",
      "Epoch 00015: val_loss improved from 0.95104 to 0.94256, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_6_conv_checkpoint/015-0.9426.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.5426 - acc: 0.8323 - val_loss: 0.9426 - val_acc: 0.7277\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8446\n",
      "Epoch 00016: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.5012 - acc: 0.8446 - val_loss: 0.9798 - val_acc: 0.7165\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8514\n",
      "Epoch 00017: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.4772 - acc: 0.8513 - val_loss: 1.0204 - val_acc: 0.7170\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8609\n",
      "Epoch 00018: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.4500 - acc: 0.8609 - val_loss: 0.9895 - val_acc: 0.7195\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8670\n",
      "Epoch 00019: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.4188 - acc: 0.8671 - val_loss: 1.0171 - val_acc: 0.7172\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8795\n",
      "Epoch 00020: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.3832 - acc: 0.8795 - val_loss: 0.9880 - val_acc: 0.7261\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8837\n",
      "Epoch 00021: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.3670 - acc: 0.8837 - val_loss: 0.9864 - val_acc: 0.7256\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8902\n",
      "Epoch 00022: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.3495 - acc: 0.8902 - val_loss: 1.0187 - val_acc: 0.7305\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8945\n",
      "Epoch 00023: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.3276 - acc: 0.8945 - val_loss: 1.0440 - val_acc: 0.7279\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8995\n",
      "Epoch 00024: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.3145 - acc: 0.8995 - val_loss: 1.0495 - val_acc: 0.7298\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9066\n",
      "Epoch 00025: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2917 - acc: 0.9066 - val_loss: 1.0929 - val_acc: 0.7254\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9130\n",
      "Epoch 00026: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.2730 - acc: 0.9129 - val_loss: 1.0548 - val_acc: 0.7312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9125\n",
      "Epoch 00027: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.2698 - acc: 0.9125 - val_loss: 1.0981 - val_acc: 0.7258\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9168\n",
      "Epoch 00028: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.2561 - acc: 0.9169 - val_loss: 1.0392 - val_acc: 0.7356\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9216\n",
      "Epoch 00029: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2415 - acc: 0.9216 - val_loss: 1.1322 - val_acc: 0.7305\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9265\n",
      "Epoch 00030: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.2296 - acc: 0.9265 - val_loss: 1.0776 - val_acc: 0.7431\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9317\n",
      "Epoch 00031: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2140 - acc: 0.9317 - val_loss: 1.0658 - val_acc: 0.7487\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9307\n",
      "Epoch 00032: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.2123 - acc: 0.9307 - val_loss: 1.1776 - val_acc: 0.7319\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9369\n",
      "Epoch 00033: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2016 - acc: 0.9369 - val_loss: 1.0938 - val_acc: 0.7449\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9363\n",
      "Epoch 00034: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1983 - acc: 0.9363 - val_loss: 1.1018 - val_acc: 0.7531\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9392\n",
      "Epoch 00035: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1892 - acc: 0.9392 - val_loss: 1.1027 - val_acc: 0.7475\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9414\n",
      "Epoch 00036: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1812 - acc: 0.9414 - val_loss: 1.2038 - val_acc: 0.7384\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9460\n",
      "Epoch 00037: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1687 - acc: 0.9460 - val_loss: 1.1439 - val_acc: 0.7526\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9438\n",
      "Epoch 00038: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1728 - acc: 0.9438 - val_loss: 1.1193 - val_acc: 0.7484\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9492\n",
      "Epoch 00039: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1611 - acc: 0.9491 - val_loss: 1.1374 - val_acc: 0.7563\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9459\n",
      "Epoch 00040: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1697 - acc: 0.9459 - val_loss: 1.1447 - val_acc: 0.7501\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9517\n",
      "Epoch 00041: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1517 - acc: 0.9517 - val_loss: 1.1300 - val_acc: 0.7587\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9533\n",
      "Epoch 00042: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1461 - acc: 0.9533 - val_loss: 1.1461 - val_acc: 0.7552\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9539\n",
      "Epoch 00043: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1438 - acc: 0.9538 - val_loss: 1.2282 - val_acc: 0.7477\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9533\n",
      "Epoch 00044: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1484 - acc: 0.9533 - val_loss: 1.1706 - val_acc: 0.7494\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9582\n",
      "Epoch 00045: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1369 - acc: 0.9582 - val_loss: 1.1558 - val_acc: 0.7680\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9582\n",
      "Epoch 00046: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1341 - acc: 0.9582 - val_loss: 1.2443 - val_acc: 0.7549\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9583\n",
      "Epoch 00047: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1328 - acc: 0.9583 - val_loss: 1.1965 - val_acc: 0.7608\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9596\n",
      "Epoch 00048: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1283 - acc: 0.9597 - val_loss: 1.1962 - val_acc: 0.7631\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9609\n",
      "Epoch 00049: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1271 - acc: 0.9609 - val_loss: 1.2137 - val_acc: 0.7589\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9622\n",
      "Epoch 00050: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1220 - acc: 0.9622 - val_loss: 1.2098 - val_acc: 0.7617\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9622\n",
      "Epoch 00051: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1190 - acc: 0.9622 - val_loss: 1.2957 - val_acc: 0.7508\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9629\n",
      "Epoch 00052: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1182 - acc: 0.9629 - val_loss: 1.2070 - val_acc: 0.7699\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9657\n",
      "Epoch 00053: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1134 - acc: 0.9657 - val_loss: 1.4257 - val_acc: 0.7424\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9642\n",
      "Epoch 00054: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.1167 - acc: 0.9642 - val_loss: 1.2360 - val_acc: 0.7713\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9658\n",
      "Epoch 00055: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1098 - acc: 0.9658 - val_loss: 1.2241 - val_acc: 0.7563\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9657\n",
      "Epoch 00056: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1094 - acc: 0.9656 - val_loss: 1.2230 - val_acc: 0.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9661\n",
      "Epoch 00057: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1088 - acc: 0.9661 - val_loss: 1.1920 - val_acc: 0.7675\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9663\n",
      "Epoch 00058: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1093 - acc: 0.9663 - val_loss: 1.2358 - val_acc: 0.7647\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9676\n",
      "Epoch 00059: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1062 - acc: 0.9676 - val_loss: 1.2034 - val_acc: 0.7738\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9705\n",
      "Epoch 00060: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1011 - acc: 0.9705 - val_loss: 1.2280 - val_acc: 0.7612\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9686\n",
      "Epoch 00061: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1002 - acc: 0.9686 - val_loss: 1.2461 - val_acc: 0.7612\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9718\n",
      "Epoch 00062: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0956 - acc: 0.9718 - val_loss: 1.2183 - val_acc: 0.7722\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9720\n",
      "Epoch 00063: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0935 - acc: 0.9720 - val_loss: 1.1978 - val_acc: 0.7761\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9716\n",
      "Epoch 00064: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0945 - acc: 0.9716 - val_loss: 1.2337 - val_acc: 0.7754\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9720\n",
      "Epoch 00065: val_loss did not improve from 0.94256\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0934 - acc: 0.9720 - val_loss: 1.2681 - val_acc: 0.7619\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 427us/sample - loss: 1.0257 - acc: 0.6993\n",
      "Loss: 1.025704760950055 Accuracy: 0.6992731\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2545 - acc: 0.2699\n",
      "Epoch 00001: val_loss improved from inf to 1.62630, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/001-1.6263.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 2.2543 - acc: 0.2700 - val_loss: 1.6263 - val_acc: 0.4759\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5802 - acc: 0.4937\n",
      "Epoch 00002: val_loss improved from 1.62630 to 1.43795, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/002-1.4380.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 1.5802 - acc: 0.4937 - val_loss: 1.4380 - val_acc: 0.5514\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3952 - acc: 0.5569\n",
      "Epoch 00003: val_loss improved from 1.43795 to 1.28941, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/003-1.2894.hdf5\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 1.3953 - acc: 0.5569 - val_loss: 1.2894 - val_acc: 0.5993\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2801 - acc: 0.5998\n",
      "Epoch 00004: val_loss improved from 1.28941 to 1.22535, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/004-1.2254.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 1.2801 - acc: 0.5998 - val_loss: 1.2254 - val_acc: 0.6217\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1883 - acc: 0.6311\n",
      "Epoch 00005: val_loss improved from 1.22535 to 1.08405, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/005-1.0840.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 1.1883 - acc: 0.6311 - val_loss: 1.0840 - val_acc: 0.6713\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0993 - acc: 0.6636\n",
      "Epoch 00006: val_loss improved from 1.08405 to 1.04794, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/006-1.0479.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 1.0992 - acc: 0.6637 - val_loss: 1.0479 - val_acc: 0.6769\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0241 - acc: 0.6867\n",
      "Epoch 00007: val_loss improved from 1.04794 to 0.96761, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/007-0.9676.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 1.0240 - acc: 0.6867 - val_loss: 0.9676 - val_acc: 0.7151\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9648 - acc: 0.7055\n",
      "Epoch 00008: val_loss did not improve from 0.96761\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.9650 - acc: 0.7054 - val_loss: 1.0001 - val_acc: 0.7067\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9035 - acc: 0.7266\n",
      "Epoch 00009: val_loss improved from 0.96761 to 0.90768, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/009-0.9077.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.9035 - acc: 0.7266 - val_loss: 0.9077 - val_acc: 0.7307\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8576 - acc: 0.7398\n",
      "Epoch 00010: val_loss improved from 0.90768 to 0.87298, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/010-0.8730.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.8577 - acc: 0.7398 - val_loss: 0.8730 - val_acc: 0.7365\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7530\n",
      "Epoch 00011: val_loss improved from 0.87298 to 0.87204, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/011-0.8720.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.8114 - acc: 0.7530 - val_loss: 0.8720 - val_acc: 0.7368\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7657\n",
      "Epoch 00012: val_loss improved from 0.87204 to 0.82970, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/012-0.8297.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.7704 - acc: 0.7657 - val_loss: 0.8297 - val_acc: 0.7501\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7317 - acc: 0.7788\n",
      "Epoch 00013: val_loss improved from 0.82970 to 0.80145, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/013-0.8015.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.7317 - acc: 0.7788 - val_loss: 0.8015 - val_acc: 0.7643\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.7893\n",
      "Epoch 00014: val_loss did not improve from 0.80145\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.6932 - acc: 0.7893 - val_loss: 0.8244 - val_acc: 0.7543\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6559 - acc: 0.7999\n",
      "Epoch 00015: val_loss improved from 0.80145 to 0.76113, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/015-0.7611.hdf5\n",
      "36805/36805 [==============================] - 34s 910us/sample - loss: 0.6558 - acc: 0.7999 - val_loss: 0.7611 - val_acc: 0.7794\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.8107\n",
      "Epoch 00016: val_loss improved from 0.76113 to 0.74545, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/016-0.7455.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.6255 - acc: 0.8107 - val_loss: 0.7455 - val_acc: 0.7789\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8170\n",
      "Epoch 00017: val_loss improved from 0.74545 to 0.74090, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/017-0.7409.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.6012 - acc: 0.8170 - val_loss: 0.7409 - val_acc: 0.7841\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.8285\n",
      "Epoch 00018: val_loss improved from 0.74090 to 0.73467, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/018-0.7347.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.5661 - acc: 0.8284 - val_loss: 0.7347 - val_acc: 0.7845\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.8351\n",
      "Epoch 00019: val_loss did not improve from 0.73467\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.5436 - acc: 0.8351 - val_loss: 0.7665 - val_acc: 0.7801\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.8418\n",
      "Epoch 00020: val_loss did not improve from 0.73467\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.5127 - acc: 0.8418 - val_loss: 0.7959 - val_acc: 0.7757\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8482\n",
      "Epoch 00021: val_loss improved from 0.73467 to 0.72508, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/021-0.7251.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.4875 - acc: 0.8482 - val_loss: 0.7251 - val_acc: 0.7897\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8569\n",
      "Epoch 00022: val_loss improved from 0.72508 to 0.69585, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_7_conv_checkpoint/022-0.6958.hdf5\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.4658 - acc: 0.8569 - val_loss: 0.6958 - val_acc: 0.7927\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8624\n",
      "Epoch 00023: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.4419 - acc: 0.8624 - val_loss: 0.7300 - val_acc: 0.7869\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8714\n",
      "Epoch 00024: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.4168 - acc: 0.8714 - val_loss: 0.8138 - val_acc: 0.7817\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8759\n",
      "Epoch 00025: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.3973 - acc: 0.8759 - val_loss: 0.7051 - val_acc: 0.7987\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8803\n",
      "Epoch 00026: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3785 - acc: 0.8803 - val_loss: 0.7324 - val_acc: 0.7997\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8862\n",
      "Epoch 00027: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.3626 - acc: 0.8862 - val_loss: 0.6992 - val_acc: 0.8022\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8923\n",
      "Epoch 00028: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.3461 - acc: 0.8923 - val_loss: 0.7085 - val_acc: 0.8036\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8938\n",
      "Epoch 00029: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3342 - acc: 0.8938 - val_loss: 0.7467 - val_acc: 0.7992\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9013\n",
      "Epoch 00030: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.3180 - acc: 0.9012 - val_loss: 0.9688 - val_acc: 0.7717\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9019\n",
      "Epoch 00031: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3126 - acc: 0.9019 - val_loss: 0.7623 - val_acc: 0.7997\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9107\n",
      "Epoch 00032: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.2832 - acc: 0.9107 - val_loss: 0.7357 - val_acc: 0.8113\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9111\n",
      "Epoch 00033: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2794 - acc: 0.9111 - val_loss: 0.7588 - val_acc: 0.7936\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9145\n",
      "Epoch 00034: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.2680 - acc: 0.9144 - val_loss: 0.7513 - val_acc: 0.8029\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9171\n",
      "Epoch 00035: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.2612 - acc: 0.9171 - val_loss: 0.7651 - val_acc: 0.8095\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9199\n",
      "Epoch 00036: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.2448 - acc: 0.9199 - val_loss: 0.7391 - val_acc: 0.8146\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9200\n",
      "Epoch 00037: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2466 - acc: 0.9200 - val_loss: 0.7199 - val_acc: 0.8164\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9252\n",
      "Epoch 00038: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.2375 - acc: 0.9252 - val_loss: 0.7213 - val_acc: 0.8148\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9264\n",
      "Epoch 00039: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2265 - acc: 0.9264 - val_loss: 0.8138 - val_acc: 0.8153\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9308\n",
      "Epoch 00040: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2154 - acc: 0.9307 - val_loss: 0.7515 - val_acc: 0.8104\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9342\n",
      "Epoch 00041: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.2047 - acc: 0.9342 - val_loss: 0.8174 - val_acc: 0.8097\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9309\n",
      "Epoch 00042: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2115 - acc: 0.9309 - val_loss: 0.7703 - val_acc: 0.8034\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9335\n",
      "Epoch 00043: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2000 - acc: 0.9335 - val_loss: 0.8639 - val_acc: 0.7964\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9357\n",
      "Epoch 00044: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1981 - acc: 0.9357 - val_loss: 0.7587 - val_acc: 0.8213\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9390\n",
      "Epoch 00045: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1888 - acc: 0.9390 - val_loss: 0.7738 - val_acc: 0.8153\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9415\n",
      "Epoch 00046: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1826 - acc: 0.9416 - val_loss: 0.8107 - val_acc: 0.8097\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9429\n",
      "Epoch 00047: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.1765 - acc: 0.9429 - val_loss: 0.8497 - val_acc: 0.7983\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9459\n",
      "Epoch 00048: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1720 - acc: 0.9459 - val_loss: 0.7653 - val_acc: 0.8230\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9458\n",
      "Epoch 00049: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1671 - acc: 0.9458 - val_loss: 0.7504 - val_acc: 0.8318\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9456\n",
      "Epoch 00050: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1668 - acc: 0.9456 - val_loss: 0.7591 - val_acc: 0.8288\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9471\n",
      "Epoch 00051: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1655 - acc: 0.9471 - val_loss: 0.8141 - val_acc: 0.8139\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9525\n",
      "Epoch 00052: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1495 - acc: 0.9525 - val_loss: 0.7793 - val_acc: 0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9484\n",
      "Epoch 00053: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1587 - acc: 0.9483 - val_loss: 0.7862 - val_acc: 0.8251\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9503\n",
      "Epoch 00054: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1506 - acc: 0.9503 - val_loss: 0.8024 - val_acc: 0.8251\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9525\n",
      "Epoch 00055: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1477 - acc: 0.9525 - val_loss: 0.8297 - val_acc: 0.8246\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9534\n",
      "Epoch 00056: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1457 - acc: 0.9534 - val_loss: 0.8158 - val_acc: 0.8230\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9533\n",
      "Epoch 00057: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1433 - acc: 0.9533 - val_loss: 0.7501 - val_acc: 0.8300\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9570\n",
      "Epoch 00058: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1358 - acc: 0.9570 - val_loss: 0.8009 - val_acc: 0.8239\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9567\n",
      "Epoch 00059: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1352 - acc: 0.9567 - val_loss: 0.8095 - val_acc: 0.8272\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9543\n",
      "Epoch 00060: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1383 - acc: 0.9544 - val_loss: 0.7638 - val_acc: 0.8314\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9569\n",
      "Epoch 00061: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.1334 - acc: 0.9569 - val_loss: 0.8057 - val_acc: 0.8332\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9560\n",
      "Epoch 00062: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1343 - acc: 0.9560 - val_loss: 0.7756 - val_acc: 0.8334\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9592\n",
      "Epoch 00063: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.1268 - acc: 0.9592 - val_loss: 0.8236 - val_acc: 0.8241\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9612\n",
      "Epoch 00064: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.1195 - acc: 0.9612 - val_loss: 0.7989 - val_acc: 0.8318\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9608\n",
      "Epoch 00065: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1263 - acc: 0.9608 - val_loss: 0.7547 - val_acc: 0.8337\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9576\n",
      "Epoch 00066: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1310 - acc: 0.9576 - val_loss: 0.8315 - val_acc: 0.8246\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9630\n",
      "Epoch 00067: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1170 - acc: 0.9630 - val_loss: 0.8054 - val_acc: 0.8293\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9611\n",
      "Epoch 00068: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1227 - acc: 0.9611 - val_loss: 0.7746 - val_acc: 0.8362\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9645\n",
      "Epoch 00069: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1161 - acc: 0.9645 - val_loss: 0.7677 - val_acc: 0.8395\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9636\n",
      "Epoch 00070: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1149 - acc: 0.9636 - val_loss: 0.7910 - val_acc: 0.8314\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9636\n",
      "Epoch 00071: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1108 - acc: 0.9636 - val_loss: 0.7894 - val_acc: 0.8328\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9644\n",
      "Epoch 00072: val_loss did not improve from 0.69585\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.1139 - acc: 0.9644 - val_loss: 0.7861 - val_acc: 0.8353\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 448us/sample - loss: 0.7964 - acc: 0.7670\n",
      "Loss: 0.7964310067217422 Accuracy: 0.7669782\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4209 - acc: 0.2016\n",
      "Epoch 00001: val_loss improved from inf to 1.75422, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/001-1.7542.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 2.4209 - acc: 0.2016 - val_loss: 1.7542 - val_acc: 0.4580\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6265 - acc: 0.4757\n",
      "Epoch 00002: val_loss improved from 1.75422 to 1.40381, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/002-1.4038.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 1.6265 - acc: 0.4757 - val_loss: 1.4038 - val_acc: 0.5535\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4333 - acc: 0.5333\n",
      "Epoch 00003: val_loss improved from 1.40381 to 1.28789, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/003-1.2879.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 1.4333 - acc: 0.5333 - val_loss: 1.2879 - val_acc: 0.6026\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3176 - acc: 0.5725\n",
      "Epoch 00004: val_loss improved from 1.28789 to 1.18360, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/004-1.1836.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 1.3175 - acc: 0.5726 - val_loss: 1.1836 - val_acc: 0.6364\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2286 - acc: 0.6029\n",
      "Epoch 00005: val_loss improved from 1.18360 to 1.11762, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/005-1.1176.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 1.2287 - acc: 0.6029 - val_loss: 1.1176 - val_acc: 0.6560\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1448 - acc: 0.6371\n",
      "Epoch 00006: val_loss improved from 1.11762 to 1.07406, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/006-1.0741.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 1.1448 - acc: 0.6371 - val_loss: 1.0741 - val_acc: 0.6716\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.6631\n",
      "Epoch 00007: val_loss improved from 1.07406 to 0.97633, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/007-0.9763.hdf5\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 1.0689 - acc: 0.6631 - val_loss: 0.9763 - val_acc: 0.7025\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9798 - acc: 0.6952\n",
      "Epoch 00008: val_loss improved from 0.97633 to 0.91364, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/008-0.9136.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.9798 - acc: 0.6952 - val_loss: 0.9136 - val_acc: 0.7291\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9042 - acc: 0.7230\n",
      "Epoch 00009: val_loss improved from 0.91364 to 0.84042, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/009-0.8404.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.9042 - acc: 0.7230 - val_loss: 0.8404 - val_acc: 0.7505\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7468\n",
      "Epoch 00010: val_loss improved from 0.84042 to 0.75969, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/010-0.7597.hdf5\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.8323 - acc: 0.7468 - val_loss: 0.7597 - val_acc: 0.7789\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7695 - acc: 0.7691\n",
      "Epoch 00011: val_loss improved from 0.75969 to 0.71585, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/011-0.7159.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.7695 - acc: 0.7691 - val_loss: 0.7159 - val_acc: 0.7859\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7831\n",
      "Epoch 00012: val_loss improved from 0.71585 to 0.69121, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/012-0.6912.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.7186 - acc: 0.7831 - val_loss: 0.6912 - val_acc: 0.7985\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6672 - acc: 0.7998\n",
      "Epoch 00013: val_loss improved from 0.69121 to 0.65944, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/013-0.6594.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.6673 - acc: 0.7998 - val_loss: 0.6594 - val_acc: 0.8032\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.8117\n",
      "Epoch 00014: val_loss improved from 0.65944 to 0.64732, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/014-0.6473.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.6320 - acc: 0.8117 - val_loss: 0.6473 - val_acc: 0.8199\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.8211\n",
      "Epoch 00015: val_loss improved from 0.64732 to 0.59279, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/015-0.5928.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.5995 - acc: 0.8211 - val_loss: 0.5928 - val_acc: 0.8332\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.8296\n",
      "Epoch 00016: val_loss improved from 0.59279 to 0.55954, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/016-0.5595.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.5662 - acc: 0.8296 - val_loss: 0.5595 - val_acc: 0.8460\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8399\n",
      "Epoch 00017: val_loss improved from 0.55954 to 0.52403, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/017-0.5240.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.5332 - acc: 0.8399 - val_loss: 0.5240 - val_acc: 0.8598\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8507\n",
      "Epoch 00018: val_loss improved from 0.52403 to 0.51491, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/018-0.5149.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.5020 - acc: 0.8507 - val_loss: 0.5149 - val_acc: 0.8619\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8555\n",
      "Epoch 00019: val_loss did not improve from 0.51491\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4817 - acc: 0.8555 - val_loss: 0.5161 - val_acc: 0.8584\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.8612\n",
      "Epoch 00020: val_loss did not improve from 0.51491\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.4654 - acc: 0.8612 - val_loss: 0.5460 - val_acc: 0.8488\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8670\n",
      "Epoch 00021: val_loss did not improve from 0.51491\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.4482 - acc: 0.8670 - val_loss: 0.5244 - val_acc: 0.8546\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8716\n",
      "Epoch 00022: val_loss improved from 0.51491 to 0.46833, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/022-0.4683.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.4241 - acc: 0.8716 - val_loss: 0.4683 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8790\n",
      "Epoch 00023: val_loss did not improve from 0.46833\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.4034 - acc: 0.8790 - val_loss: 0.4926 - val_acc: 0.8654\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8842\n",
      "Epoch 00024: val_loss improved from 0.46833 to 0.43424, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/024-0.4342.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3885 - acc: 0.8842 - val_loss: 0.4342 - val_acc: 0.8866\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8855\n",
      "Epoch 00025: val_loss did not improve from 0.43424\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.3732 - acc: 0.8855 - val_loss: 0.4575 - val_acc: 0.8826\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8903\n",
      "Epoch 00026: val_loss did not improve from 0.43424\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3615 - acc: 0.8902 - val_loss: 0.5051 - val_acc: 0.8630\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8909\n",
      "Epoch 00027: val_loss improved from 0.43424 to 0.42425, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/027-0.4243.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.3587 - acc: 0.8909 - val_loss: 0.4243 - val_acc: 0.8863\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8959\n",
      "Epoch 00028: val_loss did not improve from 0.42425\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.3370 - acc: 0.8959 - val_loss: 0.4335 - val_acc: 0.8859\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8993\n",
      "Epoch 00029: val_loss improved from 0.42425 to 0.42131, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/029-0.4213.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.3288 - acc: 0.8993 - val_loss: 0.4213 - val_acc: 0.8861\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9039\n",
      "Epoch 00030: val_loss did not improve from 0.42131\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3139 - acc: 0.9038 - val_loss: 0.4266 - val_acc: 0.8908\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9061\n",
      "Epoch 00031: val_loss improved from 0.42131 to 0.41797, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/031-0.4180.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3057 - acc: 0.9062 - val_loss: 0.4180 - val_acc: 0.8880\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9082\n",
      "Epoch 00032: val_loss improved from 0.41797 to 0.40958, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/032-0.4096.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.2948 - acc: 0.9082 - val_loss: 0.4096 - val_acc: 0.8947\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9116\n",
      "Epoch 00033: val_loss did not improve from 0.40958\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.2862 - acc: 0.9116 - val_loss: 0.4281 - val_acc: 0.8898\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9136\n",
      "Epoch 00034: val_loss improved from 0.40958 to 0.39436, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/034-0.3944.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.2780 - acc: 0.9136 - val_loss: 0.3944 - val_acc: 0.8980\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9161\n",
      "Epoch 00035: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.2665 - acc: 0.9161 - val_loss: 0.4016 - val_acc: 0.9015\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9175\n",
      "Epoch 00036: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.2599 - acc: 0.9175 - val_loss: 0.4168 - val_acc: 0.8882\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9211\n",
      "Epoch 00037: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.2520 - acc: 0.9211 - val_loss: 0.4153 - val_acc: 0.8945\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9214\n",
      "Epoch 00038: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.2500 - acc: 0.9214 - val_loss: 0.4254 - val_acc: 0.8863\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9258\n",
      "Epoch 00039: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.2375 - acc: 0.9258 - val_loss: 0.4642 - val_acc: 0.8765\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9268\n",
      "Epoch 00040: val_loss did not improve from 0.39436\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.2305 - acc: 0.9268 - val_loss: 0.4012 - val_acc: 0.8968\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9285\n",
      "Epoch 00041: val_loss improved from 0.39436 to 0.38740, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_8_conv_checkpoint/041-0.3874.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.2261 - acc: 0.9285 - val_loss: 0.3874 - val_acc: 0.9026\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9282\n",
      "Epoch 00042: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.2277 - acc: 0.9282 - val_loss: 0.4144 - val_acc: 0.8977\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9326\n",
      "Epoch 00043: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.2165 - acc: 0.9326 - val_loss: 0.3930 - val_acc: 0.9033\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9308\n",
      "Epoch 00044: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.2129 - acc: 0.9307 - val_loss: 0.4325 - val_acc: 0.8903\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9321\n",
      "Epoch 00045: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.2115 - acc: 0.9321 - val_loss: 0.3951 - val_acc: 0.9026\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9341\n",
      "Epoch 00046: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.2055 - acc: 0.9341 - val_loss: 0.4129 - val_acc: 0.8966\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9377\n",
      "Epoch 00047: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1947 - acc: 0.9377 - val_loss: 0.4575 - val_acc: 0.8877\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9378\n",
      "Epoch 00048: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1925 - acc: 0.9378 - val_loss: 0.4091 - val_acc: 0.9036\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9401\n",
      "Epoch 00049: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1851 - acc: 0.9401 - val_loss: 0.4202 - val_acc: 0.8959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9413\n",
      "Epoch 00050: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1817 - acc: 0.9413 - val_loss: 0.4599 - val_acc: 0.8977\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9411\n",
      "Epoch 00051: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1809 - acc: 0.9411 - val_loss: 0.4137 - val_acc: 0.9054\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9427\n",
      "Epoch 00052: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1773 - acc: 0.9427 - val_loss: 0.4124 - val_acc: 0.9047\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9435\n",
      "Epoch 00053: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.1749 - acc: 0.9435 - val_loss: 0.4372 - val_acc: 0.9008\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9446\n",
      "Epoch 00054: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1686 - acc: 0.9446 - val_loss: 0.4246 - val_acc: 0.9043\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9495\n",
      "Epoch 00055: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1572 - acc: 0.9495 - val_loss: 0.4465 - val_acc: 0.9010\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9473\n",
      "Epoch 00056: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1616 - acc: 0.9473 - val_loss: 0.3905 - val_acc: 0.9075\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9499\n",
      "Epoch 00057: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1557 - acc: 0.9499 - val_loss: 0.4013 - val_acc: 0.9057\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9483\n",
      "Epoch 00058: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1557 - acc: 0.9483 - val_loss: 0.4083 - val_acc: 0.8987\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9509\n",
      "Epoch 00059: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1519 - acc: 0.9509 - val_loss: 0.4247 - val_acc: 0.9033\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9521\n",
      "Epoch 00060: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1492 - acc: 0.9521 - val_loss: 0.4459 - val_acc: 0.8980\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9518\n",
      "Epoch 00061: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1495 - acc: 0.9519 - val_loss: 0.4204 - val_acc: 0.8996\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9540\n",
      "Epoch 00062: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1420 - acc: 0.9539 - val_loss: 0.4395 - val_acc: 0.9017\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9532\n",
      "Epoch 00063: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.1436 - acc: 0.9532 - val_loss: 0.4057 - val_acc: 0.9066\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9549\n",
      "Epoch 00064: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.1398 - acc: 0.9548 - val_loss: 0.4271 - val_acc: 0.9033\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9564\n",
      "Epoch 00065: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1356 - acc: 0.9564 - val_loss: 0.4601 - val_acc: 0.8945\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9565\n",
      "Epoch 00066: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1365 - acc: 0.9566 - val_loss: 0.4745 - val_acc: 0.8938\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9563\n",
      "Epoch 00067: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1357 - acc: 0.9563 - val_loss: 0.4124 - val_acc: 0.9052\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9560\n",
      "Epoch 00068: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1328 - acc: 0.9560 - val_loss: 0.4256 - val_acc: 0.9043\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9597\n",
      "Epoch 00069: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1256 - acc: 0.9597 - val_loss: 0.4343 - val_acc: 0.8966\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9580\n",
      "Epoch 00070: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1253 - acc: 0.9580 - val_loss: 0.4253 - val_acc: 0.9087\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9592\n",
      "Epoch 00071: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1225 - acc: 0.9592 - val_loss: 0.4281 - val_acc: 0.9047\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9596\n",
      "Epoch 00072: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1224 - acc: 0.9596 - val_loss: 0.4242 - val_acc: 0.9071\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9601\n",
      "Epoch 00073: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1214 - acc: 0.9601 - val_loss: 0.4442 - val_acc: 0.9043\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9601\n",
      "Epoch 00074: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.1221 - acc: 0.9601 - val_loss: 0.4230 - val_acc: 0.9052\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9623\n",
      "Epoch 00075: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1163 - acc: 0.9623 - val_loss: 0.4373 - val_acc: 0.9050\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9625\n",
      "Epoch 00076: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1164 - acc: 0.9625 - val_loss: 0.4310 - val_acc: 0.9033\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9627\n",
      "Epoch 00077: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1173 - acc: 0.9627 - val_loss: 0.4158 - val_acc: 0.9092\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9643\n",
      "Epoch 00078: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1116 - acc: 0.9643 - val_loss: 0.4505 - val_acc: 0.8977\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9647\n",
      "Epoch 00079: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.1117 - acc: 0.9647 - val_loss: 0.4128 - val_acc: 0.9052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9637\n",
      "Epoch 00080: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1106 - acc: 0.9637 - val_loss: 0.4318 - val_acc: 0.9022\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9631\n",
      "Epoch 00081: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.1129 - acc: 0.9631 - val_loss: 0.4298 - val_acc: 0.9108\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9641\n",
      "Epoch 00082: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1107 - acc: 0.9641 - val_loss: 0.4357 - val_acc: 0.9073\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9670\n",
      "Epoch 00083: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1005 - acc: 0.9670 - val_loss: 0.4422 - val_acc: 0.9108\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9655\n",
      "Epoch 00084: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1054 - acc: 0.9655 - val_loss: 0.4422 - val_acc: 0.9094\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9655\n",
      "Epoch 00085: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1031 - acc: 0.9655 - val_loss: 0.4243 - val_acc: 0.9089\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9679\n",
      "Epoch 00086: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1035 - acc: 0.9679 - val_loss: 0.4487 - val_acc: 0.9022\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9678\n",
      "Epoch 00087: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0987 - acc: 0.9678 - val_loss: 0.4428 - val_acc: 0.9071\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9665\n",
      "Epoch 00088: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1024 - acc: 0.9665 - val_loss: 0.4056 - val_acc: 0.9136\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9674\n",
      "Epoch 00089: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1003 - acc: 0.9674 - val_loss: 0.4436 - val_acc: 0.9057\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9677\n",
      "Epoch 00090: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0989 - acc: 0.9677 - val_loss: 0.4600 - val_acc: 0.8970\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9688\n",
      "Epoch 00091: val_loss did not improve from 0.38740\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0970 - acc: 0.9688 - val_loss: 0.4618 - val_acc: 0.9054\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmSUzSSY7IYQkEFBkh7AEsAi4VERocUXw696qdddirdSltba22tpW0Vqqv6q4i7hQ6oK1gqACshiVVRK2JBCy75NklvP742RhCRBCJgOZ5/163Vcyd+6c+9xZznPuufeeq7TWCCGEEACWYAcghBDixCFJQQghRDNJCkIIIZpJUhBCCNFMkoIQQohmkhSEEEI0k6QghBCimSQFIYQQzSQpCCGEaGYLVMFKqTTgJSAJ0MCzWusnD1rmTGARsKNx1jta64ePVG63bt10enp6h8crhBBd2bp164q11olHWy5gSQHwAndrrdcrpaKAdUqp/2qtNx203Aqt9Y/aWmh6ejpr167t0ECFEKKrU0rtastyAes+0lrv1Vqvb/y/CtgMpARqfUIIIY5fpxxTUEqlAyOA1a08fbpS6hul1IdKqcGdEY8QQojWBbL7CACllAt4G7hLa1150NPrgd5a62ql1FTgPaBfK2XcCNwI0KtXrwBHLIQQoUsFcuhspZQd+A+wRGv91zYsvxMYrbUuPtwyo0eP1gcfU/B4POTl5VFXV3ecEYcup9NJamoqdrs92KEIIQJAKbVOaz36aMsF8uwjBfwL2Hy4hKCU6gHs01prpdQYTHdWybGuKy8vj6ioKNLT0zGrFcdCa01JSQl5eXn06dMn2OEIIYIokN1H44GrgO+UUlmN8+4DegForecBlwI3K6W8gBuYpdux61JXVycJ4TgopUhISKCoqCjYoQghgixgSUFr/TlwxFpaa/008HRHrE8SwvGR908IASF0RbPP56a+Ph+/3xPsUIQQ4oQVMknB76+joWEvWnd8UigvL+eZZ55p12unTp1KeXl5m5d/6KGHePzxx9u1LiGEOJqQSQpKmU3V2t/hZR8pKXi93iO+9oMPPiA2NrbDYxJCiPYImaQA1sa/vg4vec6cOeTk5JCRkcE999zDsmXLmDBhAtOnT2fQoEEAXHjhhYwaNYrBgwfz7LPPNr82PT2d4uJidu7cycCBA7nhhhsYPHgwkydPxu12H3G9WVlZjBs3jmHDhnHRRRdRVlYGwNy5cxk0aBDDhg1j1qxZAHz22WdkZGSQkZHBiBEjqKqq6vD3QQhx8gv4xWudbdu2u6iuzmrlGT8+Xw0WSzhKHdtmu1wZ9Ov3xGGff/TRR9mwYQNZWWa9y5YtY/369WzYsKH5FM/nn3+e+Ph43G43mZmZXHLJJSQkJBwU+zZef/11nnvuOS677DLefvttrrzyysOu9+qrr+app55i0qRJ/PrXv+a3v/0tTzzxBI8++ig7duzA4XA0d009/vjj/P3vf2f8+PFUV1fjdDqP6T0QQoSGENpTaBK4i/X2N2bMmAPO+Z87dy7Dhw9n3Lhx5Obmsm3btkNe06dPHzIyMgAYNWoUO3fuPGz5FRUVlJeXM2nSJACuueYali9fDsCwYcO44ooreOWVV7DZTAIcP348s2fPZu7cuZSXlzfPF0KI/XW5muFwLXq/30tNTRYORxphYUkBjyMyMrL5/2XLlvHJJ5+wcuVKIiIiOPPMM1u9+trhcDT/b7Vaj9p9dDjvv/8+y5cvZ/HixTzyyCN89913zJkzh2nTpvHBBx8wfvx4lixZwoABA9pVvhCi6wqZPYVAHmiOioo6Yh99RUUFcXFxREREsGXLFlatWnXc64yJiSEuLo4VK1YA8PLLLzNp0iT8fj+5ubmcddZZPPbYY1RUVFBdXU1OTg5Dhw7l3nvvJTMzky1bthx3DEKIrqfL7SkcjkkKCq07/kBzQkIC48ePZ8iQIZx//vlMmzbtgOenTJnCvHnzGDhwIP3792fcuHEdst758+dz0003UVtbS9++fXnhhRfw+XxceeWVVFRUoLXmjjvuIDY2lgcffJClS5disVgYPHgw559/fofEIIToWgI6IF4gtDYg3ubNmxk4cOBRX1tVlYXdHofT2TtQ4Z3U2vo+CiFOPm0dEC9kuo/A7C0EovtICCG6ihBLClYCcZ2CEEJ0FSGVFED2FIQQ4khCKikoZQ3IgWYhhOgqQi4pSPeREEIcXkglBek+EkKIIwuppHAidR+5XK5jmi+EEJ0hxJKCBfBzsl2bIYQQnSWkkoIZPlvT0YPizZkzh7///e/Nj5tuhFNdXc0555zDyJEjGTp0KIsWLWpzmVpr7rnnHoYMGcLQoUN58803Adi7dy8TJ04kIyODIUOGsGLFCnw+H9dee23zsn/72986dPuEEKGj6w1zcdddkNXa0Nlg1x6s/jqwujjK7aMPlJEBTxx+6OyZM2dy1113ceuttwKwYMEClixZgtPp5N133yU6Opri4mLGjRvH9OnT23Q/5HfeeYesrCy++eYbiouLyczMZOLEibz22mucd9553H///fh8Pmpra8nKyiI/P58NGzYAHNOd3IQQYn9dLym0ieaYksJRjBgxgsLCQvbs2UNRURFxcXGkpaXh8Xi47777WL58ORaLhfz8fPbt20ePHj2OWubnn3/O5ZdfjtVqJSkpiUmTJrFmzRoyMzP5yU9+gsfj4cILLyQjI4O+ffuyfft2br/9dqZNm8bkyZM7bNuEEKGl6yWFI7TofZ4y6upyiIgYhNUa0aGrnTFjBgsXLqSgoICZM2cC8Oqrr1JUVMS6deuw2+2kp6e3OmT2sZg4cSLLly/n/fff59prr2X27NlcffXVfPPNNyxZsoR58+axYMECnn/++Y7YLCFEiAmpYwqBHD575syZvPHGGyxcuJAZM2YAZsjs7t27Y7fbWbp0Kbt27WpzeRMmTODNN9/E5/NRVFTE8uXLGTNmDLt27SIpKYkbbriB66+/nvXr11NcXIzf7+eSSy7h97//PevXr+/w7RNChIaut6dwRIG7T/PgwYOpqqoiJSWF5ORkAK644gp+/OMfM3ToUEaPHn1MN7W56KKLWLlyJcOHD0cpxZ/+9Cd69OjB/Pnz+fOf/4zdbsflcvHSSy+Rn5/Pddddh99vkt0f//jHDt8+IURoCKmhs30+N7W1G3E6+2K3xwcqxJOWDJ0tRNclQ2e3IpDdR0II0RWEVFIIZPeREEJ0BSGVFFr2FCQpCCFEa0IwKSjpPhJCiMMIqaRgyPDZQghxOCGXFOQ+zUIIcXghmBQ6fk+hvLycZ555pl2vnTp1qoxVJIQ4YYRcUoCOv6fCkZKC1+s94ms/+OADYmNjOzQeIYRor4AlBaVUmlJqqVJqk1Jqo1LqzlaWUUqpuUqpbKXUt0qpkYGKp2WdHd99NGfOHHJycsjIyOCee+5h2bJlTJgwgenTpzNo0CAALrzwQkaNGsXgwYN59tlnm1+bnp5OcXExO3fuZODAgdxwww0MHjyYyZMn43a7D1nX4sWLGTt2LCNGjOCHP/wh+/btA6C6uprrrruOoUOHMmzYMN5++20APvroI0aOHMnw4cM555xzOnS7hRBdTyCHufACd2ut1yulooB1Sqn/aq037bfM+UC/xmks8I/Gv+12hJGzAfD709Daj9V6+GUOdpSRs3n00UfZsGEDWY0rXrZsGevXr2fDhg306dMHgOeff574+HjcbjeZmZlccsklJCQkHFDOtm3beP3113nuuee47LLLePvtt7nyyisPWOaMM85g1apVKKX4f//v//GnP/2Jv/zlL/zud78jJiaG7777DoCysjKKioq44YYbWL58OX369KG0tLTtGy2ECEkBSwpa673A3sb/q5RSm4EUYP+kcAHwkjZjbaxSSsUqpZIbXxtAgR/aY8yYMc0JAWDu3Lm8++67AOTm5rJt27ZDkkKfPn3IyMgAYNSoUezcufOQcvPy8pg5cyZ79+6loaGheR2ffPIJb7zxRvNycXFxLF68mIkTJzYvEx8vQ3sIIY6sUwbEU0qlAyOA1Qc9lQLk7vc4r3Feu5PCkVr0AHV1RXg8JURFjWjvKtokMjKy+f9ly5bxySefsHLlSiIiIjjzzDNbHULb4XA0/2+1WlvtPrr99tuZPXs206dPZ9myZTz00EMBiV8IEZoCfqBZKeUC3gbu0lpXtrOMG5VSa5VSa4uKio4zHnP2UUcOBBgVFUVVVdVhn6+oqCAuLo6IiAi2bNnCqlWr2r2uiooKUlJSAJg/f37z/HPPPfeAW4KWlZUxbtw4li9fzo4dOwCk+0gIcVQBTQpKKTsmIbyqtX6nlUXygbT9Hqc2zjuA1vpZrfVorfXoxMTE44yqaZM7LikkJCQwfvx4hgwZwj333HPI81OmTMHr9TJw4EDmzJnDuHHj2r2uhx56iBkzZjBq1Ci6devWPP+BBx6grKyMIUOGMHz4cJYuXUpiYiLPPvssF198McOHD2+++Y8QQhxOwIbOVuZGxPOBUq31XYdZZhpwGzAVc4B5rtZ6zJHKPZ6hswEaGgqpr99NZORwLBZ7m14TKmTobCG6rrYOnR3IYwrjgauA75RSTecD3Qf0AtBazwM+wCSEbKAWuC6A8QAtg+KBXNUshBAHC+TZR58D6ijLaODWQMXQOmvjumX8IyGEOFjIXdFsDjRLUhBCiNaEXFJo2WTpPhJCiIOFXFKQPQUhhDi8EEwKcp9mIYQ4nJBLCifKfZpdLldQ1y+EEK0JuaQg92kWQojDC9Gk0LH3aZ4zZ84BQ0w89NBDPP7441RXV3POOecwcuRIhg4dyqJFi45a1uGG2G5tCOzDDZcthBDt1SkD4nWmuz66i6yCI4ydDfh81Shlw2JxtqnMjB4ZPDHl8CPtzZw5k7vuuotbbzWXXCxYsIAlS5bgdDp59913iY6Opri4mHHjxjF9+nTMxd6ta22Ibb/f3+oQ2K0Nly2EEMejyyWFtjniNXXHbMSIERQWFrJnzx6KioqIi4sjLS0Nj8fDfffdx/Lly7FYLOTn57Nv3z569Ohx2LJaG2K7qKio1SGwWxsuWwghjkeXSwpHatE3qanZiMXiIDz81A5b74wZM1i4cCEFBQXNA8+9+uqrFBUVsW7dOux2O+np6a0Omd2krUNsCyFEoITcMQWj4+/TPHPmTN544w0WLlzIjBkzADPMdffu3bHb7SxdupRdu3YdsYzDDbF9uCGwWxsuWwghjkdIJoVA3Kd58ODBVFVVkZKSQnJyMgBXXHEFa9euZejQobz00ksMGDDgiGUcbojtww2B3dpw2UIIcTwCNnR2oBzv0NkAbncOfr+byMghHR3eSU2Gzhai62rr0NkhuacAHb+nIIQQXUFIJgWlOv6YghBCdAVdJikctRusogI2boSGhoDcp/lkJ++FEAK6SFJwOp2UlJQcvWJzu6G+nkDcp/lkprWmpKQEp7NtF/MJIbquLnGdQmpqKnl5eRQVFR1+IY8HiosB8Dr8eL2lOBybmofSDnVOp5PU1NRghyGECLIukRTsdnvz1b6HVVUFw4fDY49RcHUSW7Zcy9ix2YSHn9I5QQohxEmgS3QftUlUFMTEQG4uVmsUYMZAEkII0SJ0kgJAWtoBScHrrQpyQEIIcWIJ6aTg80lSEEKI/YVoUjB3PZOkIIQQBwq9pFBUhM1rByQpCCHEwUIvKQDWvSYZyDEFIYQ4UGgmhT1miGk5+0gIIQ4UWkmhVy8ALPl7UcqBz1cR5ICEEOLEElpJoemK3d27iYjoR03NpuDGI4QQJ5jQSgpOJyQmQm4uUVGjqapaIwPBCSHEfkIrKUDzaalRUZl4PEXU1+cGOyIhhDhhhHBSMDcgqqpaE+SAhBDixBGyScHlGo5SdiorJSkIIUST0EwKFRVYahqIjBxGVdXao79GCCFCRGgmBdjvYPNauV+zEEI0ClhSUEo9r5QqVEptOMzzZyqlKpRSWY3TrwMVywH2SwrR0Zn4fBW43dmdsmohhDjRBXJP4UVgylGWWaG1zmicHg5gLC0O2FPIBJAuJCGEaBSwpKC1Xg6UBqr8duvZE5SC3FwiIgZhsYTLGUhCCNEo2McUTldKfaOU+lApNbhT1mi3Q3Iy5OZisdhwuUbIGUhCCNEomElhPdBbaz0ceAp473ALKqVuVEqtVUqtLSoqOv41N56WChAVlUl19df4/d7jL1cIIU5yQUsKWutKrXV14/8fAHalVLfDLPus1nq01np0YmLi8a/8gKQwGr+/ltrazcdfrhBCnOSClhSUUj2UUqrx/zGNsZR0ysqbkoLWREc3HWyWLiQhhLAFqmCl1OvAmUA3pVQe8BvADqC1ngdcCtyslPICbmCW7qzR6dLSoLYWysoIj+uH1RpNVdVakpN/0imrF0KIE1XAkoLW+vKjPP808HSg1n9E+52WquLjiYoaJXsKQghB8M8+Co79kgJATMx4qqq+pqGhAw5iCyHESUySApCYeCngo7j4neDFJIQQJ4DQTApJSeZ6hZwcACIjhxERMYDCwjeCHJgQQgRXaCYFqxXOOAMWLwatUUrRvfssyss/o75+T7CjE0KIoAnNpAAwaxZ8/z188w0AiYkzAU1R0VvBjUsIIYIodJPCxReDzQZvmC6jyMgBuFwZ0oUkhAhpoZsUunWDc881SaHx8oju3WdRWbkKt3tncGMTQoggCd2kAKYLadcuWL0aaOpCgqKiBcGMSgghgqZNSUEpdadSKloZ/1JKrVdKTQ50cAF3wQUQFgZvvglAeHg6UVFjpQtJCBGy2rqn8BOtdSUwGYgDrgIeDVhUnSUmBqZONUnB5wNMF1J19dfU1m4NcnBCCNH52poUVOPfqcDLWuuN+807uc2aBXv3wuefA9C9+0yUspGf/48gByaEEJ2vrUlhnVLqY0xSWKKUigK6xt3uf/QjiIhoPgvJ4Uime/fLKSj4Fx5PeZCDE0KIztXWpPBTYA6QqbWuxYx2el3AoupMkZEwfTq89RY0NACQmjobn6+avXufC3JwQgjRudqaFE4Htmqty5VSVwIPABWBC6uTXX01lJTAf/4DQFRUBrGxZ5OfPxe/3xPk4IQQovO0NSn8A6hVSg0H7gZygJcCFlVnmzwZevaEF15onpWWdjf19XlyhbMQIqS0NSl4G2+AcwHwtNb670BU4MLqZFYrXHMNfPAB7DFjH8XHTyEiYgC5uX+hs+79I4QQwdbWpFCllPoV5lTU95VSFhrvotZlXHst+P3w8ssAKGUhNXU21dXrKS//LLixCSFEJ2lrUpgJ1GOuVygAUoE/ByyqYDjtNDNy6gsvNA97kZR0JXZ7Irm5jwc5OCGE6BxtSgqNieBVIEYp9SOgTmvddY4pNLnuOti6FVauBMBqDScl5Q5KS9+nuvrbIAcnhBCB19ZhLi4DvgJmAJcBq5VSlwYysKCYMcOcovr8882zUlJuxWp1sXv3yX8BtxBCHE1bu4/ux1yjcI3W+mpgDPBg4MIKkqgouOwyM+xFTQ0AdnscPXveTGHhm9TWZgc5QCGECKy2JgWL1rpwv8clx/Dak8tPfgLV1TBvXvOs1NSfo5Sd3NyudRhFCCEO1taK/SOl1BKl1LVKqWuB94EPAhdWEI0fD9OmwUMPNZ+e6nAkk5x8HQUFL8rtOoUQXVpbDzTfAzwLDGucntVa3xvIwIJGKXjySfB44O67m2enpd2D1l5yc/8axOCEECKw2twFpLV+W2s9u3F6N5BBBd0pp8CcOWaQvE8/BSA8vC/du89iz555NDQUBzlAIYQIjCMmBaVUlVKqspWpSilV2VlBBsW990LfvnDbbc0D5fXu/QB+v1vORBJCdFlHTApa6yitdXQrU5TWOrqzggyK8HCYOxc2b4Y/mwPMkZEDSUq6ivz8p6mrywtygEII0fG65hlEHWXaNHOK6oMPwgJz3+b09N8Afnbt+n1wYxNCiACQpHA0L75ozki68kr45BPCw/uQnHwjBQX/kusWhBBdjiSFowkPh8WLYcAAuPBCWLuW3r0fQCk7O3c+FOzohBCiQ0lSaIvYWPjoI0hMhKlTcdSFk5p6J4WFr1Fd/V2woxNCiA4jSaGtevY0w18UFcErr5CW9svGMZEeC3ZkQgjRYSQpHIsxY2D0aJg3D7stluTkGygqelPORBJCdBkBSwpKqeeVUoVKqQ2HeV4ppeYqpbKVUt8qpUYGKpYOddNNsGEDfPklqal3oLWf/Pyngh2VEEJ0iEDuKbwITDnC8+cD/RqnGzH3gT7xzZoF0dHwj3/gdPYmMfFS9uz5J15vdbAjE0KI4xawpKC1Xg6UHmGRC4CXtLEKiFVKJQcqng4TGQlXXw1vvQXFxaSlzcbnq6Cg4IVgRyaEEMctmMcUUoDc/R7nNc478f3sZ2boixdfJDp6LNHRPyAv7wm09gU7MiGEOC4nxYFmpdSNSqm1Sqm1RUVFwQ4Hhgwx93P+5z/B7yctbTZ1ddspLl4U7MiEEOK4BDMp5ANp+z1ObZx3CK31s1rr0Vrr0YmJiZ0S3FHddBNkZ8Mnn9Ct24U4nX3Izf0zWutgRyaE6IK0Br8/8OuxBX4Vh/Vv4Dal1BvAWKBCa703iPEcm0svNcNr/+IXqHXrSEv7Jdu23Uxp6QckJEwLdnRCnPBqaqCgAMrKwOk0h+siIkzFV1dnpoYG89jvB5+vZfJ6DyzL44GqKqisNJPXaypRaKlMtTbz3W6orTVTU/m+xp7fsDBwOMxfm81MdrtZpr7eLF9bC6WlUFJiYrdazcAHERHmdUqZsnw+E0tFBZSXmxibnrNazTY7nWZ9Pp95vqHBxLj/NtfXm6muzlQ5f/hDYD+XgCUFpdTrwJlAN6VUHvAbwA6gtZ6HuXPbVCAbqAWuC1QsAeFwwN//DhdcAI8/TvK9vyA3989s334/8fHno9RJ0TMnTmLV1bBrF+zcaSq66GiIiTEVTWWlqYjKy01FFBFhKl2lzLyyMlNZud0tFbDPBxaLWcbvN5VsVZVZD5jK0WYzlWttranUmyrXpsnjMc83VcgWS+tTdXVLuZ3NYjHvRXi4qcStVjNP65aKub6+paL2eMwyTQnD6YT4eEhIMKPrN70ftbXmPW2ilPk8evY0fx2OlvfG6z2wsm8qv+k9borJYjGva1rvxImBf3/UydbdMXr0aL127dpgh9Hi0kvhP/+B775jX/RXbN58JQMHvk5S0qxgRyY6UVPFUFgI+/aZqaam5Xmvt6XVWFlplq2rM5Wy12sqEEtjO6KpJVtTYyqNpoqpqbJqaDDL7F8BHY+mys5qbWlVKwVRUSbRuFxmuaY4lDKValPLvulveLip1JRqaRE3lefzHfh/ZCT06GGmuDizXU1JpqkV7XCY8qzWlkrSam2pNJvWAeZxdHTLZLMdGMf+CWn/1nwoUUqt01qPPupykhSO0549MGgQjBqF/u/HrF03Ar/fTWbmJiwWe7CjC3lam139vDwzVVa2VKx1daYiaqqMtD6wJdv0msJCU0m5XKYya+reaKq8m1rUbe3vDQsz5TR1HzS1vpt+iuHhLS1Zp9NUjE3T/q3Gnj0hPd1MEREt3Se1taZijIszw3bt35L1+1vmx8SYdVhkpzYktDUpBPOYQtfQsyc89hjcdBPqpZfp86NH2LBhOgUFL9Cz543Bju6k5XabYaYqK1sq3aYukf27Ppqm4uKWFnpFRUufbFML+0gsFlOpWiwtrwsPh9RUSEuDkSNbWrLV1Wa5Hj3MMuHhpkXdNHXvDklJZnK5DuxDbmrFOhyBf/9Ex/H6vWitsVsPbeS5PW6UUjisDtQx7n5orcmtzCW7NJveMb3pG9f3mMsIBNlT6Ah+v+ns27oVnZ3N1znnU1e3m7Fjt2G1hgc7uqCrq4O9e2H3bsjNhfx8U7lXVLR0pzRV/OXlpmXean+zxQORRRC5D2WvJ7xsNBFOG+Hhpn+3qTKOjW3pbrDZTAWemmqm+PiWvluHA3y2SnJrtrGjfDuxzlh6x/amV0wvFIod5TvILs2muLaYs9LPonds7+ZQtNZsKNxAdmk2p8SfQr/4foTbW/+sNxdt5ovcL+if0J+MHhlEOaIOeL6moYZv9n3D+r3r2VK8Ba01NosNm8VGXHgcya5kkqOSiQqLori2mMKaQopri4kMiyQxIpHEyERqPbVsKtrEpqJN7KrYhdPmxBXmwhXmIj0mnaFJQxnafSjdI7uTW5nL7ordFFQX0MPVg75xfekb15dwWzj1vnpqPbVUN1RTVV9FZX0lFfUV5FXmsat8F7sqduH2uukW3o3EyESSXcmMTB7JiOQRhFnDDtn2Wk8tS3cs5bNdn+HxebBb7YRZw+jh6sHI5JEMTxqO0+bkq/yv+PfWf/NRzkdorekZ1ZNkVzKuMBcV9RVU1FdQ01BDlCOKeGc8CREJ2Cw2GnwNNPga8Pq9KBRKKSzKQrgtnAh7BJFhkVTVV/F96fdsK9lGYU0h43uNZ+qpUznv1PPw+X1kFWSRVZBFQXUBCREJJIQnEBkWyXf7vmN1/mrW7lmL2+smPjyepMgkXGGu5s+hxmP6CBUKp81JmDUMq8WKVVlx2ByclnAaQxKHMLj7YPzaz87yneyq2EV2aTZbirdQ3dDyRY9xxDAyeSSZPTMZmzqWcanjSHYls71sO1/lf8VX+V8xKX0SFw64sB2/Quk+6nzr1pnB8u6/n/JfnEtW1pn07fsovXrdG+zIOlxdnelW2b275awKr9e0pJsOfO7aZc4sKSw0lT0WD0TnQ8xuiNyH1ReFyxpPdFgczoR96MSNeOM24g8vJNGRRs+I3qTEJFNp28Yuz1py3GspqNt1QBwJ4QlcPPBiLh10KTaLjc1Fm9lcvJl6bz0jk0cyuudohiUNw2E7sGleWFPIY58/xmsbXqOguqDVbVQoNAf+NjJ7ZnLxwIsprCnkvS3vsaN8xwHL947tzeieozk99XROTz2dnLIcnl33LCt2rzhguVPiT8Fpc1LdUE11QzUltSXN64pxxGC32vH4PHj8Hmo9tcf02aRFp9E3ri8NvgaqG6qprK8ktzIXvz5631Zr27w/i7KQGp1KhD2C4triA+J2WB2M6jmK1OhUbBYbdoudgupBC10cAAAgAElEQVQClu1cRr2vHofVgdPmpMHXQL2vvjkehSLaEU1FfQU2i40JvSYQGRbJ3qq97KnaQ62nlhhnDDGOmOYKvtRdSom7BJ/fR5g1rLki1lqj0fj8Pup99TT4GprX0SumF6clnEasM5ZlO5dRVHvo9U6R9sjmSh4gzBrGyOSRjE0ZS5wzjsKaQgprC6mqr6JbRDe6R3YnMcKcIu/2unF73DT4GvBpHz6/j1pvLVuKt7CxcGNzuXaLnV4xvegT14dB3QYxKHEQp8Sfws7ynazbs451e9eRVZCFx+8BIMIe0fwdCLeFc9+E+3hg4gNH/Sxb/XwlKQTB5ZfDokWQnc13JT+jvHw5Y8dmExZ2glxbcRRam8r8m29g2zZYvXsdy6z3U+Jajq0uCUtNT3RlKg1rroLvpwGNu7oRRXDO/TDsFUCjsGJRViwKlEWD8uPBjebIFZMrzEVSZBJ5lXnU++qb558SdwqZKZkMSBhAkiuJ7pHd8fq9vLflPRZ/v/iA1lZUWBRh1jBK3CWA+RGOSRnDpN6TmNB7Ast2LuOpr56izlvHxQMvZnTyaPol9KNvXF8q6yvZVb6LneU78Wkfp8afyqnxp+IKc/HBtg9YuGkha/asIcwaxg/7/pAL+l9ARo8MtpdtZ2vxVjYXb2Z1/mp2lu9sjufU+FO5ceSN/Lj/j9letp31e9fzzb5v8Gu/acnbXSS5khjRYwQjkkeQEpVyQBdCnbeOguoC9lbtpaqhisSIRJJcSSSEJ1DjqaGopoii2iIcVgcDug04ZC+kqYzNRZv5rvA7St2lpEWn0SumF0muJAqqC9hetp2c0hzqvHXNretIeyTRjmiiHFFEO6LpGdWTlKiUA7pQfH4fe6r28FX+V6zMW8mqvFUU1xbj9Xvx+D1EhUVxbt9zmdpvKhN6T8BpczZ+zzT5Vfl8vfdr1u9dT35VPmf3OZspp04h1hnb1q/rUXl8HtxeN2HWsOZ1A/i1n/V71/NxzsdE2CPI6JHB8KThxIXH0eBroKS2hMr6StJj0w9pULSHX/vJrcjFZrGRHJWM5ShnJtZ568gqyGJ13mpyynIY0n0IY1LGMDhxcKtdWG0lSSEYtm83d2i79lpq/vZz1qwZSkrKLfTrNzfYkTXz+00rf9s22LHDdOds3LOTDZUryN3upLYsBnxhMPofMGQB1voEUitmQVgF9Y49VIZtpta6l95hI7m+34NUWXbzjy2/xu2t4eJTryItoRsaH77GIT8syoJC4Qpz0SumF71ietHD1YMaTw2l7lJK3aXEh8czOHGw6bZRCr/2U1hTyJ6qPaTHphMfHn/Y7XF73CzdubS5UuwZ1ROA3RW7WbNnDavzVrN893LW7VmHT/tQKGYNmcVvJv2G/t36H/P7t7dqL64wV6uVb5OC6gJW5a0izhnHxN4TT4h+YiEkKQTLnXfC00/Dhg1stTxJQcG/yMzcSETEaR2+qqKaIhZuWsiH2R9S4jatm8r6ShxWJzHWJML93QmrSyOy5Ac0ZE8gd3MPcnKgjnJI+B76fAqDFkLPdYeUHWmPZPbps7n79LuJccY0z/f4PLzy7Ss8suIRcspyAJh8ymSeOO8JBiYO7PBt7CjVDdWszltNSnQKA7oNCHY4QnQ6SQrBUlQEp5wCZ59N/Zvz+OqrfsTFTWbIkLc7pPiS2hL+vfXfLNi0gP/m/Bef9tE35lRcvt5UFUdTvMdFVW09RBaCax/E7gS7G4CI+j5oezVuS0t/ambPMcwYdCnn9zsfrTUV9RVU1Vcxqucoukd2P2wcTd03rjAX551ynrSGhTjBySmpwZKYCL/8JTz4II7195KW9kt27vw15eWfExt7RruK3Fe9j3c2v8Pbm99m2c5l+LSPJEc6mZ57qF55OZuWDcXvU7hccNZZ5nh3ejr07g0pvTwUWtfzZd4KVuevJs4Zx2kJp9Evvh8jk0eSFpN21PW3xmaxcemgS9v1WiHEiUv2FAKhuhr69IHMTHyL32L16v7Y7d0YNWpNmy9oK3OX8caGN1iwcQHLdy/Hr/3EePoTlnMJxcsvQe8Zgc2mGDvWJILJk2HsWHO6pRBCHEz2FILJ5YK774Zf/Qrr+k306zeXjRsvIS/vCXr1uueIL/1237f8adnTLNjyCh7cWEsH4P/2Adg0A4tnCCPHwNjr4Qc/gPHjW4YgEEKIjiB7CoFSVWX6cMaPRy9axIYNF1FW9jGZmRsID++L1pqsgiw+zP6Q7NJsthfnsiF/JyU6Gzzh8O0V9Mi9hXOHZTBxguKMM6B//9Acs0UIcfxkTyHYoqLg5z+HBx9EZWXRt/+TLFoxiLXLLma3OpNFWxexs3ynuRLSm0zdvl7ospH0tt3MVUOvZeYf4hk8WJKAEKJzyZ5CIFVUkDs4jZtnRvBpfCVurzkLyK5s9NHnUbn6Ygo++zEuSyJXXw233AKDBwc5ZiFElyR7CieABXlL+Nn1HrwN+7ix9xV0izuTt5/RZP33CrZ5wznzTMXDT8DMmWagNCGECDZJCgFQ6i7l7o/v5sWsFxnbYxRzf7+P+Z4LeajkEiIj/fz06ke4+OI1TJmyEItFhswUQpw4JCl0oFpPLXNXz+XRzx+lqqGKX42/n+6bfsPkMqiuVdzUazG/WXk+2jaITZt+Q3b2bE477e/BDlsIIZpJUugAWmte++417v3kXvKr8pnWbxqXJfyRv/5iKN98Az/8IcyduJCBv54B914J8+dTlfYLcnMfJzp6LD16XB3sTRBCCECSwnHbW7WXm96/iX9v/TdjUsbwwo9eY/HTE7nmKTN+/1tvwSWXgFKXgvodPPggxMfT58+PUVW1lu+//xmRkcOIisoI9qYIIYQkhePx2nevcdsHt+H2uvnL5L8wIexOrrnQyubNcPvt8Ic/HHRx2f33m7GR5s7Fsnw5g5/+I2ts17Nx4yWMGrUOu73jhg0WQoj2kLuztkOdt44bF9/IFe9cwcDEgXx9YxasnM34062Ul8OSJTB3bitXGysFTzwBCxdCYSH2CVMZ9fwYvGW72LLlanQbboQihBCBJEnhGO0q38UZz5/Bc+uf474z7uOjy5bzu7v6c/fd8KMfwXffmXGIDksp05+0eTPceSeOFxYx9s5u1GYtZvfuxzptO4QQojWSFNqouLaYJ1Y9wchnR7KtdBvvzXyPG099hEkTrbz+OjzyCLz9trlXcJtER8Pf/gaffIKtzMfoW+xUvnY/paWfBHQ7hBDiSCQpHIHWms92fsashbNI+WsKP1/ycwZ2G8ia69fizrqAUaMgJwcWL4b77mvnkBRnnYVauxbLqYMYcr+m7rrzqXjvUXPjYyGE6GSSFFqhteaT7Z8w6cVJnDn/TD7O+ZibRt3Etzd9y4Ipn3PPT/tx+eXmXjpr1sC0ace5wt69UV+uxH/dFfT4yEfMRb/CnxgLP/0p1B7bjduFEOJ4SFI4yPay7Ux4YQLnvnwu28u2M3fKXPJn5/Pk+U+Ss3IogwbBxx/D44/Dl1/CaR11l83wcKz/egX/vjx2/HU4heNq4fnn0Xfd1UErEEKIo5OksJ8vdn/B2P83lk1Fm3hm6jPk3JHD7WNvx2kL569/hYsvNkng22/N7RKs1o6PwRbbk953fkXZ365i9yxQzz2H781XO35FQgjRCkkKjV799lXOfuls4pxxrL5+NTdn3ozD5sDrhdtuM0ngkkvgs8+gX7/AxmKxhDFgwHz07x6mcgDo66+hbuuqwK5UCCGQpADAk6ue5Mp3r+T01NNZdf0q+iWYWr+kxJxm+swz5rbLb74J4eGdE5NSit6nPoj35WfB76NhxkQqSpZ3zsqFECEr5JPCkuwlzP54NhcNuIiPr/qY+PB4AFatghEjYOlSeO45eOwxsATh3YoffQPep/9E9Hce3P93Fvt2v9j5QQghQkZIJ4Xs0mxmvT2LId2H8PJFLxNmDUNrePJJmDABbDZzMPn664Mbp/O6e/D9+l56fOzH8ePr2LV2tlz9LIQIiJBNClX1VVzwxgVYlZX3Zr5HZFgkWsM998Bdd5nTTNetg1Gjgh2pYf3to/hffZnoLVa6T/8bOf+egs8np6sKITpWyCaF6xZdx9birSyYsYA+cX3QGmbPhr/8BW69Fd59F+Ligh3lgSz/dyXqs88J80aRfsV/yX4ug7q6vLYXcJLdelUI0flCMilkFWTx9ua3eejMhzi7z9lobfYOnngC7rwTnnqqnVcndwI1bhzWdRshtRf97tjGrj8NobJy9dFfuGCBGYPjootgy5bAByqEOCkFNCkopaYopbYqpbKVUnNaef5apVSRUiqrceqU3vtn1jxDuC2cWzNvBeChh8yoprNnm+GITtSE0CwtDdvKLHTmKPr/poLC+8azN+9frS9bV2d2fWbONDd4+N//YMgQuOkmKCjo3LiFECc8pQPUpaCUsgLfA+cCecAa4HKt9ab9lrkWGK21vq2t5Y4ePVqvXbu23XGV15WT8tcU/m/I//Hc9Of48ktzUPmqq+CFF06ChLC/ujr8l1+K5b338UZAfUYK4edehyU5FerrTUJ4801Yvx5+8Qtzg4eyMvjd72DePJMksrIgJibYWyKECDCl1Dqt9eijLRfIm+yMAbK11tsbA3oDuADYdMRXBdj8rPnUemq5JfMWqqvh6quhVy+zp3BSJQQApxPLwkX4F7xJzX/+jHVVFurh38P+eb5bN1i0CKZPN4+7dzf9Y7NmwaRJZi/ilVfat/4lSyAqCn7wg+PeFCHEiSGQ3UcpQO5+j/Ma5x3sEqXUt0qphUqptADGg1/7eWbtM5yeejojkkfwi1/A9u3w0ktmJOuTktWK5fL/I+bVr3GvWsjKDyJZuTCMnK9upL4k23QRNSWE/Y0fb24N+uqr8NprLfN9PvN427Yjr/ejj2DqVDP2hwzaJ0SXEewDzYuBdK31MOC/wPzWFlJK3aiUWquUWltUVNTulX2641O+L/meWzJv4cMP4Z//NL0qEya0u8gTSmLiJYw8cyNxg68it+ZfrN4wlOwd9+B257T+gvvvh9NPh5tvhp074auvIDMTrrjCXMp9uMp+82ZzjCIlBfbtM1f3CSG6hEAeUzgdeEhrfV7j418BaK3/eJjlrUCp1vqIHdzHc0zhojcv4vPdn7P7rlwG9nMSFWWGvnY621XcCc3tzmHnzofZt+9VwEdc3HmkpNxMQsKPMG91o+3bISPDHFfIz4cePeCGG+Dhh+GOO8yVfPsrKYExY6C62rx511wDW7eack70N9LfeMFfMC5NDxXV1fDOO6ZLsqwMPv3UdDGKoGvrMQW01gGZMMcrtgN9gDDgG2DwQcsk7/f/RcCqo5U7atQo3R67yndpy28tes5/5+iiIq1B67/+tV1FnVTq6vL0jh0P6S++6KmXLkWvXJmud+9+XDc0lLUs9MorWtvtWt95p9YVFWbeHXeYN+nTT1uWKy3VetIkrR0Orb/80sz79FOz3NNPd9o2tUturtaDBml92WVa+/2BX9+yZVp/8cXRl3v9da1/+9vOiSmQamu1vuUWrSMizPehd2+tldL69tuDHdnJqapK63/+U+tp07Q+4wytR4zQul8/rf/0p3YXCazVbam727JQeydgKuYMpBzg/sZ5DwPTG///I7CxMWEsBQYcrcz2JoU3N7ypnb936h1lO/SXX5otX7y4XUWdlHw+jy4sXKjXr5+oly5Ff/ZZpN627W7t8TQmgYaGA19QU2O+hL17a11WpvW8eVonJGhtsWj96qsty/n95kubmqp1XV2nbc8x2bFD6z59TOyg9VtvBXZ9H3+stc2mdXi41mvXHn65f/7TxANa//rX7VuX12s+j0WL2vf6pjLeekvrTz5p3+vz87XOzDTb8ZOfaL1ihfle3H67SQxNDYiO4vdr/dVXWm/d2v4ycnO1PvNMrR94oP0Jua5O640bzWfcNO3de+hy+fla33ij1pdeqvV992n94otab9rUepnZ2VrfeqvWUVHm/TztNK3POkvrH/9Y61mzTCOinU6IpBCIqb1JQWutK+pMBTh/vtnyLVvaXdRJrbJyvd606Sq9dKnSX3zRQxcUvKr9rf0wVq40FWlMjHnDJk7UOivr0OU+/tg8P2+e1h6P1uvXa/3ss1ovXWoeB1N2tta9emkdG2sqp5Ejte7RwyS69vjqK6337Dn88+vXa+1yaT1smFlvamrrFcW8eeY9mzpV62uuMf/Pn996mV6v1nPnan3hhVo/9ZSp0LTWeskSrYcPN6+12Y6cgFrj92v97rtmD6opOc2cqXVBgXm+vFzrP/zBbMevfqW1z3doGWvWaN2zp9aRkVq/996Bz1VWap2WpvXgwVrX1x+67pwck9AeeUTr99/XuqTk6DEvW2a+h2C+m9deq/XOnea54mKt//EPrc8/32zHb35jKtEdOw4s4+uvTcw2mynnppsO3DaPxyTIjz82v4ENG8xe3/PPaz1njtYXXaR1//5aW60t71vTZLVqffnl5rPweLR+8klTwTscppHVtE6r9dDPe9Uq8z11OLS+6irzfe3APUhJCkfwwAPm+3Tw9zTUVFSs0WvXZuqlS9Fff32mrqj46tCFfv9782V+443Df0H9fq3HjTNffpfrwB9JXJzWV1yh9cMPm1bkWWdpPXCgaTX9+c+mVVlTc2B5eXla33231t26ta1rxes9MPlUVWn90Uda//KXJgEkJJjKWmut160zH/7Pftb2N6rJ/Pmm5duzp2khHmz7drO+Xr1M6/Drr013yumnt+xFVVebfsumhFBXZ76IZ59tuvCWLj2wzHXrtB492izfo0fL+9qnT8vfF14wyad/f1P+0dTXa/3mm1qPHWvK6N9f6wULzGcUFmY+s5tuamkMDBli/l58cctnVVVlPhun0+xNfvNN6+v6z3/Max9+2MS2YIHpwktMPLRCbYpl+HCTTCIiTAWZnq71D36g9ZgxZpnkZK2feMJ8RxwOE/NZZ5n3D8z3tW9f81k1lTtpkvn83nvPfEdTUkzM995rnr/yShPfvHkt721rk81mYrzoIlORvPKK1v/+t5kWLTIxNbXyk5LM3/PO03rbNvN+NDSYPZwf/tA8N3eumf/ZZyauvn3N9ygAJCkcwcyZWp9yynEX0yX4/V6dnz9Pr1iRoJcuRX/33UW6unrDsRf0+eem8rvlFtP6+/57rd95x7TkunVrqdR+8AOtp08/8IdntWqdkWF2sa+5xvy4rVatR40yz19+udZu98GBm1bcDTdoHR1tlrPbTUXW1Bqz2Uyr8ttvD3zt7Nnm+RUrDt2OykqzDRdddGBF9+KLppKZMKEl0TS1zP1+rT/80FRGcXEHdg0sWGDWNXmyqQjCwszjadMO7G4rKzPJMirKVHDnn6/1lCkmgSUltSTlzZtN633yZJNcmsr49FMT3+GSXVWVaYk+8EBLZZWervVzzx2YUDdt0nr8eFPWJZeYpOT3m3UpZRLU3/6mdffupoxLLtF6374jfjX0rFnms2k63tC9u/mc580zibOiwuwB/OEPWl9wgfl+XHutqWB/8QtTYZ99tln3E0+Y4xdNdu3S+rrrTEV9990m+Tc1Itxu8xk+8oj5wTd934YPNw2PJo88YuY7nebvmDGmO23FCq0/+MAk0PffNxV7W/Z8y8u1fvxxrc8913z+rTVq3G6z5wdmW8PDzeefn3/08ttJksIRjBxpfm+ihcdToXfs+K1evjxaL12q9Lff/lgXFr6jfb4O2J3yeg/8ITfZt8+0sB54wFRysbHmx3HbbWaX3+83FQWYhPO//2n9zDOmFTtwoJkfEWEqmIcfNrv2d9xh+m2XLDl8q7mqyrRuTzlF69dea1lu6VJTUVosJtEoZXbjH3/c/P/DH5qWcna2WS4qyuxJNXW/pKSY5Hiwhx4yzw8ebBLSkiXmPTnYjh0mGZ1xhqkAhwwx/ctt7epqavW+954p65//1HrGDLOdTa1mpbT+0Y9MZddad5DWZn5l5aHzFy0y3URNXYmrVrUtrn37TKK7+WaTvFrb9kDz+01r/MknW06m2N+8eSYZ/e9/nXfQ3+PR+uqrWxJVYWFAVydJ4TD8frOXJidFtK6hoVjn5Nynv/iih166FL1iRYL+/vs7dFVVK8cSOprf33qf3ltvtbTiwOwNnH22aeW29gNvi2XLTCXelFjOPtv8f+qppv+4tNR0PzkcZv655x6Y2HJztR4woOUH/dJLR+6PbO8xjGNRX29aPPv3daekmK663/7W7Lnt30Juj82bO7fi7Op8PpPEO+H70dakELDrFALleMc+KiiA5GQzrMXtt3dgYF2M3++lrOy/FBS8QHHxIrRuwOUaSY8e15GYeDEOR8/ODWjbNnM9xLBhkJbWMWOS+P2wYgW8/jp8/LG5YO+Pf4TIyJZlcnPhgw/MeCgH34u1vNzENGbMiTNGSnY2/Pa35iLEyZOhf/8TJzYRVG29TiHkksKKFTBxInz4IUyZ0oGBdWEeTyn79r1GQcG/qK7OAsDlGklCwo+IihqN3Z6AzRZHWFgydntskKMVQrTmRBgQ74TUNKRPv37BjeNkYrfHk5p6G6mpt1FdvYGSksWUlPyHXbt+x/6j7yllo2fPm+nd+0HCwhKDF7AQot1CMinYbNC7d7AjOTm5XENwuYbQu/ev8HhKcLu34/WW4vGUUl6+jPz8ZygoeJFevebQo8c1OBytjYEohDhRhWRS6NvXJAZxfOz2BOz2hObHSUmXk5p6F9u3/4odO+5nx477CQtLJioqk9jYSXTvfjkOR3IQIxZCHE3IVY3ffy9dR4EUGTmQoUPfo6oqi4qK5VRVraGycg0lJf8mJ+ce4uOnkJR0FbGxkyRBCHECCqmk4PebkzPOOSfYkXR9UVEZREVlND+urd1KQcF89u17mc2bLwcgLCyF6OhMYmImEB9/HhERg1BypowQQRVSSWHPHnC7ZU8hGCIi+tO37x/o0+d3VFauprJydeNexFcUF79HTs7dhIWlEBf3Q6KjxxEdPY7IyCFYLCH1FRUi6ELqF9d05tFppwU3jlCmlJWYmB8QE9NyC8+6ut2Uln5MWdnHlJa+z7595l5LFksELtcIoqMziYoaTXT0eMLD04MUuRChISSTguwpnFiczl707Hk9PXtej9aaurodjXsTq6iqWsuePf/E738CgIiIgcTHTyU+/jxcrgw59VWIDhZyScHhMBfEihOTUorw8L6Eh/clKckce/D7vdTWbqSsbCmlpR+Qn/8UeXl/AcBuTyQycjAuVwYu10iiokYSHt5fup2EaKeQ+uVs2wannCJ3YzzZWCw2XK7huFzDSUu7C6+3msrKL6mp2UhNzUZqazeyZ8+z+P21+70mEpstCpstjtjYs+jW7UJiYydhsYQFcUuEOPGFVFL4/ns5ntAV2Gwu4uMnEx8/uXme1j5qa7dQVbUOt3s7Pl8VPl8VDQ17KSh4kT17nsFqjSE2dgKRkUMbp0E4HL2w2WLlrCchGoVMUvD5ICcHpk0LdiQiEJSyEhk5mMjIwYc85/O5KSv7hOLi96isXE1p6Udo7W1+3mKJbLzyWuPz1eL312K3JxAXN5n4+CnExp6FzebqxK0RInhCJink5kJDgxxkDkVWazjduv2Ybt1+DIDf30Bt7VZqa7dQX59LfX0e9fX5KGXBYonAao2grm5n8x4GKOz2RMLCehAWloTNFoPF4sRiCcdmiyU8/DQiIvoTETFADnyLk17IJAU580g0sVjCcLmG4nINPeJyfn89FRWfU16+goaGvTQ07KOhoYD6+nz8fjd+fx0eTyla1ze/Jjy8P/Hx5xIXdy4u13Cs1hhstiiUsgZ6s4ToECGTFMAMMS/HFERbWSwO4uLOIS7u8JfAa+2jrm43tbVbqanZQHn5p+zd+y/y858+YDmbLa75LKnIyOE4nb0JC+uO3d698ZiGHaWscmxDBF3I3U9BiEAzexhfUle3Ha+3Eq+3Ao9nH9XV31JT8y0+X/VhX6uUA7u9G2FhSYSF9Wi8V0UMNlssdntiYzfVQByONEkg4pjI/RSECBKzh3EWcNYhz2ntp65uJ/X1e/B4CmloKMTnq8Dv96B1Q2OXVFFzV1Vt7Sa83nK83krA31yO1epqHDPqfOLjz8fp7E19fT719bl4vWU4nemEh5+K1RrReRsuugRJCkJ0IqUszRfnHQut/Xg8RY0HyDdTXf0tZWX/JTv7jiO+zuFIxWqNaX5ssTgaD5gn43AkY7PFY7PFYbPFYrE4AI3WfiwWJ5GRQ3A4erRnM8VJTJKCECcBpSyNXUpJxMZObJ5fW5tNaelHeDzFOJ1pOByp2Gxx1NXtpLb2e9zubfh8Nc3L+/1uGhr2Ul29noaGQvbf+2hNWFgPIiOHY7GENZ6uW4NStsbEYiaHI5WwsBQcjhQsFmfjHk8DWvtQyobFYkepMGy2WGy2GJSSq0dPZJIUhDiJRUScSkTEbYfMj44ec9TXau1vPOZRhtdbht/f0FhhK3y+Kqqrv6W6+mtqar5Daz9WayRWqwu/30NNzQbKyj7B6y0/xogt2O3xOBy9cLmGERk5nIiI/o17KRaUsmC1RmG3mz0Yq9UFKDl+0okkKQgRopSyYLfHYrfHAn0OeT4u7uyjluHzuWlo2NN8rYfWHpQKaxxOxILWXrT24PfX4/WW4/GU4PWa27iWlHxIQcGLxxIxStmxWBwoFYbp6vI2rtNOePgphIeftt+xFHMSjda+xj2XBgCczt6Eh/drXC6qsWwNKCwWR2P5dny+GrzeCny+CkBhs8VgtUZjtUZ26b0dSQpCiHazWsMbK+NT2vX6hoZ9uN05jZW7H/Dh9VY13/fb76/BnCGpAf9+B+TrMUnCdE/5/XW43dlUVa2lqOgtDu0Ws2Kx2NHa35wc2stiCScmZjyxsWcTGzsJrf3U1++irm4nPl81Vmt0YwKJoukYDfix2eJwOns3D61iTigowOMpRJxTPN4AAAdMSURBVCkHNltUY9JxYbGEY7WGo1RYp+8lSVIQQgRN03GSjqS1D619jY8USlmaLx7UWtPQsBe3extudzY+n7u50m1KGH5/HX5/A1arq/F0YHOg3uutwOutpL5+N+Xly9ix475D1q2UHa09Hbg1FqxWV/PUs+fPSEub3YHlH0qSghCiSzEXAbZ+BblSCoejJw5HT2JjJx3XehoaCqmo+AKLJRynMx2nsxdWawQ+Xx0+XwVeb1VjwjHHSjyeYurqdlFXtxufrwK7vXvjtSiJaO3B56vE663E56tpvGLe3Xxw3+erxuer7vAE2hpJCkII0Q5hYd1JTLzokPlWqxOr1XlIBe509iYqalRnhdduXfdoiRBCiGMmSUEIIUQzSQpCCCGaBTQpKKWmKKW2KqWylVJzWnneoZR6s/H51Uqp9EDGI4QQ4sgClhSUOfz/d+B8YBBwuVJq0EGL/RQo01qfCvwNeCxQ8QghhDi6QO4pjAGytdbbtbla5A3ggoOWuQCY3/j/QuAcJdezCyFE0Pz/9u4txq6qjuP490cr0IuhXKrBFqEIQcRIkcZUEUOABwNEeABsAGNMiC8QwGBUjGAk8cHEiD4YhQCmSkO42AZiCKCFNJLIZabFS1sSG7kNae1AL1oTlMuPh7XmdGZomJOJ55xh1u/zMrPW3rOzzsra8z977b3/q5dBYQnw8rjySK074D4ui+buBY6cfCBJX5c0JGlodHS0R82NiIj3xY1m27fZXmF7xeLFWQM3IqJXevny2ivAMePKS2vdgfYZkTQXOAx47b0OOjw8/KqkF6fZpqOAV6f5t7NR+mOi9Md+6YuJZkN/HNvNTr0MCs8AJ0paRvnnvwq4bNI+DwJfBf4EXAw85inWB7U97UsFSUPdLEfXivTHROmP/dIXE7XUHz0LCrbflHQ18AgwB7jT9mZJNwNDth8E7gB+I2kbsIsSOCIiYkB6mvvI9kPAQ5Pqbhr3++vAJb1sQ0REdO99caP5/+i2QTdghkl/TJT+2C99MVEz/aEppvAjIqIhrV0pRETEe2gmKEyVh2k2k3SMpMclbZG0WdK1tf4ISb+X9Pf68/BBt7WfJM2RtEnS72p5Wc3Bta3m5Dp40G3sF0mLJN0v6TlJWyV9ttXxIekb9Tz5m6S7JR3a0thoIih0mYdpNnsTuN72J4CVwFX1838HWG/7RGB9LbfkWmDruPKPgFtqLq7dlNxcrfgZ8LDtjwOnUvqlufEhaQlwDbDC9icpT06uoqGx0URQoLs8TLOW7e22N9bf/0054ZcwMffUauCiwbSw/yQtBc4Hbq9lAWdTcnBBQ/0h6TDgC5RHxLH9P9t7aHd8zAXm1Rdq5wPbaWhstBIUusnD1ISanvw04Cngw7a31007gN4vADtz/BT4FvB2LR8J7Kk5uKCtMbIMGAV+VafTbpe0gAbHh+1XgB8DL1GCwV5gmIbGRitBIQBJC4HfAtfZ/tf4bfVN8iYeRZN0AbDT9vCg2zJDzAU+DfzC9mnAf5g0VdTK+Kj3TS6kBMqPAAuALw60UX3WSlDoJg/TrCbpA5SAsMb22lr9T0lH1+1HAzsH1b4+OwP4kqQXKFOJZ1Pm1BfVKQNoa4yMACO2n6rl+ylBosXxcS7wvO1R228AaynjpZmx0UpQ6ORhqk8NrKLkXWpCnS+/A9hq+yfjNo3lnqL+fKDfbRsE2zfYXmr7OMpYeMz25cDjlBxc0FZ/7ABelnRSrToH2EKb4+MlYKWk+fW8GeuLZsZGMy+vSTqPMo88lofphwNuUt9I+jzwR+Cv7J9D/y7lvsK9wEeBF4FLbe8aSCMHRNJZwDdtXyDpeMqVwxHAJuAK2/8dZPv6RdJyyk33g4F/AF+jfGlsbnxI+gHwZcpTe5uAKyn3EJoYG80EhYiImFor00cREdGFBIWIiOhIUIiIiI4EhYiI6EhQiIiIjgSFiD6SdNZYVtaImShBISIiOhIUIg5A0hWSnpb0rKRb69oL+yTdUnPtr5e0uO67XNKTkv4iad3YugOSTpD0B0l/lrRR0sfq4ReOW7tgTX1zNmJGSFCImETSyZQ3Ws+wvRx4C7ickhxtyPYpwAbg+/VPfg182/anKG+Nj9WvAX5u+1Tgc5Ssm1Cy1F5HWdvjeEpunYgZYe7Uu0Q05xzgdOCZ+iV+HiUZ3NvAPXWfu4C1dS2CRbY31PrVwH2SPggssb0OwPbrAPV4T9seqeVngeOAJ3r/sSKmlqAQ8W4CVtu+YUKldOOk/aabI2Z8zpy3yHkYM0imjyLebT1wsaQPQWct62Mp58tYpszLgCds7wV2Szqz1n8F2FBXuBuRdFE9xiGS5vf1U0RMQ76hRExie4uk7wGPSjoIeAO4irL4zGfqtp2U+w5QUin/sv7TH8swCiVA3Crp5nqMS/r4MSKmJVlSI7okaZ/thYNuR0QvZfooIiI6cqUQEREduVKIiIiOBIWIiOhIUIiIiI4EhYiI6EhQiIiIjgSFiIjoeAeB4Tc82e594wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 489us/sample - loss: 0.4819 - acc: 0.8685\n",
      "Loss: 0.48190565458339324 Accuracy: 0.8685358\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3981 - acc: 0.2076\n",
      "Epoch 00001: val_loss improved from inf to 1.73749, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/001-1.7375.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 2.3980 - acc: 0.2076 - val_loss: 1.7375 - val_acc: 0.4489\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6180 - acc: 0.4743\n",
      "Epoch 00002: val_loss improved from 1.73749 to 1.33921, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/002-1.3392.hdf5\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 1.6179 - acc: 0.4743 - val_loss: 1.3392 - val_acc: 0.5879\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3504 - acc: 0.5668\n",
      "Epoch 00003: val_loss improved from 1.33921 to 1.25929, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/003-1.2593.hdf5\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 1.3505 - acc: 0.5668 - val_loss: 1.2593 - val_acc: 0.6140\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2084 - acc: 0.6118\n",
      "Epoch 00004: val_loss improved from 1.25929 to 1.04914, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/004-1.0491.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 1.2083 - acc: 0.6118 - val_loss: 1.0491 - val_acc: 0.6767\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0913 - acc: 0.6550\n",
      "Epoch 00005: val_loss improved from 1.04914 to 0.95589, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/005-0.9559.hdf5\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 1.0913 - acc: 0.6550 - val_loss: 0.9559 - val_acc: 0.7126\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9844 - acc: 0.6895\n",
      "Epoch 00006: val_loss improved from 0.95589 to 0.85895, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/006-0.8590.hdf5\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.9844 - acc: 0.6895 - val_loss: 0.8590 - val_acc: 0.7421\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9066 - acc: 0.7164\n",
      "Epoch 00007: val_loss improved from 0.85895 to 0.81313, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/007-0.8131.hdf5\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.9066 - acc: 0.7164 - val_loss: 0.8131 - val_acc: 0.7591\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8267 - acc: 0.7421\n",
      "Epoch 00008: val_loss improved from 0.81313 to 0.73453, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/008-0.7345.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.8267 - acc: 0.7421 - val_loss: 0.7345 - val_acc: 0.7792\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7638 - acc: 0.7664\n",
      "Epoch 00009: val_loss improved from 0.73453 to 0.68955, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/009-0.6896.hdf5\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.7637 - acc: 0.7665 - val_loss: 0.6896 - val_acc: 0.7969\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7081 - acc: 0.7804\n",
      "Epoch 00010: val_loss improved from 0.68955 to 0.61680, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/010-0.6168.hdf5\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.7081 - acc: 0.7804 - val_loss: 0.6168 - val_acc: 0.8232\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.8003\n",
      "Epoch 00011: val_loss improved from 0.61680 to 0.57879, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/011-0.5788.hdf5\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.6495 - acc: 0.8002 - val_loss: 0.5788 - val_acc: 0.8353\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5936 - acc: 0.8173\n",
      "Epoch 00012: val_loss did not improve from 0.57879\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.5936 - acc: 0.8173 - val_loss: 0.6302 - val_acc: 0.8157\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8324\n",
      "Epoch 00013: val_loss improved from 0.57879 to 0.48014, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/013-0.4801.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.5454 - acc: 0.8324 - val_loss: 0.4801 - val_acc: 0.8621\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8437\n",
      "Epoch 00014: val_loss improved from 0.48014 to 0.45955, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/014-0.4596.hdf5\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.5044 - acc: 0.8437 - val_loss: 0.4596 - val_acc: 0.8693\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.8568\n",
      "Epoch 00015: val_loss did not improve from 0.45955\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.4617 - acc: 0.8568 - val_loss: 0.4706 - val_acc: 0.8642\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8633\n",
      "Epoch 00016: val_loss improved from 0.45955 to 0.42646, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/016-0.4265.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.4386 - acc: 0.8633 - val_loss: 0.4265 - val_acc: 0.8810\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8720\n",
      "Epoch 00017: val_loss improved from 0.42646 to 0.38768, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/017-0.3877.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.4113 - acc: 0.8720 - val_loss: 0.3877 - val_acc: 0.8921\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8803\n",
      "Epoch 00018: val_loss did not improve from 0.38768\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.3868 - acc: 0.8803 - val_loss: 0.4291 - val_acc: 0.8819\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8874\n",
      "Epoch 00019: val_loss improved from 0.38768 to 0.35523, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/019-0.3552.hdf5\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.3590 - acc: 0.8875 - val_loss: 0.3552 - val_acc: 0.9003\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8929\n",
      "Epoch 00020: val_loss improved from 0.35523 to 0.34724, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/020-0.3472.hdf5\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.3445 - acc: 0.8929 - val_loss: 0.3472 - val_acc: 0.9043\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8979\n",
      "Epoch 00021: val_loss improved from 0.34724 to 0.34172, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/021-0.3417.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3246 - acc: 0.8979 - val_loss: 0.3417 - val_acc: 0.9052\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9018\n",
      "Epoch 00022: val_loss did not improve from 0.34172\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3106 - acc: 0.9018 - val_loss: 0.3582 - val_acc: 0.8975\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9082\n",
      "Epoch 00023: val_loss improved from 0.34172 to 0.31105, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/023-0.3110.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.2936 - acc: 0.9082 - val_loss: 0.3110 - val_acc: 0.9143\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9115\n",
      "Epoch 00024: val_loss did not improve from 0.31105\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.2826 - acc: 0.9116 - val_loss: 0.3276 - val_acc: 0.9080\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9136\n",
      "Epoch 00025: val_loss did not improve from 0.31105\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.2698 - acc: 0.9135 - val_loss: 0.3135 - val_acc: 0.9143\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9190\n",
      "Epoch 00026: val_loss improved from 0.31105 to 0.30009, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/026-0.3001.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.2552 - acc: 0.9190 - val_loss: 0.3001 - val_acc: 0.9166\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9210\n",
      "Epoch 00027: val_loss did not improve from 0.30009\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.2474 - acc: 0.9209 - val_loss: 0.3011 - val_acc: 0.9189\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9230\n",
      "Epoch 00028: val_loss did not improve from 0.30009\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.2390 - acc: 0.9230 - val_loss: 0.3122 - val_acc: 0.9133\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9255\n",
      "Epoch 00029: val_loss improved from 0.30009 to 0.29614, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/029-0.2961.hdf5\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.2322 - acc: 0.9255 - val_loss: 0.2961 - val_acc: 0.9196\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9301\n",
      "Epoch 00030: val_loss did not improve from 0.29614\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.2174 - acc: 0.9301 - val_loss: 0.3066 - val_acc: 0.9143\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9322\n",
      "Epoch 00031: val_loss did not improve from 0.29614\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.2141 - acc: 0.9322 - val_loss: 0.3045 - val_acc: 0.9168\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9332\n",
      "Epoch 00032: val_loss improved from 0.29614 to 0.28870, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/032-0.2887.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.2041 - acc: 0.9331 - val_loss: 0.2887 - val_acc: 0.9243\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9335\n",
      "Epoch 00033: val_loss improved from 0.28870 to 0.28101, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/033-0.2810.hdf5\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.1998 - acc: 0.9335 - val_loss: 0.2810 - val_acc: 0.9259\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9354\n",
      "Epoch 00034: val_loss did not improve from 0.28101\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1945 - acc: 0.9354 - val_loss: 0.2932 - val_acc: 0.9215\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9378\n",
      "Epoch 00035: val_loss did not improve from 0.28101\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1870 - acc: 0.9378 - val_loss: 0.3020 - val_acc: 0.9215\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9402\n",
      "Epoch 00036: val_loss improved from 0.28101 to 0.26799, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_9_conv_checkpoint/036-0.2680.hdf5\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1816 - acc: 0.9402 - val_loss: 0.2680 - val_acc: 0.9301\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9417\n",
      "Epoch 00037: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1756 - acc: 0.9416 - val_loss: 0.2929 - val_acc: 0.9236\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9439\n",
      "Epoch 00038: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1731 - acc: 0.9439 - val_loss: 0.2817 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1624 - acc: 0.9468 - val_loss: 0.2794 - val_acc: 0.9315\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9463\n",
      "Epoch 00040: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.1619 - acc: 0.9463 - val_loss: 0.2909 - val_acc: 0.9266\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9478\n",
      "Epoch 00041: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1591 - acc: 0.9478 - val_loss: 0.2736 - val_acc: 0.9299\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9490\n",
      "Epoch 00042: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1529 - acc: 0.9490 - val_loss: 0.3224 - val_acc: 0.9231\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9520\n",
      "Epoch 00043: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1477 - acc: 0.9520 - val_loss: 0.2882 - val_acc: 0.9315\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9536\n",
      "Epoch 00044: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1393 - acc: 0.9536 - val_loss: 0.3032 - val_acc: 0.9306\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9540\n",
      "Epoch 00045: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1385 - acc: 0.9541 - val_loss: 0.2905 - val_acc: 0.9297\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9549\n",
      "Epoch 00046: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.1380 - acc: 0.9549 - val_loss: 0.2909 - val_acc: 0.9313\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9569\n",
      "Epoch 00047: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1296 - acc: 0.9569 - val_loss: 0.2980 - val_acc: 0.9259\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9583\n",
      "Epoch 00048: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1282 - acc: 0.9583 - val_loss: 0.3068 - val_acc: 0.9248\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9573\n",
      "Epoch 00049: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.1285 - acc: 0.9573 - val_loss: 0.2834 - val_acc: 0.9315\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9595\n",
      "Epoch 00050: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1229 - acc: 0.9595 - val_loss: 0.2837 - val_acc: 0.9313\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9614\n",
      "Epoch 00051: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1174 - acc: 0.9614 - val_loss: 0.2870 - val_acc: 0.9357\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9601\n",
      "Epoch 00052: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1209 - acc: 0.9601 - val_loss: 0.2915 - val_acc: 0.9341\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9628\n",
      "Epoch 00053: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1120 - acc: 0.9628 - val_loss: 0.2848 - val_acc: 0.9357\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9635\n",
      "Epoch 00054: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.1094 - acc: 0.9635 - val_loss: 0.2772 - val_acc: 0.9394\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9638\n",
      "Epoch 00055: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.1085 - acc: 0.9638 - val_loss: 0.2895 - val_acc: 0.9329\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9631\n",
      "Epoch 00056: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1084 - acc: 0.9631 - val_loss: 0.3056 - val_acc: 0.9357\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9660\n",
      "Epoch 00057: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.1023 - acc: 0.9660 - val_loss: 0.2943 - val_acc: 0.9350\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9659\n",
      "Epoch 00058: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.1039 - acc: 0.9658 - val_loss: 0.3043 - val_acc: 0.9357\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9659\n",
      "Epoch 00059: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.1008 - acc: 0.9659 - val_loss: 0.3031 - val_acc: 0.9357\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9671\n",
      "Epoch 00060: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.0995 - acc: 0.9671 - val_loss: 0.3068 - val_acc: 0.9352\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9661\n",
      "Epoch 00061: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.1008 - acc: 0.9661 - val_loss: 0.2906 - val_acc: 0.9352\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9677\n",
      "Epoch 00062: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0936 - acc: 0.9677 - val_loss: 0.2935 - val_acc: 0.9369\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9685\n",
      "Epoch 00063: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0918 - acc: 0.9685 - val_loss: 0.2802 - val_acc: 0.9387\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9701\n",
      "Epoch 00064: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0919 - acc: 0.9701 - val_loss: 0.2829 - val_acc: 0.9369\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9702\n",
      "Epoch 00065: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0881 - acc: 0.9702 - val_loss: 0.3027 - val_acc: 0.9343\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9705\n",
      "Epoch 00066: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.0892 - acc: 0.9705 - val_loss: 0.2800 - val_acc: 0.9380\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9708\n",
      "Epoch 00067: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0897 - acc: 0.9708 - val_loss: 0.2961 - val_acc: 0.9329\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9731\n",
      "Epoch 00068: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0798 - acc: 0.9731 - val_loss: 0.3123 - val_acc: 0.9390\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9720\n",
      "Epoch 00069: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0846 - acc: 0.9720 - val_loss: 0.2788 - val_acc: 0.9422\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9708\n",
      "Epoch 00070: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0847 - acc: 0.9708 - val_loss: 0.2938 - val_acc: 0.9392\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9722\n",
      "Epoch 00071: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0813 - acc: 0.9722 - val_loss: 0.3071 - val_acc: 0.9371\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9726\n",
      "Epoch 00072: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0821 - acc: 0.9726 - val_loss: 0.2884 - val_acc: 0.9390\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9734\n",
      "Epoch 00073: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0776 - acc: 0.9734 - val_loss: 0.3219 - val_acc: 0.9350\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9740\n",
      "Epoch 00074: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.0800 - acc: 0.9740 - val_loss: 0.2880 - val_acc: 0.9369\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9760\n",
      "Epoch 00075: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0739 - acc: 0.9760 - val_loss: 0.3108 - val_acc: 0.9399\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9754\n",
      "Epoch 00076: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0740 - acc: 0.9754 - val_loss: 0.3045 - val_acc: 0.9418\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9748\n",
      "Epoch 00077: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0752 - acc: 0.9748 - val_loss: 0.2928 - val_acc: 0.9390\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9772\n",
      "Epoch 00078: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.0686 - acc: 0.9772 - val_loss: 0.3147 - val_acc: 0.9371\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9746\n",
      "Epoch 00079: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0747 - acc: 0.9746 - val_loss: 0.3186 - val_acc: 0.9376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9774\n",
      "Epoch 00080: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0669 - acc: 0.9774 - val_loss: 0.3124 - val_acc: 0.9369\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9780\n",
      "Epoch 00081: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0646 - acc: 0.9780 - val_loss: 0.3309 - val_acc: 0.9406\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9776\n",
      "Epoch 00082: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0711 - acc: 0.9776 - val_loss: 0.3246 - val_acc: 0.9373\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9762\n",
      "Epoch 00083: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.0696 - acc: 0.9762 - val_loss: 0.3102 - val_acc: 0.9343\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9789\n",
      "Epoch 00084: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0625 - acc: 0.9789 - val_loss: 0.3145 - val_acc: 0.9378\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9767\n",
      "Epoch 00085: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0684 - acc: 0.9767 - val_loss: 0.3329 - val_acc: 0.9380\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9807\n",
      "Epoch 00086: val_loss did not improve from 0.26799\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0574 - acc: 0.9807 - val_loss: 0.3180 - val_acc: 0.9387\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 504us/sample - loss: 0.3154 - acc: 0.9074\n",
      "Loss: 0.3154498406597882 Accuracy: 0.9073728\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4767 - acc: 0.1797\n",
      "Epoch 00001: val_loss improved from inf to 1.82376, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/001-1.8238.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.4766 - acc: 0.1797 - val_loss: 1.8238 - val_acc: 0.4253\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6122 - acc: 0.4672\n",
      "Epoch 00002: val_loss improved from 1.82376 to 1.27557, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/002-1.2756.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 1.6123 - acc: 0.4672 - val_loss: 1.2756 - val_acc: 0.6052\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2745 - acc: 0.5851\n",
      "Epoch 00003: val_loss improved from 1.27557 to 1.03934, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/003-1.0393.hdf5\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 1.2745 - acc: 0.5851 - val_loss: 1.0393 - val_acc: 0.6837\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6475\n",
      "Epoch 00004: val_loss improved from 1.03934 to 0.88468, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/004-0.8847.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 1.0973 - acc: 0.6475 - val_loss: 0.8847 - val_acc: 0.7331\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9610 - acc: 0.6906\n",
      "Epoch 00005: val_loss improved from 0.88468 to 0.81261, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/005-0.8126.hdf5\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.9610 - acc: 0.6906 - val_loss: 0.8126 - val_acc: 0.7468\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8584 - acc: 0.7271\n",
      "Epoch 00006: val_loss improved from 0.81261 to 0.69965, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/006-0.6997.hdf5\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.8584 - acc: 0.7271 - val_loss: 0.6997 - val_acc: 0.7841\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7558\n",
      "Epoch 00007: val_loss improved from 0.69965 to 0.63537, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/007-0.6354.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.7660 - acc: 0.7557 - val_loss: 0.6354 - val_acc: 0.8081\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.7788\n",
      "Epoch 00008: val_loss improved from 0.63537 to 0.55263, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/008-0.5526.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.6981 - acc: 0.7789 - val_loss: 0.5526 - val_acc: 0.8316\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.7982\n",
      "Epoch 00009: val_loss improved from 0.55263 to 0.50412, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/009-0.5041.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.6384 - acc: 0.7982 - val_loss: 0.5041 - val_acc: 0.8537\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.8167\n",
      "Epoch 00010: val_loss improved from 0.50412 to 0.47502, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/010-0.4750.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.5763 - acc: 0.8167 - val_loss: 0.4750 - val_acc: 0.8588\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.8311\n",
      "Epoch 00011: val_loss improved from 0.47502 to 0.43803, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/011-0.4380.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.5390 - acc: 0.8311 - val_loss: 0.4380 - val_acc: 0.8714\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8424\n",
      "Epoch 00012: val_loss improved from 0.43803 to 0.41788, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/012-0.4179.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.4978 - acc: 0.8424 - val_loss: 0.4179 - val_acc: 0.8826\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8547\n",
      "Epoch 00013: val_loss did not improve from 0.41788\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.4568 - acc: 0.8547 - val_loss: 0.4433 - val_acc: 0.8689\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8614\n",
      "Epoch 00014: val_loss improved from 0.41788 to 0.36452, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/014-0.3645.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.4334 - acc: 0.8614 - val_loss: 0.3645 - val_acc: 0.8933\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8708\n",
      "Epoch 00015: val_loss improved from 0.36452 to 0.34533, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/015-0.3453.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.4051 - acc: 0.8708 - val_loss: 0.3453 - val_acc: 0.9043\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8764\n",
      "Epoch 00016: val_loss improved from 0.34533 to 0.33029, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/016-0.3303.hdf5\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.3884 - acc: 0.8764 - val_loss: 0.3303 - val_acc: 0.9043\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8823\n",
      "Epoch 00017: val_loss improved from 0.33029 to 0.30826, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/017-0.3083.hdf5\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.3665 - acc: 0.8823 - val_loss: 0.3083 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8890\n",
      "Epoch 00018: val_loss did not improve from 0.30826\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.3479 - acc: 0.8890 - val_loss: 0.3140 - val_acc: 0.9106\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8941\n",
      "Epoch 00019: val_loss improved from 0.30826 to 0.30387, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/019-0.3039.hdf5\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.3337 - acc: 0.8941 - val_loss: 0.3039 - val_acc: 0.9122\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9011\n",
      "Epoch 00020: val_loss did not improve from 0.30387\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.3106 - acc: 0.9011 - val_loss: 0.3186 - val_acc: 0.9026\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9049\n",
      "Epoch 00021: val_loss improved from 0.30387 to 0.26596, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/021-0.2660.hdf5\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.2969 - acc: 0.9049 - val_loss: 0.2660 - val_acc: 0.9210\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9093\n",
      "Epoch 00022: val_loss did not improve from 0.26596\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.2851 - acc: 0.9093 - val_loss: 0.2746 - val_acc: 0.9210\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9128\n",
      "Epoch 00023: val_loss did not improve from 0.26596\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.2742 - acc: 0.9128 - val_loss: 0.2681 - val_acc: 0.9241\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9145\n",
      "Epoch 00024: val_loss improved from 0.26596 to 0.25548, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/024-0.2555.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.2616 - acc: 0.9145 - val_loss: 0.2555 - val_acc: 0.9283\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9197\n",
      "Epoch 00025: val_loss improved from 0.25548 to 0.25529, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/025-0.2553.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.2495 - acc: 0.9197 - val_loss: 0.2553 - val_acc: 0.9280\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9215\n",
      "Epoch 00026: val_loss did not improve from 0.25529\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.2434 - acc: 0.9215 - val_loss: 0.2574 - val_acc: 0.9255\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9248\n",
      "Epoch 00027: val_loss improved from 0.25529 to 0.22112, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/027-0.2211.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.2304 - acc: 0.9248 - val_loss: 0.2211 - val_acc: 0.9343\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9279\n",
      "Epoch 00028: val_loss did not improve from 0.22112\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.2249 - acc: 0.9279 - val_loss: 0.2455 - val_acc: 0.9304\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9284\n",
      "Epoch 00029: val_loss did not improve from 0.22112\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.2180 - acc: 0.9284 - val_loss: 0.2283 - val_acc: 0.9366\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9319\n",
      "Epoch 00030: val_loss improved from 0.22112 to 0.21710, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/030-0.2171.hdf5\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.2077 - acc: 0.9319 - val_loss: 0.2171 - val_acc: 0.9350\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9351\n",
      "Epoch 00031: val_loss improved from 0.21710 to 0.19926, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/031-0.1993.hdf5\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.1977 - acc: 0.9351 - val_loss: 0.1993 - val_acc: 0.9427\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9370\n",
      "Epoch 00032: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.1922 - acc: 0.9370 - val_loss: 0.2174 - val_acc: 0.9425\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9401\n",
      "Epoch 00033: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.1858 - acc: 0.9401 - val_loss: 0.2031 - val_acc: 0.9469\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9418\n",
      "Epoch 00034: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.1765 - acc: 0.9418 - val_loss: 0.2193 - val_acc: 0.9341\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9423\n",
      "Epoch 00035: val_loss improved from 0.19926 to 0.19825, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/035-0.1983.hdf5\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.1741 - acc: 0.9423 - val_loss: 0.1983 - val_acc: 0.9471\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9448\n",
      "Epoch 00036: val_loss did not improve from 0.19825\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.1651 - acc: 0.9448 - val_loss: 0.2052 - val_acc: 0.9497\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9476\n",
      "Epoch 00037: val_loss improved from 0.19825 to 0.19406, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/037-0.1941.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.1603 - acc: 0.9476 - val_loss: 0.1941 - val_acc: 0.9469\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9471\n",
      "Epoch 00038: val_loss improved from 0.19406 to 0.19252, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/038-0.1925.hdf5\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.1626 - acc: 0.9472 - val_loss: 0.1925 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9510\n",
      "Epoch 00039: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.1504 - acc: 0.9510 - val_loss: 0.2002 - val_acc: 0.9427\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9512\n",
      "Epoch 00040: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.1506 - acc: 0.9511 - val_loss: 0.1994 - val_acc: 0.9455\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9491\n",
      "Epoch 00041: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.1530 - acc: 0.9491 - val_loss: 0.1940 - val_acc: 0.9485\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9547\n",
      "Epoch 00042: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.1394 - acc: 0.9547 - val_loss: 0.2412 - val_acc: 0.9401\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9552\n",
      "Epoch 00043: val_loss improved from 0.19252 to 0.19091, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/043-0.1909.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.1357 - acc: 0.9552 - val_loss: 0.1909 - val_acc: 0.9471\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9550\n",
      "Epoch 00044: val_loss improved from 0.19091 to 0.18030, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/044-0.1803.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.1366 - acc: 0.9550 - val_loss: 0.1803 - val_acc: 0.9518\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9569\n",
      "Epoch 00045: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.1292 - acc: 0.9569 - val_loss: 0.1939 - val_acc: 0.9464\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9576\n",
      "Epoch 00046: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.1248 - acc: 0.9576 - val_loss: 0.2042 - val_acc: 0.9483\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9589\n",
      "Epoch 00047: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.1210 - acc: 0.9589 - val_loss: 0.1877 - val_acc: 0.9534\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9612\n",
      "Epoch 00048: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.1177 - acc: 0.9612 - val_loss: 0.2426 - val_acc: 0.9413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9618\n",
      "Epoch 00049: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.1172 - acc: 0.9618 - val_loss: 0.1905 - val_acc: 0.9543\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9626\n",
      "Epoch 00050: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.1130 - acc: 0.9626 - val_loss: 0.1877 - val_acc: 0.9513\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9635\n",
      "Epoch 00051: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.1107 - acc: 0.9635 - val_loss: 0.1902 - val_acc: 0.9515\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9651\n",
      "Epoch 00052: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.1065 - acc: 0.9651 - val_loss: 0.1956 - val_acc: 0.9504\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9646\n",
      "Epoch 00053: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.1063 - acc: 0.9647 - val_loss: 0.1808 - val_acc: 0.9562\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9645\n",
      "Epoch 00054: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.1056 - acc: 0.9645 - val_loss: 0.1918 - val_acc: 0.9527\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9678\n",
      "Epoch 00055: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0991 - acc: 0.9678 - val_loss: 0.1926 - val_acc: 0.9520\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9672\n",
      "Epoch 00056: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0988 - acc: 0.9672 - val_loss: 0.2003 - val_acc: 0.9506\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9684\n",
      "Epoch 00057: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0943 - acc: 0.9684 - val_loss: 0.1981 - val_acc: 0.9469\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9684\n",
      "Epoch 00058: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0940 - acc: 0.9684 - val_loss: 0.1894 - val_acc: 0.9522\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9691\n",
      "Epoch 00059: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0933 - acc: 0.9691 - val_loss: 0.1937 - val_acc: 0.9499\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9699\n",
      "Epoch 00060: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0887 - acc: 0.9699 - val_loss: 0.2123 - val_acc: 0.9509\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9699\n",
      "Epoch 00061: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0887 - acc: 0.9699 - val_loss: 0.1952 - val_acc: 0.9543\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9700\n",
      "Epoch 00062: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0882 - acc: 0.9700 - val_loss: 0.1899 - val_acc: 0.9499\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9710\n",
      "Epoch 00063: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0859 - acc: 0.9710 - val_loss: 0.2018 - val_acc: 0.9504\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9733\n",
      "Epoch 00064: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0801 - acc: 0.9733 - val_loss: 0.1903 - val_acc: 0.9571\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9724\n",
      "Epoch 00065: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0815 - acc: 0.9724 - val_loss: 0.1928 - val_acc: 0.9550\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9733\n",
      "Epoch 00066: val_loss did not improve from 0.18030\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0760 - acc: 0.9733 - val_loss: 0.1872 - val_acc: 0.9539\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9732\n",
      "Epoch 00067: val_loss improved from 0.18030 to 0.17934, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/067-0.1793.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0781 - acc: 0.9732 - val_loss: 0.1793 - val_acc: 0.9583\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9734\n",
      "Epoch 00068: val_loss did not improve from 0.17934\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0790 - acc: 0.9734 - val_loss: 0.1936 - val_acc: 0.9543\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9742\n",
      "Epoch 00069: val_loss did not improve from 0.17934\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0751 - acc: 0.9742 - val_loss: 0.1815 - val_acc: 0.9585\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9738\n",
      "Epoch 00070: val_loss did not improve from 0.17934\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0742 - acc: 0.9738 - val_loss: 0.2099 - val_acc: 0.9541\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9739\n",
      "Epoch 00071: val_loss improved from 0.17934 to 0.17810, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_10_conv_checkpoint/071-0.1781.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0756 - acc: 0.9739 - val_loss: 0.1781 - val_acc: 0.9581\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9739\n",
      "Epoch 00072: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0783 - acc: 0.9739 - val_loss: 0.1966 - val_acc: 0.9485\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9773\n",
      "Epoch 00073: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.0642 - acc: 0.9773 - val_loss: 0.2307 - val_acc: 0.9502\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9777\n",
      "Epoch 00074: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0671 - acc: 0.9777 - val_loss: 0.1853 - val_acc: 0.9543\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9761\n",
      "Epoch 00075: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0708 - acc: 0.9761 - val_loss: 0.2039 - val_acc: 0.9557\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9774\n",
      "Epoch 00076: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0676 - acc: 0.9774 - val_loss: 0.2112 - val_acc: 0.9550\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9780\n",
      "Epoch 00077: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.0649 - acc: 0.9780 - val_loss: 0.2014 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9790\n",
      "Epoch 00078: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0645 - acc: 0.9790 - val_loss: 0.1844 - val_acc: 0.9567\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9765\n",
      "Epoch 00079: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.0715 - acc: 0.9765 - val_loss: 0.1931 - val_acc: 0.9576\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9796\n",
      "Epoch 00080: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.0599 - acc: 0.9796 - val_loss: 0.2116 - val_acc: 0.9539\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9794\n",
      "Epoch 00081: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0600 - acc: 0.9794 - val_loss: 0.1834 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9797\n",
      "Epoch 00082: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0604 - acc: 0.9797 - val_loss: 0.1988 - val_acc: 0.9529\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9793\n",
      "Epoch 00083: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0612 - acc: 0.9793 - val_loss: 0.2039 - val_acc: 0.9541\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9808\n",
      "Epoch 00084: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0555 - acc: 0.9808 - val_loss: 0.2141 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9799\n",
      "Epoch 00085: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0599 - acc: 0.9799 - val_loss: 0.2028 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9811\n",
      "Epoch 00086: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0574 - acc: 0.9811 - val_loss: 0.2056 - val_acc: 0.9571\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9806\n",
      "Epoch 00087: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0575 - acc: 0.9806 - val_loss: 0.1959 - val_acc: 0.9541\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9802\n",
      "Epoch 00088: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0587 - acc: 0.9802 - val_loss: 0.2210 - val_acc: 0.9562\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9802\n",
      "Epoch 00089: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0567 - acc: 0.9802 - val_loss: 0.2151 - val_acc: 0.9569\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9834\n",
      "Epoch 00090: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0508 - acc: 0.9834 - val_loss: 0.2095 - val_acc: 0.9546\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9823\n",
      "Epoch 00091: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.0527 - acc: 0.9823 - val_loss: 0.2005 - val_acc: 0.9574\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9818\n",
      "Epoch 00092: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0519 - acc: 0.9819 - val_loss: 0.2072 - val_acc: 0.9564\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9827\n",
      "Epoch 00093: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0525 - acc: 0.9827 - val_loss: 0.2077 - val_acc: 0.9569\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9825\n",
      "Epoch 00094: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0525 - acc: 0.9825 - val_loss: 0.2016 - val_acc: 0.9588\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9833\n",
      "Epoch 00095: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0496 - acc: 0.9833 - val_loss: 0.2098 - val_acc: 0.9553\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 00096: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.2262 - val_acc: 0.9541\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9823\n",
      "Epoch 00097: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0500 - acc: 0.9823 - val_loss: 0.1981 - val_acc: 0.9548\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9835\n",
      "Epoch 00098: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0491 - acc: 0.9835 - val_loss: 0.2233 - val_acc: 0.9548\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9827\n",
      "Epoch 00099: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0508 - acc: 0.9827 - val_loss: 0.2014 - val_acc: 0.9564\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9852\n",
      "Epoch 00100: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.0463 - acc: 0.9852 - val_loss: 0.2195 - val_acc: 0.9592\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9844\n",
      "Epoch 00101: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0450 - acc: 0.9844 - val_loss: 0.2149 - val_acc: 0.9569\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9846\n",
      "Epoch 00102: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0460 - acc: 0.9846 - val_loss: 0.2028 - val_acc: 0.9592\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9830\n",
      "Epoch 00103: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0500 - acc: 0.9830 - val_loss: 0.2102 - val_acc: 0.9562\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9847\n",
      "Epoch 00104: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0434 - acc: 0.9847 - val_loss: 0.2075 - val_acc: 0.9564\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9845\n",
      "Epoch 00105: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.0450 - acc: 0.9845 - val_loss: 0.1984 - val_acc: 0.9574\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9848\n",
      "Epoch 00106: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0450 - acc: 0.9848 - val_loss: 0.2106 - val_acc: 0.9555\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9849\n",
      "Epoch 00107: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0422 - acc: 0.9849 - val_loss: 0.2146 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9846\n",
      "Epoch 00108: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0444 - acc: 0.9846 - val_loss: 0.2212 - val_acc: 0.9534\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9853\n",
      "Epoch 00109: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.0444 - acc: 0.9853 - val_loss: 0.2211 - val_acc: 0.9553\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9862\n",
      "Epoch 00110: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0411 - acc: 0.9862 - val_loss: 0.2283 - val_acc: 0.9564\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9866\n",
      "Epoch 00111: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0399 - acc: 0.9866 - val_loss: 0.2177 - val_acc: 0.9574\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9858\n",
      "Epoch 00112: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0409 - acc: 0.9858 - val_loss: 0.2112 - val_acc: 0.9585\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9862\n",
      "Epoch 00113: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.0403 - acc: 0.9862 - val_loss: 0.2548 - val_acc: 0.9534\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9859\n",
      "Epoch 00114: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0419 - acc: 0.9859 - val_loss: 0.2049 - val_acc: 0.9595\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9872\n",
      "Epoch 00115: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.0389 - acc: 0.9872 - val_loss: 0.2205 - val_acc: 0.9592\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9851\n",
      "Epoch 00116: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0444 - acc: 0.9851 - val_loss: 0.2237 - val_acc: 0.9557\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9866\n",
      "Epoch 00117: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0412 - acc: 0.9866 - val_loss: 0.2214 - val_acc: 0.9576\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9872\n",
      "Epoch 00118: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0373 - acc: 0.9872 - val_loss: 0.2229 - val_acc: 0.9592\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9883\n",
      "Epoch 00119: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.0358 - acc: 0.9883 - val_loss: 0.2241 - val_acc: 0.9541\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9883\n",
      "Epoch 00120: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0347 - acc: 0.9883 - val_loss: 0.2411 - val_acc: 0.9581\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9872\n",
      "Epoch 00121: val_loss did not improve from 0.17810\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0380 - acc: 0.9872 - val_loss: 0.2248 - val_acc: 0.9581\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 535us/sample - loss: 0.2060 - acc: 0.9481\n",
      "Loss: 0.20598556604449128 Accuracy: 0.94807893\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4368 - acc: 0.1965\n",
      "Epoch 00001: val_loss improved from inf to 1.84902, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/001-1.8490.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.4368 - acc: 0.1965 - val_loss: 1.8490 - val_acc: 0.4123\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5392 - acc: 0.4940\n",
      "Epoch 00002: val_loss improved from 1.84902 to 1.09299, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/002-1.0930.hdf5\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 1.5392 - acc: 0.4940 - val_loss: 1.0930 - val_acc: 0.6452\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1648 - acc: 0.6163\n",
      "Epoch 00003: val_loss improved from 1.09299 to 0.85795, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/003-0.8579.hdf5\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 1.1650 - acc: 0.6162 - val_loss: 0.8579 - val_acc: 0.7384\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9753 - acc: 0.6810\n",
      "Epoch 00004: val_loss improved from 0.85795 to 0.71487, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/004-0.7149.hdf5\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.9753 - acc: 0.6810 - val_loss: 0.7149 - val_acc: 0.7890\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8330 - acc: 0.7286\n",
      "Epoch 00005: val_loss improved from 0.71487 to 0.63060, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/005-0.6306.hdf5\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.8329 - acc: 0.7286 - val_loss: 0.6306 - val_acc: 0.7969\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7620\n",
      "Epoch 00006: val_loss improved from 0.63060 to 0.54232, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/006-0.5423.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.7304 - acc: 0.7619 - val_loss: 0.5423 - val_acc: 0.8332\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.7920\n",
      "Epoch 00007: val_loss improved from 0.54232 to 0.46060, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/007-0.4606.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.6396 - acc: 0.7919 - val_loss: 0.4606 - val_acc: 0.8647\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8109\n",
      "Epoch 00008: val_loss improved from 0.46060 to 0.42776, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/008-0.4278.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.5781 - acc: 0.8109 - val_loss: 0.4278 - val_acc: 0.8640\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8335\n",
      "Epoch 00009: val_loss improved from 0.42776 to 0.40302, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/009-0.4030.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.5185 - acc: 0.8335 - val_loss: 0.4030 - val_acc: 0.8784\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8485\n",
      "Epoch 00010: val_loss improved from 0.40302 to 0.35173, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/010-0.3517.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.4707 - acc: 0.8486 - val_loss: 0.3517 - val_acc: 0.8908\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8596\n",
      "Epoch 00011: val_loss improved from 0.35173 to 0.31224, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/011-0.3122.hdf5\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.4342 - acc: 0.8596 - val_loss: 0.3122 - val_acc: 0.9059\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8690\n",
      "Epoch 00012: val_loss improved from 0.31224 to 0.28436, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/012-0.2844.hdf5\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.4026 - acc: 0.8690 - val_loss: 0.2844 - val_acc: 0.9138\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8795\n",
      "Epoch 00013: val_loss improved from 0.28436 to 0.27544, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/013-0.2754.hdf5\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3713 - acc: 0.8795 - val_loss: 0.2754 - val_acc: 0.9187\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8880\n",
      "Epoch 00014: val_loss did not improve from 0.27544\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.3471 - acc: 0.8880 - val_loss: 0.2831 - val_acc: 0.9152\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8922\n",
      "Epoch 00015: val_loss improved from 0.27544 to 0.22563, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/015-0.2256.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.3363 - acc: 0.8922 - val_loss: 0.2256 - val_acc: 0.9341\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8998\n",
      "Epoch 00016: val_loss improved from 0.22563 to 0.22377, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/016-0.2238.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.3071 - acc: 0.8999 - val_loss: 0.2238 - val_acc: 0.9324\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9052\n",
      "Epoch 00017: val_loss did not improve from 0.22377\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2970 - acc: 0.9051 - val_loss: 0.2482 - val_acc: 0.9234\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9115\n",
      "Epoch 00018: val_loss improved from 0.22377 to 0.22145, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/018-0.2215.hdf5\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.2789 - acc: 0.9115 - val_loss: 0.2215 - val_acc: 0.9327\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9141\n",
      "Epoch 00019: val_loss improved from 0.22145 to 0.19604, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/019-0.1960.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2639 - acc: 0.9141 - val_loss: 0.1960 - val_acc: 0.9392\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9209\n",
      "Epoch 00020: val_loss improved from 0.19604 to 0.19288, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/020-0.1929.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2486 - acc: 0.9209 - val_loss: 0.1929 - val_acc: 0.9418\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9221\n",
      "Epoch 00021: val_loss did not improve from 0.19288\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2389 - acc: 0.9222 - val_loss: 0.1945 - val_acc: 0.9415\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9254\n",
      "Epoch 00022: val_loss improved from 0.19288 to 0.17754, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/022-0.1775.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.2294 - acc: 0.9254 - val_loss: 0.1775 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9281\n",
      "Epoch 00023: val_loss did not improve from 0.17754\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2189 - acc: 0.9281 - val_loss: 0.1839 - val_acc: 0.9457\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9299\n",
      "Epoch 00024: val_loss did not improve from 0.17754\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.2160 - acc: 0.9300 - val_loss: 0.1798 - val_acc: 0.9439\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9353\n",
      "Epoch 00025: val_loss did not improve from 0.17754\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1978 - acc: 0.9353 - val_loss: 0.1889 - val_acc: 0.9455\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9356\n",
      "Epoch 00026: val_loss improved from 0.17754 to 0.16521, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/026-0.1652.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.1957 - acc: 0.9355 - val_loss: 0.1652 - val_acc: 0.9495\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9399\n",
      "Epoch 00027: val_loss improved from 0.16521 to 0.15957, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/027-0.1596.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.1870 - acc: 0.9400 - val_loss: 0.1596 - val_acc: 0.9527\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9417\n",
      "Epoch 00028: val_loss improved from 0.15957 to 0.15077, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/028-0.1508.hdf5\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.1793 - acc: 0.9417 - val_loss: 0.1508 - val_acc: 0.9513\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9425\n",
      "Epoch 00029: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.1756 - acc: 0.9425 - val_loss: 0.1780 - val_acc: 0.9443\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9459\n",
      "Epoch 00030: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1674 - acc: 0.9459 - val_loss: 0.1669 - val_acc: 0.9471\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9470\n",
      "Epoch 00031: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1618 - acc: 0.9470 - val_loss: 0.1731 - val_acc: 0.9474\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9495\n",
      "Epoch 00032: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.1550 - acc: 0.9495 - val_loss: 0.1560 - val_acc: 0.9539\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9493\n",
      "Epoch 00033: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.1539 - acc: 0.9492 - val_loss: 0.1533 - val_acc: 0.9550\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9523\n",
      "Epoch 00034: val_loss did not improve from 0.15077\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1433 - acc: 0.9523 - val_loss: 0.1535 - val_acc: 0.9557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9541\n",
      "Epoch 00035: val_loss improved from 0.15077 to 0.15022, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/035-0.1502.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.1399 - acc: 0.9541 - val_loss: 0.1502 - val_acc: 0.9534\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9559\n",
      "Epoch 00036: val_loss did not improve from 0.15022\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1346 - acc: 0.9559 - val_loss: 0.1577 - val_acc: 0.9560\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9562\n",
      "Epoch 00037: val_loss improved from 0.15022 to 0.14860, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/037-0.1486.hdf5\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1317 - acc: 0.9562 - val_loss: 0.1486 - val_acc: 0.9529\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9578\n",
      "Epoch 00038: val_loss improved from 0.14860 to 0.13175, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/038-0.1317.hdf5\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1279 - acc: 0.9578 - val_loss: 0.1317 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9599\n",
      "Epoch 00039: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.1226 - acc: 0.9599 - val_loss: 0.1484 - val_acc: 0.9555\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9614\n",
      "Epoch 00040: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.1173 - acc: 0.9614 - val_loss: 0.1442 - val_acc: 0.9576\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9611\n",
      "Epoch 00041: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.1172 - acc: 0.9611 - val_loss: 0.1421 - val_acc: 0.9606\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9623\n",
      "Epoch 00042: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.1108 - acc: 0.9623 - val_loss: 0.1339 - val_acc: 0.9602\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9628\n",
      "Epoch 00043: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1094 - acc: 0.9628 - val_loss: 0.1547 - val_acc: 0.9590\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9625\n",
      "Epoch 00044: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1091 - acc: 0.9625 - val_loss: 0.1358 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9633\n",
      "Epoch 00045: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1069 - acc: 0.9633 - val_loss: 0.1395 - val_acc: 0.9588\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9663\n",
      "Epoch 00046: val_loss did not improve from 0.13175\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.1001 - acc: 0.9663 - val_loss: 0.1487 - val_acc: 0.9592\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9675\n",
      "Epoch 00047: val_loss improved from 0.13175 to 0.12985, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/047-0.1299.hdf5\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.0967 - acc: 0.9675 - val_loss: 0.1299 - val_acc: 0.9590\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9680\n",
      "Epoch 00048: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0959 - acc: 0.9680 - val_loss: 0.1334 - val_acc: 0.9625\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9683\n",
      "Epoch 00049: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0964 - acc: 0.9683 - val_loss: 0.1470 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9699\n",
      "Epoch 00050: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0891 - acc: 0.9699 - val_loss: 0.1369 - val_acc: 0.9630\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9699\n",
      "Epoch 00051: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0870 - acc: 0.9699 - val_loss: 0.1812 - val_acc: 0.9520\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9687\n",
      "Epoch 00052: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0902 - acc: 0.9687 - val_loss: 0.1319 - val_acc: 0.9637\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9705\n",
      "Epoch 00053: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0881 - acc: 0.9705 - val_loss: 0.1365 - val_acc: 0.9609\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9711\n",
      "Epoch 00054: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0841 - acc: 0.9711 - val_loss: 0.1773 - val_acc: 0.9497\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9718\n",
      "Epoch 00055: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0815 - acc: 0.9718 - val_loss: 0.1501 - val_acc: 0.9597\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9721\n",
      "Epoch 00056: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0790 - acc: 0.9722 - val_loss: 0.1486 - val_acc: 0.9576\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9728\n",
      "Epoch 00057: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0774 - acc: 0.9728 - val_loss: 0.1475 - val_acc: 0.9634\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9740\n",
      "Epoch 00058: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0774 - acc: 0.9740 - val_loss: 0.1452 - val_acc: 0.9641\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9755\n",
      "Epoch 00059: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0706 - acc: 0.9755 - val_loss: 0.1375 - val_acc: 0.9646\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9763\n",
      "Epoch 00060: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0678 - acc: 0.9763 - val_loss: 0.1433 - val_acc: 0.9618\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9761\n",
      "Epoch 00061: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0692 - acc: 0.9761 - val_loss: 0.1424 - val_acc: 0.9592\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9737\n",
      "Epoch 00062: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0739 - acc: 0.9738 - val_loss: 0.1528 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9760\n",
      "Epoch 00063: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0676 - acc: 0.9760 - val_loss: 0.1372 - val_acc: 0.9644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9789\n",
      "Epoch 00064: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0633 - acc: 0.9789 - val_loss: 0.1433 - val_acc: 0.9595\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9778\n",
      "Epoch 00065: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0643 - acc: 0.9778 - val_loss: 0.1495 - val_acc: 0.9637\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9780\n",
      "Epoch 00066: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0636 - acc: 0.9780 - val_loss: 0.1430 - val_acc: 0.9637\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9783\n",
      "Epoch 00067: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0613 - acc: 0.9783 - val_loss: 0.1544 - val_acc: 0.9606\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9795\n",
      "Epoch 00068: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0598 - acc: 0.9795 - val_loss: 0.1559 - val_acc: 0.9609\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9803\n",
      "Epoch 00069: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0576 - acc: 0.9802 - val_loss: 0.1547 - val_acc: 0.9595\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9765\n",
      "Epoch 00070: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0693 - acc: 0.9765 - val_loss: 0.1401 - val_acc: 0.9625\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9810\n",
      "Epoch 00071: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0555 - acc: 0.9810 - val_loss: 0.1496 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9803\n",
      "Epoch 00072: val_loss did not improve from 0.12985\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0575 - acc: 0.9803 - val_loss: 0.1372 - val_acc: 0.9637\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9808\n",
      "Epoch 00073: val_loss improved from 0.12985 to 0.12762, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_11_conv_checkpoint/073-0.1276.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0551 - acc: 0.9808 - val_loss: 0.1276 - val_acc: 0.9674\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9811\n",
      "Epoch 00074: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0534 - acc: 0.9811 - val_loss: 0.1335 - val_acc: 0.9644\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9810\n",
      "Epoch 00075: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0535 - acc: 0.9810 - val_loss: 0.1537 - val_acc: 0.9620\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9820\n",
      "Epoch 00076: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.0521 - acc: 0.9820 - val_loss: 0.1360 - val_acc: 0.9683\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00077: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0532 - acc: 0.9815 - val_loss: 0.1320 - val_acc: 0.9674\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9830\n",
      "Epoch 00078: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0514 - acc: 0.9830 - val_loss: 0.1551 - val_acc: 0.9616\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9839\n",
      "Epoch 00079: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0469 - acc: 0.9839 - val_loss: 0.1303 - val_acc: 0.9658\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9814\n",
      "Epoch 00080: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0514 - acc: 0.9814 - val_loss: 0.1527 - val_acc: 0.9644\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9843\n",
      "Epoch 00081: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0464 - acc: 0.9843 - val_loss: 0.1514 - val_acc: 0.9658\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9846\n",
      "Epoch 00082: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0458 - acc: 0.9846 - val_loss: 0.1688 - val_acc: 0.9613\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9834\n",
      "Epoch 00083: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0494 - acc: 0.9834 - val_loss: 0.1404 - val_acc: 0.9620\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9849\n",
      "Epoch 00084: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0446 - acc: 0.9849 - val_loss: 0.1411 - val_acc: 0.9630\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9849\n",
      "Epoch 00085: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0448 - acc: 0.9849 - val_loss: 0.1410 - val_acc: 0.9669\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9842\n",
      "Epoch 00086: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0451 - acc: 0.9842 - val_loss: 0.1420 - val_acc: 0.9674\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9863\n",
      "Epoch 00087: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0415 - acc: 0.9863 - val_loss: 0.1882 - val_acc: 0.9578\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9852\n",
      "Epoch 00088: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0454 - acc: 0.9852 - val_loss: 0.1673 - val_acc: 0.9597\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9861\n",
      "Epoch 00089: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0403 - acc: 0.9861 - val_loss: 0.1592 - val_acc: 0.9653\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9852\n",
      "Epoch 00090: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0419 - acc: 0.9852 - val_loss: 0.1482 - val_acc: 0.9669\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9858\n",
      "Epoch 00091: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0417 - acc: 0.9858 - val_loss: 0.1466 - val_acc: 0.9630\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9866\n",
      "Epoch 00092: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.0398 - acc: 0.9866 - val_loss: 0.1708 - val_acc: 0.9662\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9862\n",
      "Epoch 00093: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0382 - acc: 0.9862 - val_loss: 0.1463 - val_acc: 0.9655\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9871\n",
      "Epoch 00094: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.0378 - acc: 0.9871 - val_loss: 0.1578 - val_acc: 0.9655\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9865\n",
      "Epoch 00095: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0385 - acc: 0.9865 - val_loss: 0.1658 - val_acc: 0.9632\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9876\n",
      "Epoch 00096: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0372 - acc: 0.9876 - val_loss: 0.1803 - val_acc: 0.9644\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9871\n",
      "Epoch 00097: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0378 - acc: 0.9871 - val_loss: 0.1659 - val_acc: 0.9595\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9851\n",
      "Epoch 00098: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 965us/sample - loss: 0.0434 - acc: 0.9851 - val_loss: 0.1563 - val_acc: 0.9672\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9872\n",
      "Epoch 00099: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0374 - acc: 0.9872 - val_loss: 0.1588 - val_acc: 0.9672\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9882\n",
      "Epoch 00100: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0345 - acc: 0.9882 - val_loss: 0.1585 - val_acc: 0.9658\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9883\n",
      "Epoch 00101: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0342 - acc: 0.9883 - val_loss: 0.1501 - val_acc: 0.9653\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9880\n",
      "Epoch 00102: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0343 - acc: 0.9880 - val_loss: 0.1631 - val_acc: 0.9648\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9880\n",
      "Epoch 00103: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0360 - acc: 0.9880 - val_loss: 0.1421 - val_acc: 0.9688\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9877\n",
      "Epoch 00104: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0371 - acc: 0.9877 - val_loss: 0.1358 - val_acc: 0.9679\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9888\n",
      "Epoch 00105: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0337 - acc: 0.9888 - val_loss: 0.1686 - val_acc: 0.9620\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9888\n",
      "Epoch 00106: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0347 - acc: 0.9888 - val_loss: 0.1499 - val_acc: 0.9686\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9891\n",
      "Epoch 00107: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0312 - acc: 0.9891 - val_loss: 0.1469 - val_acc: 0.9697\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9887\n",
      "Epoch 00108: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0331 - acc: 0.9888 - val_loss: 0.1758 - val_acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9891\n",
      "Epoch 00109: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0327 - acc: 0.9891 - val_loss: 0.1738 - val_acc: 0.9627\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9888\n",
      "Epoch 00110: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0329 - acc: 0.9888 - val_loss: 0.1682 - val_acc: 0.9637\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9893\n",
      "Epoch 00111: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0326 - acc: 0.9893 - val_loss: 0.1582 - val_acc: 0.9683\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9889\n",
      "Epoch 00112: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.0321 - acc: 0.9889 - val_loss: 0.1632 - val_acc: 0.9669\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9896\n",
      "Epoch 00113: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0308 - acc: 0.9896 - val_loss: 0.1577 - val_acc: 0.9688\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9887\n",
      "Epoch 00114: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0308 - acc: 0.9887 - val_loss: 0.1587 - val_acc: 0.9662\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 00115: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0262 - acc: 0.9913 - val_loss: 0.1762 - val_acc: 0.9662\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9890\n",
      "Epoch 00116: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0326 - acc: 0.9890 - val_loss: 0.1770 - val_acc: 0.9655\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9903\n",
      "Epoch 00117: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0280 - acc: 0.9903 - val_loss: 0.1622 - val_acc: 0.9662\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9907\n",
      "Epoch 00118: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0266 - acc: 0.9907 - val_loss: 0.1678 - val_acc: 0.9681\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9913\n",
      "Epoch 00119: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0279 - acc: 0.9913 - val_loss: 0.1659 - val_acc: 0.9653\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9900\n",
      "Epoch 00120: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0296 - acc: 0.9900 - val_loss: 0.1659 - val_acc: 0.9641\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9903\n",
      "Epoch 00121: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0276 - acc: 0.9903 - val_loss: 0.1666 - val_acc: 0.9676\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9912\n",
      "Epoch 00122: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0267 - acc: 0.9912 - val_loss: 0.1792 - val_acc: 0.9639\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9916\n",
      "Epoch 00123: val_loss did not improve from 0.12762\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0260 - acc: 0.9916 - val_loss: 0.1672 - val_acc: 0.9672\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 557us/sample - loss: 0.1737 - acc: 0.9564\n",
      "Loss: 0.17369112051860938 Accuracy: 0.95638627\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4245 - acc: 0.2062\n",
      "Epoch 00001: val_loss improved from inf to 1.77950, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/001-1.7795.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.4244 - acc: 0.2062 - val_loss: 1.7795 - val_acc: 0.4437\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5480 - acc: 0.4955\n",
      "Epoch 00002: val_loss improved from 1.77950 to 1.12665, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/002-1.1266.hdf5\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 1.5480 - acc: 0.4955 - val_loss: 1.1266 - val_acc: 0.6331\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1126 - acc: 0.6298\n",
      "Epoch 00003: val_loss improved from 1.12665 to 0.78522, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/003-0.7852.hdf5\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 1.1127 - acc: 0.6297 - val_loss: 0.7852 - val_acc: 0.7524\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7001\n",
      "Epoch 00004: val_loss improved from 0.78522 to 0.67490, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/004-0.6749.hdf5\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.9120 - acc: 0.7001 - val_loss: 0.6749 - val_acc: 0.7747\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7880 - acc: 0.7363\n",
      "Epoch 00005: val_loss improved from 0.67490 to 0.64314, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/005-0.6431.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.7880 - acc: 0.7363 - val_loss: 0.6431 - val_acc: 0.7820\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7036 - acc: 0.7651\n",
      "Epoch 00006: val_loss improved from 0.64314 to 0.53027, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/006-0.5303.hdf5\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.7036 - acc: 0.7651 - val_loss: 0.5303 - val_acc: 0.8295\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.7917\n",
      "Epoch 00007: val_loss improved from 0.53027 to 0.47223, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/007-0.4722.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.6314 - acc: 0.7916 - val_loss: 0.4722 - val_acc: 0.8470\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8143\n",
      "Epoch 00008: val_loss improved from 0.47223 to 0.42600, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/008-0.4260.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.5713 - acc: 0.8143 - val_loss: 0.4260 - val_acc: 0.8670\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8260\n",
      "Epoch 00009: val_loss improved from 0.42600 to 0.39583, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/009-0.3958.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.5303 - acc: 0.8259 - val_loss: 0.3958 - val_acc: 0.8703\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8401\n",
      "Epoch 00010: val_loss improved from 0.39583 to 0.38932, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/010-0.3893.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4883 - acc: 0.8401 - val_loss: 0.3893 - val_acc: 0.8812\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8512\n",
      "Epoch 00011: val_loss improved from 0.38932 to 0.36741, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/011-0.3674.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4514 - acc: 0.8512 - val_loss: 0.3674 - val_acc: 0.8835\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8622\n",
      "Epoch 00012: val_loss improved from 0.36741 to 0.31149, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/012-0.3115.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4188 - acc: 0.8622 - val_loss: 0.3115 - val_acc: 0.9029\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8699\n",
      "Epoch 00013: val_loss improved from 0.31149 to 0.30171, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/013-0.3017.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3952 - acc: 0.8699 - val_loss: 0.3017 - val_acc: 0.8996\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8847\n",
      "Epoch 00014: val_loss did not improve from 0.30171\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.3611 - acc: 0.8847 - val_loss: 0.3050 - val_acc: 0.9033\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8915\n",
      "Epoch 00015: val_loss improved from 0.30171 to 0.25046, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/015-0.2505.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3310 - acc: 0.8915 - val_loss: 0.2505 - val_acc: 0.9229\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8989\n",
      "Epoch 00016: val_loss did not improve from 0.25046\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.3107 - acc: 0.8989 - val_loss: 0.2617 - val_acc: 0.9187\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9040\n",
      "Epoch 00017: val_loss improved from 0.25046 to 0.22237, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/017-0.2224.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2927 - acc: 0.9040 - val_loss: 0.2224 - val_acc: 0.9280\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9097\n",
      "Epoch 00018: val_loss improved from 0.22237 to 0.20155, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/018-0.2016.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2773 - acc: 0.9097 - val_loss: 0.2016 - val_acc: 0.9394\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9181\n",
      "Epoch 00019: val_loss improved from 0.20155 to 0.19923, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/019-0.1992.hdf5\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.2542 - acc: 0.9181 - val_loss: 0.1992 - val_acc: 0.9420\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9211\n",
      "Epoch 00020: val_loss did not improve from 0.19923\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.2389 - acc: 0.9211 - val_loss: 0.2122 - val_acc: 0.9348\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9265\n",
      "Epoch 00021: val_loss improved from 0.19923 to 0.18884, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/021-0.1888.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2277 - acc: 0.9265 - val_loss: 0.1888 - val_acc: 0.9425\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9309\n",
      "Epoch 00022: val_loss improved from 0.18884 to 0.17830, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/022-0.1783.hdf5\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.2146 - acc: 0.9309 - val_loss: 0.1783 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9287\n",
      "Epoch 00023: val_loss improved from 0.17830 to 0.17178, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/023-0.1718.hdf5\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.2150 - acc: 0.9287 - val_loss: 0.1718 - val_acc: 0.9471\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9377\n",
      "Epoch 00024: val_loss improved from 0.17178 to 0.16277, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/024-0.1628.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1925 - acc: 0.9376 - val_loss: 0.1628 - val_acc: 0.9502\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9400\n",
      "Epoch 00025: val_loss did not improve from 0.16277\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1856 - acc: 0.9400 - val_loss: 0.1726 - val_acc: 0.9481\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9427\n",
      "Epoch 00026: val_loss improved from 0.16277 to 0.15109, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/026-0.1511.hdf5\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.1758 - acc: 0.9427 - val_loss: 0.1511 - val_acc: 0.9539\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9445\n",
      "Epoch 00027: val_loss did not improve from 0.15109\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.1659 - acc: 0.9445 - val_loss: 0.1671 - val_acc: 0.9548\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9494\n",
      "Epoch 00028: val_loss improved from 0.15109 to 0.13886, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/028-0.1389.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1561 - acc: 0.9494 - val_loss: 0.1389 - val_acc: 0.9590\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9484\n",
      "Epoch 00029: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1540 - acc: 0.9484 - val_loss: 0.1529 - val_acc: 0.9536\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9521\n",
      "Epoch 00030: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 992us/sample - loss: 0.1462 - acc: 0.9522 - val_loss: 0.1598 - val_acc: 0.9529\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9518\n",
      "Epoch 00031: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1428 - acc: 0.9518 - val_loss: 0.1527 - val_acc: 0.9560\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9566\n",
      "Epoch 00032: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.1334 - acc: 0.9566 - val_loss: 0.1803 - val_acc: 0.9483\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9551\n",
      "Epoch 00033: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1320 - acc: 0.9551 - val_loss: 0.1639 - val_acc: 0.9502\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9596\n",
      "Epoch 00034: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.1216 - acc: 0.9597 - val_loss: 0.1581 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9612\n",
      "Epoch 00035: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1186 - acc: 0.9612 - val_loss: 0.1626 - val_acc: 0.9548\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9612\n",
      "Epoch 00036: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.1160 - acc: 0.9612 - val_loss: 0.1468 - val_acc: 0.9588\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9621\n",
      "Epoch 00037: val_loss did not improve from 0.13886\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.1125 - acc: 0.9622 - val_loss: 0.1669 - val_acc: 0.9520\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9654\n",
      "Epoch 00038: val_loss improved from 0.13886 to 0.13631, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/038-0.1363.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1033 - acc: 0.9654 - val_loss: 0.1363 - val_acc: 0.9639\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9648\n",
      "Epoch 00039: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.1045 - acc: 0.9648 - val_loss: 0.1406 - val_acc: 0.9602\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9656\n",
      "Epoch 00040: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.1010 - acc: 0.9656 - val_loss: 0.1555 - val_acc: 0.9576\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9674\n",
      "Epoch 00041: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0970 - acc: 0.9674 - val_loss: 0.1401 - val_acc: 0.9604\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9695\n",
      "Epoch 00042: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0889 - acc: 0.9695 - val_loss: 0.1386 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9688\n",
      "Epoch 00043: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 994us/sample - loss: 0.0933 - acc: 0.9688 - val_loss: 0.1455 - val_acc: 0.9602\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9703\n",
      "Epoch 00044: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0880 - acc: 0.9703 - val_loss: 0.1565 - val_acc: 0.9518\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9716\n",
      "Epoch 00045: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0849 - acc: 0.9716 - val_loss: 0.1407 - val_acc: 0.9618\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9709\n",
      "Epoch 00046: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0834 - acc: 0.9709 - val_loss: 0.1541 - val_acc: 0.9590\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9735\n",
      "Epoch 00047: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0786 - acc: 0.9735 - val_loss: 0.1365 - val_acc: 0.9644\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9734\n",
      "Epoch 00048: val_loss did not improve from 0.13631\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0779 - acc: 0.9734 - val_loss: 0.1448 - val_acc: 0.9611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9752\n",
      "Epoch 00049: val_loss improved from 0.13631 to 0.12700, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_12_conv_checkpoint/049-0.1270.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0713 - acc: 0.9752 - val_loss: 0.1270 - val_acc: 0.9655\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9758\n",
      "Epoch 00050: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0725 - acc: 0.9758 - val_loss: 0.1532 - val_acc: 0.9613\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9766\n",
      "Epoch 00051: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0692 - acc: 0.9766 - val_loss: 0.1511 - val_acc: 0.9613\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9748\n",
      "Epoch 00052: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0732 - acc: 0.9748 - val_loss: 0.1376 - val_acc: 0.9644\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9757\n",
      "Epoch 00053: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0702 - acc: 0.9757 - val_loss: 0.1351 - val_acc: 0.9606\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9802\n",
      "Epoch 00054: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0606 - acc: 0.9802 - val_loss: 0.1362 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9773\n",
      "Epoch 00055: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 994us/sample - loss: 0.0682 - acc: 0.9773 - val_loss: 0.1419 - val_acc: 0.9620\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9802\n",
      "Epoch 00056: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0575 - acc: 0.9802 - val_loss: 0.1637 - val_acc: 0.9609\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9792\n",
      "Epoch 00057: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0619 - acc: 0.9792 - val_loss: 0.1639 - val_acc: 0.9595\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9817\n",
      "Epoch 00058: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0547 - acc: 0.9817 - val_loss: 0.1463 - val_acc: 0.9646\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9803\n",
      "Epoch 00059: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0572 - acc: 0.9803 - val_loss: 0.1738 - val_acc: 0.9609\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9816\n",
      "Epoch 00060: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0520 - acc: 0.9816 - val_loss: 0.1526 - val_acc: 0.9606\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9808\n",
      "Epoch 00061: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0577 - acc: 0.9808 - val_loss: 0.1563 - val_acc: 0.9623\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9815\n",
      "Epoch 00062: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0548 - acc: 0.9814 - val_loss: 0.1639 - val_acc: 0.9611\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9820\n",
      "Epoch 00063: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0548 - acc: 0.9820 - val_loss: 0.1984 - val_acc: 0.9578\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9831\n",
      "Epoch 00064: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0499 - acc: 0.9831 - val_loss: 0.1454 - val_acc: 0.9658\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9816\n",
      "Epoch 00065: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0532 - acc: 0.9816 - val_loss: 0.1598 - val_acc: 0.9595\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9841\n",
      "Epoch 00066: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0469 - acc: 0.9841 - val_loss: 0.1529 - val_acc: 0.9632\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9839\n",
      "Epoch 00067: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0483 - acc: 0.9839 - val_loss: 0.1532 - val_acc: 0.9651\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9836\n",
      "Epoch 00068: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0483 - acc: 0.9836 - val_loss: 0.1517 - val_acc: 0.9630\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9850\n",
      "Epoch 00069: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0430 - acc: 0.9850 - val_loss: 0.1727 - val_acc: 0.9611\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9847\n",
      "Epoch 00070: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 993us/sample - loss: 0.0437 - acc: 0.9847 - val_loss: 0.1630 - val_acc: 0.9648\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9851\n",
      "Epoch 00071: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0437 - acc: 0.9851 - val_loss: 0.1713 - val_acc: 0.9646\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9849\n",
      "Epoch 00072: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0439 - acc: 0.9849 - val_loss: 0.1687 - val_acc: 0.9609\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9841\n",
      "Epoch 00073: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0468 - acc: 0.9841 - val_loss: 0.1655 - val_acc: 0.9646\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9862\n",
      "Epoch 00074: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0389 - acc: 0.9863 - val_loss: 0.1559 - val_acc: 0.9639\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9862\n",
      "Epoch 00075: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0428 - acc: 0.9862 - val_loss: 0.1632 - val_acc: 0.9627\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9870\n",
      "Epoch 00076: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0381 - acc: 0.9870 - val_loss: 0.1876 - val_acc: 0.9623\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9867\n",
      "Epoch 00077: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0407 - acc: 0.9867 - val_loss: 0.1777 - val_acc: 0.9623\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9877\n",
      "Epoch 00078: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0375 - acc: 0.9877 - val_loss: 0.1688 - val_acc: 0.9620\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9881\n",
      "Epoch 00079: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0347 - acc: 0.9881 - val_loss: 0.1742 - val_acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9865\n",
      "Epoch 00080: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0410 - acc: 0.9866 - val_loss: 0.1510 - val_acc: 0.9672\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9882\n",
      "Epoch 00081: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0360 - acc: 0.9882 - val_loss: 0.1737 - val_acc: 0.9611\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9880\n",
      "Epoch 00082: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0359 - acc: 0.9880 - val_loss: 0.1837 - val_acc: 0.9606\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9878\n",
      "Epoch 00083: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0368 - acc: 0.9878 - val_loss: 0.1943 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9880\n",
      "Epoch 00084: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0374 - acc: 0.9880 - val_loss: 0.1874 - val_acc: 0.9646\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.1661 - val_acc: 0.9655\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9901\n",
      "Epoch 00086: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1000us/sample - loss: 0.0314 - acc: 0.9901 - val_loss: 0.1915 - val_acc: 0.9660\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9879\n",
      "Epoch 00087: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0355 - acc: 0.9879 - val_loss: 0.1658 - val_acc: 0.9648\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9906\n",
      "Epoch 00088: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0297 - acc: 0.9906 - val_loss: 0.1730 - val_acc: 0.9639\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9897\n",
      "Epoch 00089: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0311 - acc: 0.9897 - val_loss: 0.1646 - val_acc: 0.9655\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9900\n",
      "Epoch 00090: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.0297 - acc: 0.9900 - val_loss: 0.1834 - val_acc: 0.9632\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9893\n",
      "Epoch 00091: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0324 - acc: 0.9893 - val_loss: 0.1763 - val_acc: 0.9634\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9889\n",
      "Epoch 00092: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0325 - acc: 0.9889 - val_loss: 0.1854 - val_acc: 0.9625\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9912\n",
      "Epoch 00093: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0265 - acc: 0.9912 - val_loss: 0.1979 - val_acc: 0.9618\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9897\n",
      "Epoch 00094: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.2068 - val_acc: 0.9618\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9915\n",
      "Epoch 00095: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.1857 - val_acc: 0.9630\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9898\n",
      "Epoch 00096: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0313 - acc: 0.9898 - val_loss: 0.1646 - val_acc: 0.9660\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9905\n",
      "Epoch 00097: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0292 - acc: 0.9905 - val_loss: 0.1857 - val_acc: 0.9618\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9909\n",
      "Epoch 00098: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0275 - acc: 0.9909 - val_loss: 0.1785 - val_acc: 0.9660\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9915\n",
      "Epoch 00099: val_loss did not improve from 0.12700\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0254 - acc: 0.9915 - val_loss: 0.1720 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 579us/sample - loss: 0.2029 - acc: 0.9481\n",
      "Loss: 0.20288274491347008 Accuracy: 0.94807893\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2981 - acc: 0.2566\n",
      "Epoch 00001: val_loss improved from inf to 1.67576, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/001-1.6758.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.2981 - acc: 0.2566 - val_loss: 1.6758 - val_acc: 0.4635\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4578 - acc: 0.5254\n",
      "Epoch 00002: val_loss improved from 1.67576 to 0.97453, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/002-0.9745.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.4578 - acc: 0.5254 - val_loss: 0.9745 - val_acc: 0.6809\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.6643\n",
      "Epoch 00003: val_loss improved from 0.97453 to 0.73102, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/003-0.7310.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.0018 - acc: 0.6643 - val_loss: 0.7310 - val_acc: 0.7547\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7291\n",
      "Epoch 00004: val_loss improved from 0.73102 to 0.65584, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/004-0.6558.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.8057 - acc: 0.7291 - val_loss: 0.6558 - val_acc: 0.7813\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6897 - acc: 0.7704\n",
      "Epoch 00005: val_loss improved from 0.65584 to 0.53329, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/005-0.5333.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.6897 - acc: 0.7704 - val_loss: 0.5333 - val_acc: 0.8234\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.8014\n",
      "Epoch 00006: val_loss improved from 0.53329 to 0.45340, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/006-0.4534.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.6005 - acc: 0.8014 - val_loss: 0.4534 - val_acc: 0.8542\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.8252\n",
      "Epoch 00007: val_loss improved from 0.45340 to 0.38403, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/007-0.3840.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.5275 - acc: 0.8252 - val_loss: 0.3840 - val_acc: 0.8761\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8465\n",
      "Epoch 00008: val_loss improved from 0.38403 to 0.34910, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/008-0.3491.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.4686 - acc: 0.8465 - val_loss: 0.3491 - val_acc: 0.8849\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8645\n",
      "Epoch 00009: val_loss improved from 0.34910 to 0.31138, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/009-0.3114.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.4147 - acc: 0.8645 - val_loss: 0.3114 - val_acc: 0.9047\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8758\n",
      "Epoch 00010: val_loss improved from 0.31138 to 0.27913, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/010-0.2791.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.3781 - acc: 0.8758 - val_loss: 0.2791 - val_acc: 0.9164\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8905\n",
      "Epoch 00011: val_loss improved from 0.27913 to 0.24464, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/011-0.2446.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.3369 - acc: 0.8905 - val_loss: 0.2446 - val_acc: 0.9299\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8976\n",
      "Epoch 00012: val_loss did not improve from 0.24464\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.3109 - acc: 0.8976 - val_loss: 0.2706 - val_acc: 0.9164\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9049\n",
      "Epoch 00013: val_loss did not improve from 0.24464\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2872 - acc: 0.9049 - val_loss: 0.2643 - val_acc: 0.9203\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9126\n",
      "Epoch 00014: val_loss improved from 0.24464 to 0.20569, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/014-0.2057.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2674 - acc: 0.9125 - val_loss: 0.2057 - val_acc: 0.9366\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9181\n",
      "Epoch 00015: val_loss improved from 0.20569 to 0.19163, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/015-0.1916.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2521 - acc: 0.9181 - val_loss: 0.1916 - val_acc: 0.9399\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9259\n",
      "Epoch 00016: val_loss improved from 0.19163 to 0.19064, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/016-0.1906.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2268 - acc: 0.9259 - val_loss: 0.1906 - val_acc: 0.9415\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9289\n",
      "Epoch 00017: val_loss improved from 0.19064 to 0.18140, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/017-0.1814.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2174 - acc: 0.9289 - val_loss: 0.1814 - val_acc: 0.9406\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9339\n",
      "Epoch 00018: val_loss did not improve from 0.18140\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2019 - acc: 0.9339 - val_loss: 0.2103 - val_acc: 0.9331\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9369\n",
      "Epoch 00019: val_loss did not improve from 0.18140\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1891 - acc: 0.9369 - val_loss: 0.1987 - val_acc: 0.9336\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9420\n",
      "Epoch 00020: val_loss improved from 0.18140 to 0.16513, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/020-0.1651.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1750 - acc: 0.9420 - val_loss: 0.1651 - val_acc: 0.9474\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9440\n",
      "Epoch 00021: val_loss did not improve from 0.16513\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1687 - acc: 0.9440 - val_loss: 0.1789 - val_acc: 0.9457\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9473\n",
      "Epoch 00022: val_loss improved from 0.16513 to 0.16160, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/022-0.1616.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1586 - acc: 0.9473 - val_loss: 0.1616 - val_acc: 0.9506\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9515\n",
      "Epoch 00023: val_loss did not improve from 0.16160\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1471 - acc: 0.9514 - val_loss: 0.1795 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9507\n",
      "Epoch 00024: val_loss improved from 0.16160 to 0.15128, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/024-0.1513.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1445 - acc: 0.9507 - val_loss: 0.1513 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9558\n",
      "Epoch 00025: val_loss did not improve from 0.15128\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1331 - acc: 0.9558 - val_loss: 0.1671 - val_acc: 0.9469\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9567\n",
      "Epoch 00026: val_loss improved from 0.15128 to 0.13859, saving model to model/checkpoint/1D_CNN_custom_pool_2_ch_32_DO_13_conv_checkpoint/026-0.1386.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1290 - acc: 0.9567 - val_loss: 0.1386 - val_acc: 0.9555\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9588\n",
      "Epoch 00027: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1205 - acc: 0.9588 - val_loss: 0.1759 - val_acc: 0.9450\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9621\n",
      "Epoch 00028: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1129 - acc: 0.9621 - val_loss: 0.1688 - val_acc: 0.9509\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9642\n",
      "Epoch 00029: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1044 - acc: 0.9642 - val_loss: 0.1824 - val_acc: 0.9471\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9647\n",
      "Epoch 00030: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1029 - acc: 0.9647 - val_loss: 0.1526 - val_acc: 0.9553\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9669\n",
      "Epoch 00031: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0957 - acc: 0.9669 - val_loss: 0.1937 - val_acc: 0.9441\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9672\n",
      "Epoch 00032: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0962 - acc: 0.9672 - val_loss: 0.1745 - val_acc: 0.9511\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9687\n",
      "Epoch 00033: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0924 - acc: 0.9687 - val_loss: 0.1558 - val_acc: 0.9541\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9705\n",
      "Epoch 00034: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0831 - acc: 0.9705 - val_loss: 0.1570 - val_acc: 0.9557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9713\n",
      "Epoch 00035: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0814 - acc: 0.9713 - val_loss: 0.1553 - val_acc: 0.9590\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9737\n",
      "Epoch 00036: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0763 - acc: 0.9738 - val_loss: 0.1484 - val_acc: 0.9564\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9754\n",
      "Epoch 00037: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0709 - acc: 0.9754 - val_loss: 0.1706 - val_acc: 0.9550\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9770\n",
      "Epoch 00038: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0682 - acc: 0.9770 - val_loss: 0.1617 - val_acc: 0.9550\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9765\n",
      "Epoch 00039: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0678 - acc: 0.9765 - val_loss: 0.1451 - val_acc: 0.9611\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9764\n",
      "Epoch 00040: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0648 - acc: 0.9764 - val_loss: 0.1805 - val_acc: 0.9527\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9793\n",
      "Epoch 00041: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0604 - acc: 0.9793 - val_loss: 0.1679 - val_acc: 0.9553\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9788\n",
      "Epoch 00042: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0618 - acc: 0.9788 - val_loss: 0.1761 - val_acc: 0.9581\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9797\n",
      "Epoch 00043: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0592 - acc: 0.9797 - val_loss: 0.1803 - val_acc: 0.9569\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9805\n",
      "Epoch 00044: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0567 - acc: 0.9805 - val_loss: 0.1932 - val_acc: 0.9506\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9833\n",
      "Epoch 00045: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0501 - acc: 0.9833 - val_loss: 0.1746 - val_acc: 0.9557\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9807\n",
      "Epoch 00046: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0550 - acc: 0.9807 - val_loss: 0.1746 - val_acc: 0.9564\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9827\n",
      "Epoch 00047: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0523 - acc: 0.9827 - val_loss: 0.1891 - val_acc: 0.9553\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9819\n",
      "Epoch 00048: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0518 - acc: 0.9819 - val_loss: 0.1902 - val_acc: 0.9606\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9842\n",
      "Epoch 00049: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0463 - acc: 0.9842 - val_loss: 0.1781 - val_acc: 0.9557\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9830\n",
      "Epoch 00050: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0485 - acc: 0.9830 - val_loss: 0.1638 - val_acc: 0.9611\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9853\n",
      "Epoch 00051: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0457 - acc: 0.9853 - val_loss: 0.2126 - val_acc: 0.9511\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9862\n",
      "Epoch 00052: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0408 - acc: 0.9863 - val_loss: 0.1778 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9847\n",
      "Epoch 00053: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0469 - acc: 0.9847 - val_loss: 0.1614 - val_acc: 0.9604\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9863\n",
      "Epoch 00054: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0420 - acc: 0.9863 - val_loss: 0.1660 - val_acc: 0.9609\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9867\n",
      "Epoch 00055: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0388 - acc: 0.9867 - val_loss: 0.2010 - val_acc: 0.9564\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9869\n",
      "Epoch 00056: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0375 - acc: 0.9869 - val_loss: 0.2281 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9870\n",
      "Epoch 00057: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0375 - acc: 0.9870 - val_loss: 0.1692 - val_acc: 0.9606\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9883\n",
      "Epoch 00058: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0334 - acc: 0.9883 - val_loss: 0.1821 - val_acc: 0.9616\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9872\n",
      "Epoch 00059: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0376 - acc: 0.9872 - val_loss: 0.2107 - val_acc: 0.9543\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9889\n",
      "Epoch 00060: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0342 - acc: 0.9889 - val_loss: 0.2022 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9886\n",
      "Epoch 00061: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0341 - acc: 0.9886 - val_loss: 0.1947 - val_acc: 0.9592\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9882\n",
      "Epoch 00062: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0347 - acc: 0.9882 - val_loss: 0.2096 - val_acc: 0.9564\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9880\n",
      "Epoch 00063: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0365 - acc: 0.9879 - val_loss: 0.2102 - val_acc: 0.9560\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9864\n",
      "Epoch 00064: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0420 - acc: 0.9864 - val_loss: 0.1792 - val_acc: 0.9599\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9917\n",
      "Epoch 00065: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0261 - acc: 0.9917 - val_loss: 0.2035 - val_acc: 0.9578\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9908\n",
      "Epoch 00066: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0277 - acc: 0.9908 - val_loss: 0.2155 - val_acc: 0.9560\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 00067: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0300 - acc: 0.9900 - val_loss: 0.2365 - val_acc: 0.9567\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9905\n",
      "Epoch 00068: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0293 - acc: 0.9905 - val_loss: 0.1808 - val_acc: 0.9599\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 00069: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0311 - acc: 0.9895 - val_loss: 0.1672 - val_acc: 0.9639\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9886\n",
      "Epoch 00070: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0313 - acc: 0.9886 - val_loss: 0.1956 - val_acc: 0.9595\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9912\n",
      "Epoch 00071: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0259 - acc: 0.9913 - val_loss: 0.1952 - val_acc: 0.9599\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9917\n",
      "Epoch 00072: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0264 - acc: 0.9917 - val_loss: 0.1812 - val_acc: 0.9616\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 00073: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0272 - acc: 0.9911 - val_loss: 0.1975 - val_acc: 0.9613\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9917\n",
      "Epoch 00074: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0250 - acc: 0.9917 - val_loss: 0.2143 - val_acc: 0.9583\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9919\n",
      "Epoch 00075: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0257 - acc: 0.9919 - val_loss: 0.1921 - val_acc: 0.9578\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9903\n",
      "Epoch 00076: val_loss did not improve from 0.13859\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0278 - acc: 0.9903 - val_loss: 0.2070 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmT17QgIhCxBQQAj7rghqtVZQUauIVutSta1Vq1/92eJSq22/rVrr1mr9otWKVZRiqRtKixXRKsi+CcoOCUv2PZn1/P44k8lCAhEymSHzvF+v+0rmzp17nzvLee45595zldYaIYQQAsAS6QCEEEJED0kKQgghQiQpCCGECJGkIIQQIkSSghBCiBBJCkIIIUIkKQghhAiRpCCEECJEkoIQQogQW6QD+KYyMjJ0Xl5epMMQQogTyurVq0u01j2PttwJlxTy8vJYtWpVpMMQQogTilJqT0eWk+YjIYQQIZIUhBBChEhSEEIIEXLC9Sm0xev1UlBQQENDQ6RDOWG5XC5yc3Ox2+2RDkUIEUHdIikUFBSQlJREXl4eSqlIh3PC0VpTWlpKQUEB/fv3j3Q4QogI6hbNRw0NDaSnp0tCOEZKKdLT06WmJYToHkkBkIRwnOT9E0JAN0oKR+P31+N2FxIIeCMdihBCRK2YSQqBQAMezwG07vykUFFRwbPPPntMr50+fToVFRUdXv7BBx/kscceO6ZtCSHE0cRMUlDKCoDW/k5f95GSgs/nO+JrFy1aRGpqaqfHJIQQxyLmkgJ0flKYPXs2O3bsYNSoUdx9990sXbqUKVOmMGPGDIYOHQrAxRdfzNixY8nPz2fOnDmh1+bl5VFSUsLu3bsZMmQIN910E/n5+Zx77rnU19cfcbvr1q1j0qRJjBgxgksuuYTy8nIAnn76aYYOHcqIESO44oorAPj4448ZNWoUo0aNYvTo0VRXV3f6+yCEOPF1i1NSm9u27Q5qata18UwAv78Wi8WFUt/sXPzExFEMHPhku88//PDDbNq0iXXrzHaXLl3KmjVr2LRpU+gUzxdffJEePXpQX1/P+PHjufTSS0lPT28V+zbmzZvH888/z+WXX86bb77J1Vdf3e52r7nmGv74xz9yxhln8MADD/DQQw/x5JNP8vDDD7Nr1y6cTmeoaeqxxx7jmWeeYfLkydTU1OByub7ReyCEiA0xU1OArj27ZsKECS3O+X/66acZOXIkkyZNYt++fWzbtu2w1/Tv359Ro0YBMHbsWHbv3t3u+isrK6moqOCMM84A4Nprr2XZsmUAjBgxgquuuoq//e1v2Gwm70+ePJk777yTp59+moqKitB8IYRortuVDO0d0Wvtp6ZmLQ5HLk5n77DHkZCQEPp/6dKlLFmyhM8//5z4+HjOPPPMNq8JcDqdof+tVutRm4/a895777Fs2TLeeecd/vd//5eNGzcye/Zszj//fBYtWsTkyZNZvHgxp5xyyjGtXwjRfcVQTaFxVzu/TyEpKemIbfSVlZWkpaURHx/P1q1bWb58+XFvMyUlhbS0ND755BMAXnnlFc444wwCgQD79u3jrLPO4pFHHqGyspKamhp27NjB8OHD+fnPf8748ePZunXrcccghOh+ul1NoT3m4ixrWM4+Sk9PZ/LkyQwbNoxp06Zx/vnnt3j+vPPO47nnnmPIkCEMHjyYSZMmdcp2X375ZX784x9TV1fHgAEDeOmll/D7/Vx99dVUVlaiteanP/0pqamp/OIXv+Cjjz7CYrGQn5/PtGnTOiUGIUT3orTWkY7hGxk3bpxufZOdLVu2MGTIkKO+tqZmA1ZrEnFxMr5PWzr6PgohTjxKqdVa63FHWy6Gmo8aT0vt/JqCEEJ0FzGVFMLVfCSEEN1FTCUFpSQpCCHEkcRgUghEOgwhhIhaMZcUpE9BCCHaF1NJASzSfCSEEEcQU0nB1BQCUdGElJiY+I3mCyFEV4jBpEBUJAUhhIhGMZUUIDzDZ8+ePZtnnnkm9LjxRjg1NTWcffbZjBkzhuHDh/PWW291eJ1aa+6++26GDRvG8OHDeeONNwA4cOAAU6dOZdSoUQwbNoxPPvkEv9/PddddF1r2iSee6NT9E0LEju43zMUdd8C6tobOBpv2YQnUoywJoL5BPhw1Cp5sf+jsWbNmcccdd3DLLbcAMH/+fBYvXozL5WLhwoUkJydTUlLCpEmTmDFjRofuh/yPf/yDdevWsX79ekpKShg/fjxTp07ltdde4zvf+Q733Xcffr+furo61q1bR2FhIZs2bQL4RndyE0KI5rpfUjgCFRo+u3OH9hg9ejRFRUXs37+f4uJi0tLS6NOnD16vl3vvvZdly5ZhsVgoLCzk0KFD9O599FFaP/30U6688kqsViuZmZmcccYZrFy5kvHjx/ODH/wAr9fLxRdfzKhRoxgwYAA7d+7ktttu4/zzz+fcc8/t1P0TQsSO7pcUjnBEH/DXUl+3hbi4k7HZOvcWmDNnzmTBggUcPHiQWbNmAfDqq69SXFzM6tWrsdvt5OXltTlk9jcxdepUli1bxnvvvcd1113HnXfeyTXXXMP69etZvHgxzz33HPPnz+fFF1/sjN0SQsSYGOtTMLsbjtNSZ82axeuvv86CBQuYOXMmYIbM7tWrF3a7nY8++og9e/Z0eH1TpkzhjTfewO/3U1xczLJly5gwYQJ79uwhMzOTm266iRtvvJE1a9ZQUlJCIBDg0ksv5Te/+Q1r1qzp9P0TQsSG7ldTOIJwnn2Un59PdXU1OTk5ZGVlAXDVVVdx4YUXMnz4cMaNG/eNbmpzySWX8PnnnzNy5EiUUjz66KP07t2bl19+md///vfY7XYSExOZO3cuhYWFXH/99QQCZr9+97vfdfr+CSFiQ9iGzlZK9QHmApmYRvw5WuunWi2jgKeA6UAdcJ3W+oiHucczdHZX333tRCNDZwvRfXV06Oxw1hR8wF1a6zVKqSRgtVLq31rrL5stMw0YGJwmAn8O/g2T8N19TQghuoOw9SlorQ80HvVrrauBLUBOq8UuAuZqYzmQqpTKCldM4bz7mhBCdAdd0tGslMoDRgMrWj2VA+xr9riAwxMHSqkfKqVWKaVWFRcXH2cskhSEEKI9YU8KSqlE4E3gDq111bGsQ2s9R2s9Tms9rmfPnscZj4yUKoQQ7QlrUlBK2TEJ4VWt9T/aWKQQ6NPscW5wXhhJTUEIIdoTtqQQPLPoL8AWrfXj7Sz2NnCNMiYBlVrrA+GKycQlSUEIIdoTzprCZOD7wLeUUuuC03Sl1I+VUj8OLrMI2AlsB54HfhLGeABQytLp1ylUVFTw7LPPHtNrp0+fLmMVCSGiRthOSdVafwocceQ3bS6SuCVcMbQlHH0KjUnhJz85PKf5fD5stvbf5kWLFnVqLEIIcTxibJgLCEefwuzZs9mxYwejRo3i7rvvZunSpUyZMoUZM2YwdOhQAC6++GLGjh1Lfn4+c+bMCb02Ly+PkpISdu/ezZAhQ7jpppvIz8/n3HPPpb6+/rBtvfPOO0ycOJHRo0dzzjnncOjQIQBqamq4/vrrGT58OCNGjODNN98E4IMPPmDMmDGMHDmSs88+u1P3WwjR/XS7YS6OMHI2AIFAL7ROxWrVHKUiE3KUkbN5+OGH2bRpE+uCG166dClr1qxh06ZN9O/fH4AXX3yRHj16UF9fz/jx47n00ktJT09vsZ5t27Yxb948nn/+eS6//HLefPNNrr766hbLnH766SxfvhylFC+88AKPPvoof/jDH/j1r39NSkoKGzduBKC8vJzi4mJuuukmli1bRv/+/SkrK+vQ/gohYle3SwpHo5QiTCN7tDBhwoRQQgB4+umnWbhwIQD79u1j27ZthyWF/v37M2rUKADGjh3L7t27D1tvQUEBs2bN4sCBA3g8ntA2lixZwuuvvx5aLi0tjXfeeYepU6eGlunRo0en7qMQovvpdknhSEf0AB5PFW73bhIShmOxOMMWR0JCQuj/pUuXsmTJEj7//HPi4+M588wz2xxC2+lsisdqtbbZfHTbbbdx5513MmPGDJYuXcqDDz4YlviFELEp5voUmkZK7bx+haSkJKqrq9t9vrKykrS0NOLj49m6dSvLly8/5m1VVlaSk2Mu+n755ZdD87/97W+3uCVoeXk5kyZNYtmyZezatQtAmo+EEEclSaETpKenM3nyZIYNG8bdd9992PPnnXcePp+PIUOGMHv2bCZNmnTM23rwwQeZOXMmY8eOJSMjIzT//vvvp7y8nGHDhjFy5Eg++ugjevbsyZw5c/jud7/LyJEjQzf/EUKI9oRt6OxwOZ6hswH8/lrqwnT3tROdDJ0tRPfV0aGzY66mEM67rwkhxIku5pJCOO++JoQQJ7oYTgpSUxBCiNZiLinI3deEEKJ9MZcU5O5rQgjRvphLCiDDZwshRHtiNilEuvkoMTExotsXQoi2xGRSkOYjIYRoW0wmhc5uPpo9e3aLISYefPBBHnvsMWpqajj77LMZM2YMw4cP56233jrqutobYrutIbDbGy5bCCGOVbcbEO+OD+5g3cEjjJ0NBAL1aB3Aak044nKNRvUexZPntT/S3qxZs7jjjju45RZzv6D58+ezePFiXC4XCxcuJDk5mZKSEiZNmsSMGTOCnd1ta2uI7UAg0OYQ2G0Nly2EEMej2yWFjlFA5w3vMXr0aIqKiti/fz/FxcWkpaXRp08fvF4v9957L8uWLcNisVBYWMihQ4fo3bt3u+tqa4jt4uLiNofAbmu4bCGEOB7dLikc6Yi+UUPDPrzeYpKSxnTadmfOnMmCBQs4ePBgaOC5V199leLiYlavXo3dbicvL6/NIbMbdXSIbSGECJeY7VOAAJ05GOCsWbN4/fXXWbBgATNnzgTMMNe9evXCbrfz0UcfsWfPniOuo70httsbArut4bKFEOJ4xHBS6NyhLvLz86muriYnJ4esrCwArrrqKlatWsXw4cOZO3cup5xyyhHX0d4Q2+0Ngd3WcNlCCHE8YmfobLcbqqogLQ1PoKJL7r52opGhs4XovmTo7NZqa2HPHvB4ZFA8IYRoR+wkBVuwT93vl6QghBDt6DZJ4ajNYFaTCPD5QkkB5J4KjU60ZkQhRHh0i6TgcrkoLS09csHWrKbQdPc1X/iDOwForSktLcXlckU6FCFEhHWL6xRyc3MpKCiguLi4/YUCASgpAb8fnZSA212C3R7Aai3qukCjmMvlIjc3N9JhCCEirFskBbvdHrrat12BAAwfDvfdh++XP+fTT4czYMCj9O17d9cEKYQQJ4Bu0XzUIRYLpKZCeXlwzCMLfn9VpKMSQoioEjtJASAtDcrLUUphsyXj80lSEEKI5mIrKfToAcGhIKzWZHy+yggHJIQQ0SW2kkKwpgBgsyVL85EQQrQSs0nB1BQkKQghRHMxmxSkpiCEEIcLW1JQSr2olCpSSm1q5/kzlVKVSql1wemBcMUS0pgUtMZqTZE+BSGEaCWc1yn8FfgTMPcIy3yitb4gjDG0lJYGXi/U1UlNQQgh2hC2moLWehlQFq71H5PG21WWl0ufghBCtCHSfQqnKqXWK6XeV0rlh31rjUmhrAybLZlAoI5AQMY/EkKIRpFMCmuAflrrkcAfgX+2t6BS6odKqVVKqVVHHN/oaJrVFGy2FAD8/upjX58QQnQzEUsKWusqrXVN8P9FgF0pldHOsnO01uO01uN69ux57Btt1XwESGezEEI0E7GkoJTqrZRSwf8nBGMpDetGW9QUTFKQzmYhhGgStrOPlFLzgDOBDKVUAfBLwA6gtX4OuAy4WSnlA+qBK3S47/TSIin0BcDrja6+cCGEiKSwJQWt9ZVHef5PmFNWu05yMigF5eU4HNkAeDwHujQEIYSIZpE++6hrNRs+2+lsTAr7IxyUEEJEj9hKChC6qtlqTcZiicftLox0REIIETViNikopXA6s3G7paYghBCNYjYpADgcOdJ8JIQQzcR0UpCaghBCtBTTScHhyMbj2U+4z4QVQogTRewmBa1xOrMJBOrx+SoiHZUQQkSF2EwKweGznc4cQE5LFUKIRrGZFKDFBWzSryCEEEZMJwW5gE0IIVqK6aTgcGQByAVsQggRFNNJwWqNx2ZLleYjIYQIiumkAHIBmxBCNBfzSUEuYBNCiCaxlxRSUkLDZ0PTBWxCCCFiMSlYLCYxNKspeDwH0DoQ4cCEECLyYi8pwGFDXWjtw+stjnBQQggReR1KCkqp25VSycr4i1JqjVLq3HAHFzYtBsUzVzVLv4IQQnS8pvADrXUVcC6QBnwfeDhsUYVbq5FSQS5gE0II6HhSUMG/04FXtNabm8078bRqPgK5gE0IIaDjSWG1UupfmKSwWCmVBJy4PbNpaVBWBoDD0RtQ0nwkhBCArYPL3QCMAnZqreuUUj2A68MXVpg1Gz7bYrFjt/eS5iMhhKDjNYVTga+01hVKqauB+4HK8IUVZs2Gzwa5gE0IIRp1NCn8GahTSo0E7gJ2AHPDFlW4HTbUhVzAJoQQ0PGk4NPmnpUXAX/SWj8DJIUvrDBrc6gL6WgWQoiO9ilUK6XuwZyKOkUpZQHs4QsrzNqoKXi9RQQCXiyWE3e3hBDieHW0pjALcGOuVzgI5AK/D1tU4dajh/nb6gI2j+dgpCISQoio0KGkEEwErwIpSqkLgAatdbfpU5AL2IQQwujoMBeXA18AM4HLgRVKqcvCGVhYtdF8BHIBmxBCdLRP4T5gvNa6CEAp1RNYAiwIV2Bh1Wr47MaagpyWKoSIdR3tU7A0JoSg0m/w2ujTavhsu70nStmk+UgIEfM6WlP4QCm1GJgXfDwLWBSekLpIs/GPlLLgcGRJTUEIEfM6lBS01ncrpS4FJgdnzdFaLwxfWF2gWVIAuYBNCCGg4zUFtNZvAm+GMZau1SopOJ3Z1NV9FcGAhBAi8o7YL6CUqlZKVbUxVSulqo7y2heVUkVKqU3tPK+UUk8rpbYrpTYopcYcz458Y1JTEEKIwxwxKWitk7TWyW1MSVrr5KOs+6/AeUd4fhowMDj9EDO+Utc5rKaQg89Xgd9f16VhCCFENAnbGURa62VA2REWuQiYq43lQKpSKitc8Rym2fDZ0PwCtgNdFoIQQkSbSJ5WmgPsa/a4IDiva6SlgccD9fWAXMAmhBDwDTqaI0kp9UNMExN9+/btnJU2v6o5Pp74+FMAqKlZS2rq1M7ZhhCdyO2GykqoqDCPU1LM5HSaazH9fnOLkNpac7sQux1sNjNpDdXVTZPHAw6HmZxOc+lOTU3T83V1ZhmPx6zL7weXC+LizORwQEODOaaqrzf/+/0QCLSctG6aoOVfj8fsU+NktTbFZLeDz2fiaNxGIGDibJyUav+xUmYdcXFNcdfXw759Ztq717yPdvvhU+N7ZrWabTbuF5j5jcs1xtk4z2Ix711VlfmcqqsPf08sFrPexvX7/S2n1u+Zw9EUv8sF110Ht94a3u9ZJJNCIdCn2ePc4LzDaK3nAHMAxo0bpztl682TQk4OLlcfnM6+VFR8Qm7u7Z2yCRHdGgum+npT8Pl85q/H01Q4VlWZwrJ54dR6crtNIWS1Nk1KtdxOTY0phBoL9UCg5fJ+v9l249QYi89nptpaU/C2xW4362jv+WhltZqE1DgFAk2JyOMxBWd8fFOBaLWa97KxoG4sONt6HAiY96+hwayrUe/e0KcPDB1qioDG97lx8vub3nO/vynJNG67cfn6evNZNn9tIABJSZCcDH37mv8bk0VjomqMtXFqXHfj1Dy5QdO2GhNvfHz4P5dIJoW3gVuVUq8DE4FKrXXXNej36mX+7t8Pw4YBkJIyhfLyJWitUc1/1aLLeDym0Cwra5oqK02hWltr/rrdLY+s3O6m52pqzI+n+ZGqz2fmNU719aaQr6trOgIMUQGw14E3HnT7ras2W1Nh5XIFt+MP4HUdwBtXiLW6D9T2RmG+R4mJkJpqpr59Dz9KtFrbP2K12SAhwby2sXaglHlfGqdAwCzTODUeadd73ZR6CvFST05yNpnJqSQlKRwOU+BU1tewv3Yfbn8DQ9LzSUt2kJRkCp/QkbA9QKm7iOq6BmrqvNTUe6n3eLE7AtgdfuyOAA4HZCb1JCuxNwnOuFDBplTLCcAb8LCtdBu1vmq8fi8evwdfwEd2UjYD0wfisrkOe78DOoDX78UX8IUmp81Jgj3hqL9Vt9dHYXkJfuUmLSGBBHsCLpurw79xr99LeUM5pXWlFNUWUVxXTFFtEdXuanom9KR3Ym+yErPIiM8IxerXfnwBHw2+Buq99dT76nH73Gg0FmVBobAoC8nOZFJdqaTFpZHqSsVmiXzjTdgiUErNA84EMpRSBcAvCd6DQWv9HOaK6OnAdqCOrr7n84gR5u+6dXDuuQCkpk6hqOhV6uu3Ex8/sEvDiVZunxurxdriy+r1ezlYc5D91fvZV15EvLcPye4hlJc4KS42BVSDKmWH71O2uz/D3xCPtWIg3oMDqdpzEpXeEmoS1lObtJ76lA3o+lT03lPx7jgN/4FhoK3gqIHU3Way+KAuw0y1PbHE1WDJ3ASZG9G9NmKx+kksm0xa1VR6+IYT57JgsQbwxe+lIeFrPHF7IK4Ci7Mcl6MCu72SZFs92lpPwFqPT9VSTzl1uox6XYHGVEbjrUkk2JJJciThsruIs7uIszuJszuxWpsShi/gY1/lPnZX7Mbtd4fmZ8RnMCJzBPk987EqK1XuKirdlaG/lQ2Vob8ADqsDp82Jw+ogMyGTAWkDGJA2gJPSTkKjKagqYHtVAQVVBVQ2VOKz+fCn+fGl+ABCr3VandR569hXu4+DNc2Gg6+BuKI4cpJziLPFUVBVQHlDs9OyCxyM7j2aiTkT6Z/Wn60lW9lwaAMbizZS46np8Hcm2ZkcKiizk7LJTsomPS6d7WXbWXtwLZuLN+Pxe9p8rULRP60/g9MH49d+DtUc4lDtIYpri/Fr/2HLW5U1VLDG2eOwKAtWZcWiLNR56yiuK6as/vDzXSzKQoI9gXh7PAkO89dhdbRIOg2+Bsrry6n2VHd434+XQmG1WEP7kexMJiM+g/T4dNLj0rls6GV8b/j3whuD1p3TGtNVxo0bp1etWtU5K+vXD049FV5/HYDa2i9ZuTKfwYP/QlbWDzpnG2HkC/jYXbGbsvoyyurLKK8vp9JdSZ23LnR04vV7sVls2K12HFYHCfYEBqYPZEjGEPql9sOiTOHm9ZojztJSzbKv17N417t8UfEe+wIrQGmUtmEJxKECdny2clCtvjcBK5ScYqb0ryFzo5nvt5tCvfXygNJWkj2D8dhKqbceAsBBInblpFaXdug96JPcJ1RgAqS6UslOymZn+U4afC3bU6zKSlpcGsnOZOLt8bhsLuJscSQ4EugR14M0Vxo94nqQ6Eik1lNLlbuKKncV1Z5qGnwNuP1u8zd4xNfIoizkJOXQP7U//dP6k5OUw97KvWw4tIENRRvYXLQ5dFTYOKW4Ukh1pZLiTCHFmYJSCrfPjcfvocHfwIHqA+ws38nuit14A97QdrISs8hNziXFlYLdYsdmsWGz2NBoPH5PaB1Om5M+yX3ok9yHvil9ibPHsb96P4VVhRRWF1LnrTPPp5jnbRYbKwtXsqJwBasPrKbOW0eaK40RmSMYkTmCwemDibfHY7fasVvs2K32UOFrtVgJ6AAldSUcqD7AwZqDHKg5YKbqA+yv3k+9r56M+AxG9x7NqN6jGJk5kh5xPXBYHaF17avax9aSrWwt2crXpV9jt9rJTMg0U2ImiY5ErMocoFgtVtw+N5XuSioaKqh0V9Lga8Af8OPXfgI6gMvmold8L3ol9KJnQk9cNhe1nlpqvbXUeGqo9dRS562jzldHracWj98TisVmseG0Oenh6kGPuKapV0LT+hIdiZTUlZj9rT5ASV0JSqkW70ucLY44exxxtjicNicKhUab2kTAT7WnmvL6csobyqloqMDr97aoaVS5qyipK6G0vpTSulKuH3U9d512V4d+G4f93pRarbUed7TlIl9XiaTRo2Ht2tDD+Pgh2GzpVFZ+0uVJYXfFbt7f9j4A0wZOIy81r83ltNZ8XvA58zbOY/6X8ymqLWpzOTCFiE3Z8QV8BDj8KMvij8NS1Z+Az0pA+8HiB1c5JAbXWTABZ+G9OG1O7HH1WF312Jxukqw9SXdk0zs+h8zEngSS91Du2MCB3hvYW7+OvkkDmJA5i7EZU8lPmUBqjwBVlp1sL9/G9rLt9IjrwajeoxjacygumwutNbsrdvPZvs/4vOBzfAEf/VP70y+1H3mpeTisDkrqSiipK6G4thinzcmwXsMY1msYqa5UAPZU7OGTvZ+wbM8yDtUeYtrJ0xicPphB6YPIS80LFfYnWrOgP+CnoKrAJISkrLA2L1yefzlgDjZK60rpldCrU94vrTW13toONfWcaBIdie3+Vk9UsV1TeOghM1VVmUZfYOPGi6mt3cSkSds7ZxvtqPPW8dm+z/hg+wcs2raILSVbWjw/tOdQpp88nUHpg0JtmEW1RXxe8Dm7K3bjsrm4YOAFTM2eBjWZ1Jf3oOpQDw7tSWb71ni+3hzH/n12CLZpowJg8YKrkvg+X5Ny8hbsWVsgdbc5A8VuxemwEu90cVrWmVycP42RJ2d2SceWECL8pKbQEaNHmx7C9ethshnrLzV1CqWlb+F2H8Dp7Lxr6bTWfLbvM97f/j5Ldy/li8Iv8Aa8OKwOzuh3Bj8c+0OmD5wOwKJti1i0bRFPf/F0qO01yZ5MkrUXqf58zqr4FQ3rLuKjJ5JZ0KqVJT4ehgyBc84yZ1gMHAjp6ZCaaiEtzUmPHr1ITOwFnN5p+yaE6D4kKYBpQgomhZSUKQBUVn5Cr16XH/cmaj21vLbxNf608k9sOLQBq7IyLnscd556J2fmncnpfU8n0WFqKQ0NsGUL9N49iMk77yDl6xq27i5n56aeVFe5qAb2AwfSTIH/3e/CoEGQl2fOaOnXz5xU1c1q6EKILhTbSSE31xxGN+tXSEwcjcUSf8xJweP3sLloM2sPrmVl4Upe3/w6FQ0VjMwcyQsXvsDl+ZeT5EwKLV9RAa/+HRYuhA8+MKdWginY+/VLZPDgRM6+Hk45pWkjkTZTAAAgAElEQVTKzJSCXwgRHrGdFJQ6rLPZYrGTnHwqFRWfdHg1/oCfN7e8yZPLn2TV/lWhs0US7AlMHzid2ybcxul9T0cpRV0d/Oc/sGwZfPwxfPqpOZ88Kwu+/3341rdMwX/yyeY8eCGE6EqxnRTAJIUnn2y67h/Tr7B790P4fJXYbCntvtTtczN3/Vwe/exRtpdtZ1D6IO469S5GZ5nT7k7ucbI5X7oOXnkF/vpXkwS8XnNhz6hRcOedcMklMGFC01WMQggRKZIURo82pfSXX5pSmsZ+BU1l5Wekp09r82Xvfv0uN793MwVVBYzNGsuCmQu4+JSLsVqsoWVWr4bnn4d588wJTiefbJLAGWfAaaeZK1OFECKaSFIYE7y3z9q1oaSQnDwJpWxUVn5yWFIorSvl9g9u59WNrzKs1zBeuuglzu5/duj860AAFi2Cxx4zzUNxcTBzJtxwA0yZIn0BQojoJklh4EAzWMzatXC9GWnDao0nMXEslZUt+xUWblnIze/dTGl9KQ9MfYD7pt6Hw2qanLxeePVVePRRcwZRnz7whz+YZCA1AiHEiUJasS0WGDmyRWczQErK6VRVfYHf34DWml99/Cu+O/+7ZCdls+qmVTx01kM4rA78ftNfMGSIySkOh3m8Y4dpKpKEIIQ4kUhSANOvsG5diyEzU1OnoLWHisrP+cl7P+GXS3/JtSOvZcWNKxjZeyRaw4IFMHw4XHONuSD6rbdMbrn6ajO6pBBCnGgkKYBJCjU15vA+KDX1TLzayfcW3sBzq59j9uTZvHTRS9itdvbvhxkzTF+BUvD3v8OaNWae9BkIIU5kkhSg5ZXNQfuqy7jnyxT+VbCLP3z7EX53zu8AxcsvQ34+fPghPP44bNgAl10mp5MKIboHKcrAlPI2G6xdS5W7inuW3MMpz5zClxWV/GIIXJGXRnk5XHihuR3e8OFmuKT/+R9zcxQhhOguJCkAOJ3o/KG8WPAOg/44iIf/+zCz8mfx1a3buLD/cNas+Sennw7/+hc89RQsXWpOWhJCiO5GTkkN+uuUJG7I+C+npZ3G21e+zYScCQBsrbiX66+fgtfrY/FiG2edFeFAhRAijKSmABTVFnFXz7VM2QOffOf1UEL48EO49NJZKAVz5/6vJAQhRLcnSQH4n8X/Qw0e/u8dsPzzLQCWL4dp06BfP8X8+U+Qmvp7fL6qCEcqhBDhFfNJ4YPtH/Daxte4d8q9DBnxLbj/fqq2HeJ734PsbDOa6ZgxlxMI1HLo0GuRDlcIIcIqppNCraeWm9+7mcHpg7lnyr3w7LNQX88t537Nnj3w2muQlgZJSeNJTBzFgQP/x4l2+1IhhPgmYjopPLj0QXZX7GbOhXNw2pwweDB/O38ef9s9hV9etZ3TTjPLKaXIyvoRNTXrqK7+IrJBCyFEGMVsUviq5CueWP4EN46+kan9pgLmguafLLmEKa6V3Pff6VBfH1o+M/MqbLY09uz5TaRCFkKIsIvZpLB4x2L82s/9U+8HzCinV10FFoviby96sO7cBr/9bWh5my2JPn1+Rmnpu1RWfhapsIUQIqxiNimsKFxBVmIWfVP6AvDuu7BiBTzzDPS9crIZ1e6RR+Drr0Ovyc29Dbs9k50775W+BSFEtxS7SaFgBRNzJ4ZujjN/PmRkwKxZwQUee8yMbvfss6HXWK0J9Ot3P5WVH1NeviQCUQshRHjFZFIoqSthR/kOJuZMBKCuDt55By691AyBBEBmJlx0Efztb+b+zUHZ2TfhdPZj1y6pLQghup+YTApfFJoziBqTwvvvQ20tXH55qwV/8AMoLTUZI8hicZKX90uqq1dRUvLPrgpZCCG6RMwmBYViXPY4wDQd9eoFU6e2WvDb34acHHjppRazMzO/T1zcYHbtuh+t/V0UtRBChF9MJoUVhSvI75VPkjOJ2lrTydyi6aiR1QrXXmuqEvv3h2ZbLDb69/81dXVfcvDgy10bvBBChFHMJQWtNV8UfhFqOnrvPdOncFjTUaPrrjO36XzllRaze/a8lOTk09ix4248nqLwBi2EEF0k5pLC9rLtlNWXhZLC/PmmT3nKlHZeMHCgefLFF6FZx7JSFgYPfh6/v5rt2/+nCyIXQojwi7mksKJwBQCTcidRU2NqCpdddpQ7qF1/vble4fPPW8xOSBhK3773UlT0GqWl74cxaiGE6BqxlxQKVpDoSGRoz6G8+y40NByh6ajRzJmQkGBqC63063cP8fFD+PrrH+Pz1YQnaCGE6CKxlxQKVzAuexxWi5X58yErCyZPPsqLEhNN5njjDXPuajMWi5PBg5/H7d7L7t2/CF/gQgjRBcKaFJRS5ymlvlJKbVdKzW7j+euUUsVKqXXB6cZwxtPga2DdwXVMzJlIdTUsWtSBpqNG118PNTUwd+5hT6WkTCY7+ycUFDxFVdWKzg9cCCG6SNiSglLKCjwDTAOGAlcqpYa2segbWutRwemFcMUDsO7gOrwBLxNzJrJ0Kbjd8N3vdvDFp59uOpwfeAAqKg57esCA3+F05rJ58yw8npJOjVsIIbpKOGsKE4DtWuudWmsP8DpwURi3d1QrCsxR/MTciWzYYOaNHdvBFysFTz8NZWXw4IOHPW2zJZOf/yYezwG2bLmSQMDXOUELIUQXCmdSyAH2NXtcEJzX2qVKqQ1KqQVKqT5hjIcVhSvITc4lOymbTZsgLw+Skr7BCkaNgh/+EP70J/jyy8OeTk4ez6BBz1JevoRdu+7vtLiFEKKrRLqj+R0gT2s9Avg30OblwUqpHyqlVimlVhUXFx/zxlYUrghdn7BpEwwbdgwr+fWvTSa5/fYW1y00ysq6gaysH7Fv3yMUFS045liFECISwpkUCoHmR/65wXkhWutSrbU7+PAFoM3GHK31HK31OK31uJ49ex5TMMW1xews38nEnIl4PLB1KwwffgwrysgwiWHJEnjrrTYXGTjwKZKTJ7F163XU1m4+pniFECISwpkUVgIDlVL9lVIO4Arg7eYLKKWymj2cAWwJVzChkVFzJ/L11+DzHWNNAeDHPzYvvvNOc6FDKxaLk/z8N7HZkti48QI8nkPHEbkQQnSdsCUFrbUPuBVYjCns52utNyulfqWUmhFc7KdKqc1KqfXAT4HrwhVPn5Q+3D7xdsZmjWXjRjPvmJOCzQZPPQW7drW4ZWdzTmc2w4a9jcdziI0bZ+D31x3jxoQQouuoE+1GMePGjdOrVq06rnXcdx88+qi5Ds3hOI4VXXMNvPYafPEFjBnT5iIlJW+xadMlZGRcTH7+3zFn6gohRNdSSq3WWo872nKR7miOiI0bYdCg40wIYGoLmZlmeG23u81FMjIu4uSTn6CkZCE7dvzsODcohBDhFZNJYdOmY+xkbi0tDZ5/3qzwoYfaXSw393Zycn5KQcHj7NnzW7mNpxAiasVcUqiuNl0Bx9yf0Nr06ea2nY88YpqR2nHyyY/Tq9cV7Np1H19+eYUMnieEiEoxlxQarznrlJpCo8cfh+xs04xUX9/mIkpZGTLkNQYMeJji4gWsWTORurqvOjEIIYQ4fjGXFI77zKO2pKTAX/5iLn647DIzFEYblFL07ftzRoxYjMdziNWrx1NS8nabywohRCTEXFLYtAni46F//05e8bnnmuEv/v1vGD36sBvyNNejxzmMG7eGuLhBbNp0CYWFz3RyMEIIcWxiMink54MlHHt+yy3w3/+alU+dCo89Zu7v3AaXqy+jR39Mevr5bNt2Kzt2zEbrtpcVQoiuEnNJYePGTu5PaG38eFi7FmbMgLvvhksvNb3bbbBaE8jP/wfZ2Tezb98jbNnyfQKBtk9tFUKIrhBTSaGoyEyd2p/QltRUWLAAnngC3nkHJk2C7dvbXNRisTFw4DP07/87iopeY82ayVRUfBrmAIUQom0xlRQ2bTJ/w1pTaKQU3HEHLF4MBw+aGsTixe0squjXbzb5+QvweA6ybt0UNm26lLq6thOJEEKES0wmhbDXFJo7+2xYuRL69DHXNPzmN2Y0vjb07HkpEyd+TV7erygrW8zKlUPZtu123O6DXRiwECKWxVRS2LjRjHydmdnFGx4wwJyNNGsW/OIXphO6neYkqzWevLxfMHHiNnr3vo7CwmdYsWIA27f/Pzyeoi4OXAgRa2IqKTTeWEepCGw8IQFefdVMW7aYu7j93/+1eaMeAKczi8GD5zBhwlZ69pxJQcETLF/en+3b76K+flcXBy+EiBUxkxQCgU4c8+hYKQXf+56pspx6qrkvw4wZUFLS9vLV1cTvhyEnPc+ECV+SkXExBQVPsWLFSWzcOIOysn/LOEpCiE4VM0lh716oqeni/oT25OaaTuennoJ//cvUGpYta3re4zFnLvXrBwMHQlwc8UO/w9DbDjJ55X307XsvVVXL2bDhXFasGMjOnfdSXb1GEoQQ4rjFTFLo0jOPOsJigZ/+FJYvh7g4OOss+NWv4M03YehQc1e38ePhhRfMDSAmT4aSEuw/+xUD/pPHqafu45RT5hIX15+9ex9l9eqxrFhxMrt2/QKvtzzSeyeEOEHFzE12NmyAuXPhgQcgOTkMgR2P6mq4+WbT3wCmOvPYY/Cd77Rczu83ZzAtXWpqFhMnAuDxlFBS8k+Ki/9Oefm/sdlS6Nv3XnJybsVqjevafRGiO6irg+JiU1vvJjp6k52YSQpRT2uYP9/c8/nqq8Hazh3ayspg3DjTxLRqFfTu3eLpmpr17Nx5D2Vl7+N09qFv33tITT2L+PhBKBUzFUMRTSorYd48M1hkRkakozmygwfNGGZ//rM5WPv3v+GMMyIdVaeQpNCdrV9vOqrHjoUPP2zzFnLl5R+xc+fPqa5eCYDVmkhi4hiSkyeSkXExycmTJEmItmltCvLU1ONbj98PL70E995rjrqnToUlS8Bu75w4O0tDg2nGfeUV+NvfwOuFiy4yZwmWlMCKFXDSSeGNweuF1atN03GYmjLkdpzd2ciRZqjuTz+F666DdesOO7U1Le0sxoxZwbhxGxk8+CUyM68lEHBTUPAka9dOZsVHuZT9eDyBPr3RF1wAf/iD+VL6/ZHZJxEdAgG48UZzRP/ww+0O6HhUn34KEybATTeZe9/++temyfPuu4/8Oq3hH/8wJ1gMGAAXXwy//KXpa/vqq8Mv/Ny+HX7/e9PnlpNjas4ZGSahfetb5jVt2bEDfvtbOOcccwfFs84ytZkbbzSvWbgQ3n3X7P+FF5ok2dm0Njfmuu02cz+WU081zVUPPAClpU3Lud3w3ntwww2mNSHctNYn1DR27Fgtgu67T2vz1dK6Tx+tb75Z69df1/rtt7V+/32tP/xQ65Urta6rC73E663QZa/drd058VqDLh2Hru1jCa0n0CNV65df1joQiOCOHYfVq7V+7jmty8sjHUnnKCzU+pVXzOfp83XsNR6P1nv3ar1tm9abN2u9Zo3WO3Yc/TMNBLS+5RbzXRg50vw97zyti4o6Hq/Ho/Xdd5vX5uRo/dprTdu9/XYz/5VX2n7tjh1aT59ulhkxQuvLL9f6lFO0tjR9P7XdrvXQoVp/97taDx/eNH/MGK1vuEHrH/3I7MNPfqJ1WprWLpfWjz3W9N7t3av1jTdqbbU2beeOO8xvpqLi8Jg++khrm03r73xHa6+3/f1etkzrq64yMV9xhdbf+57WP/iB+S0VF7dc9quvtP7lL7UeONDE4HRqPXOmWfaSS8y8hAStb73VrCspycxLTjb7coyAVboDZWzEC/lvOklSaOXAAa1feEHriy7SOj6+6UfSfLJazQ/ouuuavnSDB2vfkvf0oUPz9datN+qVb2Xrzb9Al48wr6m+cKiu2LNYBwLNCiKfT+stW8wPvy1+v9YbN7ZIQp2qoMDs64IFbRdw//yn1nFxZv/i4kwhsWpVeGLpqEDAFBibNh25UK6p0Xr7dq3/+1+t33xT67vu0nrYsJafY58+Wj/0kNb79h3++qoqrefPN4VRamrb34O+fc13YO5crffvPzzOn/3MLPf//p95/Oc/mwIrO1vrpUuPvq9792p96qlmHT/6kdmn5jwerc84w3w2a9c2bXfdOq3vv98U4ImJWj/+eMsCuLZW6y++0Pqvf9X65z/X+sILTYE6ZYrWTzyh9a5dbcezf7/WM2aYeCZNMoWsw2Gm224z36eOeP55s46bbjo8QR46pPW115rn09NNEhs4UOuTTjKPwSS1yZPN+zt2rJmnlNZnnmm+z62T0aZNWl99tfndZmSYJLZokdYNDR2Ltx2SFGJRXZ3W69eb2sFnn2n98cemgLnvPq2nTdO6Vy/zw/v1rw/7ggUCAV1Ts0Xv3fWY3n9Lfx2woOsz0Rv/kKR3/m6wrrhwgPalmaQT6JmuA7ff3vTD3rfPrHPAAPOV6tdP67feajvGAwfa/3Lv2aP11Kmm8DvnHHPE9/TTWt9zT9ORa+N0zjmmEG30pz+ZH9+ECVr/5z/mB9yYJMePNz++1oXUsdizxxRECxYc/ei7qkrrK69sijk7W+trrjFHhH/+s6nZnX5624W4w6H12Wdr/cgj5kj/7383+9xYyAwapPXgwU2Tw2Gey8jQ+vrrtZ4zxxyRv/GGSZbPPqv1pZdq3aNH0zbGjTOf2/r1JtmAian5Pq1d23REO3Gi1k89ZT7D5rxec6Tdo4cp1OfNa/89OXjQ1CDy8sx70bt3UzwzZ3a8oO6oQEDrV181sVmt5kBhz55vvp677moqzCdONO/X44+bz85u13r27MO/X36/+S0+8IDWo0c3veePP96x/ayo6HjtsAM6mhSkozmWaG36DGy2oy7q+/TfcNXV2Paa8Za8KRZKJwSoGgZpqyD9c7D4wNsnFVtBJUprOPNM0wb8/POweTNccIG5QM9mM22hb7xhzpjKy4M//tE83+jjj2HmTNPpd8EFpq34q6+gqsqciTV5Mpx/PkybZtqrf/5z07784IOm/fXRR03b77x5ZkgRMO3Ac+fCc8+Zm3MnJZkzuy67zKzT7TZncfn94HKZ60Xi481yeXngdDbFt3UrPPKI6Yhs3q6dkmLutDdjBlxxBWRlmfkbN5rtbN9uYszKMmeyLFnSdLvW5GRz4czw4WZ7mZlN0+DBTfvR3I4dpvO29dhZubmmc/S009o/cw1MG/n69fD++2ZY9+XLm5679lp48cXD70BVXW3ew9deM/1XFos5Hbq+Hg4cMOPRa236uv7+d9MfcCQrVpjvSkICfPvb5tTrc8817erhUllp4m11tl6HaW363N57DxYtMoNcam36LZ55Bk455ejraGgw37MIkbOPxPGrqjIF+bBhMGECftzU12+nunolNXs+xr7wQ5I+3k/1IDg0zYp98ARSUqbgsuaQ9NflJD62EOXxobzBQnTcOFNwv/66ObPj4otN0vjnP83FeiefbP5v/IFpDYcOmcI6JaVlbIWFcOutZnkw13k8/XTbCU9r+OwzM9bU/PkmGRyNxWI6/QYNMutctMj8oG+6yWy3osLcTGndOjPYYWNh+a1vmQLz8cdNzPPmmQKwUeN4Kykp0LdvhAbiaubgQdOhWlxsOoGPdsDw5Zdmnz76yHTQ9u5tEl5enhnCpaOFXmUlJCYeOYFFs6Ii2LPHfKcj/Rl2kCQF0SV8vioqKz+jsvJjKio+prp6JVqbJOAohj7zwZsK1dMGEjfs26SmnkG87SRcf16I9X8fR3m95sh7xgxzVN+68D+at982NYXrruvYj7OszBzl2e2mJuB0msK8ocEcSdbXmwJ/+3b4+mszFRfD978Pt98OPXu2vd6tW82R9Kuvws6dJjm89loEhuQVom2SFEREBAIefL5yfL4q/P4qvN4yqqtXUVHxMVVV/8Xvrwkt6zwIJ73kxDuwJzW3TCMxeSQJCSNJSBiG3X6c58hHitZmoK3c3BP3KFh0S5IURNQJBHzU1KzD7d6Dx3MIj6cIj+cg9fVfUVOzHp+vacwmhyOL+PihJCQMJSEhP5QsbLbECO6BECeujiaFo/c4CtFJLBYbycnjgMO/l1pr3O5CamvXU1u7mbq6LdTWfsnBgy81q10o4uJOwuXKw2KJw2JxYbHEYbenExc3iPj4wcTHD8bhCHb2EkBrP2DBYpGvuhAdIb8UERWUUrhcubhcuaSnnx+ar7WmoWEPtbUbqKlZT03Nejye/Xi9pQQC9QQCDXg8RQQCdUdcv82Wht3eC4ejJw5HDklJo0lMHENS0hjs9vRw754QJwxJCiKqKaWIi8sjLi6PjIwZbS6jdQC3u5C6uq+or/8Kj+cQSlkBC0pZCQQ8eL0leL1FeL3FVFd/QXHxG6HXOxy9sVqTsFjisVrjsFqTcDiycTpzQ5PL1Qensw82WxrqBDnbRIhjIUlBnPCUsuBy9cHl6gOc06HXeL3l1NSsobp6NXV1W/H764I1j3p8virq6rbidu8HWo4FZbEk4HTmYrenYbWmYLOZSSkbJgmZRGS1JmGzpWGzpWKzpeFw9MLh6B1MQPGh9ZkLhrwoZZMBCkVUkKQgYpLdnkZa2tmkpZ3d7jJa+/F4DuF276OhYR9u9z7c7r243QX4fBX4fGU0NOzC769Caz9aB2jsx/D7q4G2T+KwWhMBC4GAG60br5lQWK2JWK3J2GzJh/11OLKJizuJuLiTiYs7GaVsuN2FeDz7cbv3o7W7WZJKxmKJRylrqMZktSbidOZgsRw+oq4QzUlSEKIdSllxOrNxOrNJTp74jV6rdQCfryp4em4ZHk8xHs9BPJ6DeL2H0FoHO8qdWCwOAgEPfn9V8DWV+P3V+P1VwQRUicdziNa1lmPYIxyO3jidfbHbe+D31+L31+D3V6O1L5hQUrFaU7DbewSb0HJwOnOw2VJoaNhHQ8NOGhp24fEcxG7PwOHICS6Tjd2eEawd9cBuT8NicQWTUuQEAm7c7gIaGvZgtSaRlDROmv+OIqxJQSl1HvAUYAVe0Fo/3Op5JzAXGAuUArO01rvDGZMQXUEpC3Z7avB6i/7Hvb5AwEtDwx7q67fT0LADrf04nTmhgtticTZLKFX4/XWAqb2YmktVsMazF7d7L15vMRZLAg5HFlbrQJSyBZNSBQ0NO6muXonHcxA4fOhshyMHh6M3tbVb8Hj2o7X3CJFbg4nPSeuR+u32NOz2TByOXtjtPfH7a/F6DwVPVz6ExeLAZkvHbk/Hbu+BxdJ4tbQKToFgDc2H1n4CATeBQF2wKbAOr7c4uA9NnM5+9Op1Ob16zSIxcQxKKQIBX7DpsC6YJM0UCDQA1mY1LkUgUN+sqbEhGEMA0ChlxeHIxuXqi9PZp0UzYWuNn4tpNjx6kgoEvPj9NShlw2ZLOuryxyNs1yko8y5+DXwbKABWAldqrb9stsxPgBFa6x8rpa4ALtFazzrSeuU6BSG6RlPzWSE+X0WwsOuH1epqtkwAr7cYt/sAPl8ZXm8ZPl8ZPl85gUADgYAn1EzWsqwJ4POVh65X8XqLsFoTcDh6hxKF1t7Q+szZZm5Mk1xw4DZlCRaqVkzycWG1xgdPGIjHZkvD5eqH09kXl6svbncBRUVvUF7+L7T2YbHEEQh4OP4aWNtstlRABROXH5Ok/cEr/hvfC2uw2TARqzUh9L6bROcLJarGxNu37z0MGPDbY4onGq5TmABs11rvDAb0OnAR8GWzZS4CHgz+vwD4k1JK6RPtijohuqHmzWftL2PB4cjE4TgxhvPo3fsavN5SiosXUlf3ZfB6lzis1sa/SaFC2mJxBmsBjYW0DiadOKzWeJRyhmoQpsbhxeMpDPY/7Q2eqEAocZnJFppMv1JDsKmwJng9jmq2jBWrNT4YSwJWayLJyRPC/h6FMynkAPuaPS4AWjfMhpbRWvuUUpVAOlASxriEEDHMbk8nO/vGsKw7Pv7ksKy3K50Q58AppX6olFqllFpVXFwc6XCEEKLbCmdSKAT6NHucG5zX5jLK1KdSMB3OLWit52itx2mtx/Vsb5RKIYQQxy2cSWElMFAp1V8p5QCuAN5utczbwLXB/y8D/iP9CUIIETlh61MI9hHcCizGnJL6otZ6s1LqV5jbwr0N/AV4RSm1HSjDJA4hhBAREtbrFLTWi4BFreY90Oz/BmBmOGMQQgjRcSdER7MQQoiuIUlBCCFEiCQFIYQQISfc7TiVUsXAnmN8eQbRf2GcxHj8oj0+iP4Yoz0+iP4Yoy2+flrro57Tf8IlheOhlFrVkbE/IkliPH7RHh9Ef4zRHh9Ef4zRHl97pPlICCFEiCQFIYQQIbGWFOZEOoAOkBiPX7THB9EfY7THB9EfY7TH16aY6lMQQghxZLFWUxBCCHEEMZMUlFLnKaW+UkptV0rNjnQ8AEqpF5VSRUqpTc3m9VBK/VsptS34Ny2C8fVRSn2klPpSKbVZKXV7FMboUkp9oZRaH4zxoeD8/kqpFcHP+43goIwRo5SyKqXWKqXejdL4diulNiql1imlVgXnRdPnnKqUWqCU2qqU2qKUOjXK4hscfO8apyql1B3RFGNHxURSCN4a9BlgGjAUuFIpNTSyUQHwV+C8VvNmAx9qrQcCHwYfR4oPuEtrPRSYBNwSfN+iKUY38C2t9UhgFHCeUmoS8AjwhNb6ZKAcuCGCMQLcDmxp9jja4gM4S2s9qtlplNH0OT8FfKC1PgUYiXkvoyY+rfVXwfduFOae83XAwmiKscO01t1+Ak4FFjd7fA9wT6TjCsaSB2xq9vgrICv4fxbwVaRjbBbbW5h7bkdljEA8sAZzh78SwNbW5x+BuHIxBcK3gHcxd52PmviCMewGMlrNi4rPGXOflV0E+0CjLb424j0X+G80x3ikKSZqCrR9a9CcCMVyNJla6wPB/w8CUXHzW6VUHjAaWEGUxRhsmlkHFAH/BnYAFdrcIR0i/3k/CfwMCAQfpxNd8YG5k/y/lFKrlVI/DM6Lls+5P1AMvBRsgntBKZUQRfG1dgUwL/h/tMbYrlhJCickbcxVdZMAAAPBSURBVA4vIn56mFIqEXgTuENrXdX8uWiIUWvt16bangtMAE6JZDzNKaUuAIq01qsjHctRnK61HoNpYr1FKTW1+ZMR/pxtwBjgz1rr0UAtrZphouF7CBDsG5oB/L31c9ES49HESlLoyK1Bo8UhpVQWQPBvUSSDUUrZMQnhVa31P4KzoyrGRlrrCuAjTHNMavAWrxDZz3syMEMptRt4HdOE9BTREx8AWuvC4N8iTFv4BKLncy4ACrTWK4KPF2CSRLTE19w0YI3W+lDwcTTGeESxkhQ6cmvQaNH8FqXXYtrxI0IppTB3x9uitX682VPRFGNPpVRq8P84TJ/HFkxyuCy4WMRi1Frfo7XO1VrnYb53/9FaXxUt8QEopRKUUkmN/2PaxDcRJZ+z1vogsE8pNTg462zgS6IkvlaupKnpCKIzxiOLdKdGV03AdOBrTHvzfZGOJxjTPOAA4MUcDd2AaW/+ENgGLAF6RDC+0zHV3Q3AuuA0PcpiHAGsDca4CXggOH8A8AWwHVOVd0bB530m8G60xReMZX1w2tz4+4iyz3kUsCr4Of8TSIum+IIxJgClQEqzeVEVY0cmuaJZCCFESKw0HwkhhOgASQpCCCFCJCkIIYQIkaQghBAiRJKCEEKIEEkKQnQhpdSZjSOlChGNJCkIIYQIkaQgRBuUUlcH79OwTin1f8FB92rU/2/v/lWjDKIwjD+vCKIEtNHGQlAbEcTKQrHyBiwUQUlhbWMngiJ4A1aClhFTiGBuwBQLKURFgoKlVSobES20iMdiJh/rpogsJC74/Krds7PDTvHt+f4w5yQPet+G5SQH+9jTSV4leZ9kaaNmfpLjSV72Xg/vkhzr08+N9QZY7DvHpZlgUpAmJDkBXAHOVSu0tw5co+1YfVtVJ4ERcK9/5Qlwq6pOAR/G4ovAw2q9Hs7Sdq9DqzZ7k9bb4yitPpI0E3ZvPUT671ygNUp500/i99IKmf0CnvUxT4EXSfYDB6pq1OMLwPNeS+hwVS0BVNUPgD7f66pa6+9XaT01VrZ/WdLWTArSZgEWqur2H8Hk7sS4aWvE/Bx7vY7HoWaIt4+kzZaBS0kOwdCr+AjteNmobHoVWKmqr8CXJOd7fB4YVdU3YC3JxT7HniT7dnQV0hQ8Q5EmVNXHJHdonch20arY3qA1dznTP/tMe+4ArSTyo/6n/wm43uPzwOMk9/scl3dwGdJUrJIq/aUk36tq7l//Dmk7eftIkjTwSkGSNPBKQZI0MClIkgYmBUnSwKQgSRqYFCRJA5OCJGnwG5rTZzFNAB3/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 591us/sample - loss: 0.2342 - acc: 0.9315\n",
      "Loss: 0.23419579769468257 Accuracy: 0.9314642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 14):\n",
    "    base = '1D_CNN_custom_pool_2_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,058,512\n",
      "Trainable params: 2,058,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 528us/sample - loss: 1.7100 - acc: 0.4746\n",
      "Loss: 1.7099558357385336 Accuracy: 0.47455868\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_94 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,039,664\n",
      "Trainable params: 1,039,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 508us/sample - loss: 1.4001 - acc: 0.5664\n",
      "Loss: 1.4001398931287408 Accuracy: 0.56635517\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,049,968\n",
      "Trainable params: 1,049,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 603us/sample - loss: 1.2685 - acc: 0.6147\n",
      "Loss: 1.2684546218359087 Accuracy: 0.61474556\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_103 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 558,512\n",
      "Trainable params: 558,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 572us/sample - loss: 1.0257 - acc: 0.6993\n",
      "Loss: 1.025704760950055 Accuracy: 0.6992731\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 323,056\n",
      "Trainable params: 323,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 618us/sample - loss: 0.7964 - acc: 0.7670\n",
      "Loss: 0.7964310067217422 Accuracy: 0.7669782\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 215,600\n",
      "Trainable params: 215,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 644us/sample - loss: 0.4819 - acc: 0.8685\n",
      "Loss: 0.48190565458339324 Accuracy: 0.8685358\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_124 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 255,664\n",
      "Trainable params: 255,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 649us/sample - loss: 0.3154 - acc: 0.9074\n",
      "Loss: 0.3154498406597882 Accuracy: 0.9073728\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_133 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 274,224\n",
      "Trainable params: 274,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.2060 - acc: 0.9481\n",
      "Loss: 0.20598556604449128 Accuracy: 0.94807893\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_143 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 323,504\n",
      "Trainable params: 323,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 691us/sample - loss: 0.1737 - acc: 0.9564\n",
      "Loss: 0.17369112051860938 Accuracy: 0.95638627\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_154 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 389,168\n",
      "Trainable params: 389,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 724us/sample - loss: 0.2029 - acc: 0.9481\n",
      "Loss: 0.20288274491347008 Accuracy: 0.94807893\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_166 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 551,216\n",
      "Trainable params: 551,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 700us/sample - loss: 0.2342 - acc: 0.9315\n",
      "Loss: 0.23419579769468257 Accuracy: 0.9314642\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_pool_2_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "    log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,058,512\n",
      "Trainable params: 2,058,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 607us/sample - loss: 3.4394 - acc: 0.4812\n",
      "Loss: 3.4394375793659058 Accuracy: 0.48120457\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_94 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,039,664\n",
      "Trainable params: 1,039,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 707us/sample - loss: 2.2473 - acc: 0.5942\n",
      "Loss: 2.247319127911719 Accuracy: 0.5941848\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,049,968\n",
      "Trainable params: 1,049,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 684us/sample - loss: 1.9840 - acc: 0.6644\n",
      "Loss: 1.9839909711730814 Accuracy: 0.66438216\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_103 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 558,512\n",
      "Trainable params: 558,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 695us/sample - loss: 1.4751 - acc: 0.7294\n",
      "Loss: 1.4751127770385266 Accuracy: 0.72938734\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 323,056\n",
      "Trainable params: 323,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.9803 - acc: 0.8064\n",
      "Loss: 0.9803158763289328 Accuracy: 0.8064382\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 215,600\n",
      "Trainable params: 215,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 733us/sample - loss: 0.5497 - acc: 0.8835\n",
      "Loss: 0.5496740678017756 Accuracy: 0.8834891\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_124 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 255,664\n",
      "Trainable params: 255,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 779us/sample - loss: 0.3348 - acc: 0.9238\n",
      "Loss: 0.3347622621282239 Accuracy: 0.92377985\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_133 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 274,224\n",
      "Trainable params: 274,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 783us/sample - loss: 0.2142 - acc: 0.9502\n",
      "Loss: 0.21417650856456777 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_143 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 323,504\n",
      "Trainable params: 323,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 768us/sample - loss: 0.2365 - acc: 0.9537\n",
      "Loss: 0.2364583510686092 Accuracy: 0.9536864\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_154 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 389,168\n",
      "Trainable params: 389,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 824us/sample - loss: 0.2326 - acc: 0.9520\n",
      "Loss: 0.2325742549711678 Accuracy: 0.95202494\n",
      "\n",
      "1D_CNN_custom_pool_2_ch_32_DO_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_166 (Conv1D)          (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 8000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 4000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 500, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 250, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 125, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 551,216\n",
      "Trainable params: 551,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 820us/sample - loss: 0.3230 - acc: 0.9364\n",
      "Loss: 0.32301166008244947 Accuracy: 0.9364486\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
