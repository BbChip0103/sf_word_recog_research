{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,248\n",
      "Trainable params: 8,195,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,739,952\n",
      "Trainable params: 2,739,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 925,488\n",
      "Trainable params: 925,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4341 - acc: 0.2012\n",
      "Epoch 00001: val_loss improved from inf to 1.96005, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/001-1.9600.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.4340 - acc: 0.2012 - val_loss: 1.9600 - val_acc: 0.3722\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8028 - acc: 0.4134\n",
      "Epoch 00002: val_loss improved from 1.96005 to 1.60336, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/002-1.6034.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.8029 - acc: 0.4134 - val_loss: 1.6034 - val_acc: 0.4691\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5568 - acc: 0.5017\n",
      "Epoch 00003: val_loss improved from 1.60336 to 1.46965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/003-1.4696.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.5570 - acc: 0.5017 - val_loss: 1.4696 - val_acc: 0.5379\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4157 - acc: 0.5519\n",
      "Epoch 00004: val_loss improved from 1.46965 to 1.35370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/004-1.3537.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.4157 - acc: 0.5519 - val_loss: 1.3537 - val_acc: 0.5751\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2949 - acc: 0.5954\n",
      "Epoch 00005: val_loss improved from 1.35370 to 1.31833, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/005-1.3183.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2950 - acc: 0.5954 - val_loss: 1.3183 - val_acc: 0.5924\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2082 - acc: 0.6265\n",
      "Epoch 00006: val_loss improved from 1.31833 to 1.24301, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/006-1.2430.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2082 - acc: 0.6265 - val_loss: 1.2430 - val_acc: 0.6110\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1360 - acc: 0.6474\n",
      "Epoch 00007: val_loss improved from 1.24301 to 1.21702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/007-1.2170.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1361 - acc: 0.6474 - val_loss: 1.2170 - val_acc: 0.6173\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0735 - acc: 0.6649\n",
      "Epoch 00008: val_loss improved from 1.21702 to 1.18738, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/008-1.1874.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.0737 - acc: 0.6648 - val_loss: 1.1874 - val_acc: 0.6362\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0091 - acc: 0.6873\n",
      "Epoch 00009: val_loss improved from 1.18738 to 1.16063, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/009-1.1606.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0090 - acc: 0.6874 - val_loss: 1.1606 - val_acc: 0.6452\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9494 - acc: 0.7083\n",
      "Epoch 00010: val_loss improved from 1.16063 to 1.15508, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/010-1.1551.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9494 - acc: 0.7082 - val_loss: 1.1551 - val_acc: 0.6415\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.7229\n",
      "Epoch 00011: val_loss improved from 1.15508 to 1.13142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/011-1.1314.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8909 - acc: 0.7229 - val_loss: 1.1314 - val_acc: 0.6611\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8490 - acc: 0.7372\n",
      "Epoch 00012: val_loss improved from 1.13142 to 1.13033, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/012-1.1303.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8490 - acc: 0.7373 - val_loss: 1.1303 - val_acc: 0.6583\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7993 - acc: 0.7507\n",
      "Epoch 00013: val_loss improved from 1.13033 to 1.12265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/013-1.1227.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7992 - acc: 0.7507 - val_loss: 1.1227 - val_acc: 0.6594\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7461 - acc: 0.7679\n",
      "Epoch 00014: val_loss did not improve from 1.12265\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7461 - acc: 0.7679 - val_loss: 1.1329 - val_acc: 0.6671\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7045 - acc: 0.7764\n",
      "Epoch 00015: val_loss did not improve from 1.12265\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7045 - acc: 0.7764 - val_loss: 1.1230 - val_acc: 0.6755\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6653 - acc: 0.7893\n",
      "Epoch 00016: val_loss improved from 1.12265 to 1.09735, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv_checkpoint/016-1.0973.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6652 - acc: 0.7893 - val_loss: 1.0973 - val_acc: 0.6785\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.7989\n",
      "Epoch 00017: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6339 - acc: 0.7989 - val_loss: 1.1139 - val_acc: 0.6727\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.8080\n",
      "Epoch 00018: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6015 - acc: 0.8080 - val_loss: 1.1377 - val_acc: 0.6739\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8137\n",
      "Epoch 00019: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5781 - acc: 0.8137 - val_loss: 1.1368 - val_acc: 0.6855\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8227\n",
      "Epoch 00020: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5498 - acc: 0.8227 - val_loss: 1.1549 - val_acc: 0.6765\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8338\n",
      "Epoch 00021: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5161 - acc: 0.8338 - val_loss: 1.1622 - val_acc: 0.6795\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8390\n",
      "Epoch 00022: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4967 - acc: 0.8390 - val_loss: 1.1950 - val_acc: 0.6748\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4821 - acc: 0.8424\n",
      "Epoch 00023: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4822 - acc: 0.8424 - val_loss: 1.1827 - val_acc: 0.6776\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8473\n",
      "Epoch 00024: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4671 - acc: 0.8472 - val_loss: 1.1762 - val_acc: 0.6839\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8574\n",
      "Epoch 00025: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4364 - acc: 0.8575 - val_loss: 1.1880 - val_acc: 0.6911\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8596\n",
      "Epoch 00026: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4285 - acc: 0.8596 - val_loss: 1.1806 - val_acc: 0.6848\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8637\n",
      "Epoch 00027: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4118 - acc: 0.8637 - val_loss: 1.2595 - val_acc: 0.6860\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8648\n",
      "Epoch 00028: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4005 - acc: 0.8648 - val_loss: 1.2532 - val_acc: 0.6820\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8699\n",
      "Epoch 00029: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3867 - acc: 0.8700 - val_loss: 1.2769 - val_acc: 0.6872\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8769\n",
      "Epoch 00030: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3725 - acc: 0.8769 - val_loss: 1.2360 - val_acc: 0.6876\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8771\n",
      "Epoch 00031: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3666 - acc: 0.8771 - val_loss: 1.2348 - val_acc: 0.6928\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8818\n",
      "Epoch 00032: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3548 - acc: 0.8819 - val_loss: 1.2658 - val_acc: 0.6869\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8863\n",
      "Epoch 00033: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3429 - acc: 0.8863 - val_loss: 1.2748 - val_acc: 0.6902\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8869\n",
      "Epoch 00034: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3387 - acc: 0.8869 - val_loss: 1.2477 - val_acc: 0.6928\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8914\n",
      "Epoch 00035: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3271 - acc: 0.8915 - val_loss: 1.2991 - val_acc: 0.6860\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.8947\n",
      "Epoch 00036: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3150 - acc: 0.8947 - val_loss: 1.2598 - val_acc: 0.6981\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.8928\n",
      "Epoch 00037: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3160 - acc: 0.8927 - val_loss: 1.2453 - val_acc: 0.6990\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8973\n",
      "Epoch 00038: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3071 - acc: 0.8973 - val_loss: 1.3184 - val_acc: 0.6869\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9014\n",
      "Epoch 00039: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2960 - acc: 0.9014 - val_loss: 1.2976 - val_acc: 0.6958\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9033\n",
      "Epoch 00040: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2867 - acc: 0.9033 - val_loss: 1.2958 - val_acc: 0.7014\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9057\n",
      "Epoch 00041: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2819 - acc: 0.9057 - val_loss: 1.3206 - val_acc: 0.6946\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9074\n",
      "Epoch 00042: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2752 - acc: 0.9075 - val_loss: 1.2872 - val_acc: 0.7056\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9098\n",
      "Epoch 00043: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2651 - acc: 0.9098 - val_loss: 1.3424 - val_acc: 0.6972\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9079\n",
      "Epoch 00044: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2752 - acc: 0.9079 - val_loss: 1.3029 - val_acc: 0.7042\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9084\n",
      "Epoch 00045: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2730 - acc: 0.9084 - val_loss: 1.3006 - val_acc: 0.7044\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9125\n",
      "Epoch 00046: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2643 - acc: 0.9125 - val_loss: 1.2982 - val_acc: 0.7072\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9158\n",
      "Epoch 00047: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2522 - acc: 0.9158 - val_loss: 1.3466 - val_acc: 0.7072\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9137\n",
      "Epoch 00048: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2574 - acc: 0.9137 - val_loss: 1.2974 - val_acc: 0.7095\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9154\n",
      "Epoch 00049: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2485 - acc: 0.9154 - val_loss: 1.3336 - val_acc: 0.7100\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9171\n",
      "Epoch 00050: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2468 - acc: 0.9170 - val_loss: 1.3433 - val_acc: 0.7135\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9208\n",
      "Epoch 00051: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2380 - acc: 0.9209 - val_loss: 1.3305 - val_acc: 0.7109\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9184\n",
      "Epoch 00052: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2398 - acc: 0.9184 - val_loss: 1.3736 - val_acc: 0.7081\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9200\n",
      "Epoch 00053: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2338 - acc: 0.9200 - val_loss: 1.3432 - val_acc: 0.7091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9215\n",
      "Epoch 00054: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2340 - acc: 0.9215 - val_loss: 1.3580 - val_acc: 0.7100\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9243\n",
      "Epoch 00055: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2257 - acc: 0.9243 - val_loss: 1.3427 - val_acc: 0.7000\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9260\n",
      "Epoch 00056: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2229 - acc: 0.9260 - val_loss: 1.3157 - val_acc: 0.7049\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9255\n",
      "Epoch 00057: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2239 - acc: 0.9255 - val_loss: 1.3686 - val_acc: 0.7060\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9273\n",
      "Epoch 00058: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2196 - acc: 0.9273 - val_loss: 1.3781 - val_acc: 0.7028\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9267\n",
      "Epoch 00059: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2178 - acc: 0.9267 - val_loss: 1.3398 - val_acc: 0.7098\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9287\n",
      "Epoch 00060: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2166 - acc: 0.9287 - val_loss: 1.3497 - val_acc: 0.7121\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9267\n",
      "Epoch 00061: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2196 - acc: 0.9267 - val_loss: 1.3331 - val_acc: 0.7100\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9290\n",
      "Epoch 00062: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2097 - acc: 0.9291 - val_loss: 1.3712 - val_acc: 0.7072\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9314\n",
      "Epoch 00063: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2080 - acc: 0.9314 - val_loss: 1.3691 - val_acc: 0.7116\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9294\n",
      "Epoch 00064: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2091 - acc: 0.9294 - val_loss: 1.3516 - val_acc: 0.7128\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9294\n",
      "Epoch 00065: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2061 - acc: 0.9294 - val_loss: 1.3757 - val_acc: 0.7212\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9338\n",
      "Epoch 00066: val_loss did not improve from 1.09735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1998 - acc: 0.9338 - val_loss: 1.3529 - val_acc: 0.7156\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 489us/sample - loss: 1.1753 - acc: 0.6438\n",
      "Loss: 1.1752867400212947 Accuracy: 0.6438214\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4382 - acc: 0.2060\n",
      "Epoch 00001: val_loss improved from inf to 1.80592, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/001-1.8059.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.4381 - acc: 0.2061 - val_loss: 1.8059 - val_acc: 0.4323\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6761 - acc: 0.4586\n",
      "Epoch 00002: val_loss improved from 1.80592 to 1.59064, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/002-1.5906.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.6762 - acc: 0.4586 - val_loss: 1.5906 - val_acc: 0.4868\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4859 - acc: 0.5182\n",
      "Epoch 00003: val_loss improved from 1.59064 to 1.43636, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/003-1.4364.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4859 - acc: 0.5182 - val_loss: 1.4364 - val_acc: 0.5390\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3847 - acc: 0.5561\n",
      "Epoch 00004: val_loss improved from 1.43636 to 1.27952, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/004-1.2795.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.3847 - acc: 0.5561 - val_loss: 1.2795 - val_acc: 0.5966\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3037 - acc: 0.5890\n",
      "Epoch 00005: val_loss improved from 1.27952 to 1.20928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/005-1.2093.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.3036 - acc: 0.5891 - val_loss: 1.2093 - val_acc: 0.6273\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2163 - acc: 0.6219\n",
      "Epoch 00006: val_loss did not improve from 1.20928\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.2163 - acc: 0.6218 - val_loss: 1.2579 - val_acc: 0.6231\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1500 - acc: 0.6425\n",
      "Epoch 00007: val_loss improved from 1.20928 to 1.09426, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/007-1.0943.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1499 - acc: 0.6425 - val_loss: 1.0943 - val_acc: 0.6611\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0811 - acc: 0.6671\n",
      "Epoch 00008: val_loss improved from 1.09426 to 1.01595, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/008-1.0160.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0811 - acc: 0.6671 - val_loss: 1.0160 - val_acc: 0.6962\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0166 - acc: 0.6887\n",
      "Epoch 00009: val_loss improved from 1.01595 to 1.00484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/009-1.0048.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0167 - acc: 0.6887 - val_loss: 1.0048 - val_acc: 0.6946\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9629 - acc: 0.7053\n",
      "Epoch 00010: val_loss improved from 1.00484 to 0.98977, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/010-0.9898.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.9629 - acc: 0.7053 - val_loss: 0.9898 - val_acc: 0.6925\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9092 - acc: 0.7221\n",
      "Epoch 00011: val_loss improved from 0.98977 to 0.94495, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/011-0.9449.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.9091 - acc: 0.7221 - val_loss: 0.9449 - val_acc: 0.7251\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8567 - acc: 0.7385\n",
      "Epoch 00012: val_loss improved from 0.94495 to 0.91596, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/012-0.9160.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8568 - acc: 0.7384 - val_loss: 0.9160 - val_acc: 0.7251\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.7491\n",
      "Epoch 00013: val_loss improved from 0.91596 to 0.90707, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/013-0.9071.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8159 - acc: 0.7491 - val_loss: 0.9071 - val_acc: 0.7389\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7745 - acc: 0.7638\n",
      "Epoch 00014: val_loss improved from 0.90707 to 0.84875, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/014-0.8487.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7744 - acc: 0.7638 - val_loss: 0.8487 - val_acc: 0.7517\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7344 - acc: 0.7748\n",
      "Epoch 00015: val_loss improved from 0.84875 to 0.82348, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/015-0.8235.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7343 - acc: 0.7748 - val_loss: 0.8235 - val_acc: 0.7533\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7098 - acc: 0.7826\n",
      "Epoch 00016: val_loss improved from 0.82348 to 0.80367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/016-0.8037.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7097 - acc: 0.7826 - val_loss: 0.8037 - val_acc: 0.7689\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.7949\n",
      "Epoch 00017: val_loss improved from 0.80367 to 0.77630, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/017-0.7763.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6737 - acc: 0.7950 - val_loss: 0.7763 - val_acc: 0.7752\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.8014\n",
      "Epoch 00018: val_loss improved from 0.77630 to 0.77452, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/018-0.7745.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6507 - acc: 0.8014 - val_loss: 0.7745 - val_acc: 0.7782\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6180 - acc: 0.8096\n",
      "Epoch 00019: val_loss improved from 0.77452 to 0.77160, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/019-0.7716.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6182 - acc: 0.8095 - val_loss: 0.7716 - val_acc: 0.7773\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.8189\n",
      "Epoch 00020: val_loss did not improve from 0.77160\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5929 - acc: 0.8189 - val_loss: 0.7729 - val_acc: 0.7715\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8255\n",
      "Epoch 00021: val_loss improved from 0.77160 to 0.74959, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/021-0.7496.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5683 - acc: 0.8255 - val_loss: 0.7496 - val_acc: 0.7845\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.8302\n",
      "Epoch 00022: val_loss did not improve from 0.74959\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5493 - acc: 0.8302 - val_loss: 0.7834 - val_acc: 0.7792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8392\n",
      "Epoch 00023: val_loss did not improve from 0.74959\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5243 - acc: 0.8392 - val_loss: 0.7575 - val_acc: 0.7866\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8456\n",
      "Epoch 00024: val_loss did not improve from 0.74959\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5022 - acc: 0.8456 - val_loss: 0.7849 - val_acc: 0.7834\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8518\n",
      "Epoch 00025: val_loss did not improve from 0.74959\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4796 - acc: 0.8519 - val_loss: 0.7521 - val_acc: 0.7906\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8529\n",
      "Epoch 00026: val_loss improved from 0.74959 to 0.73960, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/026-0.7396.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4695 - acc: 0.8529 - val_loss: 0.7396 - val_acc: 0.7948\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8617\n",
      "Epoch 00027: val_loss did not improve from 0.73960\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4478 - acc: 0.8617 - val_loss: 0.7679 - val_acc: 0.7887\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8624\n",
      "Epoch 00028: val_loss did not improve from 0.73960\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4346 - acc: 0.8624 - val_loss: 0.8341 - val_acc: 0.7708\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8669\n",
      "Epoch 00029: val_loss improved from 0.73960 to 0.73650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv_checkpoint/029-0.7365.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4212 - acc: 0.8669 - val_loss: 0.7365 - val_acc: 0.7962\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8718\n",
      "Epoch 00030: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4089 - acc: 0.8718 - val_loss: 0.7653 - val_acc: 0.7964\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8786\n",
      "Epoch 00031: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3899 - acc: 0.8787 - val_loss: 0.7565 - val_acc: 0.8018\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8792\n",
      "Epoch 00032: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3811 - acc: 0.8792 - val_loss: 0.7481 - val_acc: 0.8013\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8825\n",
      "Epoch 00033: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3673 - acc: 0.8825 - val_loss: 0.8667 - val_acc: 0.7801\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8861\n",
      "Epoch 00034: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3596 - acc: 0.8861 - val_loss: 0.7417 - val_acc: 0.8029\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8902\n",
      "Epoch 00035: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3429 - acc: 0.8902 - val_loss: 0.8058 - val_acc: 0.7966\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8935\n",
      "Epoch 00036: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3359 - acc: 0.8935 - val_loss: 0.7686 - val_acc: 0.8032\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8962\n",
      "Epoch 00037: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3256 - acc: 0.8962 - val_loss: 0.7669 - val_acc: 0.8076\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8972\n",
      "Epoch 00038: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3219 - acc: 0.8972 - val_loss: 0.7729 - val_acc: 0.8085\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9018\n",
      "Epoch 00039: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3064 - acc: 0.9018 - val_loss: 0.7701 - val_acc: 0.8085\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.9020\n",
      "Epoch 00040: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3003 - acc: 0.9021 - val_loss: 0.7620 - val_acc: 0.8102\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9057\n",
      "Epoch 00041: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2932 - acc: 0.9057 - val_loss: 0.7580 - val_acc: 0.8109\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9086\n",
      "Epoch 00042: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2833 - acc: 0.9085 - val_loss: 0.7887 - val_acc: 0.7999\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9068\n",
      "Epoch 00043: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2826 - acc: 0.9069 - val_loss: 0.7878 - val_acc: 0.8078\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9092\n",
      "Epoch 00044: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2753 - acc: 0.9092 - val_loss: 0.7951 - val_acc: 0.8111\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9105\n",
      "Epoch 00045: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2714 - acc: 0.9106 - val_loss: 0.7755 - val_acc: 0.8139\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9167\n",
      "Epoch 00046: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2558 - acc: 0.9168 - val_loss: 0.8019 - val_acc: 0.8057\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9163\n",
      "Epoch 00047: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2561 - acc: 0.9163 - val_loss: 0.8195 - val_acc: 0.8174\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9189\n",
      "Epoch 00048: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2515 - acc: 0.9189 - val_loss: 0.7905 - val_acc: 0.8139\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9178\n",
      "Epoch 00049: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2489 - acc: 0.9178 - val_loss: 0.8178 - val_acc: 0.7992\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9157\n",
      "Epoch 00050: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2548 - acc: 0.9157 - val_loss: 0.7902 - val_acc: 0.8176\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9211\n",
      "Epoch 00051: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2372 - acc: 0.9211 - val_loss: 0.7817 - val_acc: 0.8148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9239\n",
      "Epoch 00052: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2308 - acc: 0.9239 - val_loss: 0.8012 - val_acc: 0.8178\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9243\n",
      "Epoch 00053: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2306 - acc: 0.9243 - val_loss: 0.8405 - val_acc: 0.8111\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9259\n",
      "Epoch 00054: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2298 - acc: 0.9259 - val_loss: 0.8396 - val_acc: 0.8125\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9285\n",
      "Epoch 00055: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2193 - acc: 0.9285 - val_loss: 0.8220 - val_acc: 0.8109\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9296\n",
      "Epoch 00056: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2193 - acc: 0.9297 - val_loss: 0.8211 - val_acc: 0.8223\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9308\n",
      "Epoch 00057: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2137 - acc: 0.9308 - val_loss: 0.8202 - val_acc: 0.8211\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9276\n",
      "Epoch 00058: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2187 - acc: 0.9276 - val_loss: 0.8069 - val_acc: 0.8139\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9331\n",
      "Epoch 00059: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2085 - acc: 0.9330 - val_loss: 0.7932 - val_acc: 0.8155\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9301\n",
      "Epoch 00060: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2129 - acc: 0.9301 - val_loss: 0.8314 - val_acc: 0.8213\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9351\n",
      "Epoch 00061: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1987 - acc: 0.9351 - val_loss: 0.8033 - val_acc: 0.8241\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9364\n",
      "Epoch 00062: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1932 - acc: 0.9364 - val_loss: 0.8214 - val_acc: 0.8192\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9377\n",
      "Epoch 00063: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1942 - acc: 0.9377 - val_loss: 0.8142 - val_acc: 0.8211\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9352\n",
      "Epoch 00064: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2001 - acc: 0.9352 - val_loss: 0.8047 - val_acc: 0.8253\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9376\n",
      "Epoch 00065: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1917 - acc: 0.9376 - val_loss: 0.8451 - val_acc: 0.8197\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9365\n",
      "Epoch 00066: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1958 - acc: 0.9365 - val_loss: 0.8448 - val_acc: 0.8220\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9393\n",
      "Epoch 00067: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1872 - acc: 0.9393 - val_loss: 0.8451 - val_acc: 0.8234\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9396\n",
      "Epoch 00068: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1874 - acc: 0.9396 - val_loss: 0.8193 - val_acc: 0.8244\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9425\n",
      "Epoch 00069: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1796 - acc: 0.9425 - val_loss: 0.8268 - val_acc: 0.8260\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9410\n",
      "Epoch 00070: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1813 - acc: 0.9410 - val_loss: 0.8528 - val_acc: 0.8220\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9430\n",
      "Epoch 00071: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1802 - acc: 0.9429 - val_loss: 0.8065 - val_acc: 0.8227\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9435\n",
      "Epoch 00072: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1736 - acc: 0.9435 - val_loss: 0.8536 - val_acc: 0.8155\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9428\n",
      "Epoch 00073: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1785 - acc: 0.9428 - val_loss: 0.8569 - val_acc: 0.8211\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9428\n",
      "Epoch 00074: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1790 - acc: 0.9428 - val_loss: 0.8651 - val_acc: 0.8216\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9450\n",
      "Epoch 00075: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1720 - acc: 0.9450 - val_loss: 0.8231 - val_acc: 0.8293\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9424\n",
      "Epoch 00076: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1763 - acc: 0.9424 - val_loss: 0.8475 - val_acc: 0.8258\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9474\n",
      "Epoch 00077: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1635 - acc: 0.9474 - val_loss: 0.8579 - val_acc: 0.8183\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9469\n",
      "Epoch 00078: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1625 - acc: 0.9469 - val_loss: 0.8462 - val_acc: 0.8269\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9476\n",
      "Epoch 00079: val_loss did not improve from 0.73650\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1612 - acc: 0.9476 - val_loss: 0.8606 - val_acc: 0.8330\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 501us/sample - loss: 0.8568 - acc: 0.7601\n",
      "Loss: 0.8567534625963507 Accuracy: 0.7601246\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5785 - acc: 0.1427\n",
      "Epoch 00001: val_loss improved from inf to 2.07253, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/001-2.0725.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.5785 - acc: 0.1426 - val_loss: 2.0725 - val_acc: 0.3303\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8758 - acc: 0.3762\n",
      "Epoch 00002: val_loss improved from 2.07253 to 1.53205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/002-1.5321.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.8758 - acc: 0.3762 - val_loss: 1.5321 - val_acc: 0.5111\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5603 - acc: 0.4878\n",
      "Epoch 00003: val_loss improved from 1.53205 to 1.35360, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/003-1.3536.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.5603 - acc: 0.4878 - val_loss: 1.3536 - val_acc: 0.5877\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4201 - acc: 0.5384\n",
      "Epoch 00004: val_loss improved from 1.35360 to 1.24693, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/004-1.2469.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.4199 - acc: 0.5384 - val_loss: 1.2469 - val_acc: 0.6138\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3408 - acc: 0.5677\n",
      "Epoch 00005: val_loss improved from 1.24693 to 1.20315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/005-1.2031.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3407 - acc: 0.5677 - val_loss: 1.2031 - val_acc: 0.6254\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2775 - acc: 0.5875\n",
      "Epoch 00006: val_loss improved from 1.20315 to 1.15313, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/006-1.1531.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2775 - acc: 0.5875 - val_loss: 1.1531 - val_acc: 0.6403\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2261 - acc: 0.6070\n",
      "Epoch 00007: val_loss improved from 1.15313 to 1.10483, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/007-1.1048.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2260 - acc: 0.6071 - val_loss: 1.1048 - val_acc: 0.6655\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6236\n",
      "Epoch 00008: val_loss did not improve from 1.10483\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1741 - acc: 0.6236 - val_loss: 1.2198 - val_acc: 0.6040\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1204 - acc: 0.6461\n",
      "Epoch 00009: val_loss improved from 1.10483 to 1.06416, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/009-1.0642.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1204 - acc: 0.6461 - val_loss: 1.0642 - val_acc: 0.6650\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6653\n",
      "Epoch 00010: val_loss improved from 1.06416 to 0.99007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/010-0.9901.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0699 - acc: 0.6653 - val_loss: 0.9901 - val_acc: 0.6965\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0231 - acc: 0.6764\n",
      "Epoch 00011: val_loss improved from 0.99007 to 0.93385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/011-0.9339.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0231 - acc: 0.6765 - val_loss: 0.9339 - val_acc: 0.7112\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9556 - acc: 0.7010\n",
      "Epoch 00012: val_loss improved from 0.93385 to 0.83864, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/012-0.8386.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9556 - acc: 0.7010 - val_loss: 0.8386 - val_acc: 0.7519\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7176\n",
      "Epoch 00013: val_loss improved from 0.83864 to 0.81953, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/013-0.8195.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9074 - acc: 0.7176 - val_loss: 0.8195 - val_acc: 0.7540\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8540 - acc: 0.7371\n",
      "Epoch 00014: val_loss improved from 0.81953 to 0.74751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/014-0.7475.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8541 - acc: 0.7371 - val_loss: 0.7475 - val_acc: 0.7743\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8016 - acc: 0.7515\n",
      "Epoch 00015: val_loss improved from 0.74751 to 0.73405, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/015-0.7341.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8016 - acc: 0.7516 - val_loss: 0.7341 - val_acc: 0.7850\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7645 - acc: 0.7660\n",
      "Epoch 00016: val_loss improved from 0.73405 to 0.72167, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/016-0.7217.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7644 - acc: 0.7661 - val_loss: 0.7217 - val_acc: 0.7775\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7249 - acc: 0.7797\n",
      "Epoch 00017: val_loss improved from 0.72167 to 0.64575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/017-0.6458.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7249 - acc: 0.7797 - val_loss: 0.6458 - val_acc: 0.8062\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6850 - acc: 0.7892\n",
      "Epoch 00018: val_loss improved from 0.64575 to 0.61111, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/018-0.6111.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6850 - acc: 0.7892 - val_loss: 0.6111 - val_acc: 0.8241\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.8012\n",
      "Epoch 00019: val_loss did not improve from 0.61111\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6538 - acc: 0.8012 - val_loss: 0.6150 - val_acc: 0.8127\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8080\n",
      "Epoch 00020: val_loss improved from 0.61111 to 0.58156, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/020-0.5816.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6243 - acc: 0.8080 - val_loss: 0.5816 - val_acc: 0.8307\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.8186\n",
      "Epoch 00021: val_loss improved from 0.58156 to 0.53902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/021-0.5390.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5995 - acc: 0.8186 - val_loss: 0.5390 - val_acc: 0.8446\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8264\n",
      "Epoch 00022: val_loss improved from 0.53902 to 0.53383, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/022-0.5338.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5755 - acc: 0.8264 - val_loss: 0.5338 - val_acc: 0.8414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8303\n",
      "Epoch 00023: val_loss improved from 0.53383 to 0.53025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/023-0.5302.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5550 - acc: 0.8303 - val_loss: 0.5302 - val_acc: 0.8467\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8390\n",
      "Epoch 00024: val_loss improved from 0.53025 to 0.49122, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/024-0.4912.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5314 - acc: 0.8391 - val_loss: 0.4912 - val_acc: 0.8581\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8444\n",
      "Epoch 00025: val_loss did not improve from 0.49122\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5136 - acc: 0.8444 - val_loss: 0.5037 - val_acc: 0.8553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8508\n",
      "Epoch 00026: val_loss improved from 0.49122 to 0.47481, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/026-0.4748.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4891 - acc: 0.8509 - val_loss: 0.4748 - val_acc: 0.8619\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8543\n",
      "Epoch 00027: val_loss improved from 0.47481 to 0.45955, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/027-0.4595.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4783 - acc: 0.8543 - val_loss: 0.4595 - val_acc: 0.8698\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.8587\n",
      "Epoch 00028: val_loss improved from 0.45955 to 0.44714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/028-0.4471.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4589 - acc: 0.8588 - val_loss: 0.4471 - val_acc: 0.8682\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8645\n",
      "Epoch 00029: val_loss did not improve from 0.44714\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4431 - acc: 0.8645 - val_loss: 0.4619 - val_acc: 0.8686\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8686\n",
      "Epoch 00030: val_loss improved from 0.44714 to 0.43184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/030-0.4318.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4322 - acc: 0.8686 - val_loss: 0.4318 - val_acc: 0.8786\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8736\n",
      "Epoch 00031: val_loss did not improve from 0.43184\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4182 - acc: 0.8736 - val_loss: 0.4392 - val_acc: 0.8789\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8754\n",
      "Epoch 00032: val_loss did not improve from 0.43184\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4126 - acc: 0.8753 - val_loss: 0.4608 - val_acc: 0.8679\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8799\n",
      "Epoch 00033: val_loss improved from 0.43184 to 0.41175, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/033-0.4117.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3956 - acc: 0.8799 - val_loss: 0.4117 - val_acc: 0.8877\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8809\n",
      "Epoch 00034: val_loss did not improve from 0.41175\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3883 - acc: 0.8809 - val_loss: 0.4167 - val_acc: 0.8854\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8885\n",
      "Epoch 00035: val_loss did not improve from 0.41175\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3657 - acc: 0.8885 - val_loss: 0.4200 - val_acc: 0.8903\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8889\n",
      "Epoch 00036: val_loss did not improve from 0.41175\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3630 - acc: 0.8889 - val_loss: 0.4139 - val_acc: 0.8819\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8906\n",
      "Epoch 00037: val_loss did not improve from 0.41175\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3593 - acc: 0.8906 - val_loss: 0.4250 - val_acc: 0.8810\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8940\n",
      "Epoch 00038: val_loss improved from 0.41175 to 0.40691, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/038-0.4069.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3459 - acc: 0.8941 - val_loss: 0.4069 - val_acc: 0.8945\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8954\n",
      "Epoch 00039: val_loss did not improve from 0.40691\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3414 - acc: 0.8953 - val_loss: 0.4187 - val_acc: 0.8840\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8977\n",
      "Epoch 00040: val_loss did not improve from 0.40691\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3319 - acc: 0.8977 - val_loss: 0.4131 - val_acc: 0.8863\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8962\n",
      "Epoch 00041: val_loss improved from 0.40691 to 0.39590, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/041-0.3959.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3295 - acc: 0.8962 - val_loss: 0.3959 - val_acc: 0.8935\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9029\n",
      "Epoch 00042: val_loss did not improve from 0.39590\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3125 - acc: 0.9029 - val_loss: 0.4350 - val_acc: 0.8859\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9037\n",
      "Epoch 00043: val_loss improved from 0.39590 to 0.38673, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/043-0.3867.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3092 - acc: 0.9037 - val_loss: 0.3867 - val_acc: 0.8977\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9049\n",
      "Epoch 00044: val_loss improved from 0.38673 to 0.38160, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/044-0.3816.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3096 - acc: 0.9049 - val_loss: 0.3816 - val_acc: 0.8989\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9072\n",
      "Epoch 00045: val_loss did not improve from 0.38160\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2988 - acc: 0.9072 - val_loss: 0.4259 - val_acc: 0.8882\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9095\n",
      "Epoch 00046: val_loss did not improve from 0.38160\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2896 - acc: 0.9095 - val_loss: 0.3934 - val_acc: 0.8926\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9104\n",
      "Epoch 00047: val_loss did not improve from 0.38160\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2877 - acc: 0.9103 - val_loss: 0.3939 - val_acc: 0.8970\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9118\n",
      "Epoch 00048: val_loss did not improve from 0.38160\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2800 - acc: 0.9118 - val_loss: 0.3850 - val_acc: 0.9008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9136\n",
      "Epoch 00049: val_loss improved from 0.38160 to 0.36956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/049-0.3696.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2725 - acc: 0.9137 - val_loss: 0.3696 - val_acc: 0.9075\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9156\n",
      "Epoch 00050: val_loss did not improve from 0.36956\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2727 - acc: 0.9156 - val_loss: 0.3763 - val_acc: 0.9075\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9156\n",
      "Epoch 00051: val_loss improved from 0.36956 to 0.36187, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/051-0.3619.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2651 - acc: 0.9156 - val_loss: 0.3619 - val_acc: 0.9068\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9158\n",
      "Epoch 00052: val_loss did not improve from 0.36187\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2656 - acc: 0.9158 - val_loss: 0.3644 - val_acc: 0.9071\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9196\n",
      "Epoch 00053: val_loss did not improve from 0.36187\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2562 - acc: 0.9197 - val_loss: 0.3914 - val_acc: 0.9003\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9192\n",
      "Epoch 00054: val_loss did not improve from 0.36187\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2556 - acc: 0.9192 - val_loss: 0.3744 - val_acc: 0.9068\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9209\n",
      "Epoch 00055: val_loss improved from 0.36187 to 0.35945, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/055-0.3594.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2511 - acc: 0.9209 - val_loss: 0.3594 - val_acc: 0.9094\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9237\n",
      "Epoch 00056: val_loss did not improve from 0.35945\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2427 - acc: 0.9238 - val_loss: 0.3597 - val_acc: 0.9092\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9244\n",
      "Epoch 00057: val_loss did not improve from 0.35945\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2350 - acc: 0.9244 - val_loss: 0.3605 - val_acc: 0.9073\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9236\n",
      "Epoch 00058: val_loss did not improve from 0.35945\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2422 - acc: 0.9236 - val_loss: 0.3900 - val_acc: 0.9033\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9250\n",
      "Epoch 00059: val_loss did not improve from 0.35945\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2338 - acc: 0.9250 - val_loss: 0.3718 - val_acc: 0.9078\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9255\n",
      "Epoch 00060: val_loss improved from 0.35945 to 0.35135, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/060-0.3513.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2315 - acc: 0.9255 - val_loss: 0.3513 - val_acc: 0.9161\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9256\n",
      "Epoch 00061: val_loss did not improve from 0.35135\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2301 - acc: 0.9256 - val_loss: 0.3543 - val_acc: 0.9122\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9279\n",
      "Epoch 00062: val_loss did not improve from 0.35135\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2255 - acc: 0.9279 - val_loss: 0.3573 - val_acc: 0.9045\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9285\n",
      "Epoch 00063: val_loss did not improve from 0.35135\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2210 - acc: 0.9285 - val_loss: 0.3537 - val_acc: 0.9168\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9296\n",
      "Epoch 00064: val_loss did not improve from 0.35135\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2196 - acc: 0.9297 - val_loss: 0.3754 - val_acc: 0.9080\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9309\n",
      "Epoch 00065: val_loss improved from 0.35135 to 0.35006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/065-0.3501.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2109 - acc: 0.9309 - val_loss: 0.3501 - val_acc: 0.9168\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9316\n",
      "Epoch 00066: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2114 - acc: 0.9316 - val_loss: 0.3818 - val_acc: 0.9075\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9329\n",
      "Epoch 00067: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2081 - acc: 0.9329 - val_loss: 0.3606 - val_acc: 0.9071\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9352\n",
      "Epoch 00068: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1998 - acc: 0.9352 - val_loss: 0.3588 - val_acc: 0.9119\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9344\n",
      "Epoch 00069: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2004 - acc: 0.9344 - val_loss: 0.3684 - val_acc: 0.9192\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9361\n",
      "Epoch 00070: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2026 - acc: 0.9361 - val_loss: 0.3632 - val_acc: 0.9124\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9371\n",
      "Epoch 00071: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1949 - acc: 0.9370 - val_loss: 0.3932 - val_acc: 0.9085\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9351\n",
      "Epoch 00072: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1960 - acc: 0.9351 - val_loss: 0.3640 - val_acc: 0.9157\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9379\n",
      "Epoch 00073: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1930 - acc: 0.9379 - val_loss: 0.3671 - val_acc: 0.9106\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9383\n",
      "Epoch 00074: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1885 - acc: 0.9384 - val_loss: 0.3514 - val_acc: 0.9187\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9389\n",
      "Epoch 00075: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1867 - acc: 0.9389 - val_loss: 0.3640 - val_acc: 0.9199\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9397\n",
      "Epoch 00076: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1874 - acc: 0.9397 - val_loss: 0.3661 - val_acc: 0.9161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9411\n",
      "Epoch 00077: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1828 - acc: 0.9411 - val_loss: 0.3772 - val_acc: 0.9210\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9414\n",
      "Epoch 00078: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1799 - acc: 0.9414 - val_loss: 0.3701 - val_acc: 0.9178\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9425\n",
      "Epoch 00079: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1738 - acc: 0.9425 - val_loss: 0.3657 - val_acc: 0.9171\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9423\n",
      "Epoch 00080: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1739 - acc: 0.9423 - val_loss: 0.3632 - val_acc: 0.9178\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9440\n",
      "Epoch 00081: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1735 - acc: 0.9441 - val_loss: 0.3566 - val_acc: 0.9206\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9422\n",
      "Epoch 00082: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1754 - acc: 0.9422 - val_loss: 0.3713 - val_acc: 0.9108\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9448\n",
      "Epoch 00083: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1706 - acc: 0.9448 - val_loss: 0.3659 - val_acc: 0.9147\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9471\n",
      "Epoch 00084: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1638 - acc: 0.9471 - val_loss: 0.3925 - val_acc: 0.9092\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9448\n",
      "Epoch 00085: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1682 - acc: 0.9448 - val_loss: 0.3797 - val_acc: 0.9171\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9464\n",
      "Epoch 00086: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1647 - acc: 0.9464 - val_loss: 0.3812 - val_acc: 0.9173\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9465\n",
      "Epoch 00087: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1631 - acc: 0.9465 - val_loss: 0.3708 - val_acc: 0.9173\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9483\n",
      "Epoch 00088: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1606 - acc: 0.9483 - val_loss: 0.3552 - val_acc: 0.9168\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9468\n",
      "Epoch 00089: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1598 - acc: 0.9468 - val_loss: 0.3636 - val_acc: 0.9213\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9492\n",
      "Epoch 00090: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1535 - acc: 0.9491 - val_loss: 0.3592 - val_acc: 0.9220\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9474\n",
      "Epoch 00091: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1587 - acc: 0.9474 - val_loss: 0.3594 - val_acc: 0.9171\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9486\n",
      "Epoch 00092: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1574 - acc: 0.9486 - val_loss: 0.3746 - val_acc: 0.9166\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9505\n",
      "Epoch 00093: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1533 - acc: 0.9505 - val_loss: 0.3855 - val_acc: 0.9147\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9507\n",
      "Epoch 00094: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1515 - acc: 0.9507 - val_loss: 0.3563 - val_acc: 0.9166\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9506\n",
      "Epoch 00095: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1506 - acc: 0.9506 - val_loss: 0.4093 - val_acc: 0.9147\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9516\n",
      "Epoch 00096: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1479 - acc: 0.9516 - val_loss: 0.3652 - val_acc: 0.9208\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9530\n",
      "Epoch 00097: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1420 - acc: 0.9529 - val_loss: 0.4113 - val_acc: 0.9092\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9506\n",
      "Epoch 00098: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1513 - acc: 0.9506 - val_loss: 0.3698 - val_acc: 0.9180\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9512\n",
      "Epoch 00099: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1446 - acc: 0.9512 - val_loss: 0.3746 - val_acc: 0.9210\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9525\n",
      "Epoch 00100: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1455 - acc: 0.9525 - val_loss: 0.3573 - val_acc: 0.9252\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9523\n",
      "Epoch 00101: val_loss did not improve from 0.35006\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1438 - acc: 0.9523 - val_loss: 0.3858 - val_acc: 0.9152\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9538\n",
      "Epoch 00102: val_loss improved from 0.35006 to 0.34778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv_checkpoint/102-0.3478.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1411 - acc: 0.9538 - val_loss: 0.3478 - val_acc: 0.9262\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9547\n",
      "Epoch 00103: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1368 - acc: 0.9547 - val_loss: 0.3655 - val_acc: 0.9224\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9557\n",
      "Epoch 00104: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1353 - acc: 0.9556 - val_loss: 0.3625 - val_acc: 0.9194\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9515\n",
      "Epoch 00105: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1439 - acc: 0.9515 - val_loss: 0.3693 - val_acc: 0.9175\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9548\n",
      "Epoch 00106: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1342 - acc: 0.9548 - val_loss: 0.3583 - val_acc: 0.9264\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9538\n",
      "Epoch 00107: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1383 - acc: 0.9538 - val_loss: 0.3654 - val_acc: 0.9222\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9568\n",
      "Epoch 00108: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1338 - acc: 0.9569 - val_loss: 0.3994 - val_acc: 0.9196\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9542\n",
      "Epoch 00109: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1373 - acc: 0.9542 - val_loss: 0.3782 - val_acc: 0.9229\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9550\n",
      "Epoch 00110: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1329 - acc: 0.9550 - val_loss: 0.3714 - val_acc: 0.9234\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9570\n",
      "Epoch 00111: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1297 - acc: 0.9570 - val_loss: 0.3673 - val_acc: 0.9215\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9585\n",
      "Epoch 00112: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1261 - acc: 0.9585 - val_loss: 0.3723 - val_acc: 0.9208\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9567\n",
      "Epoch 00113: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1299 - acc: 0.9567 - val_loss: 0.3749 - val_acc: 0.9180\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9593\n",
      "Epoch 00114: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1258 - acc: 0.9593 - val_loss: 0.3831 - val_acc: 0.9203\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9588\n",
      "Epoch 00115: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1253 - acc: 0.9588 - val_loss: 0.3634 - val_acc: 0.9283\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9588\n",
      "Epoch 00116: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1254 - acc: 0.9588 - val_loss: 0.3614 - val_acc: 0.9238\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9593\n",
      "Epoch 00117: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1203 - acc: 0.9593 - val_loss: 0.3677 - val_acc: 0.9194\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9570\n",
      "Epoch 00118: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1295 - acc: 0.9570 - val_loss: 0.3641 - val_acc: 0.9252\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9597\n",
      "Epoch 00119: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1183 - acc: 0.9597 - val_loss: 0.3697 - val_acc: 0.9220\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9603\n",
      "Epoch 00120: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1191 - acc: 0.9603 - val_loss: 0.3693 - val_acc: 0.9220\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9609\n",
      "Epoch 00121: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1169 - acc: 0.9609 - val_loss: 0.3534 - val_acc: 0.9243\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9631\n",
      "Epoch 00122: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1164 - acc: 0.9631 - val_loss: 0.3843 - val_acc: 0.9222\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9593\n",
      "Epoch 00123: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1220 - acc: 0.9593 - val_loss: 0.3525 - val_acc: 0.9241\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9601\n",
      "Epoch 00124: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1206 - acc: 0.9601 - val_loss: 0.3817 - val_acc: 0.9227\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9638\n",
      "Epoch 00125: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1101 - acc: 0.9638 - val_loss: 0.3690 - val_acc: 0.9231\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9617\n",
      "Epoch 00126: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1145 - acc: 0.9617 - val_loss: 0.4116 - val_acc: 0.9087\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9622\n",
      "Epoch 00127: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1130 - acc: 0.9622 - val_loss: 0.3571 - val_acc: 0.9271\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9615\n",
      "Epoch 00128: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1172 - acc: 0.9616 - val_loss: 0.3629 - val_acc: 0.9262\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9608\n",
      "Epoch 00129: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1172 - acc: 0.9607 - val_loss: 0.3826 - val_acc: 0.9269\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9620\n",
      "Epoch 00130: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1120 - acc: 0.9620 - val_loss: 0.4061 - val_acc: 0.9203\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9613\n",
      "Epoch 00131: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1190 - acc: 0.9613 - val_loss: 0.3833 - val_acc: 0.9222\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9621\n",
      "Epoch 00132: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1125 - acc: 0.9621 - val_loss: 0.3820 - val_acc: 0.9224\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9650\n",
      "Epoch 00133: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1077 - acc: 0.9650 - val_loss: 0.3880 - val_acc: 0.9243\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9629\n",
      "Epoch 00134: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1135 - acc: 0.9629 - val_loss: 0.3820 - val_acc: 0.9217\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9651\n",
      "Epoch 00135: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1076 - acc: 0.9651 - val_loss: 0.3911 - val_acc: 0.9185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9640\n",
      "Epoch 00136: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1077 - acc: 0.9640 - val_loss: 0.3877 - val_acc: 0.9199\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9638\n",
      "Epoch 00137: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1074 - acc: 0.9638 - val_loss: 0.3822 - val_acc: 0.9241\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9624\n",
      "Epoch 00138: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1159 - acc: 0.9624 - val_loss: 0.3585 - val_acc: 0.9257\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9660\n",
      "Epoch 00139: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1028 - acc: 0.9660 - val_loss: 0.3662 - val_acc: 0.9280\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9650\n",
      "Epoch 00140: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1089 - acc: 0.9650 - val_loss: 0.3643 - val_acc: 0.9269\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9646\n",
      "Epoch 00141: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1061 - acc: 0.9646 - val_loss: 0.3691 - val_acc: 0.9271\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9657\n",
      "Epoch 00142: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1050 - acc: 0.9657 - val_loss: 0.3501 - val_acc: 0.9248\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9660\n",
      "Epoch 00143: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1042 - acc: 0.9660 - val_loss: 0.3938 - val_acc: 0.9241\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9653\n",
      "Epoch 00144: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1032 - acc: 0.9653 - val_loss: 0.3683 - val_acc: 0.9273\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9665\n",
      "Epoch 00145: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1002 - acc: 0.9664 - val_loss: 0.3775 - val_acc: 0.9276\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9646\n",
      "Epoch 00146: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1032 - acc: 0.9646 - val_loss: 0.3793 - val_acc: 0.9243\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9658\n",
      "Epoch 00147: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1017 - acc: 0.9658 - val_loss: 0.3807 - val_acc: 0.9311\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9665\n",
      "Epoch 00148: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1024 - acc: 0.9665 - val_loss: 0.3953 - val_acc: 0.9224\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9661\n",
      "Epoch 00149: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0997 - acc: 0.9661 - val_loss: 0.3676 - val_acc: 0.9273\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9671\n",
      "Epoch 00150: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0991 - acc: 0.9672 - val_loss: 0.3696 - val_acc: 0.9201\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9681\n",
      "Epoch 00151: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0972 - acc: 0.9681 - val_loss: 0.3937 - val_acc: 0.9248\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9675\n",
      "Epoch 00152: val_loss did not improve from 0.34778\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0959 - acc: 0.9675 - val_loss: 0.3820 - val_acc: 0.9317\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 543us/sample - loss: 0.4141 - acc: 0.8910\n",
      "Loss: 0.4140792941131077 Accuracy: 0.89096576\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6965 - acc: 0.0925\n",
      "Epoch 00001: val_loss improved from inf to 2.51387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/001-2.5139.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.6964 - acc: 0.0925 - val_loss: 2.5139 - val_acc: 0.1794\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1497 - acc: 0.2882\n",
      "Epoch 00002: val_loss improved from 2.51387 to 1.55297, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/002-1.5530.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.1495 - acc: 0.2882 - val_loss: 1.5530 - val_acc: 0.4899\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6054 - acc: 0.4607\n",
      "Epoch 00003: val_loss improved from 1.55297 to 1.35062, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/003-1.3506.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.6054 - acc: 0.4607 - val_loss: 1.3506 - val_acc: 0.5500\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4419 - acc: 0.5167\n",
      "Epoch 00004: val_loss improved from 1.35062 to 1.22400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/004-1.2240.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4417 - acc: 0.5167 - val_loss: 1.2240 - val_acc: 0.5982\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3329 - acc: 0.5568\n",
      "Epoch 00005: val_loss improved from 1.22400 to 1.12760, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/005-1.1276.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3329 - acc: 0.5569 - val_loss: 1.1276 - val_acc: 0.6364\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2473 - acc: 0.5882\n",
      "Epoch 00006: val_loss improved from 1.12760 to 1.02528, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/006-1.0253.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2473 - acc: 0.5881 - val_loss: 1.0253 - val_acc: 0.6795\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1627 - acc: 0.6196\n",
      "Epoch 00007: val_loss improved from 1.02528 to 0.99139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/007-0.9914.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1628 - acc: 0.6196 - val_loss: 0.9914 - val_acc: 0.6972\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6485\n",
      "Epoch 00008: val_loss improved from 0.99139 to 0.98584, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/008-0.9858.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0949 - acc: 0.6484 - val_loss: 0.9858 - val_acc: 0.6972\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0297 - acc: 0.6667\n",
      "Epoch 00009: val_loss improved from 0.98584 to 0.86620, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/009-0.8662.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0298 - acc: 0.6667 - val_loss: 0.8662 - val_acc: 0.7347\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9725 - acc: 0.6877\n",
      "Epoch 00010: val_loss improved from 0.86620 to 0.80148, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/010-0.8015.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9726 - acc: 0.6877 - val_loss: 0.8015 - val_acc: 0.7512\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9282 - acc: 0.7043\n",
      "Epoch 00011: val_loss improved from 0.80148 to 0.76025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/011-0.7602.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9281 - acc: 0.7043 - val_loss: 0.7602 - val_acc: 0.7680\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7229\n",
      "Epoch 00012: val_loss improved from 0.76025 to 0.72104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/012-0.7210.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8773 - acc: 0.7229 - val_loss: 0.7210 - val_acc: 0.7768\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7349\n",
      "Epoch 00013: val_loss improved from 0.72104 to 0.70655, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/013-0.7066.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8389 - acc: 0.7349 - val_loss: 0.7066 - val_acc: 0.7806\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8010 - acc: 0.7450\n",
      "Epoch 00014: val_loss did not improve from 0.70655\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8009 - acc: 0.7450 - val_loss: 0.7144 - val_acc: 0.7766\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.7572\n",
      "Epoch 00015: val_loss improved from 0.70655 to 0.60189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/015-0.6019.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7732 - acc: 0.7572 - val_loss: 0.6019 - val_acc: 0.8192\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7336 - acc: 0.7690\n",
      "Epoch 00016: val_loss did not improve from 0.60189\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7336 - acc: 0.7691 - val_loss: 0.6094 - val_acc: 0.8125\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7042 - acc: 0.7768\n",
      "Epoch 00017: val_loss improved from 0.60189 to 0.56299, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/017-0.5630.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7043 - acc: 0.7768 - val_loss: 0.5630 - val_acc: 0.8404\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.7865\n",
      "Epoch 00018: val_loss improved from 0.56299 to 0.52778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/018-0.5278.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6777 - acc: 0.7866 - val_loss: 0.5278 - val_acc: 0.8432\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6418 - acc: 0.8002\n",
      "Epoch 00019: val_loss improved from 0.52778 to 0.49902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/019-0.4990.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6418 - acc: 0.8003 - val_loss: 0.4990 - val_acc: 0.8453\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.8070\n",
      "Epoch 00020: val_loss improved from 0.49902 to 0.45747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/020-0.4575.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6133 - acc: 0.8069 - val_loss: 0.4575 - val_acc: 0.8635\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8151\n",
      "Epoch 00021: val_loss did not improve from 0.45747\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5935 - acc: 0.8151 - val_loss: 0.4877 - val_acc: 0.8523\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8215\n",
      "Epoch 00022: val_loss improved from 0.45747 to 0.45652, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/022-0.4565.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5738 - acc: 0.8215 - val_loss: 0.4565 - val_acc: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8292\n",
      "Epoch 00023: val_loss improved from 0.45652 to 0.42751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/023-0.4275.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5464 - acc: 0.8292 - val_loss: 0.4275 - val_acc: 0.8712\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8359\n",
      "Epoch 00024: val_loss did not improve from 0.42751\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5277 - acc: 0.8359 - val_loss: 0.4445 - val_acc: 0.8668\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.8418\n",
      "Epoch 00025: val_loss improved from 0.42751 to 0.39403, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/025-0.3940.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5080 - acc: 0.8418 - val_loss: 0.3940 - val_acc: 0.8817\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8459\n",
      "Epoch 00026: val_loss improved from 0.39403 to 0.38276, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/026-0.3828.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4880 - acc: 0.8459 - val_loss: 0.3828 - val_acc: 0.8873\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8502\n",
      "Epoch 00027: val_loss improved from 0.38276 to 0.38048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/027-0.3805.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4760 - acc: 0.8502 - val_loss: 0.3805 - val_acc: 0.8908\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8564\n",
      "Epoch 00028: val_loss improved from 0.38048 to 0.35760, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/028-0.3576.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4605 - acc: 0.8564 - val_loss: 0.3576 - val_acc: 0.8984\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8615\n",
      "Epoch 00029: val_loss improved from 0.35760 to 0.34447, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/029-0.3445.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4444 - acc: 0.8615 - val_loss: 0.3445 - val_acc: 0.8998\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8645\n",
      "Epoch 00030: val_loss improved from 0.34447 to 0.33246, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/030-0.3325.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4307 - acc: 0.8645 - val_loss: 0.3325 - val_acc: 0.9036\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8697\n",
      "Epoch 00031: val_loss improved from 0.33246 to 0.32588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/031-0.3259.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4131 - acc: 0.8697 - val_loss: 0.3259 - val_acc: 0.9064\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8721\n",
      "Epoch 00032: val_loss improved from 0.32588 to 0.32109, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/032-0.3211.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4098 - acc: 0.8721 - val_loss: 0.3211 - val_acc: 0.9075\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8759\n",
      "Epoch 00033: val_loss improved from 0.32109 to 0.30989, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/033-0.3099.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3956 - acc: 0.8759 - val_loss: 0.3099 - val_acc: 0.9136\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8795\n",
      "Epoch 00034: val_loss did not improve from 0.30989\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3880 - acc: 0.8794 - val_loss: 0.3421 - val_acc: 0.8996\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8838\n",
      "Epoch 00035: val_loss improved from 0.30989 to 0.29630, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/035-0.2963.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3753 - acc: 0.8838 - val_loss: 0.2963 - val_acc: 0.9117\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8860\n",
      "Epoch 00036: val_loss improved from 0.29630 to 0.28908, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/036-0.2891.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3707 - acc: 0.8860 - val_loss: 0.2891 - val_acc: 0.9166\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8908\n",
      "Epoch 00037: val_loss did not improve from 0.28908\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3540 - acc: 0.8908 - val_loss: 0.3169 - val_acc: 0.9057\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8887\n",
      "Epoch 00038: val_loss improved from 0.28908 to 0.27803, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/038-0.2780.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3531 - acc: 0.8887 - val_loss: 0.2780 - val_acc: 0.9175\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8931\n",
      "Epoch 00039: val_loss improved from 0.27803 to 0.27649, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/039-0.2765.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3432 - acc: 0.8931 - val_loss: 0.2765 - val_acc: 0.9201\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8984\n",
      "Epoch 00040: val_loss improved from 0.27649 to 0.27570, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/040-0.2757.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3281 - acc: 0.8984 - val_loss: 0.2757 - val_acc: 0.9206\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8975\n",
      "Epoch 00041: val_loss improved from 0.27570 to 0.25799, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/041-0.2580.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3269 - acc: 0.8975 - val_loss: 0.2580 - val_acc: 0.9236\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9014\n",
      "Epoch 00042: val_loss did not improve from 0.25799\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3208 - acc: 0.9014 - val_loss: 0.2663 - val_acc: 0.9222\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9015\n",
      "Epoch 00043: val_loss did not improve from 0.25799\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3162 - acc: 0.9015 - val_loss: 0.2615 - val_acc: 0.9234\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9003\n",
      "Epoch 00044: val_loss did not improve from 0.25799\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3131 - acc: 0.9003 - val_loss: 0.2878 - val_acc: 0.9131\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9057\n",
      "Epoch 00045: val_loss did not improve from 0.25799\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2975 - acc: 0.9057 - val_loss: 0.2590 - val_acc: 0.9259\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9070\n",
      "Epoch 00046: val_loss improved from 0.25799 to 0.24085, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/046-0.2409.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2970 - acc: 0.9070 - val_loss: 0.2409 - val_acc: 0.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9089\n",
      "Epoch 00047: val_loss did not improve from 0.24085\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2882 - acc: 0.9088 - val_loss: 0.2425 - val_acc: 0.9280\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9079\n",
      "Epoch 00048: val_loss did not improve from 0.24085\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2894 - acc: 0.9079 - val_loss: 0.2490 - val_acc: 0.9238\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9104\n",
      "Epoch 00049: val_loss did not improve from 0.24085\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2787 - acc: 0.9103 - val_loss: 0.2458 - val_acc: 0.9350\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9125\n",
      "Epoch 00050: val_loss improved from 0.24085 to 0.23821, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/050-0.2382.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2778 - acc: 0.9125 - val_loss: 0.2382 - val_acc: 0.9297\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9138\n",
      "Epoch 00051: val_loss did not improve from 0.23821\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2761 - acc: 0.9138 - val_loss: 0.2401 - val_acc: 0.9313\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9137\n",
      "Epoch 00052: val_loss improved from 0.23821 to 0.23480, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/052-0.2348.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2735 - acc: 0.9137 - val_loss: 0.2348 - val_acc: 0.9324\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9167\n",
      "Epoch 00053: val_loss improved from 0.23480 to 0.22921, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/053-0.2292.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2592 - acc: 0.9166 - val_loss: 0.2292 - val_acc: 0.9355\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9191\n",
      "Epoch 00054: val_loss did not improve from 0.22921\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2593 - acc: 0.9191 - val_loss: 0.2353 - val_acc: 0.9329\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9195\n",
      "Epoch 00055: val_loss improved from 0.22921 to 0.21905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/055-0.2190.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2588 - acc: 0.9195 - val_loss: 0.2190 - val_acc: 0.9364\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9210\n",
      "Epoch 00056: val_loss did not improve from 0.21905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2503 - acc: 0.9210 - val_loss: 0.2294 - val_acc: 0.9345\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9236\n",
      "Epoch 00057: val_loss did not improve from 0.21905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2411 - acc: 0.9235 - val_loss: 0.2308 - val_acc: 0.9343\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9216\n",
      "Epoch 00058: val_loss did not improve from 0.21905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2469 - acc: 0.9216 - val_loss: 0.2201 - val_acc: 0.9362\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9219\n",
      "Epoch 00059: val_loss did not improve from 0.21905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2437 - acc: 0.9219 - val_loss: 0.2254 - val_acc: 0.9392\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9240\n",
      "Epoch 00060: val_loss improved from 0.21905 to 0.21666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/060-0.2167.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2394 - acc: 0.9240 - val_loss: 0.2167 - val_acc: 0.9392\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9257\n",
      "Epoch 00061: val_loss improved from 0.21666 to 0.21204, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/061-0.2120.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2327 - acc: 0.9257 - val_loss: 0.2120 - val_acc: 0.9385\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9280\n",
      "Epoch 00062: val_loss did not improve from 0.21204\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2289 - acc: 0.9280 - val_loss: 0.2202 - val_acc: 0.9408\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9265\n",
      "Epoch 00063: val_loss improved from 0.21204 to 0.20768, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/063-0.2077.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2317 - acc: 0.9265 - val_loss: 0.2077 - val_acc: 0.9397\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9270\n",
      "Epoch 00064: val_loss did not improve from 0.20768\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2259 - acc: 0.9270 - val_loss: 0.2088 - val_acc: 0.9413\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9298\n",
      "Epoch 00065: val_loss improved from 0.20768 to 0.20184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/065-0.2018.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2219 - acc: 0.9298 - val_loss: 0.2018 - val_acc: 0.9453\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9301\n",
      "Epoch 00066: val_loss improved from 0.20184 to 0.20023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/066-0.2002.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2230 - acc: 0.9301 - val_loss: 0.2002 - val_acc: 0.9453\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9316\n",
      "Epoch 00067: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2152 - acc: 0.9316 - val_loss: 0.2070 - val_acc: 0.9364\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9323\n",
      "Epoch 00068: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2101 - acc: 0.9322 - val_loss: 0.2119 - val_acc: 0.9406\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9322\n",
      "Epoch 00069: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2096 - acc: 0.9322 - val_loss: 0.2073 - val_acc: 0.9441\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9331\n",
      "Epoch 00070: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2098 - acc: 0.9331 - val_loss: 0.2009 - val_acc: 0.9425\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9352\n",
      "Epoch 00071: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2014 - acc: 0.9352 - val_loss: 0.2026 - val_acc: 0.9446\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9344\n",
      "Epoch 00072: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2015 - acc: 0.9344 - val_loss: 0.2105 - val_acc: 0.9415\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9361\n",
      "Epoch 00073: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1968 - acc: 0.9361 - val_loss: 0.2021 - val_acc: 0.9471\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9348\n",
      "Epoch 00074: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2002 - acc: 0.9347 - val_loss: 0.2091 - val_acc: 0.9406\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9353\n",
      "Epoch 00075: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1994 - acc: 0.9353 - val_loss: 0.2015 - val_acc: 0.9422\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9386\n",
      "Epoch 00076: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1913 - acc: 0.9386 - val_loss: 0.2170 - val_acc: 0.9436\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9361\n",
      "Epoch 00077: val_loss improved from 0.20023 to 0.19615, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/077-0.1961.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1973 - acc: 0.9361 - val_loss: 0.1961 - val_acc: 0.9457\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9367\n",
      "Epoch 00078: val_loss did not improve from 0.19615\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1917 - acc: 0.9367 - val_loss: 0.2172 - val_acc: 0.9425\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9390\n",
      "Epoch 00079: val_loss did not improve from 0.19615\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1859 - acc: 0.9390 - val_loss: 0.2059 - val_acc: 0.9399\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9389\n",
      "Epoch 00080: val_loss did not improve from 0.19615\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1848 - acc: 0.9389 - val_loss: 0.2075 - val_acc: 0.9460\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9394\n",
      "Epoch 00081: val_loss improved from 0.19615 to 0.18624, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/081-0.1862.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1841 - acc: 0.9394 - val_loss: 0.1862 - val_acc: 0.9485\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9423\n",
      "Epoch 00082: val_loss did not improve from 0.18624\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1786 - acc: 0.9423 - val_loss: 0.1876 - val_acc: 0.9490\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9417\n",
      "Epoch 00083: val_loss did not improve from 0.18624\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1787 - acc: 0.9417 - val_loss: 0.1877 - val_acc: 0.9497\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9418\n",
      "Epoch 00084: val_loss did not improve from 0.18624\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1807 - acc: 0.9418 - val_loss: 0.1950 - val_acc: 0.9471\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9412\n",
      "Epoch 00085: val_loss did not improve from 0.18624\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1771 - acc: 0.9412 - val_loss: 0.1950 - val_acc: 0.9469\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9440\n",
      "Epoch 00086: val_loss improved from 0.18624 to 0.18591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/086-0.1859.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1704 - acc: 0.9440 - val_loss: 0.1859 - val_acc: 0.9495\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9432\n",
      "Epoch 00087: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1752 - acc: 0.9431 - val_loss: 0.1890 - val_acc: 0.9464\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9465\n",
      "Epoch 00088: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1654 - acc: 0.9465 - val_loss: 0.1966 - val_acc: 0.9481\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9443\n",
      "Epoch 00089: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1686 - acc: 0.9443 - val_loss: 0.1967 - val_acc: 0.9469\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9462\n",
      "Epoch 00090: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1656 - acc: 0.9462 - val_loss: 0.1892 - val_acc: 0.9478\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9465\n",
      "Epoch 00091: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1655 - acc: 0.9465 - val_loss: 0.1957 - val_acc: 0.9446\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9457\n",
      "Epoch 00092: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1662 - acc: 0.9457 - val_loss: 0.1909 - val_acc: 0.9432\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9450\n",
      "Epoch 00093: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1626 - acc: 0.9450 - val_loss: 0.2215 - val_acc: 0.9376\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9466\n",
      "Epoch 00094: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1610 - acc: 0.9466 - val_loss: 0.1942 - val_acc: 0.9485\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9486\n",
      "Epoch 00095: val_loss did not improve from 0.18591\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1561 - acc: 0.9486 - val_loss: 0.1979 - val_acc: 0.9502\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9485\n",
      "Epoch 00096: val_loss improved from 0.18591 to 0.17905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/096-0.1791.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1544 - acc: 0.9484 - val_loss: 0.1791 - val_acc: 0.9464\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9483\n",
      "Epoch 00097: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1556 - acc: 0.9483 - val_loss: 0.1937 - val_acc: 0.9483\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9481\n",
      "Epoch 00098: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1543 - acc: 0.9481 - val_loss: 0.2011 - val_acc: 0.9481\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9492\n",
      "Epoch 00099: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1519 - acc: 0.9492 - val_loss: 0.1928 - val_acc: 0.9483\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9494\n",
      "Epoch 00100: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1518 - acc: 0.9494 - val_loss: 0.1881 - val_acc: 0.9471\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9513\n",
      "Epoch 00101: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1492 - acc: 0.9512 - val_loss: 0.1953 - val_acc: 0.9446\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9506\n",
      "Epoch 00102: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1486 - acc: 0.9506 - val_loss: 0.1828 - val_acc: 0.9476\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9527\n",
      "Epoch 00103: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1432 - acc: 0.9527 - val_loss: 0.2027 - val_acc: 0.9483\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9534\n",
      "Epoch 00104: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1426 - acc: 0.9534 - val_loss: 0.1936 - val_acc: 0.9504\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9506\n",
      "Epoch 00105: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1470 - acc: 0.9506 - val_loss: 0.1962 - val_acc: 0.9476\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9532\n",
      "Epoch 00106: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1411 - acc: 0.9532 - val_loss: 0.1953 - val_acc: 0.9527\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9541\n",
      "Epoch 00107: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1373 - acc: 0.9541 - val_loss: 0.1950 - val_acc: 0.9499\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9534\n",
      "Epoch 00108: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1387 - acc: 0.9534 - val_loss: 0.1825 - val_acc: 0.9495\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9537\n",
      "Epoch 00109: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1370 - acc: 0.9537 - val_loss: 0.1982 - val_acc: 0.9488\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9540\n",
      "Epoch 00110: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1381 - acc: 0.9540 - val_loss: 0.1819 - val_acc: 0.9509\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9531\n",
      "Epoch 00111: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1378 - acc: 0.9531 - val_loss: 0.2128 - val_acc: 0.9520\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9552\n",
      "Epoch 00112: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1339 - acc: 0.9552 - val_loss: 0.1912 - val_acc: 0.9462\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9559\n",
      "Epoch 00113: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1320 - acc: 0.9559 - val_loss: 0.2021 - val_acc: 0.9450\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9549\n",
      "Epoch 00114: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1342 - acc: 0.9549 - val_loss: 0.1985 - val_acc: 0.9502\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9545\n",
      "Epoch 00115: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1339 - acc: 0.9545 - val_loss: 0.1801 - val_acc: 0.9520\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9564\n",
      "Epoch 00116: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1268 - acc: 0.9564 - val_loss: 0.1972 - val_acc: 0.9502\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9555\n",
      "Epoch 00117: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1330 - acc: 0.9555 - val_loss: 0.2054 - val_acc: 0.9478\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9565\n",
      "Epoch 00118: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1315 - acc: 0.9565 - val_loss: 0.1890 - val_acc: 0.9515\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9575\n",
      "Epoch 00119: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1248 - acc: 0.9575 - val_loss: 0.2082 - val_acc: 0.9476\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9576\n",
      "Epoch 00120: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1259 - acc: 0.9576 - val_loss: 0.1974 - val_acc: 0.9518\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9568\n",
      "Epoch 00121: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1271 - acc: 0.9569 - val_loss: 0.1871 - val_acc: 0.9481\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9570\n",
      "Epoch 00122: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1263 - acc: 0.9570 - val_loss: 0.1918 - val_acc: 0.9506\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9597\n",
      "Epoch 00123: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1211 - acc: 0.9597 - val_loss: 0.1982 - val_acc: 0.9497\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9578\n",
      "Epoch 00124: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1239 - acc: 0.9578 - val_loss: 0.2197 - val_acc: 0.9492\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9585\n",
      "Epoch 00125: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1219 - acc: 0.9584 - val_loss: 0.1938 - val_acc: 0.9504\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9583\n",
      "Epoch 00126: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1193 - acc: 0.9583 - val_loss: 0.1911 - val_acc: 0.9546\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9585\n",
      "Epoch 00127: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1190 - acc: 0.9585 - val_loss: 0.2012 - val_acc: 0.9509\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9590\n",
      "Epoch 00128: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1192 - acc: 0.9590 - val_loss: 0.1839 - val_acc: 0.9529\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9600\n",
      "Epoch 00129: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1176 - acc: 0.9600 - val_loss: 0.1810 - val_acc: 0.9539\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9623\n",
      "Epoch 00130: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1126 - acc: 0.9623 - val_loss: 0.1914 - val_acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9608\n",
      "Epoch 00131: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1152 - acc: 0.9608 - val_loss: 0.1941 - val_acc: 0.9550\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9629\n",
      "Epoch 00132: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1135 - acc: 0.9629 - val_loss: 0.1849 - val_acc: 0.9543\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9621\n",
      "Epoch 00133: val_loss improved from 0.17905 to 0.17881, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/133-0.1788.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1119 - acc: 0.9621 - val_loss: 0.1788 - val_acc: 0.9534\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9619\n",
      "Epoch 00134: val_loss improved from 0.17881 to 0.17808, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/134-0.1781.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1142 - acc: 0.9619 - val_loss: 0.1781 - val_acc: 0.9548\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9640\n",
      "Epoch 00135: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1079 - acc: 0.9640 - val_loss: 0.2085 - val_acc: 0.9543\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9634\n",
      "Epoch 00136: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1100 - acc: 0.9634 - val_loss: 0.2096 - val_acc: 0.9520\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9617\n",
      "Epoch 00137: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1140 - acc: 0.9617 - val_loss: 0.1947 - val_acc: 0.9536\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9637\n",
      "Epoch 00138: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1066 - acc: 0.9637 - val_loss: 0.1832 - val_acc: 0.9522\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9621\n",
      "Epoch 00139: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1122 - acc: 0.9621 - val_loss: 0.2151 - val_acc: 0.9462\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9619\n",
      "Epoch 00140: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1119 - acc: 0.9619 - val_loss: 0.1897 - val_acc: 0.9511\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9636\n",
      "Epoch 00141: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1069 - acc: 0.9636 - val_loss: 0.1913 - val_acc: 0.9539\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9646\n",
      "Epoch 00142: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1053 - acc: 0.9646 - val_loss: 0.1925 - val_acc: 0.9536\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9652\n",
      "Epoch 00143: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1039 - acc: 0.9652 - val_loss: 0.2062 - val_acc: 0.9532\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9633\n",
      "Epoch 00144: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1070 - acc: 0.9633 - val_loss: 0.2055 - val_acc: 0.9527\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9654\n",
      "Epoch 00145: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1007 - acc: 0.9654 - val_loss: 0.2027 - val_acc: 0.9506\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9653\n",
      "Epoch 00146: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1034 - acc: 0.9653 - val_loss: 0.2056 - val_acc: 0.9460\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9652\n",
      "Epoch 00147: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1014 - acc: 0.9652 - val_loss: 0.1923 - val_acc: 0.9539\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9659\n",
      "Epoch 00148: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1017 - acc: 0.9659 - val_loss: 0.1900 - val_acc: 0.9536\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9665\n",
      "Epoch 00149: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1019 - acc: 0.9666 - val_loss: 0.1938 - val_acc: 0.9536\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9658\n",
      "Epoch 00150: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1016 - acc: 0.9658 - val_loss: 0.2028 - val_acc: 0.9532\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9679\n",
      "Epoch 00151: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0949 - acc: 0.9679 - val_loss: 0.1837 - val_acc: 0.9543\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9641\n",
      "Epoch 00152: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1037 - acc: 0.9641 - val_loss: 0.1971 - val_acc: 0.9527\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9659\n",
      "Epoch 00153: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0994 - acc: 0.9659 - val_loss: 0.2135 - val_acc: 0.9511\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9664\n",
      "Epoch 00154: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0986 - acc: 0.9664 - val_loss: 0.1956 - val_acc: 0.9555\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9674\n",
      "Epoch 00155: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0952 - acc: 0.9673 - val_loss: 0.1934 - val_acc: 0.9553\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9689\n",
      "Epoch 00156: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0939 - acc: 0.9689 - val_loss: 0.2081 - val_acc: 0.9515\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9684\n",
      "Epoch 00157: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0931 - acc: 0.9684 - val_loss: 0.1987 - val_acc: 0.9557\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9677\n",
      "Epoch 00158: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0932 - acc: 0.9677 - val_loss: 0.1948 - val_acc: 0.9539\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9671\n",
      "Epoch 00159: val_loss did not improve from 0.17808\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0931 - acc: 0.9671 - val_loss: 0.1952 - val_acc: 0.9539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9677\n",
      "Epoch 00160: val_loss improved from 0.17808 to 0.17798, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv_checkpoint/160-0.1780.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0936 - acc: 0.9677 - val_loss: 0.1780 - val_acc: 0.9571\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9673\n",
      "Epoch 00161: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0947 - acc: 0.9673 - val_loss: 0.2128 - val_acc: 0.9543\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9682\n",
      "Epoch 00162: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0911 - acc: 0.9682 - val_loss: 0.1857 - val_acc: 0.9560\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9700\n",
      "Epoch 00163: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0874 - acc: 0.9700 - val_loss: 0.2169 - val_acc: 0.9483\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9702\n",
      "Epoch 00164: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0876 - acc: 0.9701 - val_loss: 0.2009 - val_acc: 0.9574\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9683\n",
      "Epoch 00165: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0930 - acc: 0.9683 - val_loss: 0.2035 - val_acc: 0.9527\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9703\n",
      "Epoch 00166: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0859 - acc: 0.9703 - val_loss: 0.2003 - val_acc: 0.9539\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9709\n",
      "Epoch 00167: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0865 - acc: 0.9709 - val_loss: 0.1944 - val_acc: 0.9529\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9708\n",
      "Epoch 00168: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0860 - acc: 0.9708 - val_loss: 0.2029 - val_acc: 0.9529\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9691\n",
      "Epoch 00169: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0881 - acc: 0.9691 - val_loss: 0.1961 - val_acc: 0.9555\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9704\n",
      "Epoch 00170: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0883 - acc: 0.9704 - val_loss: 0.1952 - val_acc: 0.9546\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9710\n",
      "Epoch 00171: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0827 - acc: 0.9710 - val_loss: 0.2111 - val_acc: 0.9539\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9715\n",
      "Epoch 00172: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0853 - acc: 0.9715 - val_loss: 0.2024 - val_acc: 0.9555\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9718\n",
      "Epoch 00173: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0837 - acc: 0.9718 - val_loss: 0.2169 - val_acc: 0.9529\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9705\n",
      "Epoch 00174: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0849 - acc: 0.9705 - val_loss: 0.1986 - val_acc: 0.9511\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9696\n",
      "Epoch 00175: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0863 - acc: 0.9696 - val_loss: 0.2247 - val_acc: 0.9511\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9711\n",
      "Epoch 00176: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0841 - acc: 0.9711 - val_loss: 0.1977 - val_acc: 0.9525\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9716\n",
      "Epoch 00177: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0825 - acc: 0.9716 - val_loss: 0.2136 - val_acc: 0.9541\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9729\n",
      "Epoch 00178: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0814 - acc: 0.9729 - val_loss: 0.2042 - val_acc: 0.9576\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9719\n",
      "Epoch 00179: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0815 - acc: 0.9719 - val_loss: 0.2058 - val_acc: 0.9553\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9712\n",
      "Epoch 00180: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0829 - acc: 0.9713 - val_loss: 0.2015 - val_acc: 0.9557\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9727\n",
      "Epoch 00181: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0805 - acc: 0.9727 - val_loss: 0.1869 - val_acc: 0.9534\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9736\n",
      "Epoch 00182: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0792 - acc: 0.9736 - val_loss: 0.1936 - val_acc: 0.9574\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9723\n",
      "Epoch 00183: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0826 - acc: 0.9723 - val_loss: 0.1919 - val_acc: 0.9581\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9725\n",
      "Epoch 00184: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0813 - acc: 0.9725 - val_loss: 0.1995 - val_acc: 0.9562\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9732\n",
      "Epoch 00185: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0780 - acc: 0.9732 - val_loss: 0.1929 - val_acc: 0.9518\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9751\n",
      "Epoch 00186: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0754 - acc: 0.9751 - val_loss: 0.1968 - val_acc: 0.9529\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9746\n",
      "Epoch 00187: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0779 - acc: 0.9745 - val_loss: 0.2110 - val_acc: 0.9581\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9716\n",
      "Epoch 00188: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0808 - acc: 0.9716 - val_loss: 0.1947 - val_acc: 0.9562\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9734\n",
      "Epoch 00189: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0807 - acc: 0.9734 - val_loss: 0.1951 - val_acc: 0.9541\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9743\n",
      "Epoch 00190: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0758 - acc: 0.9743 - val_loss: 0.1903 - val_acc: 0.9543\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9734\n",
      "Epoch 00191: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0783 - acc: 0.9734 - val_loss: 0.1921 - val_acc: 0.9509\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9740\n",
      "Epoch 00192: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0761 - acc: 0.9740 - val_loss: 0.2091 - val_acc: 0.9548\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9744\n",
      "Epoch 00193: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0748 - acc: 0.9744 - val_loss: 0.2097 - val_acc: 0.9515\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9742\n",
      "Epoch 00194: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0756 - acc: 0.9742 - val_loss: 0.2043 - val_acc: 0.9548\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9744\n",
      "Epoch 00195: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0736 - acc: 0.9744 - val_loss: 0.1987 - val_acc: 0.9578\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9748\n",
      "Epoch 00196: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0753 - acc: 0.9748 - val_loss: 0.2136 - val_acc: 0.9543\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9740\n",
      "Epoch 00197: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0755 - acc: 0.9740 - val_loss: 0.2026 - val_acc: 0.9562\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9710\n",
      "Epoch 00198: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0864 - acc: 0.9710 - val_loss: 0.2134 - val_acc: 0.9536\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9748\n",
      "Epoch 00199: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0725 - acc: 0.9748 - val_loss: 0.2053 - val_acc: 0.9581\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9764\n",
      "Epoch 00200: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0695 - acc: 0.9764 - val_loss: 0.2080 - val_acc: 0.9543\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9754\n",
      "Epoch 00201: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0701 - acc: 0.9754 - val_loss: 0.2069 - val_acc: 0.9525\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9760\n",
      "Epoch 00202: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0695 - acc: 0.9760 - val_loss: 0.2054 - val_acc: 0.9578\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9743\n",
      "Epoch 00203: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0719 - acc: 0.9743 - val_loss: 0.2068 - val_acc: 0.9543\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9760\n",
      "Epoch 00204: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0663 - acc: 0.9760 - val_loss: 0.2221 - val_acc: 0.9522\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9756\n",
      "Epoch 00205: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0710 - acc: 0.9756 - val_loss: 0.1996 - val_acc: 0.9562\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9762\n",
      "Epoch 00206: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0674 - acc: 0.9763 - val_loss: 0.2136 - val_acc: 0.9562\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9758\n",
      "Epoch 00207: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0710 - acc: 0.9758 - val_loss: 0.2182 - val_acc: 0.9562\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9755\n",
      "Epoch 00208: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0692 - acc: 0.9755 - val_loss: 0.2191 - val_acc: 0.9578\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9750\n",
      "Epoch 00209: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0757 - acc: 0.9750 - val_loss: 0.2088 - val_acc: 0.9532\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9757\n",
      "Epoch 00210: val_loss did not improve from 0.17798\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0707 - acc: 0.9757 - val_loss: 0.2132 - val_acc: 0.9557\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 567us/sample - loss: 0.1959 - acc: 0.9456\n",
      "Loss: 0.1959358832523273 Accuracy: 0.9455867\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6519 - acc: 0.1134\n",
      "Epoch 00001: val_loss improved from inf to 2.35360, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/001-2.3536.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.6519 - acc: 0.1134 - val_loss: 2.3536 - val_acc: 0.2364\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1326 - acc: 0.2992\n",
      "Epoch 00002: val_loss improved from 2.35360 to 1.55364, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/002-1.5536.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.1325 - acc: 0.2992 - val_loss: 1.5536 - val_acc: 0.5155\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6227 - acc: 0.4611\n",
      "Epoch 00003: val_loss improved from 1.55364 to 1.32184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/003-1.3218.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.6226 - acc: 0.4612 - val_loss: 1.3218 - val_acc: 0.5740\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3879 - acc: 0.5374\n",
      "Epoch 00004: val_loss improved from 1.32184 to 1.10737, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/004-1.1074.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.3879 - acc: 0.5373 - val_loss: 1.1074 - val_acc: 0.6506\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2447 - acc: 0.5864\n",
      "Epoch 00005: val_loss improved from 1.10737 to 0.94085, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/005-0.9408.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2447 - acc: 0.5864 - val_loss: 0.9408 - val_acc: 0.7046\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1364 - acc: 0.6233\n",
      "Epoch 00006: val_loss improved from 0.94085 to 0.92434, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/006-0.9243.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1365 - acc: 0.6233 - val_loss: 0.9243 - val_acc: 0.7035\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0512 - acc: 0.6526\n",
      "Epoch 00007: val_loss improved from 0.92434 to 0.80277, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/007-0.8028.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0514 - acc: 0.6525 - val_loss: 0.8028 - val_acc: 0.7428\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9873 - acc: 0.6744\n",
      "Epoch 00008: val_loss improved from 0.80277 to 0.72269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/008-0.7227.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9873 - acc: 0.6743 - val_loss: 0.7227 - val_acc: 0.7682\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9245 - acc: 0.6963\n",
      "Epoch 00009: val_loss improved from 0.72269 to 0.70078, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/009-0.7008.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9246 - acc: 0.6963 - val_loss: 0.7008 - val_acc: 0.7827\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8823 - acc: 0.7101\n",
      "Epoch 00010: val_loss improved from 0.70078 to 0.68074, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/010-0.6807.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8824 - acc: 0.7101 - val_loss: 0.6807 - val_acc: 0.7827\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8289 - acc: 0.7266\n",
      "Epoch 00011: val_loss did not improve from 0.68074\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8289 - acc: 0.7266 - val_loss: 0.7361 - val_acc: 0.7570\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7951 - acc: 0.7400\n",
      "Epoch 00012: val_loss improved from 0.68074 to 0.60778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/012-0.6078.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7951 - acc: 0.7400 - val_loss: 0.6078 - val_acc: 0.8083\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7572 - acc: 0.7508\n",
      "Epoch 00013: val_loss improved from 0.60778 to 0.55797, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/013-0.5580.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7572 - acc: 0.7509 - val_loss: 0.5580 - val_acc: 0.8190\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7260 - acc: 0.7621\n",
      "Epoch 00014: val_loss did not improve from 0.55797\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7260 - acc: 0.7621 - val_loss: 0.5647 - val_acc: 0.8155\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7721\n",
      "Epoch 00015: val_loss improved from 0.55797 to 0.50745, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/015-0.5074.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6962 - acc: 0.7722 - val_loss: 0.5074 - val_acc: 0.8393\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6667 - acc: 0.7826\n",
      "Epoch 00016: val_loss improved from 0.50745 to 0.49889, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/016-0.4989.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6666 - acc: 0.7826 - val_loss: 0.4989 - val_acc: 0.8411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.7902\n",
      "Epoch 00017: val_loss improved from 0.49889 to 0.47490, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/017-0.4749.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6445 - acc: 0.7902 - val_loss: 0.4749 - val_acc: 0.8500\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.7998\n",
      "Epoch 00018: val_loss improved from 0.47490 to 0.45074, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/018-0.4507.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6200 - acc: 0.7998 - val_loss: 0.4507 - val_acc: 0.8593\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.8071\n",
      "Epoch 00019: val_loss did not improve from 0.45074\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5988 - acc: 0.8071 - val_loss: 0.5167 - val_acc: 0.8355\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8130\n",
      "Epoch 00020: val_loss improved from 0.45074 to 0.41597, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/020-0.4160.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5781 - acc: 0.8130 - val_loss: 0.4160 - val_acc: 0.8663\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8205\n",
      "Epoch 00021: val_loss did not improve from 0.41597\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5555 - acc: 0.8205 - val_loss: 0.4249 - val_acc: 0.8768\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8309\n",
      "Epoch 00022: val_loss improved from 0.41597 to 0.37834, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/022-0.3783.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5280 - acc: 0.8309 - val_loss: 0.3783 - val_acc: 0.8800\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8334\n",
      "Epoch 00023: val_loss did not improve from 0.37834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5149 - acc: 0.8334 - val_loss: 0.3854 - val_acc: 0.8859\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8398\n",
      "Epoch 00024: val_loss improved from 0.37834 to 0.35885, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/024-0.3589.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5009 - acc: 0.8398 - val_loss: 0.3589 - val_acc: 0.8926\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8480\n",
      "Epoch 00025: val_loss improved from 0.35885 to 0.32668, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/025-0.3267.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4708 - acc: 0.8480 - val_loss: 0.3267 - val_acc: 0.9038\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8549\n",
      "Epoch 00026: val_loss did not improve from 0.32668\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4621 - acc: 0.8549 - val_loss: 0.3694 - val_acc: 0.8945\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8616\n",
      "Epoch 00027: val_loss improved from 0.32668 to 0.30920, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/027-0.3092.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4418 - acc: 0.8616 - val_loss: 0.3092 - val_acc: 0.9047\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8647\n",
      "Epoch 00028: val_loss improved from 0.30920 to 0.30166, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/028-0.3017.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4250 - acc: 0.8647 - val_loss: 0.3017 - val_acc: 0.9054\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8690\n",
      "Epoch 00029: val_loss improved from 0.30166 to 0.27381, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/029-0.2738.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4086 - acc: 0.8690 - val_loss: 0.2738 - val_acc: 0.9173\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8750\n",
      "Epoch 00030: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3910 - acc: 0.8750 - val_loss: 0.2832 - val_acc: 0.9147\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8781\n",
      "Epoch 00031: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3854 - acc: 0.8780 - val_loss: 0.2766 - val_acc: 0.9175\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8842\n",
      "Epoch 00032: val_loss improved from 0.27381 to 0.26871, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/032-0.2687.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3673 - acc: 0.8842 - val_loss: 0.2687 - val_acc: 0.9168\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8863\n",
      "Epoch 00033: val_loss improved from 0.26871 to 0.25143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/033-0.2514.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3621 - acc: 0.8863 - val_loss: 0.2514 - val_acc: 0.9266\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8886\n",
      "Epoch 00034: val_loss improved from 0.25143 to 0.24917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/034-0.2492.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3500 - acc: 0.8886 - val_loss: 0.2492 - val_acc: 0.9262\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8917\n",
      "Epoch 00035: val_loss improved from 0.24917 to 0.24677, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/035-0.2468.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3394 - acc: 0.8916 - val_loss: 0.2468 - val_acc: 0.9317\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8906\n",
      "Epoch 00036: val_loss improved from 0.24677 to 0.23006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/036-0.2301.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3405 - acc: 0.8906 - val_loss: 0.2301 - val_acc: 0.9299\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8984\n",
      "Epoch 00037: val_loss did not improve from 0.23006\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3175 - acc: 0.8984 - val_loss: 0.2586 - val_acc: 0.9231\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8977\n",
      "Epoch 00038: val_loss improved from 0.23006 to 0.22381, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/038-0.2238.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3182 - acc: 0.8976 - val_loss: 0.2238 - val_acc: 0.9362\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9020\n",
      "Epoch 00039: val_loss improved from 0.22381 to 0.22067, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/039-0.2207.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3085 - acc: 0.9021 - val_loss: 0.2207 - val_acc: 0.9359\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9062\n",
      "Epoch 00040: val_loss did not improve from 0.22067\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2931 - acc: 0.9062 - val_loss: 0.2405 - val_acc: 0.9329\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9065\n",
      "Epoch 00041: val_loss improved from 0.22067 to 0.21952, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/041-0.2195.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2911 - acc: 0.9065 - val_loss: 0.2195 - val_acc: 0.9350\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9125\n",
      "Epoch 00042: val_loss improved from 0.21952 to 0.20768, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/042-0.2077.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2797 - acc: 0.9125 - val_loss: 0.2077 - val_acc: 0.9383\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9154\n",
      "Epoch 00043: val_loss improved from 0.20768 to 0.19662, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/043-0.1966.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2680 - acc: 0.9154 - val_loss: 0.1966 - val_acc: 0.9448\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9149\n",
      "Epoch 00044: val_loss improved from 0.19662 to 0.18897, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/044-0.1890.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2699 - acc: 0.9149 - val_loss: 0.1890 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9186\n",
      "Epoch 00045: val_loss did not improve from 0.18897\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2591 - acc: 0.9185 - val_loss: 0.1918 - val_acc: 0.9464\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9173\n",
      "Epoch 00046: val_loss did not improve from 0.18897\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2610 - acc: 0.9173 - val_loss: 0.1923 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9208\n",
      "Epoch 00047: val_loss did not improve from 0.18897\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2500 - acc: 0.9208 - val_loss: 0.1949 - val_acc: 0.9457\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9223\n",
      "Epoch 00048: val_loss improved from 0.18897 to 0.17913, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/048-0.1791.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2440 - acc: 0.9223 - val_loss: 0.1791 - val_acc: 0.9485\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9238\n",
      "Epoch 00049: val_loss did not improve from 0.17913\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2402 - acc: 0.9238 - val_loss: 0.1862 - val_acc: 0.9481\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9268\n",
      "Epoch 00050: val_loss improved from 0.17913 to 0.17873, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/050-0.1787.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2285 - acc: 0.9268 - val_loss: 0.1787 - val_acc: 0.9495\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9264\n",
      "Epoch 00051: val_loss improved from 0.17873 to 0.17418, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/051-0.1742.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2298 - acc: 0.9264 - val_loss: 0.1742 - val_acc: 0.9522\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9265\n",
      "Epoch 00052: val_loss did not improve from 0.17418\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2248 - acc: 0.9265 - val_loss: 0.1830 - val_acc: 0.9485\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9300\n",
      "Epoch 00053: val_loss improved from 0.17418 to 0.17002, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/053-0.1700.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2203 - acc: 0.9300 - val_loss: 0.1700 - val_acc: 0.9511\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9281\n",
      "Epoch 00054: val_loss did not improve from 0.17002\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2221 - acc: 0.9281 - val_loss: 0.1980 - val_acc: 0.9460\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9329\n",
      "Epoch 00055: val_loss improved from 0.17002 to 0.16144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/055-0.1614.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2119 - acc: 0.9329 - val_loss: 0.1614 - val_acc: 0.9562\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9326\n",
      "Epoch 00056: val_loss did not improve from 0.16144\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2090 - acc: 0.9326 - val_loss: 0.1703 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9343\n",
      "Epoch 00057: val_loss did not improve from 0.16144\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2057 - acc: 0.9343 - val_loss: 0.1716 - val_acc: 0.9506\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9365\n",
      "Epoch 00058: val_loss did not improve from 0.16144\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1992 - acc: 0.9365 - val_loss: 0.1710 - val_acc: 0.9518\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9368\n",
      "Epoch 00059: val_loss did not improve from 0.16144\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1961 - acc: 0.9368 - val_loss: 0.1697 - val_acc: 0.9543\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9372\n",
      "Epoch 00060: val_loss improved from 0.16144 to 0.15936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/060-0.1594.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1966 - acc: 0.9372 - val_loss: 0.1594 - val_acc: 0.9536\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9380\n",
      "Epoch 00061: val_loss improved from 0.15936 to 0.15394, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/061-0.1539.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1924 - acc: 0.9380 - val_loss: 0.1539 - val_acc: 0.9557\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9388\n",
      "Epoch 00062: val_loss did not improve from 0.15394\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1890 - acc: 0.9388 - val_loss: 0.1546 - val_acc: 0.9557\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9418\n",
      "Epoch 00063: val_loss improved from 0.15394 to 0.15304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/063-0.1530.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1824 - acc: 0.9418 - val_loss: 0.1530 - val_acc: 0.9588\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9402\n",
      "Epoch 00064: val_loss did not improve from 0.15304\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1841 - acc: 0.9402 - val_loss: 0.1652 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9418\n",
      "Epoch 00065: val_loss did not improve from 0.15304\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1808 - acc: 0.9418 - val_loss: 0.1535 - val_acc: 0.9578\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9432\n",
      "Epoch 00066: val_loss did not improve from 0.15304\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1765 - acc: 0.9432 - val_loss: 0.1629 - val_acc: 0.9564\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9433\n",
      "Epoch 00067: val_loss did not improve from 0.15304\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1711 - acc: 0.9433 - val_loss: 0.1630 - val_acc: 0.9560\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9449\n",
      "Epoch 00068: val_loss improved from 0.15304 to 0.15259, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/068-0.1526.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1704 - acc: 0.9449 - val_loss: 0.1526 - val_acc: 0.9576\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9465\n",
      "Epoch 00069: val_loss improved from 0.15259 to 0.14752, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/069-0.1475.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1665 - acc: 0.9465 - val_loss: 0.1475 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9463\n",
      "Epoch 00070: val_loss did not improve from 0.14752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1667 - acc: 0.9463 - val_loss: 0.1535 - val_acc: 0.9550\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9467\n",
      "Epoch 00071: val_loss did not improve from 0.14752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1644 - acc: 0.9467 - val_loss: 0.1518 - val_acc: 0.9578\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9476\n",
      "Epoch 00072: val_loss did not improve from 0.14752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1576 - acc: 0.9476 - val_loss: 0.1512 - val_acc: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9480\n",
      "Epoch 00073: val_loss improved from 0.14752 to 0.14620, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/073-0.1462.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1588 - acc: 0.9480 - val_loss: 0.1462 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9489\n",
      "Epoch 00074: val_loss did not improve from 0.14620\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1539 - acc: 0.9489 - val_loss: 0.1582 - val_acc: 0.9569\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9493\n",
      "Epoch 00075: val_loss did not improve from 0.14620\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1561 - acc: 0.9493 - val_loss: 0.1537 - val_acc: 0.9553\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9495\n",
      "Epoch 00076: val_loss did not improve from 0.14620\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1543 - acc: 0.9495 - val_loss: 0.1464 - val_acc: 0.9581\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9510\n",
      "Epoch 00077: val_loss did not improve from 0.14620\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1472 - acc: 0.9510 - val_loss: 0.1512 - val_acc: 0.9588\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9519\n",
      "Epoch 00078: val_loss did not improve from 0.14620\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1461 - acc: 0.9519 - val_loss: 0.1480 - val_acc: 0.9592\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9507\n",
      "Epoch 00079: val_loss improved from 0.14620 to 0.14408, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/079-0.1441.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1488 - acc: 0.9507 - val_loss: 0.1441 - val_acc: 0.9592\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9527\n",
      "Epoch 00080: val_loss did not improve from 0.14408\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1445 - acc: 0.9527 - val_loss: 0.1574 - val_acc: 0.9562\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9530\n",
      "Epoch 00081: val_loss did not improve from 0.14408\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1418 - acc: 0.9530 - val_loss: 0.1444 - val_acc: 0.9585\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9540\n",
      "Epoch 00082: val_loss improved from 0.14408 to 0.14295, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/082-0.1430.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1384 - acc: 0.9540 - val_loss: 0.1430 - val_acc: 0.9616\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9540\n",
      "Epoch 00083: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1411 - acc: 0.9540 - val_loss: 0.1544 - val_acc: 0.9571\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9546\n",
      "Epoch 00084: val_loss improved from 0.14295 to 0.13655, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/084-0.1366.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1353 - acc: 0.9546 - val_loss: 0.1366 - val_acc: 0.9606\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9577\n",
      "Epoch 00085: val_loss did not improve from 0.13655\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1307 - acc: 0.9577 - val_loss: 0.1427 - val_acc: 0.9604\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9567\n",
      "Epoch 00086: val_loss did not improve from 0.13655\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1303 - acc: 0.9567 - val_loss: 0.1576 - val_acc: 0.9578\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9562\n",
      "Epoch 00087: val_loss did not improve from 0.13655\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1288 - acc: 0.9562 - val_loss: 0.1554 - val_acc: 0.9595\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9586\n",
      "Epoch 00088: val_loss improved from 0.13655 to 0.13264, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/088-0.1326.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1290 - acc: 0.9586 - val_loss: 0.1326 - val_acc: 0.9637\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9581\n",
      "Epoch 00089: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1251 - acc: 0.9581 - val_loss: 0.1900 - val_acc: 0.9504\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9583\n",
      "Epoch 00090: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1253 - acc: 0.9583 - val_loss: 0.1428 - val_acc: 0.9611\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9581\n",
      "Epoch 00091: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1257 - acc: 0.9581 - val_loss: 0.1361 - val_acc: 0.9632\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9587\n",
      "Epoch 00092: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1243 - acc: 0.9587 - val_loss: 0.1534 - val_acc: 0.9581\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9586\n",
      "Epoch 00093: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1205 - acc: 0.9586 - val_loss: 0.1466 - val_acc: 0.9606\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9605\n",
      "Epoch 00094: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1164 - acc: 0.9605 - val_loss: 0.1565 - val_acc: 0.9613\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9594\n",
      "Epoch 00095: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1202 - acc: 0.9594 - val_loss: 0.1591 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9612\n",
      "Epoch 00096: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1173 - acc: 0.9612 - val_loss: 0.1336 - val_acc: 0.9630\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9621\n",
      "Epoch 00097: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1122 - acc: 0.9621 - val_loss: 0.1418 - val_acc: 0.9602\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9624\n",
      "Epoch 00098: val_loss did not improve from 0.13264\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1138 - acc: 0.9625 - val_loss: 0.1410 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9616\n",
      "Epoch 00099: val_loss improved from 0.13264 to 0.13231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/099-0.1323.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1146 - acc: 0.9616 - val_loss: 0.1323 - val_acc: 0.9639\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9620\n",
      "Epoch 00100: val_loss did not improve from 0.13231\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1129 - acc: 0.9620 - val_loss: 0.1511 - val_acc: 0.9641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9638\n",
      "Epoch 00101: val_loss did not improve from 0.13231\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1064 - acc: 0.9638 - val_loss: 0.1336 - val_acc: 0.9660\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9638\n",
      "Epoch 00102: val_loss did not improve from 0.13231\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1078 - acc: 0.9638 - val_loss: 0.1393 - val_acc: 0.9646\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9639\n",
      "Epoch 00103: val_loss improved from 0.13231 to 0.13056, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/103-0.1306.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1051 - acc: 0.9639 - val_loss: 0.1306 - val_acc: 0.9655\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9631\n",
      "Epoch 00104: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1083 - acc: 0.9630 - val_loss: 0.1466 - val_acc: 0.9611\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9642\n",
      "Epoch 00105: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1066 - acc: 0.9642 - val_loss: 0.1397 - val_acc: 0.9641\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9655\n",
      "Epoch 00106: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1015 - acc: 0.9654 - val_loss: 0.1437 - val_acc: 0.9634\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9646\n",
      "Epoch 00107: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1077 - acc: 0.9646 - val_loss: 0.1336 - val_acc: 0.9625\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9669\n",
      "Epoch 00108: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1013 - acc: 0.9669 - val_loss: 0.1424 - val_acc: 0.9646\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9654\n",
      "Epoch 00109: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1013 - acc: 0.9654 - val_loss: 0.1368 - val_acc: 0.9625\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9664\n",
      "Epoch 00110: val_loss did not improve from 0.13056\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0996 - acc: 0.9664 - val_loss: 0.1416 - val_acc: 0.9611\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9655\n",
      "Epoch 00111: val_loss improved from 0.13056 to 0.12915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv_checkpoint/111-0.1292.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1006 - acc: 0.9655 - val_loss: 0.1292 - val_acc: 0.9646\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9676\n",
      "Epoch 00112: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0938 - acc: 0.9676 - val_loss: 0.1611 - val_acc: 0.9623\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9671\n",
      "Epoch 00113: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0959 - acc: 0.9672 - val_loss: 0.1373 - val_acc: 0.9644\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9672\n",
      "Epoch 00114: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0936 - acc: 0.9672 - val_loss: 0.1432 - val_acc: 0.9625\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9686\n",
      "Epoch 00115: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0916 - acc: 0.9686 - val_loss: 0.1486 - val_acc: 0.9630\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9678\n",
      "Epoch 00116: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0956 - acc: 0.9678 - val_loss: 0.1315 - val_acc: 0.9644\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9700\n",
      "Epoch 00117: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0876 - acc: 0.9700 - val_loss: 0.1501 - val_acc: 0.9634\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9689\n",
      "Epoch 00118: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0902 - acc: 0.9689 - val_loss: 0.1496 - val_acc: 0.9604\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9697\n",
      "Epoch 00119: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0909 - acc: 0.9697 - val_loss: 0.1523 - val_acc: 0.9665\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9694\n",
      "Epoch 00120: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0898 - acc: 0.9694 - val_loss: 0.1550 - val_acc: 0.9618\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9712\n",
      "Epoch 00121: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0840 - acc: 0.9712 - val_loss: 0.1439 - val_acc: 0.9646\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9702\n",
      "Epoch 00122: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0880 - acc: 0.9702 - val_loss: 0.1453 - val_acc: 0.9648\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9711\n",
      "Epoch 00123: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0865 - acc: 0.9711 - val_loss: 0.1374 - val_acc: 0.9672\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9702\n",
      "Epoch 00124: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0880 - acc: 0.9702 - val_loss: 0.1451 - val_acc: 0.9651\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9704\n",
      "Epoch 00125: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0873 - acc: 0.9704 - val_loss: 0.1446 - val_acc: 0.9658\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9718\n",
      "Epoch 00126: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0826 - acc: 0.9717 - val_loss: 0.1489 - val_acc: 0.9644\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9707\n",
      "Epoch 00127: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0850 - acc: 0.9707 - val_loss: 0.1469 - val_acc: 0.9630\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9701\n",
      "Epoch 00128: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0884 - acc: 0.9701 - val_loss: 0.1393 - val_acc: 0.9641\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9726\n",
      "Epoch 00129: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0779 - acc: 0.9726 - val_loss: 0.1470 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9727\n",
      "Epoch 00130: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0812 - acc: 0.9727 - val_loss: 0.1524 - val_acc: 0.9665\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9723\n",
      "Epoch 00131: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0802 - acc: 0.9723 - val_loss: 0.1544 - val_acc: 0.9637\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9723\n",
      "Epoch 00132: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0820 - acc: 0.9723 - val_loss: 0.1532 - val_acc: 0.9681\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9733\n",
      "Epoch 00133: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0778 - acc: 0.9733 - val_loss: 0.1659 - val_acc: 0.9595\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9735\n",
      "Epoch 00134: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0786 - acc: 0.9735 - val_loss: 0.1600 - val_acc: 0.9646\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9739\n",
      "Epoch 00135: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0778 - acc: 0.9739 - val_loss: 0.1582 - val_acc: 0.9651\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9729\n",
      "Epoch 00136: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0802 - acc: 0.9729 - val_loss: 0.1336 - val_acc: 0.9651\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9728\n",
      "Epoch 00137: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0806 - acc: 0.9728 - val_loss: 0.1528 - val_acc: 0.9648\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9751\n",
      "Epoch 00138: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0737 - acc: 0.9751 - val_loss: 0.1427 - val_acc: 0.9627\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9741\n",
      "Epoch 00139: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0750 - acc: 0.9741 - val_loss: 0.1525 - val_acc: 0.9630\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9755\n",
      "Epoch 00140: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0718 - acc: 0.9755 - val_loss: 0.1490 - val_acc: 0.9653\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9745\n",
      "Epoch 00141: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0737 - acc: 0.9745 - val_loss: 0.1489 - val_acc: 0.9676\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9740\n",
      "Epoch 00142: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0739 - acc: 0.9740 - val_loss: 0.1523 - val_acc: 0.9632\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9739\n",
      "Epoch 00143: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0772 - acc: 0.9739 - val_loss: 0.1488 - val_acc: 0.9688\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9736\n",
      "Epoch 00144: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0741 - acc: 0.9736 - val_loss: 0.1303 - val_acc: 0.9651\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9761\n",
      "Epoch 00145: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0695 - acc: 0.9761 - val_loss: 0.1466 - val_acc: 0.9639\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9754\n",
      "Epoch 00146: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0705 - acc: 0.9754 - val_loss: 0.1371 - val_acc: 0.9672\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9752\n",
      "Epoch 00147: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0739 - acc: 0.9752 - val_loss: 0.1428 - val_acc: 0.9632\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9768\n",
      "Epoch 00148: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0673 - acc: 0.9768 - val_loss: 0.1508 - val_acc: 0.9662\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9754\n",
      "Epoch 00149: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0724 - acc: 0.9754 - val_loss: 0.1625 - val_acc: 0.9665\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9765\n",
      "Epoch 00150: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0678 - acc: 0.9765 - val_loss: 0.1528 - val_acc: 0.9632\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9779\n",
      "Epoch 00151: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0667 - acc: 0.9779 - val_loss: 0.1599 - val_acc: 0.9639\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9776\n",
      "Epoch 00152: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0654 - acc: 0.9776 - val_loss: 0.1555 - val_acc: 0.9653\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9766\n",
      "Epoch 00153: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0680 - acc: 0.9766 - val_loss: 0.1546 - val_acc: 0.9641\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9785\n",
      "Epoch 00154: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0640 - acc: 0.9785 - val_loss: 0.1443 - val_acc: 0.9660\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9769\n",
      "Epoch 00155: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0646 - acc: 0.9769 - val_loss: 0.1566 - val_acc: 0.9658\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9760\n",
      "Epoch 00156: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0668 - acc: 0.9760 - val_loss: 0.1475 - val_acc: 0.9674\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9794\n",
      "Epoch 00157: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0601 - acc: 0.9794 - val_loss: 0.1666 - val_acc: 0.9634\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9776\n",
      "Epoch 00158: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0622 - acc: 0.9776 - val_loss: 0.1845 - val_acc: 0.9602\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9762\n",
      "Epoch 00159: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0689 - acc: 0.9762 - val_loss: 0.1610 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9780\n",
      "Epoch 00160: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0630 - acc: 0.9780 - val_loss: 0.1394 - val_acc: 0.9662\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9801\n",
      "Epoch 00161: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0577 - acc: 0.9801 - val_loss: 0.1589 - val_acc: 0.9634\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 579us/sample - loss: 0.1792 - acc: 0.9481\n",
      "Loss: 0.1792294374352799 Accuracy: 0.94807893\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4881 - acc: 0.1809\n",
      "Epoch 00001: val_loss improved from inf to 1.93513, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/001-1.9351.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 2.4880 - acc: 0.1810 - val_loss: 1.9351 - val_acc: 0.3958\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8247 - acc: 0.4126\n",
      "Epoch 00002: val_loss improved from 1.93513 to 1.39202, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/002-1.3920.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.8248 - acc: 0.4126 - val_loss: 1.3920 - val_acc: 0.5537\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4172 - acc: 0.5420\n",
      "Epoch 00003: val_loss improved from 1.39202 to 1.04666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/003-1.0467.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4171 - acc: 0.5420 - val_loss: 1.0467 - val_acc: 0.6753\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0815 - acc: 0.6478\n",
      "Epoch 00004: val_loss improved from 1.04666 to 0.78144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/004-0.7814.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0816 - acc: 0.6478 - val_loss: 0.7814 - val_acc: 0.7498\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9083 - acc: 0.7006\n",
      "Epoch 00005: val_loss improved from 0.78144 to 0.70530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/005-0.7053.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9082 - acc: 0.7006 - val_loss: 0.7053 - val_acc: 0.7701\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7293\n",
      "Epoch 00006: val_loss improved from 0.70530 to 0.62057, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/006-0.6206.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8228 - acc: 0.7293 - val_loss: 0.6206 - val_acc: 0.7997\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7514\n",
      "Epoch 00007: val_loss improved from 0.62057 to 0.57333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/007-0.5733.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7614 - acc: 0.7514 - val_loss: 0.5733 - val_acc: 0.8181\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.7680\n",
      "Epoch 00008: val_loss improved from 0.57333 to 0.52110, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/008-0.5211.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7074 - acc: 0.7680 - val_loss: 0.5211 - val_acc: 0.8344\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.7849\n",
      "Epoch 00009: val_loss improved from 0.52110 to 0.49237, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/009-0.4924.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6581 - acc: 0.7849 - val_loss: 0.4924 - val_acc: 0.8428\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.7958\n",
      "Epoch 00010: val_loss improved from 0.49237 to 0.46552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/010-0.4655.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6222 - acc: 0.7958 - val_loss: 0.4655 - val_acc: 0.8528\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.8083\n",
      "Epoch 00011: val_loss improved from 0.46552 to 0.46437, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/011-0.4644.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5866 - acc: 0.8084 - val_loss: 0.4644 - val_acc: 0.8586\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8177\n",
      "Epoch 00012: val_loss improved from 0.46437 to 0.40842, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/012-0.4084.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5621 - acc: 0.8177 - val_loss: 0.4084 - val_acc: 0.8751\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.8290\n",
      "Epoch 00013: val_loss did not improve from 0.40842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5293 - acc: 0.8289 - val_loss: 0.4846 - val_acc: 0.8444\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8353\n",
      "Epoch 00014: val_loss improved from 0.40842 to 0.38006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/014-0.3801.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5166 - acc: 0.8353 - val_loss: 0.3801 - val_acc: 0.8861\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8444\n",
      "Epoch 00015: val_loss improved from 0.38006 to 0.37328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/015-0.3733.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4796 - acc: 0.8444 - val_loss: 0.3733 - val_acc: 0.8861\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8502\n",
      "Epoch 00016: val_loss did not improve from 0.37328\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4596 - acc: 0.8502 - val_loss: 0.3865 - val_acc: 0.8758\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8594\n",
      "Epoch 00017: val_loss improved from 0.37328 to 0.32643, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/017-0.3264.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4419 - acc: 0.8594 - val_loss: 0.3264 - val_acc: 0.8973\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8636\n",
      "Epoch 00018: val_loss did not improve from 0.32643\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4279 - acc: 0.8636 - val_loss: 0.3414 - val_acc: 0.8882\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8689\n",
      "Epoch 00019: val_loss did not improve from 0.32643\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4091 - acc: 0.8688 - val_loss: 0.3278 - val_acc: 0.8954\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8713\n",
      "Epoch 00020: val_loss improved from 0.32643 to 0.31182, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/020-0.3118.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3955 - acc: 0.8713 - val_loss: 0.3118 - val_acc: 0.8994\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8779\n",
      "Epoch 00021: val_loss improved from 0.31182 to 0.30316, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/021-0.3032.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3782 - acc: 0.8778 - val_loss: 0.3032 - val_acc: 0.8998\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8817\n",
      "Epoch 00022: val_loss improved from 0.30316 to 0.28463, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/022-0.2846.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3611 - acc: 0.8818 - val_loss: 0.2846 - val_acc: 0.9078\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8864\n",
      "Epoch 00023: val_loss did not improve from 0.28463\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3473 - acc: 0.8864 - val_loss: 0.2869 - val_acc: 0.9110\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8907\n",
      "Epoch 00024: val_loss improved from 0.28463 to 0.28361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/024-0.2836.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3358 - acc: 0.8907 - val_loss: 0.2836 - val_acc: 0.9094\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8926\n",
      "Epoch 00025: val_loss improved from 0.28361 to 0.27006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/025-0.2701.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3288 - acc: 0.8926 - val_loss: 0.2701 - val_acc: 0.9147\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8954\n",
      "Epoch 00026: val_loss improved from 0.27006 to 0.26750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/026-0.2675.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3193 - acc: 0.8954 - val_loss: 0.2675 - val_acc: 0.9143\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8992\n",
      "Epoch 00027: val_loss improved from 0.26750 to 0.26065, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/027-0.2607.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3102 - acc: 0.8993 - val_loss: 0.2607 - val_acc: 0.9159\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9022\n",
      "Epoch 00028: val_loss did not improve from 0.26065\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2987 - acc: 0.9022 - val_loss: 0.2729 - val_acc: 0.9113\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9031\n",
      "Epoch 00029: val_loss improved from 0.26065 to 0.25808, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/029-0.2581.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2977 - acc: 0.9031 - val_loss: 0.2581 - val_acc: 0.9203\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9074\n",
      "Epoch 00030: val_loss improved from 0.25808 to 0.24310, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/030-0.2431.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2807 - acc: 0.9075 - val_loss: 0.2431 - val_acc: 0.9220\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9091\n",
      "Epoch 00031: val_loss improved from 0.24310 to 0.23262, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/031-0.2326.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2751 - acc: 0.9091 - val_loss: 0.2326 - val_acc: 0.9255\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9151\n",
      "Epoch 00032: val_loss did not improve from 0.23262\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2620 - acc: 0.9151 - val_loss: 0.2528 - val_acc: 0.9222\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9170\n",
      "Epoch 00033: val_loss improved from 0.23262 to 0.22504, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/033-0.2250.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2557 - acc: 0.9170 - val_loss: 0.2250 - val_acc: 0.9276\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9183\n",
      "Epoch 00034: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2478 - acc: 0.9183 - val_loss: 0.2324 - val_acc: 0.9273\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9196\n",
      "Epoch 00035: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2444 - acc: 0.9196 - val_loss: 0.2278 - val_acc: 0.9273\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9220\n",
      "Epoch 00036: val_loss improved from 0.22504 to 0.21800, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/036-0.2180.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2379 - acc: 0.9220 - val_loss: 0.2180 - val_acc: 0.9317\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9235\n",
      "Epoch 00037: val_loss did not improve from 0.21800\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2292 - acc: 0.9235 - val_loss: 0.2562 - val_acc: 0.9217\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9266\n",
      "Epoch 00038: val_loss did not improve from 0.21800\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2227 - acc: 0.9266 - val_loss: 0.2215 - val_acc: 0.9285\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9269\n",
      "Epoch 00039: val_loss improved from 0.21800 to 0.21116, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/039-0.2112.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2226 - acc: 0.9269 - val_loss: 0.2112 - val_acc: 0.9364\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9310\n",
      "Epoch 00040: val_loss improved from 0.21116 to 0.20244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/040-0.2024.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2089 - acc: 0.9310 - val_loss: 0.2024 - val_acc: 0.9341\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9325\n",
      "Epoch 00041: val_loss did not improve from 0.20244\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1984 - acc: 0.9324 - val_loss: 0.2334 - val_acc: 0.9269\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9303\n",
      "Epoch 00042: val_loss did not improve from 0.20244\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2038 - acc: 0.9303 - val_loss: 0.2149 - val_acc: 0.9364\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9349\n",
      "Epoch 00043: val_loss did not improve from 0.20244\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1973 - acc: 0.9349 - val_loss: 0.2047 - val_acc: 0.9357\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9369\n",
      "Epoch 00044: val_loss improved from 0.20244 to 0.20035, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/044-0.2003.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1897 - acc: 0.9369 - val_loss: 0.2003 - val_acc: 0.9364\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9380\n",
      "Epoch 00045: val_loss did not improve from 0.20035\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1869 - acc: 0.9380 - val_loss: 0.2032 - val_acc: 0.9383\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9404\n",
      "Epoch 00046: val_loss did not improve from 0.20035\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1795 - acc: 0.9404 - val_loss: 0.2090 - val_acc: 0.9343\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9414\n",
      "Epoch 00047: val_loss did not improve from 0.20035\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1762 - acc: 0.9414 - val_loss: 0.2068 - val_acc: 0.9343\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9433\n",
      "Epoch 00048: val_loss did not improve from 0.20035\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1677 - acc: 0.9433 - val_loss: 0.2172 - val_acc: 0.9338\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9430\n",
      "Epoch 00049: val_loss did not improve from 0.20035\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1672 - acc: 0.9429 - val_loss: 0.2045 - val_acc: 0.9373\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9436\n",
      "Epoch 00050: val_loss improved from 0.20035 to 0.19942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/050-0.1994.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1682 - acc: 0.9436 - val_loss: 0.1994 - val_acc: 0.9380\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9472\n",
      "Epoch 00051: val_loss did not improve from 0.19942\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1575 - acc: 0.9472 - val_loss: 0.2119 - val_acc: 0.9362\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9479\n",
      "Epoch 00052: val_loss improved from 0.19942 to 0.19887, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/052-0.1989.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1572 - acc: 0.9479 - val_loss: 0.1989 - val_acc: 0.9394\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9474\n",
      "Epoch 00053: val_loss improved from 0.19887 to 0.19389, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/053-0.1939.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1525 - acc: 0.9475 - val_loss: 0.1939 - val_acc: 0.9446\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9513\n",
      "Epoch 00054: val_loss improved from 0.19389 to 0.18918, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/054-0.1892.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1470 - acc: 0.9513 - val_loss: 0.1892 - val_acc: 0.9434\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9508\n",
      "Epoch 00055: val_loss did not improve from 0.18918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1443 - acc: 0.9508 - val_loss: 0.1980 - val_acc: 0.9394\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9525\n",
      "Epoch 00056: val_loss did not improve from 0.18918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1398 - acc: 0.9525 - val_loss: 0.1914 - val_acc: 0.9439\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9534\n",
      "Epoch 00057: val_loss improved from 0.18918 to 0.18700, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/057-0.1870.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1367 - acc: 0.9534 - val_loss: 0.1870 - val_acc: 0.9427\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9539\n",
      "Epoch 00058: val_loss improved from 0.18700 to 0.18483, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/058-0.1848.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1349 - acc: 0.9539 - val_loss: 0.1848 - val_acc: 0.9441\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9561\n",
      "Epoch 00059: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1306 - acc: 0.9561 - val_loss: 0.2013 - val_acc: 0.9422\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9561\n",
      "Epoch 00060: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1279 - acc: 0.9561 - val_loss: 0.1908 - val_acc: 0.9427\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9583\n",
      "Epoch 00061: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1210 - acc: 0.9583 - val_loss: 0.1989 - val_acc: 0.9443\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9589\n",
      "Epoch 00062: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1215 - acc: 0.9589 - val_loss: 0.2027 - val_acc: 0.9450\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9592\n",
      "Epoch 00063: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1193 - acc: 0.9592 - val_loss: 0.1997 - val_acc: 0.9462\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9596\n",
      "Epoch 00064: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1179 - acc: 0.9597 - val_loss: 0.1873 - val_acc: 0.9474\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9616\n",
      "Epoch 00065: val_loss improved from 0.18483 to 0.17801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/065-0.1780.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1097 - acc: 0.9616 - val_loss: 0.1780 - val_acc: 0.9485\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9623\n",
      "Epoch 00066: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1117 - acc: 0.9623 - val_loss: 0.1898 - val_acc: 0.9474\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9630\n",
      "Epoch 00067: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1080 - acc: 0.9630 - val_loss: 0.1902 - val_acc: 0.9422\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9628\n",
      "Epoch 00068: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1090 - acc: 0.9628 - val_loss: 0.1795 - val_acc: 0.9513\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1041 - acc: 0.9650 - val_loss: 0.1822 - val_acc: 0.9495\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9635\n",
      "Epoch 00070: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1050 - acc: 0.9635 - val_loss: 0.2008 - val_acc: 0.9476\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9654\n",
      "Epoch 00071: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1015 - acc: 0.9654 - val_loss: 0.1943 - val_acc: 0.9488\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9677\n",
      "Epoch 00072: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0942 - acc: 0.9677 - val_loss: 0.1958 - val_acc: 0.9485\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9673\n",
      "Epoch 00073: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0938 - acc: 0.9673 - val_loss: 0.1867 - val_acc: 0.9513\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9673\n",
      "Epoch 00074: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0945 - acc: 0.9673 - val_loss: 0.1793 - val_acc: 0.9518\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9677\n",
      "Epoch 00075: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0927 - acc: 0.9677 - val_loss: 0.1998 - val_acc: 0.9497\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9681\n",
      "Epoch 00076: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0920 - acc: 0.9681 - val_loss: 0.2024 - val_acc: 0.9497\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9691\n",
      "Epoch 00077: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0886 - acc: 0.9691 - val_loss: 0.1898 - val_acc: 0.9481\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9699\n",
      "Epoch 00078: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0870 - acc: 0.9699 - val_loss: 0.2023 - val_acc: 0.9506\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9693\n",
      "Epoch 00079: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0882 - acc: 0.9693 - val_loss: 0.1820 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9714\n",
      "Epoch 00080: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0820 - acc: 0.9714 - val_loss: 0.2123 - val_acc: 0.9515\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9709\n",
      "Epoch 00081: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0833 - acc: 0.9709 - val_loss: 0.1879 - val_acc: 0.9520\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9720\n",
      "Epoch 00082: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0810 - acc: 0.9720 - val_loss: 0.1902 - val_acc: 0.9525\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9721\n",
      "Epoch 00083: val_loss improved from 0.17801 to 0.17635, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv_checkpoint/083-0.1763.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0800 - acc: 0.9721 - val_loss: 0.1763 - val_acc: 0.9525\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9735\n",
      "Epoch 00084: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0769 - acc: 0.9735 - val_loss: 0.2101 - val_acc: 0.9495\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9714\n",
      "Epoch 00085: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0843 - acc: 0.9714 - val_loss: 0.1887 - val_acc: 0.9529\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9745\n",
      "Epoch 00086: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0718 - acc: 0.9745 - val_loss: 0.1818 - val_acc: 0.9543\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9743\n",
      "Epoch 00087: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0722 - acc: 0.9743 - val_loss: 0.1990 - val_acc: 0.9513\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9737\n",
      "Epoch 00088: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0737 - acc: 0.9737 - val_loss: 0.2171 - val_acc: 0.9518\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9761\n",
      "Epoch 00089: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0695 - acc: 0.9761 - val_loss: 0.1866 - val_acc: 0.9543\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9764\n",
      "Epoch 00090: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0696 - acc: 0.9764 - val_loss: 0.1977 - val_acc: 0.9527\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9756\n",
      "Epoch 00091: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0699 - acc: 0.9756 - val_loss: 0.2066 - val_acc: 0.9499\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9767\n",
      "Epoch 00092: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0678 - acc: 0.9767 - val_loss: 0.1911 - val_acc: 0.9520\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9774\n",
      "Epoch 00093: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0660 - acc: 0.9774 - val_loss: 0.1897 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9774\n",
      "Epoch 00094: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0644 - acc: 0.9774 - val_loss: 0.2115 - val_acc: 0.9518\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9752\n",
      "Epoch 00095: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0684 - acc: 0.9752 - val_loss: 0.1933 - val_acc: 0.9546\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9789\n",
      "Epoch 00096: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0610 - acc: 0.9789 - val_loss: 0.2072 - val_acc: 0.9546\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9770\n",
      "Epoch 00097: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0680 - acc: 0.9770 - val_loss: 0.1792 - val_acc: 0.9562\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9790\n",
      "Epoch 00098: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0603 - acc: 0.9790 - val_loss: 0.2371 - val_acc: 0.9511\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9790\n",
      "Epoch 00099: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0606 - acc: 0.9790 - val_loss: 0.1969 - val_acc: 0.9548\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9798\n",
      "Epoch 00100: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0569 - acc: 0.9798 - val_loss: 0.1995 - val_acc: 0.9567\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9774\n",
      "Epoch 00101: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0647 - acc: 0.9774 - val_loss: 0.2041 - val_acc: 0.9560\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9802\n",
      "Epoch 00102: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0547 - acc: 0.9802 - val_loss: 0.1922 - val_acc: 0.9581\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9801\n",
      "Epoch 00103: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0568 - acc: 0.9801 - val_loss: 0.1914 - val_acc: 0.9546\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9793\n",
      "Epoch 00104: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0582 - acc: 0.9794 - val_loss: 0.1909 - val_acc: 0.9569\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9787\n",
      "Epoch 00105: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0602 - acc: 0.9788 - val_loss: 0.1907 - val_acc: 0.9557\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9799\n",
      "Epoch 00106: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0570 - acc: 0.9799 - val_loss: 0.1881 - val_acc: 0.9562\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9811\n",
      "Epoch 00107: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0547 - acc: 0.9811 - val_loss: 0.1916 - val_acc: 0.9588\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9805\n",
      "Epoch 00108: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0568 - acc: 0.9805 - val_loss: 0.1872 - val_acc: 0.9595\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9813\n",
      "Epoch 00109: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0533 - acc: 0.9813 - val_loss: 0.1938 - val_acc: 0.9557\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9809\n",
      "Epoch 00110: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0546 - acc: 0.9809 - val_loss: 0.1958 - val_acc: 0.9567\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9827\n",
      "Epoch 00111: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0511 - acc: 0.9826 - val_loss: 0.1796 - val_acc: 0.9578\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9805\n",
      "Epoch 00112: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0572 - acc: 0.9805 - val_loss: 0.1956 - val_acc: 0.9555\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9830\n",
      "Epoch 00113: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0487 - acc: 0.9830 - val_loss: 0.2031 - val_acc: 0.9578\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9814\n",
      "Epoch 00114: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0542 - acc: 0.9814 - val_loss: 0.2082 - val_acc: 0.9532\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9822\n",
      "Epoch 00115: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0535 - acc: 0.9822 - val_loss: 0.1994 - val_acc: 0.9576\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9836\n",
      "Epoch 00116: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0465 - acc: 0.9836 - val_loss: 0.1891 - val_acc: 0.9571\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9844\n",
      "Epoch 00117: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0442 - acc: 0.9844 - val_loss: 0.2051 - val_acc: 0.9569\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9826\n",
      "Epoch 00118: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0515 - acc: 0.9826 - val_loss: 0.2186 - val_acc: 0.9546\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9819\n",
      "Epoch 00119: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0516 - acc: 0.9819 - val_loss: 0.1983 - val_acc: 0.9585\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9846\n",
      "Epoch 00120: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0451 - acc: 0.9846 - val_loss: 0.1794 - val_acc: 0.9597\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9845\n",
      "Epoch 00121: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0464 - acc: 0.9845 - val_loss: 0.2214 - val_acc: 0.9564\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9849\n",
      "Epoch 00122: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0439 - acc: 0.9849 - val_loss: 0.2236 - val_acc: 0.9548\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9840\n",
      "Epoch 00123: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0460 - acc: 0.9840 - val_loss: 0.2044 - val_acc: 0.9590\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9842\n",
      "Epoch 00124: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0461 - acc: 0.9842 - val_loss: 0.2375 - val_acc: 0.9550\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9848\n",
      "Epoch 00125: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0438 - acc: 0.9848 - val_loss: 0.1926 - val_acc: 0.9590\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9851\n",
      "Epoch 00126: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0447 - acc: 0.9851 - val_loss: 0.1956 - val_acc: 0.9597\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9838\n",
      "Epoch 00127: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0465 - acc: 0.9838 - val_loss: 0.2149 - val_acc: 0.9564\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9861\n",
      "Epoch 00128: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0414 - acc: 0.9861 - val_loss: 0.1934 - val_acc: 0.9602\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9860\n",
      "Epoch 00129: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0405 - acc: 0.9860 - val_loss: 0.2330 - val_acc: 0.9567\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9849\n",
      "Epoch 00130: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0436 - acc: 0.9849 - val_loss: 0.2037 - val_acc: 0.9604\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9860\n",
      "Epoch 00131: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0401 - acc: 0.9860 - val_loss: 0.2101 - val_acc: 0.9564\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9851\n",
      "Epoch 00132: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0438 - acc: 0.9851 - val_loss: 0.2303 - val_acc: 0.9583\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9864\n",
      "Epoch 00133: val_loss did not improve from 0.17635\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0389 - acc: 0.9864 - val_loss: 0.2273 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 610us/sample - loss: 0.2719 - acc: 0.9302\n",
      "Loss: 0.27185838637972176 Accuracy: 0.93021804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO'\n",
    "    \n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 743us/sample - loss: 1.1753 - acc: 0.6438\n",
      "Loss: 1.1752867400212947 Accuracy: 0.6438214\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 512us/sample - loss: 0.8568 - acc: 0.7601\n",
      "Loss: 0.8567534625963507 Accuracy: 0.7601246\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 541us/sample - loss: 0.4141 - acc: 0.8910\n",
      "Loss: 0.4140792941131077 Accuracy: 0.89096576\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 523us/sample - loss: 0.1959 - acc: 0.9456\n",
      "Loss: 0.1959358832523273 Accuracy: 0.9455867\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 535us/sample - loss: 0.1792 - acc: 0.9481\n",
      "Loss: 0.1792294374352799 Accuracy: 0.94807893\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 602us/sample - loss: 0.2719 - acc: 0.9302\n",
      "Loss: 0.27185838637972176 Accuracy: 0.93021804\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 579us/sample - loss: 1.4369 - acc: 0.6924\n",
      "Loss: 1.4368617728739388 Accuracy: 0.6924195\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 600us/sample - loss: 1.0402 - acc: 0.7884\n",
      "Loss: 1.040188212929485 Accuracy: 0.78836966\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 629us/sample - loss: 0.4269 - acc: 0.9022\n",
      "Loss: 0.4269444535918696 Accuracy: 0.9021807\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 660us/sample - loss: 0.2256 - acc: 0.9485\n",
      "Loss: 0.22561302653650211 Accuracy: 0.9484943\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 661us/sample - loss: 0.2002 - acc: 0.9483\n",
      "Loss: 0.20021873861270825 Accuracy: 0.9482866\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 675us/sample - loss: 0.3246 - acc: 0.9389\n",
      "Loss: 0.32459399479568035 Accuracy: 0.9389408\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
