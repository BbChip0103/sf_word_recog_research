{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 512000)            2048000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 10,278,352\n",
      "Trainable params: 9,253,840\n",
      "Non-trainable params: 1,024,512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 256000)            1024000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 5,183,568\n",
      "Trainable params: 4,670,800\n",
      "Non-trainable params: 512,768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,648,784\n",
      "Trainable params: 2,391,760\n",
      "Non-trainable params: 257,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,723,792\n",
      "Trainable params: 2,466,256\n",
      "Non-trainable params: 257,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,543,376\n",
      "Trainable params: 1,413,328\n",
      "Non-trainable params: 130,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 1,002,960\n",
      "Trainable params: 936,400\n",
      "Non-trainable params: 66,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 782,544\n",
      "Trainable params: 747,472\n",
      "Non-trainable params: 35,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 15872)             63488     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,077,456\n",
      "Trainable params: 1,041,616\n",
      "Non-trainable params: 35,840\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,314,512\n",
      "Trainable params: 1,293,520\n",
      "Non-trainable params: 20,992\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 3840)              15360     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,628,368\n",
      "Trainable params: 1,614,544\n",
      "Non-trainable params: 13,824\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 1792)              7168      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,983,184\n",
      "Trainable params: 1,972,432\n",
      "Non-trainable params: 10,752\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,162,832\n",
      "Trainable params: 3,150,544\n",
      "Non-trainable params: 12,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8037 - acc: 0.4039\n",
      "Epoch 00001: val_loss improved from inf to 2.35966, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_3_conv_checkpoint/001-2.3597.hdf5\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 2.8041 - acc: 0.4039 - val_loss: 2.3597 - val_acc: 0.3960\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7441 - acc: 0.8249\n",
      "Epoch 00002: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.7442 - acc: 0.8249 - val_loss: 2.5809 - val_acc: 0.4428\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9468\n",
      "Epoch 00003: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2740 - acc: 0.9468 - val_loss: 2.5112 - val_acc: 0.4934\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9740\n",
      "Epoch 00004: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1657 - acc: 0.9739 - val_loss: 2.5534 - val_acc: 0.4887\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9630\n",
      "Epoch 00005: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1894 - acc: 0.9630 - val_loss: 2.9865 - val_acc: 0.4507\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9755\n",
      "Epoch 00006: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1320 - acc: 0.9755 - val_loss: 2.8404 - val_acc: 0.4955\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9758\n",
      "Epoch 00007: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1316 - acc: 0.9757 - val_loss: 3.1242 - val_acc: 0.4594\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9788\n",
      "Epoch 00008: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1116 - acc: 0.9788 - val_loss: 3.6771 - val_acc: 0.4610\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9777\n",
      "Epoch 00009: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1163 - acc: 0.9777 - val_loss: 3.7386 - val_acc: 0.4591\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9690\n",
      "Epoch 00010: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1516 - acc: 0.9689 - val_loss: 4.3723 - val_acc: 0.4358\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9663\n",
      "Epoch 00011: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1573 - acc: 0.9662 - val_loss: 4.0695 - val_acc: 0.4491\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9796\n",
      "Epoch 00012: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1010 - acc: 0.9796 - val_loss: 3.8683 - val_acc: 0.4729\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9788\n",
      "Epoch 00013: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1120 - acc: 0.9787 - val_loss: 4.3754 - val_acc: 0.4382\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9706\n",
      "Epoch 00014: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1420 - acc: 0.9706 - val_loss: 4.3230 - val_acc: 0.4517\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9783\n",
      "Epoch 00015: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1099 - acc: 0.9783 - val_loss: 3.9117 - val_acc: 0.4892\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9806\n",
      "Epoch 00016: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1067 - acc: 0.9805 - val_loss: 4.8823 - val_acc: 0.4375\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9781\n",
      "Epoch 00017: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1148 - acc: 0.9781 - val_loss: 4.8466 - val_acc: 0.4542\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9821\n",
      "Epoch 00018: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0969 - acc: 0.9821 - val_loss: 4.6467 - val_acc: 0.4633\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9847\n",
      "Epoch 00019: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0860 - acc: 0.9847 - val_loss: 4.7082 - val_acc: 0.4642\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9854\n",
      "Epoch 00020: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0880 - acc: 0.9854 - val_loss: 4.6187 - val_acc: 0.4705\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9865\n",
      "Epoch 00021: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0812 - acc: 0.9864 - val_loss: 4.8830 - val_acc: 0.4540\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9833\n",
      "Epoch 00022: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0942 - acc: 0.9833 - val_loss: 4.8553 - val_acc: 0.4675\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9867\n",
      "Epoch 00023: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0853 - acc: 0.9867 - val_loss: 5.3308 - val_acc: 0.4563\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9830\n",
      "Epoch 00024: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0985 - acc: 0.9830 - val_loss: 5.1385 - val_acc: 0.4449\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9791\n",
      "Epoch 00025: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1229 - acc: 0.9791 - val_loss: 5.2015 - val_acc: 0.4698\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9841\n",
      "Epoch 00026: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0931 - acc: 0.9841 - val_loss: 5.2150 - val_acc: 0.4747\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9873\n",
      "Epoch 00027: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0819 - acc: 0.9873 - val_loss: 5.2328 - val_acc: 0.4736\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9849\n",
      "Epoch 00028: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0878 - acc: 0.9849 - val_loss: 5.2878 - val_acc: 0.4752\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9867\n",
      "Epoch 00029: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0820 - acc: 0.9867 - val_loss: 5.2358 - val_acc: 0.4738\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9883\n",
      "Epoch 00030: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0750 - acc: 0.9883 - val_loss: 5.2165 - val_acc: 0.4845\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9865\n",
      "Epoch 00031: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0853 - acc: 0.9865 - val_loss: 5.5023 - val_acc: 0.4705\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9872\n",
      "Epoch 00032: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0814 - acc: 0.9872 - val_loss: 5.4905 - val_acc: 0.4708\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9879\n",
      "Epoch 00033: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0767 - acc: 0.9879 - val_loss: 5.2837 - val_acc: 0.4771\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9839\n",
      "Epoch 00034: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0977 - acc: 0.9839 - val_loss: 5.4300 - val_acc: 0.4771\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9889\n",
      "Epoch 00035: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0715 - acc: 0.9889 - val_loss: 5.3555 - val_acc: 0.4808\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9895\n",
      "Epoch 00036: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0758 - acc: 0.9895 - val_loss: 5.3020 - val_acc: 0.4994\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9896\n",
      "Epoch 00037: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0680 - acc: 0.9896 - val_loss: 5.8469 - val_acc: 0.4559\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9908\n",
      "Epoch 00038: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0614 - acc: 0.9908 - val_loss: 5.7221 - val_acc: 0.4708\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9883\n",
      "Epoch 00039: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0775 - acc: 0.9882 - val_loss: 5.8338 - val_acc: 0.4621\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9860\n",
      "Epoch 00040: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0914 - acc: 0.9860 - val_loss: 5.6942 - val_acc: 0.4663\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9852\n",
      "Epoch 00041: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0911 - acc: 0.9852 - val_loss: 5.8694 - val_acc: 0.4624\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9903\n",
      "Epoch 00042: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0671 - acc: 0.9903 - val_loss: 5.7589 - val_acc: 0.4750\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9923\n",
      "Epoch 00043: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0560 - acc: 0.9922 - val_loss: 5.6574 - val_acc: 0.4768\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9851\n",
      "Epoch 00044: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0926 - acc: 0.9851 - val_loss: 5.4402 - val_acc: 0.4934\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9893\n",
      "Epoch 00045: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0719 - acc: 0.9893 - val_loss: 5.4525 - val_acc: 0.4885\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9914\n",
      "Epoch 00046: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0591 - acc: 0.9914 - val_loss: 5.7938 - val_acc: 0.4826\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9892\n",
      "Epoch 00047: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0776 - acc: 0.9891 - val_loss: 5.4934 - val_acc: 0.4931\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9898\n",
      "Epoch 00048: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0776 - acc: 0.9897 - val_loss: 5.5684 - val_acc: 0.5003\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9881\n",
      "Epoch 00049: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0846 - acc: 0.9881 - val_loss: 5.5882 - val_acc: 0.4868\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9898\n",
      "Epoch 00050: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0747 - acc: 0.9897 - val_loss: 5.7619 - val_acc: 0.4812\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9871\n",
      "Epoch 00051: val_loss did not improve from 2.35966\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0856 - acc: 0.9871 - val_loss: 5.8824 - val_acc: 0.4754\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 2.3640 - acc: 0.3902\n",
      "Loss: 2.3640441237951735 Accuracy: 0.39023885\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3123 - acc: 0.4218\n",
      "Epoch 00001: val_loss improved from inf to 2.58423, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv_checkpoint/001-2.5842.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 2.3124 - acc: 0.4218 - val_loss: 2.5842 - val_acc: 0.3580\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8621 - acc: 0.7552\n",
      "Epoch 00002: val_loss improved from 2.58423 to 1.97691, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv_checkpoint/002-1.9769.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.8622 - acc: 0.7551 - val_loss: 1.9769 - val_acc: 0.5127\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9198\n",
      "Epoch 00003: val_loss improved from 1.97691 to 1.92384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv_checkpoint/003-1.9238.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3135 - acc: 0.9197 - val_loss: 1.9238 - val_acc: 0.5511\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9694\n",
      "Epoch 00004: val_loss improved from 1.92384 to 1.91319, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv_checkpoint/004-1.9132.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1569 - acc: 0.9694 - val_loss: 1.9132 - val_acc: 0.5721\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9843\n",
      "Epoch 00005: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0996 - acc: 0.9843 - val_loss: 1.9276 - val_acc: 0.5628\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9850\n",
      "Epoch 00006: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0902 - acc: 0.9849 - val_loss: 2.1573 - val_acc: 0.5581\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9764\n",
      "Epoch 00007: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1114 - acc: 0.9763 - val_loss: 2.4075 - val_acc: 0.5327\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9689\n",
      "Epoch 00008: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1257 - acc: 0.9689 - val_loss: 2.4350 - val_acc: 0.5320\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9871\n",
      "Epoch 00009: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0683 - acc: 0.9871 - val_loss: 2.4659 - val_acc: 0.5521\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9899\n",
      "Epoch 00010: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0541 - acc: 0.9899 - val_loss: 2.8286 - val_acc: 0.5048\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9822\n",
      "Epoch 00011: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0824 - acc: 0.9821 - val_loss: 2.8386 - val_acc: 0.5213\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9729\n",
      "Epoch 00012: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1043 - acc: 0.9728 - val_loss: 2.7936 - val_acc: 0.5374\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9774\n",
      "Epoch 00013: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0928 - acc: 0.9774 - val_loss: 2.7213 - val_acc: 0.5458\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9893\n",
      "Epoch 00014: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0562 - acc: 0.9893 - val_loss: 2.6658 - val_acc: 0.5516\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9901\n",
      "Epoch 00015: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0500 - acc: 0.9901 - val_loss: 2.9215 - val_acc: 0.5376\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9829\n",
      "Epoch 00016: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0722 - acc: 0.9829 - val_loss: 3.1860 - val_acc: 0.5264\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9873\n",
      "Epoch 00017: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0589 - acc: 0.9873 - val_loss: 3.0588 - val_acc: 0.5420\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9816\n",
      "Epoch 00018: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0766 - acc: 0.9816 - val_loss: 3.0330 - val_acc: 0.5323\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9910\n",
      "Epoch 00019: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0443 - acc: 0.9910 - val_loss: 3.2860 - val_acc: 0.5176\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9868\n",
      "Epoch 00020: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0602 - acc: 0.9867 - val_loss: 3.3415 - val_acc: 0.5346\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9818\n",
      "Epoch 00021: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0735 - acc: 0.9818 - val_loss: 3.3512 - val_acc: 0.5344\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9880\n",
      "Epoch 00022: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0547 - acc: 0.9880 - val_loss: 3.3493 - val_acc: 0.5427\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9901\n",
      "Epoch 00023: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0489 - acc: 0.9901 - val_loss: 3.2928 - val_acc: 0.5425\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9935\n",
      "Epoch 00024: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0367 - acc: 0.9934 - val_loss: 3.4875 - val_acc: 0.5397\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9839\n",
      "Epoch 00025: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0697 - acc: 0.9839 - val_loss: 3.3778 - val_acc: 0.5360\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9888\n",
      "Epoch 00026: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0555 - acc: 0.9888 - val_loss: 3.6961 - val_acc: 0.5264\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9866\n",
      "Epoch 00027: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0641 - acc: 0.9866 - val_loss: 4.1053 - val_acc: 0.4990\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9867\n",
      "Epoch 00028: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0648 - acc: 0.9866 - val_loss: 3.4990 - val_acc: 0.5528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9902\n",
      "Epoch 00029: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0481 - acc: 0.9901 - val_loss: 3.5750 - val_acc: 0.5444\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9899\n",
      "Epoch 00030: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0531 - acc: 0.9898 - val_loss: 3.6216 - val_acc: 0.5493\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9930\n",
      "Epoch 00031: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0384 - acc: 0.9930 - val_loss: 3.8347 - val_acc: 0.5372\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9925\n",
      "Epoch 00032: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0418 - acc: 0.9925 - val_loss: 3.5822 - val_acc: 0.5523\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9902\n",
      "Epoch 00033: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0490 - acc: 0.9902 - val_loss: 3.7228 - val_acc: 0.5516\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9942\n",
      "Epoch 00034: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0366 - acc: 0.9941 - val_loss: 3.5898 - val_acc: 0.5684\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9889\n",
      "Epoch 00035: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0524 - acc: 0.9888 - val_loss: 3.7486 - val_acc: 0.5570\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9895\n",
      "Epoch 00036: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0548 - acc: 0.9895 - val_loss: 3.8824 - val_acc: 0.5425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9881\n",
      "Epoch 00037: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0604 - acc: 0.9881 - val_loss: 3.7812 - val_acc: 0.5621\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9935\n",
      "Epoch 00038: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0418 - acc: 0.9935 - val_loss: 3.8190 - val_acc: 0.5474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9930\n",
      "Epoch 00039: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0411 - acc: 0.9930 - val_loss: 4.1984 - val_acc: 0.5316\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9900\n",
      "Epoch 00040: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0530 - acc: 0.9900 - val_loss: 3.9036 - val_acc: 0.5467\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9921\n",
      "Epoch 00041: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0443 - acc: 0.9921 - val_loss: 4.2433 - val_acc: 0.5353\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9926\n",
      "Epoch 00042: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0436 - acc: 0.9926 - val_loss: 3.8597 - val_acc: 0.5695\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0480 - acc: 0.9917 - val_loss: 4.3510 - val_acc: 0.5127\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9893\n",
      "Epoch 00044: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0576 - acc: 0.9893 - val_loss: 4.1340 - val_acc: 0.5462\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9911\n",
      "Epoch 00045: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0486 - acc: 0.9911 - val_loss: 3.9147 - val_acc: 0.5574\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9926\n",
      "Epoch 00046: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0437 - acc: 0.9926 - val_loss: 4.0369 - val_acc: 0.5553\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9937\n",
      "Epoch 00047: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0373 - acc: 0.9937 - val_loss: 3.9638 - val_acc: 0.5551\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9913\n",
      "Epoch 00048: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0509 - acc: 0.9912 - val_loss: 4.0490 - val_acc: 0.5544\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9877\n",
      "Epoch 00049: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0623 - acc: 0.9877 - val_loss: 4.2518 - val_acc: 0.5446\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9905\n",
      "Epoch 00050: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0548 - acc: 0.9905 - val_loss: 4.1613 - val_acc: 0.5439\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0558 - acc: 0.9908 - val_loss: 4.1765 - val_acc: 0.5628\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9936\n",
      "Epoch 00052: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0415 - acc: 0.9936 - val_loss: 4.0600 - val_acc: 0.5660\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9913\n",
      "Epoch 00053: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0471 - acc: 0.9913 - val_loss: 4.1040 - val_acc: 0.5595\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9945\n",
      "Epoch 00054: val_loss did not improve from 1.91319\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0361 - acc: 0.9945 - val_loss: 4.1875 - val_acc: 0.5625\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 2.0532 - acc: 0.5306\n",
      "Loss: 2.0531809329243833 Accuracy: 0.53063345\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2051 - acc: 0.4428\n",
      "Epoch 00001: val_loss improved from inf to 1.86560, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv_checkpoint/001-1.8656.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 2.2048 - acc: 0.4429 - val_loss: 1.8656 - val_acc: 0.4547\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9057 - acc: 0.7475\n",
      "Epoch 00002: val_loss did not improve from 1.86560\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.9061 - acc: 0.7475 - val_loss: 2.0336 - val_acc: 0.5215\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8987\n",
      "Epoch 00003: val_loss improved from 1.86560 to 1.78576, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv_checkpoint/003-1.7858.hdf5\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.3663 - acc: 0.8987 - val_loss: 1.7858 - val_acc: 0.5751\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9624\n",
      "Epoch 00004: val_loss improved from 1.78576 to 1.68659, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv_checkpoint/004-1.6866.hdf5\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1690 - acc: 0.9623 - val_loss: 1.6866 - val_acc: 0.6254\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9730\n",
      "Epoch 00005: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1256 - acc: 0.9730 - val_loss: 1.8085 - val_acc: 0.6157\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9774\n",
      "Epoch 00006: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1062 - acc: 0.9774 - val_loss: 1.8337 - val_acc: 0.6133\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9849\n",
      "Epoch 00007: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0799 - acc: 0.9848 - val_loss: 1.8165 - val_acc: 0.6217\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9732\n",
      "Epoch 00008: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1109 - acc: 0.9732 - val_loss: 2.1140 - val_acc: 0.5779\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9758\n",
      "Epoch 00009: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1061 - acc: 0.9757 - val_loss: 2.0518 - val_acc: 0.6073\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9686\n",
      "Epoch 00010: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1173 - acc: 0.9686 - val_loss: 2.0727 - val_acc: 0.6038\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9709\n",
      "Epoch 00011: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1091 - acc: 0.9708 - val_loss: 2.1188 - val_acc: 0.6210\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9793\n",
      "Epoch 00012: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0883 - acc: 0.9793 - val_loss: 2.3718 - val_acc: 0.6019\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9732\n",
      "Epoch 00013: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.1067 - acc: 0.9732 - val_loss: 2.4558 - val_acc: 0.6075\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9783\n",
      "Epoch 00014: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0892 - acc: 0.9783 - val_loss: 2.3786 - val_acc: 0.6168\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9873\n",
      "Epoch 00015: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0573 - acc: 0.9873 - val_loss: 2.4035 - val_acc: 0.6271\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9802\n",
      "Epoch 00016: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0807 - acc: 0.9802 - val_loss: 2.6191 - val_acc: 0.6075\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9799\n",
      "Epoch 00017: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0829 - acc: 0.9798 - val_loss: 2.5341 - val_acc: 0.6182\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9819\n",
      "Epoch 00018: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0781 - acc: 0.9819 - val_loss: 2.4778 - val_acc: 0.6261\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9863\n",
      "Epoch 00019: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0615 - acc: 0.9863 - val_loss: 2.4821 - val_acc: 0.6355\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9882\n",
      "Epoch 00020: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0529 - acc: 0.9882 - val_loss: 2.4817 - val_acc: 0.6373\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9840\n",
      "Epoch 00021: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0685 - acc: 0.9839 - val_loss: 3.0034 - val_acc: 0.5961\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9768\n",
      "Epoch 00022: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0962 - acc: 0.9767 - val_loss: 2.7491 - val_acc: 0.6217\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9814\n",
      "Epoch 00023: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0783 - acc: 0.9813 - val_loss: 2.6278 - val_acc: 0.6464\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9808\n",
      "Epoch 00024: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0857 - acc: 0.9807 - val_loss: 2.8074 - val_acc: 0.6410\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9844\n",
      "Epoch 00025: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0698 - acc: 0.9843 - val_loss: 2.9324 - val_acc: 0.6147\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9837\n",
      "Epoch 00026: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0701 - acc: 0.9836 - val_loss: 2.9716 - val_acc: 0.6238\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9837\n",
      "Epoch 00027: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0710 - acc: 0.9837 - val_loss: 2.7947 - val_acc: 0.6438\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9912\n",
      "Epoch 00028: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0470 - acc: 0.9912 - val_loss: 2.8239 - val_acc: 0.6233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9883\n",
      "Epoch 00029: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0597 - acc: 0.9883 - val_loss: 2.8511 - val_acc: 0.6427\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9925\n",
      "Epoch 00030: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0418 - acc: 0.9924 - val_loss: 2.8827 - val_acc: 0.6322\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9837\n",
      "Epoch 00031: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0764 - acc: 0.9837 - val_loss: 3.0899 - val_acc: 0.6175\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9907\n",
      "Epoch 00032: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0467 - acc: 0.9907 - val_loss: 2.8912 - val_acc: 0.6336\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9868\n",
      "Epoch 00033: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0633 - acc: 0.9868 - val_loss: 4.2641 - val_acc: 0.5344\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9873\n",
      "Epoch 00034: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0643 - acc: 0.9872 - val_loss: 2.9312 - val_acc: 0.6338\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9842\n",
      "Epoch 00035: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0757 - acc: 0.9842 - val_loss: 3.2316 - val_acc: 0.6208\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9883\n",
      "Epoch 00036: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0598 - acc: 0.9883 - val_loss: 3.1251 - val_acc: 0.6217\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9916\n",
      "Epoch 00037: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0447 - acc: 0.9916 - val_loss: 3.1367 - val_acc: 0.6285\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9873\n",
      "Epoch 00038: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0650 - acc: 0.9873 - val_loss: 3.2256 - val_acc: 0.6164\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9844\n",
      "Epoch 00039: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0755 - acc: 0.9844 - val_loss: 3.2112 - val_acc: 0.6222\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9890\n",
      "Epoch 00040: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0568 - acc: 0.9889 - val_loss: 3.4930 - val_acc: 0.5998\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9864\n",
      "Epoch 00041: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0679 - acc: 0.9863 - val_loss: 3.2960 - val_acc: 0.6217\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9849\n",
      "Epoch 00042: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0784 - acc: 0.9849 - val_loss: 3.0877 - val_acc: 0.6373\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9924\n",
      "Epoch 00043: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0447 - acc: 0.9923 - val_loss: 3.1487 - val_acc: 0.6350\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9858\n",
      "Epoch 00044: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0767 - acc: 0.9858 - val_loss: 3.3518 - val_acc: 0.6110\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9909\n",
      "Epoch 00045: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0497 - acc: 0.9909 - val_loss: 3.1127 - val_acc: 0.6420\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9930\n",
      "Epoch 00046: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0416 - acc: 0.9930 - val_loss: 3.2676 - val_acc: 0.6294\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9874\n",
      "Epoch 00047: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0684 - acc: 0.9874 - val_loss: 3.3836 - val_acc: 0.6236\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9885\n",
      "Epoch 00048: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0610 - acc: 0.9885 - val_loss: 3.2280 - val_acc: 0.6329\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9924\n",
      "Epoch 00049: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0446 - acc: 0.9923 - val_loss: 3.2229 - val_acc: 0.6422\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9909\n",
      "Epoch 00050: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0525 - acc: 0.9909 - val_loss: 3.2745 - val_acc: 0.6352\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9904\n",
      "Epoch 00051: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0527 - acc: 0.9904 - val_loss: 3.3444 - val_acc: 0.6322\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9900\n",
      "Epoch 00052: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0543 - acc: 0.9899 - val_loss: 3.4602 - val_acc: 0.6257\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9870\n",
      "Epoch 00053: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0686 - acc: 0.9869 - val_loss: 3.2674 - val_acc: 0.6413\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9887\n",
      "Epoch 00054: val_loss did not improve from 1.68659\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 0.0626 - acc: 0.9886 - val_loss: 3.2582 - val_acc: 0.6387\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.8076 - acc: 0.5923\n",
      "Loss: 1.8075694526838737 Accuracy: 0.5923157\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9833 - acc: 0.4558\n",
      "Epoch 00001: val_loss improved from inf to 1.73785, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv_checkpoint/001-1.7379.hdf5\n",
      "36805/36805 [==============================] - 230s 6ms/sample - loss: 1.9832 - acc: 0.4558 - val_loss: 1.7379 - val_acc: 0.4929\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0963 - acc: 0.6887\n",
      "Epoch 00002: val_loss improved from 1.73785 to 1.38826, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv_checkpoint/002-1.3883.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 1.0963 - acc: 0.6887 - val_loss: 1.3883 - val_acc: 0.6452\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8085\n",
      "Epoch 00003: val_loss improved from 1.38826 to 1.28666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv_checkpoint/003-1.2867.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.6512 - acc: 0.8085 - val_loss: 1.2867 - val_acc: 0.6622\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8964\n",
      "Epoch 00004: val_loss improved from 1.28666 to 1.23154, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv_checkpoint/004-1.2315.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.3629 - acc: 0.8964 - val_loss: 1.2315 - val_acc: 0.6725\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9385\n",
      "Epoch 00005: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.2339 - acc: 0.9385 - val_loss: 1.3236 - val_acc: 0.6681\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9641\n",
      "Epoch 00006: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1538 - acc: 0.9641 - val_loss: 1.2518 - val_acc: 0.6937\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9764\n",
      "Epoch 00007: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1113 - acc: 0.9764 - val_loss: 1.4846 - val_acc: 0.6473\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9739\n",
      "Epoch 00008: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1150 - acc: 0.9739 - val_loss: 1.7546 - val_acc: 0.6392\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9725\n",
      "Epoch 00009: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1157 - acc: 0.9724 - val_loss: 1.7111 - val_acc: 0.6410\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9744\n",
      "Epoch 00010: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1066 - acc: 0.9744 - val_loss: 1.5026 - val_acc: 0.6860\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9784\n",
      "Epoch 00011: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0875 - acc: 0.9784 - val_loss: 1.6389 - val_acc: 0.6713\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9773\n",
      "Epoch 00012: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0914 - acc: 0.9772 - val_loss: 1.4602 - val_acc: 0.6925\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9700\n",
      "Epoch 00013: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1107 - acc: 0.9699 - val_loss: 1.5388 - val_acc: 0.6900\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9799\n",
      "Epoch 00014: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0818 - acc: 0.9799 - val_loss: 1.7935 - val_acc: 0.6758\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9904\n",
      "Epoch 00015: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0473 - acc: 0.9904 - val_loss: 1.6640 - val_acc: 0.6923\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9901\n",
      "Epoch 00016: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0530 - acc: 0.9901 - val_loss: 1.7239 - val_acc: 0.6811\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9797\n",
      "Epoch 00017: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0827 - acc: 0.9796 - val_loss: 2.0978 - val_acc: 0.6350\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9821\n",
      "Epoch 00018: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0701 - acc: 0.9821 - val_loss: 1.8028 - val_acc: 0.6806\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9806\n",
      "Epoch 00019: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0764 - acc: 0.9806 - val_loss: 1.7894 - val_acc: 0.6769\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9880\n",
      "Epoch 00020: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0509 - acc: 0.9880 - val_loss: 1.7419 - val_acc: 0.6883\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9885\n",
      "Epoch 00021: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0511 - acc: 0.9885 - val_loss: 2.0862 - val_acc: 0.6459\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9823\n",
      "Epoch 00022: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0708 - acc: 0.9823 - val_loss: 1.8866 - val_acc: 0.6902\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9911\n",
      "Epoch 00023: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0381 - acc: 0.9911 - val_loss: 1.9091 - val_acc: 0.6860\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9926\n",
      "Epoch 00024: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0343 - acc: 0.9926 - val_loss: 1.9736 - val_acc: 0.6804\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9891\n",
      "Epoch 00025: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0453 - acc: 0.9891 - val_loss: 2.0782 - val_acc: 0.6662\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9838\n",
      "Epoch 00026: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0614 - acc: 0.9838 - val_loss: 2.2272 - val_acc: 0.6613\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9828\n",
      "Epoch 00027: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0665 - acc: 0.9827 - val_loss: 1.9967 - val_acc: 0.6827\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9878\n",
      "Epoch 00028: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0521 - acc: 0.9878 - val_loss: 1.9654 - val_acc: 0.6902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9887\n",
      "Epoch 00029: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0498 - acc: 0.9886 - val_loss: 1.9344 - val_acc: 0.6848\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9850\n",
      "Epoch 00030: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0589 - acc: 0.9850 - val_loss: 2.0061 - val_acc: 0.6818\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9858\n",
      "Epoch 00031: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0574 - acc: 0.9858 - val_loss: 2.0570 - val_acc: 0.6825\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9865\n",
      "Epoch 00032: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0536 - acc: 0.9864 - val_loss: 1.9544 - val_acc: 0.6988\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9866\n",
      "Epoch 00033: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0569 - acc: 0.9866 - val_loss: 2.0133 - val_acc: 0.6949\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9914\n",
      "Epoch 00034: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0364 - acc: 0.9914 - val_loss: 2.2753 - val_acc: 0.6727\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9933\n",
      "Epoch 00035: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0337 - acc: 0.9933 - val_loss: 2.2022 - val_acc: 0.6748\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9925\n",
      "Epoch 00036: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0373 - acc: 0.9924 - val_loss: 2.3598 - val_acc: 0.6613\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9851\n",
      "Epoch 00037: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0577 - acc: 0.9850 - val_loss: 2.0935 - val_acc: 0.6918\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9890\n",
      "Epoch 00038: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0456 - acc: 0.9889 - val_loss: 2.0990 - val_acc: 0.6923\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9919\n",
      "Epoch 00039: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0362 - acc: 0.9919 - val_loss: 2.1465 - val_acc: 0.6860\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9876\n",
      "Epoch 00040: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0487 - acc: 0.9876 - val_loss: 2.1171 - val_acc: 0.6979\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9897\n",
      "Epoch 00041: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0434 - acc: 0.9897 - val_loss: 2.2606 - val_acc: 0.6827\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9883\n",
      "Epoch 00042: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0485 - acc: 0.9882 - val_loss: 2.1312 - val_acc: 0.6879\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9882\n",
      "Epoch 00043: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0500 - acc: 0.9882 - val_loss: 2.0556 - val_acc: 0.6988\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9950\n",
      "Epoch 00044: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0261 - acc: 0.9949 - val_loss: 2.1629 - val_acc: 0.6967\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9882\n",
      "Epoch 00045: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0498 - acc: 0.9882 - val_loss: 2.4667 - val_acc: 0.6543\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9907\n",
      "Epoch 00046: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0397 - acc: 0.9907 - val_loss: 2.1914 - val_acc: 0.6907\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9947\n",
      "Epoch 00047: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0284 - acc: 0.9946 - val_loss: 2.1852 - val_acc: 0.6939\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9910\n",
      "Epoch 00048: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0398 - acc: 0.9910 - val_loss: 2.2288 - val_acc: 0.6937\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9879\n",
      "Epoch 00049: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0489 - acc: 0.9879 - val_loss: 2.2305 - val_acc: 0.6881\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9941\n",
      "Epoch 00050: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0290 - acc: 0.9940 - val_loss: 2.3237 - val_acc: 0.6890\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9889\n",
      "Epoch 00051: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0463 - acc: 0.9889 - val_loss: 2.2232 - val_acc: 0.6953\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9950\n",
      "Epoch 00052: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0270 - acc: 0.9949 - val_loss: 2.3405 - val_acc: 0.6881\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9896\n",
      "Epoch 00053: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0435 - acc: 0.9896 - val_loss: 2.1953 - val_acc: 0.7018\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9949\n",
      "Epoch 00054: val_loss did not improve from 1.23154\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0258 - acc: 0.9949 - val_loss: 2.5878 - val_acc: 0.6671\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.3120 - acc: 0.6417\n",
      "Loss: 1.3119915772078565 Accuracy: 0.64174455\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8750 - acc: 0.4489\n",
      "Epoch 00001: val_loss improved from inf to 1.76630, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv_checkpoint/001-1.7663.hdf5\n",
      "36805/36805 [==============================] - 244s 7ms/sample - loss: 1.8750 - acc: 0.4489 - val_loss: 1.7663 - val_acc: 0.4610\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1118 - acc: 0.6760\n",
      "Epoch 00002: val_loss improved from 1.76630 to 1.22384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv_checkpoint/002-1.2238.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 1.1118 - acc: 0.6760 - val_loss: 1.2238 - val_acc: 0.6769\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8448 - acc: 0.7529\n",
      "Epoch 00003: val_loss improved from 1.22384 to 1.10063, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv_checkpoint/003-1.1006.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.8447 - acc: 0.7528 - val_loss: 1.1006 - val_acc: 0.6895\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.8099\n",
      "Epoch 00004: val_loss improved from 1.10063 to 0.93877, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv_checkpoint/004-0.9388.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.6469 - acc: 0.8098 - val_loss: 0.9388 - val_acc: 0.7414\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8485\n",
      "Epoch 00005: val_loss did not improve from 0.93877\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.5067 - acc: 0.8486 - val_loss: 1.0504 - val_acc: 0.7160\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8894\n",
      "Epoch 00006: val_loss did not improve from 0.93877\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.3744 - acc: 0.8894 - val_loss: 0.9896 - val_acc: 0.7354\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9153\n",
      "Epoch 00007: val_loss did not improve from 0.93877\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.2874 - acc: 0.9152 - val_loss: 1.2664 - val_acc: 0.6914\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9375\n",
      "Epoch 00008: val_loss improved from 0.93877 to 0.87241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv_checkpoint/008-0.8724.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.2209 - acc: 0.9374 - val_loss: 0.8724 - val_acc: 0.7701\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9498\n",
      "Epoch 00009: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1807 - acc: 0.9498 - val_loss: 0.9137 - val_acc: 0.7608\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9707\n",
      "Epoch 00010: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1223 - acc: 0.9707 - val_loss: 1.0975 - val_acc: 0.7375\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9735\n",
      "Epoch 00011: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1085 - acc: 0.9735 - val_loss: 0.9169 - val_acc: 0.7850\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9762\n",
      "Epoch 00012: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0994 - acc: 0.9761 - val_loss: 1.0436 - val_acc: 0.7556\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9666\n",
      "Epoch 00013: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1247 - acc: 0.9666 - val_loss: 1.1312 - val_acc: 0.7484\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9833\n",
      "Epoch 00014: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0739 - acc: 0.9833 - val_loss: 1.0944 - val_acc: 0.7526\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9848\n",
      "Epoch 00015: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0674 - acc: 0.9848 - val_loss: 1.0558 - val_acc: 0.7657\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9864\n",
      "Epoch 00016: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0633 - acc: 0.9864 - val_loss: 1.4271 - val_acc: 0.7044\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9723\n",
      "Epoch 00017: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1013 - acc: 0.9723 - val_loss: 1.1021 - val_acc: 0.7591\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9790\n",
      "Epoch 00018: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0775 - acc: 0.9789 - val_loss: 1.0348 - val_acc: 0.7738\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9805\n",
      "Epoch 00019: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0739 - acc: 0.9805 - val_loss: 1.2164 - val_acc: 0.7496\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9880\n",
      "Epoch 00020: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0513 - acc: 0.9880 - val_loss: 1.1346 - val_acc: 0.7736\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9926\n",
      "Epoch 00021: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0374 - acc: 0.9926 - val_loss: 1.0613 - val_acc: 0.7782\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9818\n",
      "Epoch 00022: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0691 - acc: 0.9818 - val_loss: 1.1090 - val_acc: 0.7720\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9841\n",
      "Epoch 00023: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0628 - acc: 0.9841 - val_loss: 1.2472 - val_acc: 0.7421\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9833\n",
      "Epoch 00024: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0642 - acc: 0.9833 - val_loss: 1.0872 - val_acc: 0.7792\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9899\n",
      "Epoch 00025: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0439 - acc: 0.9899 - val_loss: 1.1411 - val_acc: 0.7750\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9926\n",
      "Epoch 00026: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0340 - acc: 0.9926 - val_loss: 1.3994 - val_acc: 0.7438\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9859\n",
      "Epoch 00027: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0536 - acc: 0.9858 - val_loss: 1.3233 - val_acc: 0.7454\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9869\n",
      "Epoch 00028: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0520 - acc: 0.9869 - val_loss: 1.2014 - val_acc: 0.7722\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9893\n",
      "Epoch 00029: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0435 - acc: 0.9892 - val_loss: 1.3005 - val_acc: 0.7626\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9894\n",
      "Epoch 00030: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0430 - acc: 0.9894 - val_loss: 1.1192 - val_acc: 0.7862\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9932\n",
      "Epoch 00031: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0319 - acc: 0.9932 - val_loss: 1.1215 - val_acc: 0.7857\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9880\n",
      "Epoch 00032: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0462 - acc: 0.9880 - val_loss: 1.4872 - val_acc: 0.7324\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9925\n",
      "Epoch 00033: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0329 - acc: 0.9925 - val_loss: 1.6535 - val_acc: 0.7242\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9928\n",
      "Epoch 00034: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0323 - acc: 0.9927 - val_loss: 1.4687 - val_acc: 0.7368\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9808\n",
      "Epoch 00035: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0673 - acc: 0.9808 - val_loss: 1.4078 - val_acc: 0.7533\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9866\n",
      "Epoch 00036: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0508 - acc: 0.9866 - val_loss: 1.2576 - val_acc: 0.7696\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9908\n",
      "Epoch 00037: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0374 - acc: 0.9908 - val_loss: 1.3286 - val_acc: 0.7622\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9871\n",
      "Epoch 00038: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0477 - acc: 0.9871 - val_loss: 1.3930 - val_acc: 0.7547\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9936\n",
      "Epoch 00039: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0291 - acc: 0.9936 - val_loss: 1.3365 - val_acc: 0.7682\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9915\n",
      "Epoch 00040: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0357 - acc: 0.9914 - val_loss: 1.3039 - val_acc: 0.7727\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9938\n",
      "Epoch 00041: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0289 - acc: 0.9938 - val_loss: 1.4059 - val_acc: 0.7512\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9895\n",
      "Epoch 00042: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0400 - acc: 0.9894 - val_loss: 1.3462 - val_acc: 0.7603\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9872\n",
      "Epoch 00043: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0473 - acc: 0.9872 - val_loss: 1.4026 - val_acc: 0.7589\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9907\n",
      "Epoch 00044: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0377 - acc: 0.9906 - val_loss: 1.2796 - val_acc: 0.7727\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9889\n",
      "Epoch 00045: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0423 - acc: 0.9889 - val_loss: 1.3745 - val_acc: 0.7638\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9897\n",
      "Epoch 00046: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0395 - acc: 0.9896 - val_loss: 1.2217 - val_acc: 0.7836\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0403 - acc: 0.9898 - val_loss: 1.2852 - val_acc: 0.7836\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9967\n",
      "Epoch 00048: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0184 - acc: 0.9967 - val_loss: 1.2628 - val_acc: 0.7852\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9963\n",
      "Epoch 00049: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0203 - acc: 0.9963 - val_loss: 1.3215 - val_acc: 0.7738\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9910\n",
      "Epoch 00050: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0368 - acc: 0.9910 - val_loss: 1.6693 - val_acc: 0.7377\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9898\n",
      "Epoch 00051: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0398 - acc: 0.9897 - val_loss: 1.3502 - val_acc: 0.7738\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9868\n",
      "Epoch 00052: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0491 - acc: 0.9868 - val_loss: 1.3299 - val_acc: 0.7794\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9967\n",
      "Epoch 00053: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0177 - acc: 0.9967 - val_loss: 1.2399 - val_acc: 0.7897\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9906\n",
      "Epoch 00054: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0352 - acc: 0.9906 - val_loss: 1.4882 - val_acc: 0.7531\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9919\n",
      "Epoch 00055: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0322 - acc: 0.9919 - val_loss: 1.3509 - val_acc: 0.7785\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0204 - acc: 0.9959 - val_loss: 1.4051 - val_acc: 0.7706\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9912\n",
      "Epoch 00057: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0330 - acc: 0.9913 - val_loss: 1.6841 - val_acc: 0.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.87241\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0203 - acc: 0.9958 - val_loss: 1.4525 - val_acc: 0.7731\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.9608 - acc: 0.7462\n",
      "Loss: 0.9608338521895998 Accuracy: 0.74620974\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8906 - acc: 0.4351\n",
      "Epoch 00001: val_loss improved from inf to 1.62951, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/001-1.6295.hdf5\n",
      "36805/36805 [==============================] - 258s 7ms/sample - loss: 1.8907 - acc: 0.4351 - val_loss: 1.6295 - val_acc: 0.4987\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1102 - acc: 0.6683\n",
      "Epoch 00002: val_loss improved from 1.62951 to 1.00611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/002-1.0061.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 1.1105 - acc: 0.6683 - val_loss: 1.0061 - val_acc: 0.7056\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8711 - acc: 0.7454\n",
      "Epoch 00003: val_loss did not improve from 1.00611\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.8712 - acc: 0.7453 - val_loss: 1.1446 - val_acc: 0.6925\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7912\n",
      "Epoch 00004: val_loss improved from 1.00611 to 0.84559, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/004-0.8456.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.7235 - acc: 0.7912 - val_loss: 0.8456 - val_acc: 0.7731\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6131 - acc: 0.8237\n",
      "Epoch 00005: val_loss did not improve from 0.84559\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.6133 - acc: 0.8237 - val_loss: 0.8522 - val_acc: 0.7724\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8477\n",
      "Epoch 00006: val_loss improved from 0.84559 to 0.75292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/006-0.7529.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.5214 - acc: 0.8477 - val_loss: 0.7529 - val_acc: 0.7943\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4345 - acc: 0.8723\n",
      "Epoch 00007: val_loss did not improve from 0.75292\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.4346 - acc: 0.8722 - val_loss: 0.9321 - val_acc: 0.7519\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8884\n",
      "Epoch 00008: val_loss improved from 0.75292 to 0.74922, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/008-0.7492.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.3810 - acc: 0.8884 - val_loss: 0.7492 - val_acc: 0.7901\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.9038\n",
      "Epoch 00009: val_loss improved from 0.74922 to 0.64546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/009-0.6455.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.3292 - acc: 0.9038 - val_loss: 0.6455 - val_acc: 0.8248\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9222\n",
      "Epoch 00010: val_loss did not improve from 0.64546\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.2644 - acc: 0.9222 - val_loss: 0.6996 - val_acc: 0.8206\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9329\n",
      "Epoch 00011: val_loss improved from 0.64546 to 0.59898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/011-0.5990.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.2353 - acc: 0.9329 - val_loss: 0.5990 - val_acc: 0.8348\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9464\n",
      "Epoch 00012: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1867 - acc: 0.9464 - val_loss: 0.6851 - val_acc: 0.8223\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9487\n",
      "Epoch 00013: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1794 - acc: 0.9487 - val_loss: 0.7035 - val_acc: 0.8178\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9609\n",
      "Epoch 00014: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1419 - acc: 0.9609 - val_loss: 0.6792 - val_acc: 0.8248\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9620\n",
      "Epoch 00015: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1341 - acc: 0.9619 - val_loss: 0.6124 - val_acc: 0.8474\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9711\n",
      "Epoch 00016: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1081 - acc: 0.9711 - val_loss: 0.6368 - val_acc: 0.8463\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9754\n",
      "Epoch 00017: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0944 - acc: 0.9754 - val_loss: 0.6254 - val_acc: 0.8495\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9795\n",
      "Epoch 00018: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0827 - acc: 0.9795 - val_loss: 0.6505 - val_acc: 0.8502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9706\n",
      "Epoch 00019: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1064 - acc: 0.9705 - val_loss: 0.7487 - val_acc: 0.8202\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9773\n",
      "Epoch 00020: val_loss did not improve from 0.59898\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0856 - acc: 0.9773 - val_loss: 0.8267 - val_acc: 0.8118\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9867\n",
      "Epoch 00021: val_loss improved from 0.59898 to 0.59698, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv_checkpoint/021-0.5970.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0582 - acc: 0.9867 - val_loss: 0.5970 - val_acc: 0.8572\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9874\n",
      "Epoch 00022: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0532 - acc: 0.9874 - val_loss: 0.6944 - val_acc: 0.8477\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9740\n",
      "Epoch 00023: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0920 - acc: 0.9739 - val_loss: 0.7102 - val_acc: 0.8395\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9812\n",
      "Epoch 00024: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0699 - acc: 0.9812 - val_loss: 0.7157 - val_acc: 0.8404\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9906\n",
      "Epoch 00025: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0423 - acc: 0.9906 - val_loss: 0.7391 - val_acc: 0.8411\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9888\n",
      "Epoch 00026: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0480 - acc: 0.9888 - val_loss: 0.7519 - val_acc: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9667\n",
      "Epoch 00027: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1099 - acc: 0.9667 - val_loss: 0.6471 - val_acc: 0.8549\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9859\n",
      "Epoch 00028: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0539 - acc: 0.9858 - val_loss: 0.6978 - val_acc: 0.8474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9854\n",
      "Epoch 00029: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0579 - acc: 0.9854 - val_loss: 0.7388 - val_acc: 0.8365\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0329 - acc: 0.9932 - val_loss: 0.7024 - val_acc: 0.8449\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9860\n",
      "Epoch 00031: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0529 - acc: 0.9860 - val_loss: 0.6688 - val_acc: 0.8570\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9886\n",
      "Epoch 00032: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0469 - acc: 0.9886 - val_loss: 0.6640 - val_acc: 0.8595\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9932\n",
      "Epoch 00033: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0317 - acc: 0.9932 - val_loss: 0.6324 - val_acc: 0.8670\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9923\n",
      "Epoch 00034: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0350 - acc: 0.9922 - val_loss: 0.7957 - val_acc: 0.8449\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9838\n",
      "Epoch 00035: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0600 - acc: 0.9837 - val_loss: 0.7528 - val_acc: 0.8395\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9862\n",
      "Epoch 00036: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0513 - acc: 0.9862 - val_loss: 0.7297 - val_acc: 0.8472\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9841\n",
      "Epoch 00037: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0568 - acc: 0.9841 - val_loss: 0.6624 - val_acc: 0.8675\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9877\n",
      "Epoch 00038: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0479 - acc: 0.9876 - val_loss: 0.6280 - val_acc: 0.8689\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9920\n",
      "Epoch 00039: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0342 - acc: 0.9920 - val_loss: 0.8127 - val_acc: 0.8481\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9923\n",
      "Epoch 00040: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0346 - acc: 0.9922 - val_loss: 0.7786 - val_acc: 0.8495\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9859\n",
      "Epoch 00041: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0502 - acc: 0.9859 - val_loss: 0.7196 - val_acc: 0.8512\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9936\n",
      "Epoch 00042: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0297 - acc: 0.9936 - val_loss: 0.7312 - val_acc: 0.8523\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9959\n",
      "Epoch 00043: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0224 - acc: 0.9959 - val_loss: 0.7412 - val_acc: 0.8553\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9890\n",
      "Epoch 00044: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0432 - acc: 0.9890 - val_loss: 0.7775 - val_acc: 0.8491\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9958\n",
      "Epoch 00045: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0211 - acc: 0.9958 - val_loss: 0.7218 - val_acc: 0.8549\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9897\n",
      "Epoch 00046: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0402 - acc: 0.9897 - val_loss: 0.8290 - val_acc: 0.8376\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9956\n",
      "Epoch 00047: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0207 - acc: 0.9956 - val_loss: 0.6910 - val_acc: 0.8584\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9933\n",
      "Epoch 00048: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0270 - acc: 0.9933 - val_loss: 0.7119 - val_acc: 0.8556\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9915\n",
      "Epoch 00049: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0314 - acc: 0.9916 - val_loss: 0.8919 - val_acc: 0.8309\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9942\n",
      "Epoch 00050: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0256 - acc: 0.9942 - val_loss: 0.7021 - val_acc: 0.8616\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9891\n",
      "Epoch 00051: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0413 - acc: 0.9891 - val_loss: 0.7821 - val_acc: 0.8479\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9928\n",
      "Epoch 00052: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0288 - acc: 0.9928 - val_loss: 0.7511 - val_acc: 0.8558\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9955\n",
      "Epoch 00053: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0213 - acc: 0.9955 - val_loss: 0.7269 - val_acc: 0.8626\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9892\n",
      "Epoch 00054: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0404 - acc: 0.9892 - val_loss: 0.6697 - val_acc: 0.8616\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9931\n",
      "Epoch 00055: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0287 - acc: 0.9931 - val_loss: 0.7935 - val_acc: 0.8549\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9961\n",
      "Epoch 00056: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0196 - acc: 0.9961 - val_loss: 0.8314 - val_acc: 0.8453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0227 - acc: 0.9950 - val_loss: 0.7438 - val_acc: 0.8509\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9878\n",
      "Epoch 00058: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0402 - acc: 0.9878 - val_loss: 0.7655 - val_acc: 0.8586\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9958\n",
      "Epoch 00059: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0187 - acc: 0.9958 - val_loss: 0.7359 - val_acc: 0.8558\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9961\n",
      "Epoch 00060: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0202 - acc: 0.9961 - val_loss: 0.7502 - val_acc: 0.8600\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9901\n",
      "Epoch 00061: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0367 - acc: 0.9900 - val_loss: 0.7187 - val_acc: 0.8609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9890\n",
      "Epoch 00062: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0382 - acc: 0.9891 - val_loss: 0.7577 - val_acc: 0.8686\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9955\n",
      "Epoch 00063: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0190 - acc: 0.9955 - val_loss: 0.6846 - val_acc: 0.8733\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9947\n",
      "Epoch 00064: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0222 - acc: 0.9946 - val_loss: 0.8893 - val_acc: 0.8346\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9948\n",
      "Epoch 00065: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0223 - acc: 0.9947 - val_loss: 0.8312 - val_acc: 0.8526\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9897\n",
      "Epoch 00066: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0416 - acc: 0.9896 - val_loss: 0.8281 - val_acc: 0.8570\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9908\n",
      "Epoch 00067: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0352 - acc: 0.9907 - val_loss: 0.7289 - val_acc: 0.8630\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9940\n",
      "Epoch 00068: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0243 - acc: 0.9940 - val_loss: 0.7218 - val_acc: 0.8689\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9918\n",
      "Epoch 00069: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0321 - acc: 0.9918 - val_loss: 0.8854 - val_acc: 0.8379\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9967\n",
      "Epoch 00070: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0162 - acc: 0.9967 - val_loss: 0.7610 - val_acc: 0.8605\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9927\n",
      "Epoch 00071: val_loss did not improve from 0.59698\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0282 - acc: 0.9927 - val_loss: 0.7391 - val_acc: 0.8633\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.6860 - acc: 0.8411\n",
      "Loss: 0.6860222424302146 Accuracy: 0.8411215\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8610 - acc: 0.4469\n",
      "Epoch 00001: val_loss improved from inf to 1.76715, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/001-1.7672.hdf5\n",
      "36805/36805 [==============================] - 266s 7ms/sample - loss: 1.8609 - acc: 0.4470 - val_loss: 1.7672 - val_acc: 0.4642\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0821 - acc: 0.6776\n",
      "Epoch 00002: val_loss improved from 1.76715 to 0.98031, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/002-0.9803.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 1.0821 - acc: 0.6775 - val_loss: 0.9803 - val_acc: 0.7156\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7648\n",
      "Epoch 00003: val_loss improved from 0.98031 to 0.88916, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/003-0.8892.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.8026 - acc: 0.7648 - val_loss: 0.8892 - val_acc: 0.7547\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.8251\n",
      "Epoch 00004: val_loss improved from 0.88916 to 0.62009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/004-0.6201.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.6031 - acc: 0.8251 - val_loss: 0.6201 - val_acc: 0.8330\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8540\n",
      "Epoch 00005: val_loss did not improve from 0.62009\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.4977 - acc: 0.8540 - val_loss: 0.6774 - val_acc: 0.8053\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8764\n",
      "Epoch 00006: val_loss improved from 0.62009 to 0.49361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/006-0.4936.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.4214 - acc: 0.8764 - val_loss: 0.4936 - val_acc: 0.8528\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8977\n",
      "Epoch 00007: val_loss did not improve from 0.49361\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.3515 - acc: 0.8977 - val_loss: 0.5394 - val_acc: 0.8493\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9110\n",
      "Epoch 00008: val_loss did not improve from 0.49361\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.2979 - acc: 0.9110 - val_loss: 0.5808 - val_acc: 0.8474\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9222\n",
      "Epoch 00009: val_loss did not improve from 0.49361\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.2559 - acc: 0.9222 - val_loss: 0.6663 - val_acc: 0.8244\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9283\n",
      "Epoch 00010: val_loss improved from 0.49361 to 0.47648, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/010-0.4765.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.2364 - acc: 0.9282 - val_loss: 0.4765 - val_acc: 0.8758\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9360\n",
      "Epoch 00011: val_loss did not improve from 0.47648\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.2101 - acc: 0.9359 - val_loss: 0.6537 - val_acc: 0.8244\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9465\n",
      "Epoch 00012: val_loss did not improve from 0.47648\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1775 - acc: 0.9464 - val_loss: 0.6101 - val_acc: 0.8549\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9449\n",
      "Epoch 00013: val_loss did not improve from 0.47648\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1798 - acc: 0.9448 - val_loss: 0.5054 - val_acc: 0.8805\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9563\n",
      "Epoch 00014: val_loss improved from 0.47648 to 0.45525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/014-0.4553.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1409 - acc: 0.9562 - val_loss: 0.4553 - val_acc: 0.8903\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9629\n",
      "Epoch 00015: val_loss improved from 0.45525 to 0.44166, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/015-0.4417.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1228 - acc: 0.9629 - val_loss: 0.4417 - val_acc: 0.8819\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9681\n",
      "Epoch 00016: val_loss did not improve from 0.44166\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1078 - acc: 0.9680 - val_loss: 0.4777 - val_acc: 0.8924\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9703\n",
      "Epoch 00017: val_loss improved from 0.44166 to 0.42904, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/017-0.4290.hdf5\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0998 - acc: 0.9702 - val_loss: 0.4290 - val_acc: 0.8959\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9680\n",
      "Epoch 00018: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.1040 - acc: 0.9680 - val_loss: 0.4614 - val_acc: 0.8956\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9759\n",
      "Epoch 00019: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0813 - acc: 0.9759 - val_loss: 0.5550 - val_acc: 0.8789\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9779\n",
      "Epoch 00020: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0747 - acc: 0.9779 - val_loss: 0.5305 - val_acc: 0.8805\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9807\n",
      "Epoch 00021: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0640 - acc: 0.9807 - val_loss: 0.5091 - val_acc: 0.8870\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9784\n",
      "Epoch 00022: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0689 - acc: 0.9783 - val_loss: 0.4872 - val_acc: 0.8952\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9775\n",
      "Epoch 00023: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0737 - acc: 0.9775 - val_loss: 0.4682 - val_acc: 0.8994\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9817\n",
      "Epoch 00024: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0603 - acc: 0.9816 - val_loss: 0.5149 - val_acc: 0.8891\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9770\n",
      "Epoch 00025: val_loss did not improve from 0.42904\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0767 - acc: 0.9770 - val_loss: 0.4418 - val_acc: 0.9089\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9870\n",
      "Epoch 00026: val_loss improved from 0.42904 to 0.42122, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv_checkpoint/026-0.4212.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0470 - acc: 0.9870 - val_loss: 0.4212 - val_acc: 0.9154\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9872\n",
      "Epoch 00027: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0438 - acc: 0.9872 - val_loss: 0.4272 - val_acc: 0.9043\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9910\n",
      "Epoch 00028: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0330 - acc: 0.9910 - val_loss: 0.5508 - val_acc: 0.8861\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9846\n",
      "Epoch 00029: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0538 - acc: 0.9846 - val_loss: 0.4984 - val_acc: 0.9022\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9913\n",
      "Epoch 00030: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0313 - acc: 0.9913 - val_loss: 0.5865 - val_acc: 0.8873\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9838\n",
      "Epoch 00031: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0539 - acc: 0.9838 - val_loss: 0.5850 - val_acc: 0.8928\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9901\n",
      "Epoch 00032: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0335 - acc: 0.9901 - val_loss: 0.5757 - val_acc: 0.8891\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9896\n",
      "Epoch 00033: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0362 - acc: 0.9896 - val_loss: 0.7152 - val_acc: 0.8707\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9779\n",
      "Epoch 00034: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0708 - acc: 0.9779 - val_loss: 0.4400 - val_acc: 0.9078\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9933\n",
      "Epoch 00035: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0252 - acc: 0.9933 - val_loss: 0.4503 - val_acc: 0.9108\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9885\n",
      "Epoch 00036: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0382 - acc: 0.9884 - val_loss: 0.5136 - val_acc: 0.8980\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9849\n",
      "Epoch 00037: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0492 - acc: 0.9849 - val_loss: 0.4984 - val_acc: 0.8973\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9913\n",
      "Epoch 00038: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0302 - acc: 0.9913 - val_loss: 0.4537 - val_acc: 0.9124\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9942\n",
      "Epoch 00039: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0222 - acc: 0.9942 - val_loss: 0.5207 - val_acc: 0.9005\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9926\n",
      "Epoch 00040: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0266 - acc: 0.9926 - val_loss: 0.5910 - val_acc: 0.8856\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9842\n",
      "Epoch 00041: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0526 - acc: 0.9841 - val_loss: 0.4410 - val_acc: 0.9143\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9870\n",
      "Epoch 00042: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0449 - acc: 0.9870 - val_loss: 0.4504 - val_acc: 0.9147\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9910\n",
      "Epoch 00043: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0317 - acc: 0.9910 - val_loss: 0.4784 - val_acc: 0.9089\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9877\n",
      "Epoch 00044: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0417 - acc: 0.9877 - val_loss: 0.4718 - val_acc: 0.9101\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9920\n",
      "Epoch 00045: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0273 - acc: 0.9919 - val_loss: 0.4851 - val_acc: 0.9129\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9876\n",
      "Epoch 00046: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0404 - acc: 0.9876 - val_loss: 0.4702 - val_acc: 0.9071\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9947\n",
      "Epoch 00047: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0186 - acc: 0.9947 - val_loss: 0.4988 - val_acc: 0.9092\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9949\n",
      "Epoch 00048: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0183 - acc: 0.9949 - val_loss: 0.5031 - val_acc: 0.9129\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9871\n",
      "Epoch 00049: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0422 - acc: 0.9871 - val_loss: 0.5825 - val_acc: 0.8912\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9906\n",
      "Epoch 00050: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0342 - acc: 0.9905 - val_loss: 0.5071 - val_acc: 0.9138\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9858\n",
      "Epoch 00051: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0477 - acc: 0.9858 - val_loss: 0.5317 - val_acc: 0.9057\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0249 - acc: 0.9927 - val_loss: 0.5131 - val_acc: 0.9075\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9923\n",
      "Epoch 00053: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0260 - acc: 0.9923 - val_loss: 0.4406 - val_acc: 0.9210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9950\n",
      "Epoch 00054: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0180 - acc: 0.9950 - val_loss: 0.5151 - val_acc: 0.9066\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9904\n",
      "Epoch 00055: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0318 - acc: 0.9904 - val_loss: 0.6017 - val_acc: 0.9017\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9928\n",
      "Epoch 00056: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0242 - acc: 0.9928 - val_loss: 0.4754 - val_acc: 0.9136\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9937\n",
      "Epoch 00057: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0226 - acc: 0.9937 - val_loss: 0.5236 - val_acc: 0.9080\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9910\n",
      "Epoch 00058: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0315 - acc: 0.9910 - val_loss: 0.5257 - val_acc: 0.9057\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9949\n",
      "Epoch 00059: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0182 - acc: 0.9948 - val_loss: 0.5527 - val_acc: 0.9071\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9863\n",
      "Epoch 00060: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0465 - acc: 0.9863 - val_loss: 0.5612 - val_acc: 0.9075\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 00061: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.5337 - val_acc: 0.9092\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0139 - acc: 0.9963 - val_loss: 0.6320 - val_acc: 0.9038\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9935\n",
      "Epoch 00063: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0217 - acc: 0.9935 - val_loss: 0.5302 - val_acc: 0.9113\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9925\n",
      "Epoch 00064: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0259 - acc: 0.9925 - val_loss: 0.4758 - val_acc: 0.9164\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9872\n",
      "Epoch 00065: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0451 - acc: 0.9872 - val_loss: 0.4422 - val_acc: 0.9189\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960\n",
      "Epoch 00066: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0157 - acc: 0.9959 - val_loss: 0.5349 - val_acc: 0.9194\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9893\n",
      "Epoch 00067: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0368 - acc: 0.9892 - val_loss: 0.5532 - val_acc: 0.9094\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9936\n",
      "Epoch 00068: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0231 - acc: 0.9936 - val_loss: 0.4647 - val_acc: 0.9222\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0123 - acc: 0.9967 - val_loss: 0.5367 - val_acc: 0.9136\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9958\n",
      "Epoch 00070: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.6902 - val_acc: 0.8903\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9883\n",
      "Epoch 00071: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0401 - acc: 0.9882 - val_loss: 0.5066 - val_acc: 0.9157\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0236 - acc: 0.9928 - val_loss: 0.4543 - val_acc: 0.9229\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9910\n",
      "Epoch 00073: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0303 - acc: 0.9910 - val_loss: 0.4957 - val_acc: 0.9182\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9962\n",
      "Epoch 00074: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0140 - acc: 0.9962 - val_loss: 0.5263 - val_acc: 0.9122\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9959\n",
      "Epoch 00075: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0163 - acc: 0.9959 - val_loss: 0.4516 - val_acc: 0.9236\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9967\n",
      "Epoch 00076: val_loss did not improve from 0.42122\n",
      "36805/36805 [==============================] - 207s 6ms/sample - loss: 0.0122 - acc: 0.9967 - val_loss: 0.4843 - val_acc: 0.9185\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.4995 - acc: 0.8935\n",
      "Loss: 0.4994610071677409 Accuracy: 0.89345795\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9132 - acc: 0.4232\n",
      "Epoch 00001: val_loss improved from inf to 1.84038, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/001-1.8404.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 1.9132 - acc: 0.4232 - val_loss: 1.8404 - val_acc: 0.4205\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.7017\n",
      "Epoch 00002: val_loss improved from 1.84038 to 0.79146, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/002-0.7915.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 1.0052 - acc: 0.7017 - val_loss: 0.7915 - val_acc: 0.7626\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.8080\n",
      "Epoch 00003: val_loss improved from 0.79146 to 0.67430, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/003-0.6743.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.6515 - acc: 0.8079 - val_loss: 0.6743 - val_acc: 0.8018\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8569\n",
      "Epoch 00004: val_loss improved from 0.67430 to 0.49431, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/004-0.4943.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.4877 - acc: 0.8569 - val_loss: 0.4943 - val_acc: 0.8572\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8882\n",
      "Epoch 00005: val_loss did not improve from 0.49431\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3808 - acc: 0.8882 - val_loss: 0.5615 - val_acc: 0.8321\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.9056\n",
      "Epoch 00006: val_loss did not improve from 0.49431\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3187 - acc: 0.9056 - val_loss: 0.7899 - val_acc: 0.7806\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9167\n",
      "Epoch 00007: val_loss improved from 0.49431 to 0.38994, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/007-0.3899.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.2753 - acc: 0.9167 - val_loss: 0.3899 - val_acc: 0.8959\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9314\n",
      "Epoch 00008: val_loss did not improve from 0.38994\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2313 - acc: 0.9313 - val_loss: 0.4315 - val_acc: 0.8796\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9365\n",
      "Epoch 00009: val_loss did not improve from 0.38994\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.2057 - acc: 0.9366 - val_loss: 0.4374 - val_acc: 0.8724\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9476\n",
      "Epoch 00010: val_loss did not improve from 0.38994\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1717 - acc: 0.9476 - val_loss: 0.4636 - val_acc: 0.8693\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9556\n",
      "Epoch 00011: val_loss improved from 0.38994 to 0.31871, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/011-0.3187.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1465 - acc: 0.9555 - val_loss: 0.3187 - val_acc: 0.9124\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9515\n",
      "Epoch 00012: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1535 - acc: 0.9515 - val_loss: 0.3989 - val_acc: 0.8945\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9584\n",
      "Epoch 00013: val_loss improved from 0.31871 to 0.27857, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/013-0.2786.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1351 - acc: 0.9584 - val_loss: 0.2786 - val_acc: 0.9206\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9708\n",
      "Epoch 00014: val_loss did not improve from 0.27857\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0962 - acc: 0.9707 - val_loss: 0.3692 - val_acc: 0.9057\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9665\n",
      "Epoch 00015: val_loss did not improve from 0.27857\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.1064 - acc: 0.9666 - val_loss: 0.3138 - val_acc: 0.9222\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9725\n",
      "Epoch 00016: val_loss did not improve from 0.27857\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0846 - acc: 0.9724 - val_loss: 0.4460 - val_acc: 0.8852\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9746\n",
      "Epoch 00017: val_loss improved from 0.27857 to 0.24943, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/017-0.2494.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0816 - acc: 0.9745 - val_loss: 0.2494 - val_acc: 0.9313\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9748\n",
      "Epoch 00018: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0824 - acc: 0.9747 - val_loss: 0.3510 - val_acc: 0.9052\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9796\n",
      "Epoch 00019: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0692 - acc: 0.9796 - val_loss: 0.3117 - val_acc: 0.9271\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9782\n",
      "Epoch 00020: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0697 - acc: 0.9781 - val_loss: 0.2596 - val_acc: 0.9390\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9824\n",
      "Epoch 00021: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0558 - acc: 0.9823 - val_loss: 0.3758 - val_acc: 0.9045\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9817\n",
      "Epoch 00022: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0599 - acc: 0.9817 - val_loss: 0.2883 - val_acc: 0.9227\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9885\n",
      "Epoch 00023: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0388 - acc: 0.9885 - val_loss: 0.2586 - val_acc: 0.9338\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9820\n",
      "Epoch 00024: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0573 - acc: 0.9819 - val_loss: 0.2514 - val_acc: 0.9380\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9817\n",
      "Epoch 00025: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0580 - acc: 0.9816 - val_loss: 0.2981 - val_acc: 0.9320\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9870\n",
      "Epoch 00026: val_loss did not improve from 0.24943\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0448 - acc: 0.9869 - val_loss: 0.2576 - val_acc: 0.9343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9842\n",
      "Epoch 00027: val_loss improved from 0.24943 to 0.24610, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/027-0.2461.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0510 - acc: 0.9842 - val_loss: 0.2461 - val_acc: 0.9338\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9917\n",
      "Epoch 00028: val_loss did not improve from 0.24610\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0289 - acc: 0.9917 - val_loss: 0.2483 - val_acc: 0.9404\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00029: val_loss improved from 0.24610 to 0.24162, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/029-0.2416.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.2416 - val_acc: 0.9406\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9936\n",
      "Epoch 00030: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0234 - acc: 0.9936 - val_loss: 0.4224 - val_acc: 0.9145\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9844\n",
      "Epoch 00031: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0465 - acc: 0.9844 - val_loss: 0.2554 - val_acc: 0.9383\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9910\n",
      "Epoch 00032: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0300 - acc: 0.9910 - val_loss: 0.2569 - val_acc: 0.9453\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9934\n",
      "Epoch 00033: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0226 - acc: 0.9934 - val_loss: 0.2466 - val_acc: 0.9450\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9918\n",
      "Epoch 00034: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0261 - acc: 0.9918 - val_loss: 0.2493 - val_acc: 0.9418\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9929\n",
      "Epoch 00035: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0240 - acc: 0.9929 - val_loss: 0.2878 - val_acc: 0.9294\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9870\n",
      "Epoch 00036: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0420 - acc: 0.9870 - val_loss: 0.2640 - val_acc: 0.9413\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9947\n",
      "Epoch 00037: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0186 - acc: 0.9947 - val_loss: 0.2642 - val_acc: 0.9441\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9951\n",
      "Epoch 00038: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.2811 - val_acc: 0.9420\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9865\n",
      "Epoch 00039: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0417 - acc: 0.9865 - val_loss: 0.2934 - val_acc: 0.9359\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 00040: val_loss did not improve from 0.24162\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.2781 - val_acc: 0.9404\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9938\n",
      "Epoch 00041: val_loss improved from 0.24162 to 0.23934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/041-0.2393.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0205 - acc: 0.9938 - val_loss: 0.2393 - val_acc: 0.9434\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00042: val_loss did not improve from 0.23934\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.2539 - val_acc: 0.9434\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9910\n",
      "Epoch 00043: val_loss did not improve from 0.23934\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0275 - acc: 0.9909 - val_loss: 0.3872 - val_acc: 0.9168\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9875\n",
      "Epoch 00044: val_loss did not improve from 0.23934\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0409 - acc: 0.9875 - val_loss: 0.2721 - val_acc: 0.9422\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9973\n",
      "Epoch 00045: val_loss improved from 0.23934 to 0.23786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/045-0.2379.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.2379 - val_acc: 0.9441\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9917\n",
      "Epoch 00046: val_loss improved from 0.23786 to 0.22498, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/046-0.2250.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0271 - acc: 0.9916 - val_loss: 0.2250 - val_acc: 0.9495\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9911\n",
      "Epoch 00047: val_loss did not improve from 0.22498\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0289 - acc: 0.9911 - val_loss: 0.2572 - val_acc: 0.9406\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9915\n",
      "Epoch 00048: val_loss did not improve from 0.22498\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0273 - acc: 0.9915 - val_loss: 0.2618 - val_acc: 0.9397\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 00049: val_loss improved from 0.22498 to 0.20803, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv_checkpoint/049-0.2080.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.2080 - val_acc: 0.9555\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00050: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 0.2858 - val_acc: 0.9446\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.3239 - val_acc: 0.9348\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9913\n",
      "Epoch 00052: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0269 - acc: 0.9912 - val_loss: 0.2545 - val_acc: 0.9441\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9902\n",
      "Epoch 00053: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0325 - acc: 0.9902 - val_loss: 0.2317 - val_acc: 0.9511\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9976\n",
      "Epoch 00054: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.2649 - val_acc: 0.9439\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 00055: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.2860 - val_acc: 0.9406\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 00056: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0199 - acc: 0.9939 - val_loss: 0.2579 - val_acc: 0.9462\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9982\n",
      "Epoch 00057: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0071 - acc: 0.9982 - val_loss: 0.2565 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.3964 - val_acc: 0.9252\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.2438 - val_acc: 0.9455\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9932\n",
      "Epoch 00060: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0211 - acc: 0.9932 - val_loss: 0.3098 - val_acc: 0.9425\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9958\n",
      "Epoch 00061: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0121 - acc: 0.9958 - val_loss: 0.2347 - val_acc: 0.9485\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 00062: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0117 - acc: 0.9964 - val_loss: 0.3437 - val_acc: 0.9329\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 00063: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0141 - acc: 0.9954 - val_loss: 0.3097 - val_acc: 0.9404\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9924\n",
      "Epoch 00064: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0240 - acc: 0.9923 - val_loss: 0.3002 - val_acc: 0.9390\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9930\n",
      "Epoch 00065: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0221 - acc: 0.9930 - val_loss: 0.2508 - val_acc: 0.9495\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 00066: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.2512 - val_acc: 0.9495\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9938\n",
      "Epoch 00067: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0204 - acc: 0.9938 - val_loss: 0.2475 - val_acc: 0.9474\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9932\n",
      "Epoch 00068: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0244 - acc: 0.9932 - val_loss: 0.2393 - val_acc: 0.9495\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980\n",
      "Epoch 00069: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0071 - acc: 0.9980 - val_loss: 0.2426 - val_acc: 0.9490\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00070: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2676 - val_acc: 0.9506\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9957\n",
      "Epoch 00071: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0140 - acc: 0.9957 - val_loss: 0.2635 - val_acc: 0.9490\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9963\n",
      "Epoch 00072: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0123 - acc: 0.9963 - val_loss: 0.3139 - val_acc: 0.9383\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9962\n",
      "Epoch 00073: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0120 - acc: 0.9963 - val_loss: 0.3325 - val_acc: 0.9394\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9960\n",
      "Epoch 00074: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0122 - acc: 0.9960 - val_loss: 0.3090 - val_acc: 0.9378\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9965\n",
      "Epoch 00075: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.2274 - val_acc: 0.9532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9972\n",
      "Epoch 00076: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.2913 - val_acc: 0.9455\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9969\n",
      "Epoch 00077: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0105 - acc: 0.9969 - val_loss: 0.3667 - val_acc: 0.9334\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 00078: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0127 - acc: 0.9958 - val_loss: 0.3379 - val_acc: 0.9387\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9980\n",
      "Epoch 00079: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 0.3690 - val_acc: 0.9315\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9930\n",
      "Epoch 00080: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0215 - acc: 0.9930 - val_loss: 0.2386 - val_acc: 0.9534\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 00081: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.2818 - val_acc: 0.9460\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00082: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.2307 - val_acc: 0.9564\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00083: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.2597 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9939\n",
      "Epoch 00084: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0190 - acc: 0.9939 - val_loss: 0.3108 - val_acc: 0.9390\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9926\n",
      "Epoch 00085: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0213 - acc: 0.9926 - val_loss: 0.2220 - val_acc: 0.9548\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00086: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2588 - val_acc: 0.9536\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00087: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.2135 - val_acc: 0.9569\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9964\n",
      "Epoch 00088: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.2996 - val_acc: 0.9446\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9933\n",
      "Epoch 00089: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.2769 - val_acc: 0.9515\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9931\n",
      "Epoch 00090: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0221 - acc: 0.9931 - val_loss: 0.2211 - val_acc: 0.9569\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 00091: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.2271 - val_acc: 0.9564\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9989\n",
      "Epoch 00092: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2876 - val_acc: 0.9443\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00093: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.2512 - val_acc: 0.9567\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9965\n",
      "Epoch 00094: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.3444 - val_acc: 0.9364\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9945\n",
      "Epoch 00095: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0158 - acc: 0.9945 - val_loss: 0.3016 - val_acc: 0.9450\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9983\n",
      "Epoch 00096: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0053 - acc: 0.9983 - val_loss: 0.2333 - val_acc: 0.9574\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00097: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.2652 - val_acc: 0.9490\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9922\n",
      "Epoch 00098: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0253 - acc: 0.9922 - val_loss: 0.2711 - val_acc: 0.9527\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9958\n",
      "Epoch 00099: val_loss did not improve from 0.20803\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0136 - acc: 0.9958 - val_loss: 0.2279 - val_acc: 0.9541\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2687 - acc: 0.9398\n",
      "Loss: 0.26865348473349865 Accuracy: 0.93977153\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8724 - acc: 0.4321\n",
      "Epoch 00001: val_loss improved from inf to 1.50611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/001-1.5061.hdf5\n",
      "36805/36805 [==============================] - 315s 9ms/sample - loss: 1.8724 - acc: 0.4320 - val_loss: 1.5061 - val_acc: 0.5309\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7499\n",
      "Epoch 00002: val_loss improved from 1.50611 to 0.73238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/002-0.7324.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.8266 - acc: 0.7499 - val_loss: 0.7324 - val_acc: 0.7857\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.8413\n",
      "Epoch 00003: val_loss improved from 0.73238 to 0.48646, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/003-0.4865.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.5233 - acc: 0.8413 - val_loss: 0.4865 - val_acc: 0.8549\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8808\n",
      "Epoch 00004: val_loss did not improve from 0.48646\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.3857 - acc: 0.8807 - val_loss: 0.4974 - val_acc: 0.8528\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9057\n",
      "Epoch 00005: val_loss improved from 0.48646 to 0.45946, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/005-0.4595.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.3103 - acc: 0.9057 - val_loss: 0.4595 - val_acc: 0.8770\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9237\n",
      "Epoch 00006: val_loss improved from 0.45946 to 0.27915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/006-0.2792.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.2460 - acc: 0.9238 - val_loss: 0.2792 - val_acc: 0.9147\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9328\n",
      "Epoch 00007: val_loss did not improve from 0.27915\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.2125 - acc: 0.9328 - val_loss: 0.3086 - val_acc: 0.9113\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9437\n",
      "Epoch 00008: val_loss did not improve from 0.27915\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1823 - acc: 0.9437 - val_loss: 0.2880 - val_acc: 0.9133\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9519\n",
      "Epoch 00009: val_loss did not improve from 0.27915\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1541 - acc: 0.9518 - val_loss: 0.2973 - val_acc: 0.9108\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9486\n",
      "Epoch 00010: val_loss improved from 0.27915 to 0.27654, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/010-0.2765.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1631 - acc: 0.9485 - val_loss: 0.2765 - val_acc: 0.9185\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9627\n",
      "Epoch 00011: val_loss did not improve from 0.27654\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1192 - acc: 0.9627 - val_loss: 0.2929 - val_acc: 0.9189\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9639\n",
      "Epoch 00012: val_loss improved from 0.27654 to 0.21610, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/012-0.2161.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1145 - acc: 0.9639 - val_loss: 0.2161 - val_acc: 0.9362\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9677\n",
      "Epoch 00013: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1019 - acc: 0.9676 - val_loss: 0.2206 - val_acc: 0.9371\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9691\n",
      "Epoch 00014: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1003 - acc: 0.9691 - val_loss: 0.2731 - val_acc: 0.9203\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9702\n",
      "Epoch 00015: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0924 - acc: 0.9702 - val_loss: 0.2777 - val_acc: 0.9301\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9806\n",
      "Epoch 00016: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0644 - acc: 0.9806 - val_loss: 0.2193 - val_acc: 0.9362\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9811\n",
      "Epoch 00017: val_loss improved from 0.21610 to 0.20248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/017-0.2025.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0602 - acc: 0.9811 - val_loss: 0.2025 - val_acc: 0.9399\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9817\n",
      "Epoch 00018: val_loss improved from 0.20248 to 0.18728, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/018-0.1873.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0586 - acc: 0.9816 - val_loss: 0.1873 - val_acc: 0.9455\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9798\n",
      "Epoch 00019: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0655 - acc: 0.9798 - val_loss: 0.2424 - val_acc: 0.9320\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9871\n",
      "Epoch 00020: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0439 - acc: 0.9871 - val_loss: 0.2001 - val_acc: 0.9434\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9858\n",
      "Epoch 00021: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0444 - acc: 0.9858 - val_loss: 0.2536 - val_acc: 0.9378\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9837\n",
      "Epoch 00022: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0523 - acc: 0.9837 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9873\n",
      "Epoch 00023: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0427 - acc: 0.9873 - val_loss: 0.2041 - val_acc: 0.9457\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9912\n",
      "Epoch 00024: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0310 - acc: 0.9911 - val_loss: 0.2453 - val_acc: 0.9443\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9817\n",
      "Epoch 00025: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0564 - acc: 0.9816 - val_loss: 0.2333 - val_acc: 0.9436\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9889\n",
      "Epoch 00026: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0377 - acc: 0.9889 - val_loss: 0.1979 - val_acc: 0.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9955\n",
      "Epoch 00027: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0194 - acc: 0.9955 - val_loss: 0.2261 - val_acc: 0.9446\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 00028: val_loss did not improve from 0.18728\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0326 - acc: 0.9899 - val_loss: 0.2161 - val_acc: 0.9411\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9902\n",
      "Epoch 00029: val_loss improved from 0.18728 to 0.18299, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/029-0.1830.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0336 - acc: 0.9901 - val_loss: 0.1830 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9902\n",
      "Epoch 00030: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0326 - acc: 0.9902 - val_loss: 0.2217 - val_acc: 0.9443\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9947\n",
      "Epoch 00031: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0191 - acc: 0.9947 - val_loss: 0.2073 - val_acc: 0.9485\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9943\n",
      "Epoch 00032: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 0.2715 - val_acc: 0.9362\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9896\n",
      "Epoch 00033: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0340 - acc: 0.9896 - val_loss: 0.1909 - val_acc: 0.9509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9952\n",
      "Epoch 00034: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0167 - acc: 0.9952 - val_loss: 0.2215 - val_acc: 0.9474\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9949\n",
      "Epoch 00035: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0178 - acc: 0.9949 - val_loss: 0.2203 - val_acc: 0.9495\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9888\n",
      "Epoch 00036: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0364 - acc: 0.9888 - val_loss: 0.1839 - val_acc: 0.9557\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00037: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.2056 - val_acc: 0.9548\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9958\n",
      "Epoch 00038: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0153 - acc: 0.9958 - val_loss: 0.3429 - val_acc: 0.9248\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9873\n",
      "Epoch 00039: val_loss did not improve from 0.18299\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0394 - acc: 0.9873 - val_loss: 0.1875 - val_acc: 0.9525\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9972\n",
      "Epoch 00040: val_loss improved from 0.18299 to 0.16306, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/040-0.1631.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.1631 - val_acc: 0.9637\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9965\n",
      "Epoch 00041: val_loss did not improve from 0.16306\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0131 - acc: 0.9965 - val_loss: 0.1952 - val_acc: 0.9546\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9961\n",
      "Epoch 00042: val_loss did not improve from 0.16306\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0133 - acc: 0.9961 - val_loss: 0.4377 - val_acc: 0.9092\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9936\n",
      "Epoch 00043: val_loss did not improve from 0.16306\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.2343 - val_acc: 0.9485\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9917\n",
      "Epoch 00044: val_loss did not improve from 0.16306\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0276 - acc: 0.9917 - val_loss: 0.1832 - val_acc: 0.9613\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 00045: val_loss improved from 0.16306 to 0.15786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/045-0.1579.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0071 - acc: 0.9984 - val_loss: 0.1579 - val_acc: 0.9609\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9981\n",
      "Epoch 00046: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0081 - acc: 0.9981 - val_loss: 0.2232 - val_acc: 0.9492\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 00047: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0240 - acc: 0.9925 - val_loss: 0.2489 - val_acc: 0.9467\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9960\n",
      "Epoch 00048: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0140 - acc: 0.9960 - val_loss: 0.1816 - val_acc: 0.9569\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00049: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.3019 - val_acc: 0.9343\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9960\n",
      "Epoch 00050: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.2025 - val_acc: 0.9557\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.2248 - val_acc: 0.9541\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 00052: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1969 - val_acc: 0.9536\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9963\n",
      "Epoch 00053: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0125 - acc: 0.9963 - val_loss: 0.2384 - val_acc: 0.9499\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9947\n",
      "Epoch 00054: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0188 - acc: 0.9946 - val_loss: 0.2416 - val_acc: 0.9427\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9931\n",
      "Epoch 00055: val_loss did not improve from 0.15786\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0224 - acc: 0.9931 - val_loss: 0.1811 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 00056: val_loss improved from 0.15786 to 0.15750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/056-0.1575.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.1575 - val_acc: 0.9623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00057: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1953 - val_acc: 0.9564\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9953\n",
      "Epoch 00058: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0156 - acc: 0.9953 - val_loss: 0.2376 - val_acc: 0.9483\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0116 - acc: 0.9960 - val_loss: 0.2246 - val_acc: 0.9518\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9974\n",
      "Epoch 00060: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0088 - acc: 0.9974 - val_loss: 0.1660 - val_acc: 0.9637\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9972\n",
      "Epoch 00061: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.1936 - val_acc: 0.9581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 00062: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0122 - acc: 0.9964 - val_loss: 0.2001 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9916\n",
      "Epoch 00063: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0267 - acc: 0.9916 - val_loss: 0.2015 - val_acc: 0.9555\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 00064: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.1995 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00065: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1832 - val_acc: 0.9599\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9952\n",
      "Epoch 00066: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0160 - acc: 0.9952 - val_loss: 0.1741 - val_acc: 0.9595\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9980\n",
      "Epoch 00067: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0067 - acc: 0.9980 - val_loss: 0.2158 - val_acc: 0.9529\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9982\n",
      "Epoch 00068: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1840 - val_acc: 0.9562\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00069: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.3456 - val_acc: 0.9348\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0128 - acc: 0.9959 - val_loss: 0.1860 - val_acc: 0.9571\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9978\n",
      "Epoch 00071: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0079 - acc: 0.9978 - val_loss: 0.1914 - val_acc: 0.9576\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9967\n",
      "Epoch 00072: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 0.2159 - val_acc: 0.9539\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9967\n",
      "Epoch 00073: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0108 - acc: 0.9967 - val_loss: 0.2551 - val_acc: 0.9408\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9970\n",
      "Epoch 00074: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0091 - acc: 0.9970 - val_loss: 0.1904 - val_acc: 0.9569\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9952\n",
      "Epoch 00075: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0155 - acc: 0.9952 - val_loss: 0.1866 - val_acc: 0.9620\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2345 - val_acc: 0.9478\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 00077: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1952 - val_acc: 0.9581\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00078: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0073 - acc: 0.9977 - val_loss: 0.3037 - val_acc: 0.9401\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9928\n",
      "Epoch 00079: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0238 - acc: 0.9928 - val_loss: 0.1955 - val_acc: 0.9595\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00080: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.1642 - val_acc: 0.9655\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00081: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1852 - val_acc: 0.9618\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 00082: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.2077 - val_acc: 0.9588\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9952\n",
      "Epoch 00083: val_loss did not improve from 0.15750\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0150 - acc: 0.9952 - val_loss: 0.2387 - val_acc: 0.9509\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9985\n",
      "Epoch 00084: val_loss improved from 0.15750 to 0.15155, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv_checkpoint/084-0.1516.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1516 - val_acc: 0.9679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9944\n",
      "Epoch 00085: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0178 - acc: 0.9944 - val_loss: 0.1743 - val_acc: 0.9646\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00086: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0044 - acc: 0.9989 - val_loss: 0.1574 - val_acc: 0.9632\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00087: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.2224 - val_acc: 0.9546\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9943\n",
      "Epoch 00088: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.1833 - val_acc: 0.9623\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 00089: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.2703 - val_acc: 0.9432\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9943\n",
      "Epoch 00090: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0191 - acc: 0.9943 - val_loss: 0.1780 - val_acc: 0.9599\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 00091: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1695 - val_acc: 0.9655\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991\n",
      "Epoch 00092: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1896 - val_acc: 0.9632\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 00093: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.1845 - val_acc: 0.9611\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00094: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1951 - val_acc: 0.9571\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 00095: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.1930 - val_acc: 0.9592\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00096: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0066 - acc: 0.9984 - val_loss: 0.1988 - val_acc: 0.9569\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9963\n",
      "Epoch 00097: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0115 - acc: 0.9963 - val_loss: 0.1726 - val_acc: 0.9606\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9975\n",
      "Epoch 00098: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.1836 - val_acc: 0.9625\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 00099: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0145 - acc: 0.9958 - val_loss: 0.1843 - val_acc: 0.9585\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9983\n",
      "Epoch 00100: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0051 - acc: 0.9983 - val_loss: 0.2696 - val_acc: 0.9467\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00101: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1954 - val_acc: 0.9648\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9969\n",
      "Epoch 00102: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0107 - acc: 0.9969 - val_loss: 0.1903 - val_acc: 0.9658\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00103: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1837 - val_acc: 0.9630\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 00104: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1810 - val_acc: 0.9632\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00105: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.2622 - val_acc: 0.9483\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 00106: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0127 - acc: 0.9958 - val_loss: 0.1991 - val_acc: 0.9606\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9969\n",
      "Epoch 00107: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.1698 - val_acc: 0.9641\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9985\n",
      "Epoch 00108: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0050 - acc: 0.9984 - val_loss: 0.2270 - val_acc: 0.9541\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9964\n",
      "Epoch 00109: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0113 - acc: 0.9964 - val_loss: 0.1898 - val_acc: 0.9623\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00110: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1743 - val_acc: 0.9632\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00111: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.2342 - val_acc: 0.9522\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 00112: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0171 - acc: 0.9948 - val_loss: 0.2160 - val_acc: 0.9560\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 00113: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.1702 - val_acc: 0.9665\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00114: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1679 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00115: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.1735 - val_acc: 0.9634\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00116: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1671 - val_acc: 0.9655\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 00117: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.4973 - val_acc: 0.9220\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 00118: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0165 - acc: 0.9947 - val_loss: 0.1823 - val_acc: 0.9644\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9969\n",
      "Epoch 00119: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0099 - acc: 0.9969 - val_loss: 0.1906 - val_acc: 0.9599\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00120: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1829 - val_acc: 0.9627\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00121: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.2255 - val_acc: 0.9536\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00122: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.2100 - val_acc: 0.9581\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00123: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1915 - val_acc: 0.9606\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00124: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1750 - val_acc: 0.9644\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00125: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 0.2191 - val_acc: 0.9536\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00126: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1992 - val_acc: 0.9592\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 00127: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.2160 - val_acc: 0.9590\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00128: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.2176 - val_acc: 0.9574\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9974\n",
      "Epoch 00129: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0076 - acc: 0.9973 - val_loss: 0.2198 - val_acc: 0.9578\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9962\n",
      "Epoch 00130: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0122 - acc: 0.9963 - val_loss: 0.1762 - val_acc: 0.9660\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00131: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2140 - val_acc: 0.9613\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00132: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1786 - val_acc: 0.9599\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9976\n",
      "Epoch 00133: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.1931 - val_acc: 0.9609\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 00134: val_loss did not improve from 0.15155\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0065 - acc: 0.9980 - val_loss: 0.2039 - val_acc: 0.9620\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.1970 - acc: 0.9535\n",
      "Loss: 0.19696606612016726 Accuracy: 0.9534787\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8138 - acc: 0.4434\n",
      "Epoch 00001: val_loss improved from inf to 1.47763, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/001-1.4776.hdf5\n",
      "36805/36805 [==============================] - 340s 9ms/sample - loss: 1.8139 - acc: 0.4434 - val_loss: 1.4776 - val_acc: 0.5222\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7229 - acc: 0.7749\n",
      "Epoch 00002: val_loss improved from 1.47763 to 0.51095, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/002-0.5109.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.7229 - acc: 0.7749 - val_loss: 0.5109 - val_acc: 0.8425\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8579\n",
      "Epoch 00003: val_loss improved from 0.51095 to 0.47869, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/003-0.4787.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.4544 - acc: 0.8579 - val_loss: 0.4787 - val_acc: 0.8484\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8969\n",
      "Epoch 00004: val_loss improved from 0.47869 to 0.31158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/004-0.3116.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.3326 - acc: 0.8969 - val_loss: 0.3116 - val_acc: 0.9068\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9182\n",
      "Epoch 00005: val_loss improved from 0.31158 to 0.27541, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/005-0.2754.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.2643 - acc: 0.9182 - val_loss: 0.2754 - val_acc: 0.9206\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9285\n",
      "Epoch 00006: val_loss improved from 0.27541 to 0.25508, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/006-0.2551.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.2256 - acc: 0.9285 - val_loss: 0.2551 - val_acc: 0.9217\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9443\n",
      "Epoch 00007: val_loss did not improve from 0.25508\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1789 - acc: 0.9442 - val_loss: 0.2620 - val_acc: 0.9234\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9490\n",
      "Epoch 00008: val_loss improved from 0.25508 to 0.23764, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/008-0.2376.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1631 - acc: 0.9491 - val_loss: 0.2376 - val_acc: 0.9292\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9576\n",
      "Epoch 00009: val_loss improved from 0.23764 to 0.21345, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/009-0.2134.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1342 - acc: 0.9576 - val_loss: 0.2134 - val_acc: 0.9338\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9602\n",
      "Epoch 00010: val_loss improved from 0.21345 to 0.20902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/010-0.2090.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.1256 - acc: 0.9602 - val_loss: 0.2090 - val_acc: 0.9322\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9653\n",
      "Epoch 00011: val_loss improved from 0.20902 to 0.18854, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/011-0.1885.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1114 - acc: 0.9653 - val_loss: 0.1885 - val_acc: 0.9446\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9718\n",
      "Epoch 00012: val_loss improved from 0.18854 to 0.17634, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/012-0.1763.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0885 - acc: 0.9718 - val_loss: 0.1763 - val_acc: 0.9506\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9754\n",
      "Epoch 00013: val_loss did not improve from 0.17634\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0795 - acc: 0.9754 - val_loss: 0.1908 - val_acc: 0.9413\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9773\n",
      "Epoch 00014: val_loss did not improve from 0.17634\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0727 - acc: 0.9773 - val_loss: 0.2049 - val_acc: 0.9392\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9731\n",
      "Epoch 00015: val_loss improved from 0.17634 to 0.17616, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/015-0.1762.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.0856 - acc: 0.9730 - val_loss: 0.1762 - val_acc: 0.9504\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9810\n",
      "Epoch 00016: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0623 - acc: 0.9809 - val_loss: 0.1764 - val_acc: 0.9509\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9822\n",
      "Epoch 00017: val_loss improved from 0.17616 to 0.14905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/017-0.1491.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0574 - acc: 0.9822 - val_loss: 0.1491 - val_acc: 0.9553\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9891\n",
      "Epoch 00018: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0382 - acc: 0.9891 - val_loss: 0.1535 - val_acc: 0.9553\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9858\n",
      "Epoch 00019: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0472 - acc: 0.9858 - val_loss: 0.2107 - val_acc: 0.9425\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9799\n",
      "Epoch 00020: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0627 - acc: 0.9799 - val_loss: 0.1825 - val_acc: 0.9446\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9853\n",
      "Epoch 00021: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1643 - val_acc: 0.9497\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9877\n",
      "Epoch 00022: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0386 - acc: 0.9877 - val_loss: 0.2004 - val_acc: 0.9429\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9858\n",
      "Epoch 00023: val_loss improved from 0.14905 to 0.13865, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/023-0.1387.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0447 - acc: 0.9858 - val_loss: 0.1387 - val_acc: 0.9616\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9878\n",
      "Epoch 00024: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0419 - acc: 0.9878 - val_loss: 0.1844 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9874\n",
      "Epoch 00025: val_loss improved from 0.13865 to 0.13812, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/025-0.1381.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0409 - acc: 0.9874 - val_loss: 0.1381 - val_acc: 0.9588\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9933\n",
      "Epoch 00026: val_loss did not improve from 0.13812\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0230 - acc: 0.9933 - val_loss: 0.1858 - val_acc: 0.9455\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9911\n",
      "Epoch 00027: val_loss improved from 0.13812 to 0.13642, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/027-0.1364.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0303 - acc: 0.9911 - val_loss: 0.1364 - val_acc: 0.9632\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9955\n",
      "Epoch 00028: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0171 - acc: 0.9955 - val_loss: 0.1891 - val_acc: 0.9534\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9858\n",
      "Epoch 00029: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0461 - acc: 0.9858 - val_loss: 0.1572 - val_acc: 0.9550\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9888\n",
      "Epoch 00030: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0375 - acc: 0.9888 - val_loss: 0.1787 - val_acc: 0.9543\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9949\n",
      "Epoch 00031: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0179 - acc: 0.9949 - val_loss: 0.1617 - val_acc: 0.9576\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 00032: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0139 - acc: 0.9965 - val_loss: 0.1513 - val_acc: 0.9634\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9903\n",
      "Epoch 00033: val_loss did not improve from 0.13642\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0323 - acc: 0.9903 - val_loss: 0.1644 - val_acc: 0.9567\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9928\n",
      "Epoch 00034: val_loss improved from 0.13642 to 0.13215, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/034-0.1321.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0262 - acc: 0.9928 - val_loss: 0.1321 - val_acc: 0.9627\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9962\n",
      "Epoch 00035: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0144 - acc: 0.9962 - val_loss: 0.1601 - val_acc: 0.9630\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9957\n",
      "Epoch 00036: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.1679 - val_acc: 0.9592\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9959\n",
      "Epoch 00037: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0148 - acc: 0.9959 - val_loss: 0.2119 - val_acc: 0.9471\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9924\n",
      "Epoch 00038: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0244 - acc: 0.9924 - val_loss: 0.1606 - val_acc: 0.9604\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9907\n",
      "Epoch 00039: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0298 - acc: 0.9907 - val_loss: 0.1590 - val_acc: 0.9564\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9979\n",
      "Epoch 00040: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0086 - acc: 0.9978 - val_loss: 0.1460 - val_acc: 0.9634\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9927\n",
      "Epoch 00041: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0239 - acc: 0.9927 - val_loss: 0.1843 - val_acc: 0.9548\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9924\n",
      "Epoch 00042: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0245 - acc: 0.9924 - val_loss: 0.1766 - val_acc: 0.9571\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9926\n",
      "Epoch 00043: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0243 - acc: 0.9926 - val_loss: 0.1356 - val_acc: 0.9665\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9979\n",
      "Epoch 00044: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0085 - acc: 0.9979 - val_loss: 0.1687 - val_acc: 0.9602\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976\n",
      "Epoch 00045: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0089 - acc: 0.9976 - val_loss: 0.1732 - val_acc: 0.9588\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9952\n",
      "Epoch 00046: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0152 - acc: 0.9952 - val_loss: 0.2013 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9949\n",
      "Epoch 00047: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0173 - acc: 0.9949 - val_loss: 0.1644 - val_acc: 0.9583\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00048: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0072 - acc: 0.9982 - val_loss: 0.1672 - val_acc: 0.9625\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9966\n",
      "Epoch 00049: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0124 - acc: 0.9966 - val_loss: 0.1526 - val_acc: 0.9632\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9947\n",
      "Epoch 00050: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0168 - acc: 0.9947 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9969\n",
      "Epoch 00051: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0110 - acc: 0.9969 - val_loss: 0.1458 - val_acc: 0.9620\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00052: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0106 - acc: 0.9971 - val_loss: 0.2657 - val_acc: 0.9434\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9949\n",
      "Epoch 00053: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.1754 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9940\n",
      "Epoch 00054: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0203 - acc: 0.9940 - val_loss: 0.1502 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00055: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.1559 - val_acc: 0.9620\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9982\n",
      "Epoch 00056: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1975 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 00057: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0131 - acc: 0.9962 - val_loss: 0.1754 - val_acc: 0.9569\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9934\n",
      "Epoch 00058: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0211 - acc: 0.9934 - val_loss: 0.1442 - val_acc: 0.9644\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0125 - acc: 0.9960 - val_loss: 0.1486 - val_acc: 0.9644\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00060: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0134 - acc: 0.9961 - val_loss: 0.1411 - val_acc: 0.9630\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9961\n",
      "Epoch 00061: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0149 - acc: 0.9960 - val_loss: 0.1589 - val_acc: 0.9623\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9970\n",
      "Epoch 00062: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.1344 - val_acc: 0.9679\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00063: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1646 - val_acc: 0.9632\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n",
      "Epoch 00064: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1513 - val_acc: 0.9627\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00065: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.1754 - val_acc: 0.9602\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9929\n",
      "Epoch 00066: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0240 - acc: 0.9929 - val_loss: 0.1586 - val_acc: 0.9662\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00067: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1424 - val_acc: 0.9679\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00068: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1648 - val_acc: 0.9634\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00069: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1806 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 00070: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0189 - acc: 0.9943 - val_loss: 0.1516 - val_acc: 0.9646\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00071: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.1378 - val_acc: 0.9676\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9946\n",
      "Epoch 00072: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 0.1370 - val_acc: 0.9637\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00073: val_loss did not improve from 0.13215\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1523 - val_acc: 0.9648\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00074: val_loss improved from 0.13215 to 0.12927, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/074-0.1293.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1293 - val_acc: 0.9700\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9966\n",
      "Epoch 00075: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0113 - acc: 0.9966 - val_loss: 0.1825 - val_acc: 0.9602\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 00076: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.1491 - val_acc: 0.9669\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9946\n",
      "Epoch 00077: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0173 - acc: 0.9946 - val_loss: 0.1803 - val_acc: 0.9588\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9966\n",
      "Epoch 00078: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0118 - acc: 0.9966 - val_loss: 0.1488 - val_acc: 0.9672\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9965\n",
      "Epoch 00079: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1646 - val_acc: 0.9602\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 00080: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1368 - val_acc: 0.9674\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 00081: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1455 - val_acc: 0.9648\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.2077 - val_acc: 0.9543\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00083: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 0.1601 - val_acc: 0.9627\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 00084: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0075 - acc: 0.9976 - val_loss: 0.1807 - val_acc: 0.9590\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 00085: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.1419 - val_acc: 0.9665\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965\n",
      "Epoch 00086: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.1359 - val_acc: 0.9667\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00087: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1528 - val_acc: 0.9637\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00088: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1647 - val_acc: 0.9651\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9970\n",
      "Epoch 00089: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0113 - acc: 0.9969 - val_loss: 0.1692 - val_acc: 0.9613\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9955\n",
      "Epoch 00090: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0140 - acc: 0.9955 - val_loss: 0.1479 - val_acc: 0.9630\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9970\n",
      "Epoch 00091: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0111 - acc: 0.9970 - val_loss: 0.1311 - val_acc: 0.9693\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00092: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1532 - val_acc: 0.9669\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00093: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1368 - val_acc: 0.9688\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00094: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1344 - val_acc: 0.9686\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9949\n",
      "Epoch 00095: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0152 - acc: 0.9949 - val_loss: 0.1592 - val_acc: 0.9639\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9964\n",
      "Epoch 00096: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0125 - acc: 0.9964 - val_loss: 0.1559 - val_acc: 0.9655\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00097: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1685 - val_acc: 0.9646\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00098: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.1509 - val_acc: 0.9683\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 00099: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1538 - val_acc: 0.9674\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00100: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.1783 - val_acc: 0.9602\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 00101: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1529 - val_acc: 0.9665\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00102: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0101 - acc: 0.9968 - val_loss: 0.2299 - val_acc: 0.9518\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9963\n",
      "Epoch 00103: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0123 - acc: 0.9963 - val_loss: 0.1386 - val_acc: 0.9665\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00104: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1925 - val_acc: 0.9567\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9966\n",
      "Epoch 00105: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0108 - acc: 0.9966 - val_loss: 0.1710 - val_acc: 0.9651\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
      "Epoch 00106: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0057 - acc: 0.9984 - val_loss: 0.1369 - val_acc: 0.9702\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00107: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1390 - val_acc: 0.9706\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9972\n",
      "Epoch 00108: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.1885 - val_acc: 0.9602\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00109: val_loss did not improve from 0.12927\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1341 - val_acc: 0.9676\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00110: val_loss improved from 0.12927 to 0.12578, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv_checkpoint/110-0.1258.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1258 - val_acc: 0.9697\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9946\n",
      "Epoch 00111: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0158 - acc: 0.9946 - val_loss: 0.1397 - val_acc: 0.9651\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00112: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1450 - val_acc: 0.9683\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9951\n",
      "Epoch 00113: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0153 - acc: 0.9951 - val_loss: 0.1557 - val_acc: 0.9634\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00114: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1372 - val_acc: 0.9688\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00115: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1388 - val_acc: 0.9697\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00116: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1417 - val_acc: 0.9686\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
      "Epoch 00117: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.2263 - val_acc: 0.9511\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9960\n",
      "Epoch 00118: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0135 - acc: 0.9959 - val_loss: 0.1890 - val_acc: 0.9602\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9965\n",
      "Epoch 00119: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0108 - acc: 0.9965 - val_loss: 0.1376 - val_acc: 0.9676\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00120: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1434 - val_acc: 0.9655\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00121: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.1355 - val_acc: 0.9702\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9990\n",
      "Epoch 00122: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0026 - acc: 0.9990 - val_loss: 0.1554 - val_acc: 0.9672\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00123: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1411 - val_acc: 0.9679\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00124: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1596 - val_acc: 0.9655\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9970\n",
      "Epoch 00125: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.1567 - val_acc: 0.9662\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00126: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1498 - val_acc: 0.9667\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00127: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.1694 - val_acc: 0.9667\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9964\n",
      "Epoch 00128: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0116 - acc: 0.9964 - val_loss: 0.1891 - val_acc: 0.9606\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9966\n",
      "Epoch 00129: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.1695 - val_acc: 0.9653\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9972\n",
      "Epoch 00130: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0086 - acc: 0.9971 - val_loss: 0.1278 - val_acc: 0.9697\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9975\n",
      "Epoch 00131: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0096 - acc: 0.9975 - val_loss: 0.1336 - val_acc: 0.9693\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00132: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1447 - val_acc: 0.9688\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00133: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1362 - val_acc: 0.9716\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9980\n",
      "Epoch 00134: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0069 - acc: 0.9980 - val_loss: 0.1549 - val_acc: 0.9674\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 00135: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.1630 - val_acc: 0.9660\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00136: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1413 - val_acc: 0.9697\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9963\n",
      "Epoch 00137: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0120 - acc: 0.9963 - val_loss: 0.1313 - val_acc: 0.9702\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00138: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.1504 - val_acc: 0.9681\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.8195e-04 - acc: 0.9999\n",
      "Epoch 00139: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 7.8297e-04 - acc: 0.9999 - val_loss: 0.1301 - val_acc: 0.9706\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 00140: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1471 - val_acc: 0.9690\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9948\n",
      "Epoch 00141: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1277 - val_acc: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00142: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1502 - val_acc: 0.9665\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9965\n",
      "Epoch 00143: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0108 - acc: 0.9965 - val_loss: 0.1402 - val_acc: 0.9681\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 00144: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1458 - val_acc: 0.9686\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00145: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1349 - val_acc: 0.9688\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.8806e-04 - acc: 0.9999\n",
      "Epoch 00146: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 7.8908e-04 - acc: 0.9999 - val_loss: 0.1309 - val_acc: 0.9713\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.6728e-04 - acc: 0.9998\n",
      "Epoch 00147: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 6.6719e-04 - acc: 0.9998 - val_loss: 0.1561 - val_acc: 0.9662\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9968\n",
      "Epoch 00148: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0104 - acc: 0.9968 - val_loss: 0.2388 - val_acc: 0.9460\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00149: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1622 - val_acc: 0.9679\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00150: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1551 - val_acc: 0.9697\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00151: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1505 - val_acc: 0.9669\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.4226e-04 - acc: 0.9998\n",
      "Epoch 00152: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 8.4215e-04 - acc: 0.9998 - val_loss: 0.1536 - val_acc: 0.9693\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.2267e-04 - acc: 0.9999\n",
      "Epoch 00153: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 6.6228e-04 - acc: 0.9999 - val_loss: 0.1552 - val_acc: 0.9702\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9970\n",
      "Epoch 00154: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1792 - val_acc: 0.9658\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9987\n",
      "Epoch 00155: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1586 - val_acc: 0.9641\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 00156: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0023 - acc: 0.9992 - val_loss: 0.1621 - val_acc: 0.9639\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
      "Epoch 00157: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1385 - val_acc: 0.9704\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.7726e-04 - acc: 0.9999\n",
      "Epoch 00158: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 8.7715e-04 - acc: 0.9999 - val_loss: 0.1352 - val_acc: 0.9716\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 00159: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0032 - acc: 0.9989 - val_loss: 0.1876 - val_acc: 0.9634\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 00160: val_loss did not improve from 0.12578\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.0113 - acc: 0.9962 - val_loss: 0.1652 - val_acc: 0.9686\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.1618 - acc: 0.9657\n",
      "Loss: 0.16177561581853778 Accuracy: 0.9657321\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5891 - acc: 0.5093\n",
      "Epoch 00001: val_loss improved from inf to 1.23948, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/001-1.2395.hdf5\n",
      "36805/36805 [==============================] - 371s 10ms/sample - loss: 1.5893 - acc: 0.5093 - val_loss: 1.2395 - val_acc: 0.5865\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6382 - acc: 0.7953\n",
      "Epoch 00002: val_loss improved from 1.23948 to 0.47971, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/002-0.4797.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.6382 - acc: 0.7953 - val_loss: 0.4797 - val_acc: 0.8474\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8687\n",
      "Epoch 00003: val_loss improved from 0.47971 to 0.36773, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/003-0.3677.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.4023 - acc: 0.8687 - val_loss: 0.3677 - val_acc: 0.8856\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9066\n",
      "Epoch 00004: val_loss improved from 0.36773 to 0.29553, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/004-0.2955.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.2861 - acc: 0.9066 - val_loss: 0.2955 - val_acc: 0.9066\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9253\n",
      "Epoch 00005: val_loss improved from 0.29553 to 0.27947, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/005-0.2795.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.2274 - acc: 0.9253 - val_loss: 0.2795 - val_acc: 0.9175\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9402\n",
      "Epoch 00006: val_loss did not improve from 0.27947\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1868 - acc: 0.9401 - val_loss: 0.2926 - val_acc: 0.9117\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9441\n",
      "Epoch 00007: val_loss improved from 0.27947 to 0.25109, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/007-0.2511.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.1731 - acc: 0.9441 - val_loss: 0.2511 - val_acc: 0.9238\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9532\n",
      "Epoch 00008: val_loss did not improve from 0.25109\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1437 - acc: 0.9532 - val_loss: 0.2935 - val_acc: 0.9138\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9600\n",
      "Epoch 00009: val_loss improved from 0.25109 to 0.21439, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/009-0.2144.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.1235 - acc: 0.9600 - val_loss: 0.2144 - val_acc: 0.9352\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9629\n",
      "Epoch 00010: val_loss did not improve from 0.21439\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1089 - acc: 0.9629 - val_loss: 0.2617 - val_acc: 0.9252\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9663\n",
      "Epoch 00011: val_loss improved from 0.21439 to 0.19879, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/011-0.1988.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.1019 - acc: 0.9663 - val_loss: 0.1988 - val_acc: 0.9429\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9757\n",
      "Epoch 00012: val_loss did not improve from 0.19879\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0765 - acc: 0.9757 - val_loss: 0.2396 - val_acc: 0.9345\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9780\n",
      "Epoch 00013: val_loss did not improve from 0.19879\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0685 - acc: 0.9780 - val_loss: 0.2897 - val_acc: 0.9171\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9712\n",
      "Epoch 00014: val_loss did not improve from 0.19879\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0879 - acc: 0.9712 - val_loss: 0.2084 - val_acc: 0.9392\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9783\n",
      "Epoch 00015: val_loss improved from 0.19879 to 0.18914, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/015-0.1891.hdf5\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0677 - acc: 0.9783 - val_loss: 0.1891 - val_acc: 0.9443\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9847\n",
      "Epoch 00016: val_loss did not improve from 0.18914\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0500 - acc: 0.9847 - val_loss: 0.2723 - val_acc: 0.9285\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9777\n",
      "Epoch 00017: val_loss improved from 0.18914 to 0.18196, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/017-0.1820.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0681 - acc: 0.9777 - val_loss: 0.1820 - val_acc: 0.9532\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9885\n",
      "Epoch 00018: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0362 - acc: 0.9885 - val_loss: 0.2318 - val_acc: 0.9357\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9859\n",
      "Epoch 00019: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0455 - acc: 0.9858 - val_loss: 0.2400 - val_acc: 0.9364\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9860\n",
      "Epoch 00020: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0456 - acc: 0.9860 - val_loss: 0.2208 - val_acc: 0.9420\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9847\n",
      "Epoch 00021: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0504 - acc: 0.9846 - val_loss: 0.2586 - val_acc: 0.9336\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9889\n",
      "Epoch 00022: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0361 - acc: 0.9889 - val_loss: 0.1949 - val_acc: 0.9481\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9922\n",
      "Epoch 00023: val_loss did not improve from 0.18196\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0246 - acc: 0.9922 - val_loss: 0.2279 - val_acc: 0.9404\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9853\n",
      "Epoch 00024: val_loss improved from 0.18196 to 0.16649, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/024-0.1665.hdf5\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0459 - acc: 0.9853 - val_loss: 0.1665 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9934\n",
      "Epoch 00025: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0211 - acc: 0.9934 - val_loss: 0.1905 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9836\n",
      "Epoch 00026: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0512 - acc: 0.9836 - val_loss: 0.1901 - val_acc: 0.9518\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9937\n",
      "Epoch 00027: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0210 - acc: 0.9937 - val_loss: 0.2104 - val_acc: 0.9490\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9929\n",
      "Epoch 00028: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 0.1806 - val_acc: 0.9504\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9894\n",
      "Epoch 00029: val_loss improved from 0.16649 to 0.15843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/029-0.1584.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0329 - acc: 0.9894 - val_loss: 0.1584 - val_acc: 0.9562\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9944\n",
      "Epoch 00030: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0188 - acc: 0.9944 - val_loss: 0.1936 - val_acc: 0.9527\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9927\n",
      "Epoch 00031: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0227 - acc: 0.9927 - val_loss: 0.2084 - val_acc: 0.9436\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9884\n",
      "Epoch 00032: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0372 - acc: 0.9884 - val_loss: 0.1770 - val_acc: 0.9574\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9975\n",
      "Epoch 00033: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0097 - acc: 0.9975 - val_loss: 0.1605 - val_acc: 0.9599\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00034: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0106 - acc: 0.9966 - val_loss: 0.2402 - val_acc: 0.9481\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9882\n",
      "Epoch 00035: val_loss did not improve from 0.15843\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0361 - acc: 0.9882 - val_loss: 0.1627 - val_acc: 0.9618\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9915\n",
      "Epoch 00036: val_loss improved from 0.15843 to 0.15189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/036-0.1519.hdf5\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0266 - acc: 0.9915 - val_loss: 0.1519 - val_acc: 0.9627\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00037: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.1925 - val_acc: 0.9550\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9950\n",
      "Epoch 00038: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0154 - acc: 0.9950 - val_loss: 0.2017 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9918\n",
      "Epoch 00039: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0247 - acc: 0.9918 - val_loss: 0.2898 - val_acc: 0.9404\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9945\n",
      "Epoch 00040: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0197 - acc: 0.9945 - val_loss: 0.1581 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 00041: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0083 - acc: 0.9979 - val_loss: 0.1603 - val_acc: 0.9625\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00042: val_loss did not improve from 0.15189\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.1914 - val_acc: 0.9583\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9902\n",
      "Epoch 00043: val_loss improved from 0.15189 to 0.15119, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/043-0.1512.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0302 - acc: 0.9902 - val_loss: 0.1512 - val_acc: 0.9620\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 00044: val_loss did not improve from 0.15119\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.1514 - val_acc: 0.9681\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00045: val_loss did not improve from 0.15119\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1821 - val_acc: 0.9613\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 0.15119\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0149 - acc: 0.9951 - val_loss: 0.2335 - val_acc: 0.9476\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9943\n",
      "Epoch 00047: val_loss did not improve from 0.15119\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0168 - acc: 0.9943 - val_loss: 0.2070 - val_acc: 0.9504\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9937\n",
      "Epoch 00048: val_loss did not improve from 0.15119\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.1545 - val_acc: 0.9627\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9991\n",
      "Epoch 00049: val_loss improved from 0.15119 to 0.14567, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/049-0.1457.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.1457 - val_acc: 0.9674\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9935\n",
      "Epoch 00050: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0195 - acc: 0.9934 - val_loss: 0.2304 - val_acc: 0.9457\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9936\n",
      "Epoch 00051: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0205 - acc: 0.9935 - val_loss: 0.1741 - val_acc: 0.9606\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9932\n",
      "Epoch 00052: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0225 - acc: 0.9932 - val_loss: 0.1538 - val_acc: 0.9634\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9941\n",
      "Epoch 00053: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0200 - acc: 0.9941 - val_loss: 0.1698 - val_acc: 0.9571\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 00054: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0089 - acc: 0.9977 - val_loss: 0.1612 - val_acc: 0.9604\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00055: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1525 - val_acc: 0.9646\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0143 - acc: 0.9958 - val_loss: 0.1777 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00057: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0190 - acc: 0.9942 - val_loss: 0.1562 - val_acc: 0.9618\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 00058: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1932 - val_acc: 0.9595\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00059: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.2028 - val_acc: 0.9548\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9907\n",
      "Epoch 00060: val_loss did not improve from 0.14567\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.1685 - val_acc: 0.9606\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9957\n",
      "Epoch 00061: val_loss improved from 0.14567 to 0.14422, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv_checkpoint/061-0.1442.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.1442 - val_acc: 0.9676\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00062: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1687 - val_acc: 0.9646\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 00063: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.2118 - val_acc: 0.9550\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0109 - acc: 0.9970 - val_loss: 0.1562 - val_acc: 0.9653\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9966\n",
      "Epoch 00065: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.1839 - val_acc: 0.9597\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9942\n",
      "Epoch 00066: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0197 - acc: 0.9942 - val_loss: 0.1743 - val_acc: 0.9630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9987\n",
      "Epoch 00067: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0039 - acc: 0.9987 - val_loss: 0.1560 - val_acc: 0.9620\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9957\n",
      "Epoch 00068: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0160 - acc: 0.9957 - val_loss: 0.1724 - val_acc: 0.9637\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9986\n",
      "Epoch 00069: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.2040 - val_acc: 0.9585\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9951\n",
      "Epoch 00070: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0167 - acc: 0.9951 - val_loss: 0.1805 - val_acc: 0.9590\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00071: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0037 - acc: 0.9989 - val_loss: 0.1880 - val_acc: 0.9623\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 00072: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1960 - val_acc: 0.9576\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9939\n",
      "Epoch 00073: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0184 - acc: 0.9939 - val_loss: 0.1931 - val_acc: 0.9627\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 00074: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1739 - val_acc: 0.9627\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9958\n",
      "Epoch 00075: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.1703 - val_acc: 0.9639\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9966\n",
      "Epoch 00076: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0111 - acc: 0.9966 - val_loss: 0.1585 - val_acc: 0.9646\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00077: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1540 - val_acc: 0.9662\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00078: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0014 - acc: 0.9995 - val_loss: 0.1556 - val_acc: 0.9679\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 00079: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.2218 - val_acc: 0.9548\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9945\n",
      "Epoch 00080: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0156 - acc: 0.9945 - val_loss: 0.1971 - val_acc: 0.9606\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00081: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1914 - val_acc: 0.9578\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9952\n",
      "Epoch 00082: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0143 - acc: 0.9952 - val_loss: 0.2002 - val_acc: 0.9576\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00083: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1808 - val_acc: 0.9571\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
      "Epoch 00084: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1777 - val_acc: 0.9618\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00085: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.2017 - val_acc: 0.9609\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 00086: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.1712 - val_acc: 0.9648\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00087: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1737 - val_acc: 0.9644\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9959\n",
      "Epoch 00088: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0133 - acc: 0.9958 - val_loss: 0.1635 - val_acc: 0.9667\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9966\n",
      "Epoch 00089: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0115 - acc: 0.9966 - val_loss: 0.1575 - val_acc: 0.9658\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993\n",
      "Epoch 00090: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1526 - val_acc: 0.9658\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00091: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1691 - val_acc: 0.9653\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00092: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0027 - acc: 0.9992 - val_loss: 0.1905 - val_acc: 0.9632\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9934\n",
      "Epoch 00093: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0224 - acc: 0.9934 - val_loss: 0.1723 - val_acc: 0.9620\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00094: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1764 - val_acc: 0.9620\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9980\n",
      "Epoch 00095: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0067 - acc: 0.9980 - val_loss: 0.2310 - val_acc: 0.9543\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9971\n",
      "Epoch 00096: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0081 - acc: 0.9971 - val_loss: 0.1961 - val_acc: 0.9606\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
      "Epoch 00097: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.1822 - val_acc: 0.9655\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00098: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.1475 - val_acc: 0.9651\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9980\n",
      "Epoch 00099: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0066 - acc: 0.9980 - val_loss: 0.1669 - val_acc: 0.9660\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00100: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0037 - acc: 0.9989 - val_loss: 0.1852 - val_acc: 0.9618\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00101: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.1458 - val_acc: 0.9683\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9986\n",
      "Epoch 00102: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.1662 - val_acc: 0.9662\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00103: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1647 - val_acc: 0.9655\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00104: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1679 - val_acc: 0.9648\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9959\n",
      "Epoch 00105: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.1883 - val_acc: 0.9585\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00106: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1546 - val_acc: 0.9665\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 00107: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1725 - val_acc: 0.9653\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.1612 - val_acc: 0.9667\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00109: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1855 - val_acc: 0.9632\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9961\n",
      "Epoch 00110: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0129 - acc: 0.9961 - val_loss: 0.1690 - val_acc: 0.9604\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9969\n",
      "Epoch 00111: val_loss did not improve from 0.14422\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1532 - val_acc: 0.9648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1764 - acc: 0.9601\n",
      "Loss: 0.1764103547913295 Accuracy: 0.9601246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_BN'\n",
    "\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 256000)            1024000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 5,183,568\n",
      "Trainable params: 4,670,800\n",
      "Non-trainable params: 512,768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 2.3640 - acc: 0.3902\n",
      "Loss: 2.3640441237951735 Accuracy: 0.39023885\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,648,784\n",
      "Trainable params: 2,391,760\n",
      "Non-trainable params: 257,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 2.0532 - acc: 0.5306\n",
      "Loss: 2.0531809329243833 Accuracy: 0.53063345\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,723,792\n",
      "Trainable params: 2,466,256\n",
      "Non-trainable params: 257,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 1.8076 - acc: 0.5923\n",
      "Loss: 1.8075694526838737 Accuracy: 0.5923157\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,543,376\n",
      "Trainable params: 1,413,328\n",
      "Non-trainable params: 130,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 1.3120 - acc: 0.6417\n",
      "Loss: 1.3119915772078565 Accuracy: 0.64174455\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 1,002,960\n",
      "Trainable params: 936,400\n",
      "Non-trainable params: 66,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.9608 - acc: 0.7462\n",
      "Loss: 0.9608338521895998 Accuracy: 0.74620974\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 782,544\n",
      "Trainable params: 747,472\n",
      "Non-trainable params: 35,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.6860 - acc: 0.8411\n",
      "Loss: 0.6860222424302146 Accuracy: 0.8411215\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 15872)             63488     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,077,456\n",
      "Trainable params: 1,041,616\n",
      "Non-trainable params: 35,840\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.4995 - acc: 0.8935\n",
      "Loss: 0.4994610071677409 Accuracy: 0.89345795\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,314,512\n",
      "Trainable params: 1,293,520\n",
      "Non-trainable params: 20,992\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.2687 - acc: 0.9398\n",
      "Loss: 0.26865348473349865 Accuracy: 0.93977153\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 3840)              15360     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,628,368\n",
      "Trainable params: 1,614,544\n",
      "Non-trainable params: 13,824\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 0.1970 - acc: 0.9535\n",
      "Loss: 0.19696606612016726 Accuracy: 0.9534787\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 1792)              7168      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,983,184\n",
      "Trainable params: 1,972,432\n",
      "Non-trainable params: 10,752\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 0.1618 - acc: 0.9657\n",
      "Loss: 0.16177561581853778 Accuracy: 0.9657321\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_364 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_365 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_366 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_367 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_368 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_369 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_370 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_371 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_372 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_373 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_374 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_375 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_376 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_377 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_378 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_379 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_380 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_381 ( (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,162,832\n",
      "Trainable params: 3,150,544\n",
      "Non-trainable params: 12,288\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 0.1764 - acc: 0.9601\n",
      "Loss: 0.1764103547913295 Accuracy: 0.9601246\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 256000)            1024000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 5,183,568\n",
      "Trainable params: 4,670,800\n",
      "Non-trainable params: 512,768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 6.3048 - acc: 0.4492\n",
      "Loss: 6.3047530986314495 Accuracy: 0.4492212\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,648,784\n",
      "Trainable params: 2,391,760\n",
      "Non-trainable params: 257,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 4.5059 - acc: 0.5256\n",
      "Loss: 4.505895039943644 Accuracy: 0.525649\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,723,792\n",
      "Trainable params: 2,466,256\n",
      "Non-trainable params: 257,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 3.7695 - acc: 0.5961\n",
      "Loss: 3.7694590808198956 Accuracy: 0.596054\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,543,376\n",
      "Trainable params: 1,413,328\n",
      "Non-trainable params: 130,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 2.7659 - acc: 0.6451\n",
      "Loss: 2.7659393429384798 Accuracy: 0.6450675\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 1,002,960\n",
      "Trainable params: 936,400\n",
      "Non-trainable params: 66,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 1.6026 - acc: 0.7483\n",
      "Loss: 1.6025987649632392 Accuracy: 0.7482866\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 782,544\n",
      "Trainable params: 747,472\n",
      "Non-trainable params: 35,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 17s 4ms/sample - loss: 0.7888 - acc: 0.8546\n",
      "Loss: 0.7887698325288878 Accuracy: 0.854621\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 15872)             63488     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,077,456\n",
      "Trainable params: 1,041,616\n",
      "Non-trainable params: 35,840\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.5586 - acc: 0.8993\n",
      "Loss: 0.5586357116265955 Accuracy: 0.8992731\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,314,512\n",
      "Trainable params: 1,293,520\n",
      "Non-trainable params: 20,992\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.2807 - acc: 0.9431\n",
      "Loss: 0.2807180250763723 Accuracy: 0.9430945\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 3840)              15360     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,628,368\n",
      "Trainable params: 1,614,544\n",
      "Non-trainable params: 13,824\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.2152 - acc: 0.9566\n",
      "Loss: 0.2151849320509076 Accuracy: 0.956594\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 1792)              7168      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,983,184\n",
      "Trainable params: 1,972,432\n",
      "Non-trainable params: 10,752\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1956 - acc: 0.9612\n",
      "Loss: 0.19560887005813213 Accuracy: 0.96116304\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_364 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_365 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_366 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_367 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_368 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_369 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_370 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_371 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_372 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_373 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_374 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_375 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_376 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_377 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_378 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_379 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_380 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_381 ( (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,162,832\n",
      "Trainable params: 3,150,544\n",
      "Non-trainable params: 12,288\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1801 - acc: 0.9616\n",
      "Loss: 0.18005223404200263 Accuracy: 0.9615784\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
