{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.3959 - acc: 0.1327\n",
      "Epoch 00001: val_loss improved from inf to 12.84533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/001-12.8453.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 13.3957 - acc: 0.1327 - val_loss: 12.8453 - val_acc: 0.1628\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.8406 - acc: 0.1765\n",
      "Epoch 00002: val_loss did not improve from 12.84533\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.8407 - acc: 0.1765 - val_loss: 13.3236 - val_acc: 0.1421\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8079 - acc: 0.1333\n",
      "Epoch 00003: val_loss did not improve from 12.84533\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 13.8082 - acc: 0.1333 - val_loss: 13.8101 - val_acc: 0.1307\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7532 - acc: 0.1404\n",
      "Epoch 00004: val_loss did not improve from 12.84533\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 13.7531 - acc: 0.1404 - val_loss: 13.9607 - val_acc: 0.1099\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.0772 - acc: 0.1771\n",
      "Epoch 00005: val_loss did not improve from 12.84533\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 13.0772 - acc: 0.1771 - val_loss: 13.0810 - val_acc: 0.1666\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.6467 - acc: 0.2038\n",
      "Epoch 00006: val_loss did not improve from 12.84533\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.6463 - acc: 0.2038 - val_loss: 13.1201 - val_acc: 0.1612\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.5888 - acc: 0.2087\n",
      "Epoch 00007: val_loss improved from 12.84533 to 12.83385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/007-12.8339.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.5889 - acc: 0.2087 - val_loss: 12.8339 - val_acc: 0.1815\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.5449 - acc: 0.2127\n",
      "Epoch 00008: val_loss improved from 12.83385 to 12.82184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/008-12.8218.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.5441 - acc: 0.2127 - val_loss: 12.8218 - val_acc: 0.1836\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.5164 - acc: 0.2170\n",
      "Epoch 00009: val_loss did not improve from 12.82184\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.5169 - acc: 0.2170 - val_loss: 12.9081 - val_acc: 0.1773\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.4819 - acc: 0.2189\n",
      "Epoch 00010: val_loss did not improve from 12.82184\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.4820 - acc: 0.2189 - val_loss: 12.9476 - val_acc: 0.1728\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.4629 - acc: 0.2187\n",
      "Epoch 00011: val_loss did not improve from 12.82184\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.4629 - acc: 0.2187 - val_loss: 12.8292 - val_acc: 0.1824\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.3338 - acc: 0.2245\n",
      "Epoch 00012: val_loss improved from 12.82184 to 12.78063, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/012-12.7806.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.3330 - acc: 0.2245 - val_loss: 12.7806 - val_acc: 0.1752\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.1684 - acc: 0.2318\n",
      "Epoch 00013: val_loss improved from 12.78063 to 12.55189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/013-12.5519.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.1683 - acc: 0.2318 - val_loss: 12.5519 - val_acc: 0.1912\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.0471 - acc: 0.2417\n",
      "Epoch 00014: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 12.0472 - acc: 0.2417 - val_loss: 12.8102 - val_acc: 0.1677\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.9953 - acc: 0.2455\n",
      "Epoch 00015: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.9954 - acc: 0.2455 - val_loss: 13.1051 - val_acc: 0.1649\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.9506 - acc: 0.2504\n",
      "Epoch 00016: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.9507 - acc: 0.2504 - val_loss: 12.7411 - val_acc: 0.1721\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.9123 - acc: 0.2543\n",
      "Epoch 00017: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.9124 - acc: 0.2543 - val_loss: 12.7226 - val_acc: 0.1870\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8768 - acc: 0.2588\n",
      "Epoch 00018: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8760 - acc: 0.2589 - val_loss: 12.6899 - val_acc: 0.1810\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8695 - acc: 0.2595\n",
      "Epoch 00019: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8692 - acc: 0.2595 - val_loss: 12.8729 - val_acc: 0.1747\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8662 - acc: 0.2595\n",
      "Epoch 00020: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8668 - acc: 0.2595 - val_loss: 12.5902 - val_acc: 0.1952\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8550 - acc: 0.2609\n",
      "Epoch 00021: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8547 - acc: 0.2609 - val_loss: 12.6867 - val_acc: 0.1817\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8508 - acc: 0.2610\n",
      "Epoch 00022: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8509 - acc: 0.2610 - val_loss: 12.6411 - val_acc: 0.1891\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8487 - acc: 0.2612\n",
      "Epoch 00023: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8484 - acc: 0.2613 - val_loss: 12.7288 - val_acc: 0.1882\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8413 - acc: 0.2621\n",
      "Epoch 00024: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8419 - acc: 0.2620 - val_loss: 12.8306 - val_acc: 0.1775\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8469 - acc: 0.2614\n",
      "Epoch 00025: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8475 - acc: 0.2614 - val_loss: 12.6926 - val_acc: 0.1826\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8373 - acc: 0.2624\n",
      "Epoch 00026: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8379 - acc: 0.2624 - val_loss: 12.6887 - val_acc: 0.1815\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8367 - acc: 0.2628\n",
      "Epoch 00027: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8360 - acc: 0.2628 - val_loss: 12.9448 - val_acc: 0.1756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8332 - acc: 0.2633\n",
      "Epoch 00028: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8333 - acc: 0.2633 - val_loss: 12.6921 - val_acc: 0.1861\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8277 - acc: 0.2638\n",
      "Epoch 00029: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8283 - acc: 0.2638 - val_loss: 12.6217 - val_acc: 0.1877\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.8372 - acc: 0.2629\n",
      "Epoch 00030: val_loss did not improve from 12.55189\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.8374 - acc: 0.2629 - val_loss: 12.9163 - val_acc: 0.1640\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.2873 - acc: 0.2761\n",
      "Epoch 00031: val_loss improved from 12.55189 to 12.30216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/031-12.3022.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 11.2876 - acc: 0.2761 - val_loss: 12.3022 - val_acc: 0.1973\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.4870 - acc: 0.3118\n",
      "Epoch 00032: val_loss improved from 12.30216 to 11.66649, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/032-11.6665.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.4873 - acc: 0.3118 - val_loss: 11.6665 - val_acc: 0.1971\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0171 - acc: 0.3463\n",
      "Epoch 00033: val_loss improved from 11.66649 to 11.23873, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/033-11.2387.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0157 - acc: 0.3464 - val_loss: 11.2387 - val_acc: 0.2273\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.8107 - acc: 0.3674\n",
      "Epoch 00034: val_loss did not improve from 11.23873\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 9.8107 - acc: 0.3674 - val_loss: 11.2914 - val_acc: 0.2204\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.1467 - acc: 0.3925\n",
      "Epoch 00035: val_loss improved from 11.23873 to 10.58686, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/035-10.5869.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 9.1467 - acc: 0.3925 - val_loss: 10.5869 - val_acc: 0.2376\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.5713 - acc: 0.4306\n",
      "Epoch 00036: val_loss improved from 10.58686 to 10.46056, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/036-10.4606.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.5710 - acc: 0.4306 - val_loss: 10.4606 - val_acc: 0.2315\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.4006 - acc: 0.4532\n",
      "Epoch 00037: val_loss improved from 10.46056 to 10.32286, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/037-10.3229.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.4004 - acc: 0.4532 - val_loss: 10.3229 - val_acc: 0.2476\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.3035 - acc: 0.4654\n",
      "Epoch 00038: val_loss did not improve from 10.32286\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.3032 - acc: 0.4654 - val_loss: 10.5284 - val_acc: 0.2357\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.3952 - acc: 0.4809\n",
      "Epoch 00039: val_loss improved from 10.32286 to 9.04581, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/039-9.0458.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 7.3951 - acc: 0.4809 - val_loss: 9.0458 - val_acc: 0.2595\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.1768 - acc: 0.5403\n",
      "Epoch 00040: val_loss improved from 9.04581 to 8.36689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/040-8.3669.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.1769 - acc: 0.5403 - val_loss: 8.3669 - val_acc: 0.2788\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7375 - acc: 0.5976\n",
      "Epoch 00041: val_loss did not improve from 8.36689\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.7381 - acc: 0.5976 - val_loss: 8.4119 - val_acc: 0.2751\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.5239 - acc: 0.6259\n",
      "Epoch 00042: val_loss improved from 8.36689 to 8.33372, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/042-8.3337.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.5240 - acc: 0.6259 - val_loss: 8.3337 - val_acc: 0.2886\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.4297 - acc: 0.6383\n",
      "Epoch 00043: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.4303 - acc: 0.6382 - val_loss: 8.6796 - val_acc: 0.2609\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.3487 - acc: 0.6489\n",
      "Epoch 00044: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.3480 - acc: 0.6489 - val_loss: 9.4003 - val_acc: 0.2311\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.3128 - acc: 0.6531\n",
      "Epoch 00045: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.3125 - acc: 0.6531 - val_loss: 8.5671 - val_acc: 0.2756\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2753 - acc: 0.6580\n",
      "Epoch 00046: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.2755 - acc: 0.6580 - val_loss: 8.8042 - val_acc: 0.2630\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2456 - acc: 0.6609\n",
      "Epoch 00047: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.2457 - acc: 0.6609 - val_loss: 8.8326 - val_acc: 0.2576\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2174 - acc: 0.6658\n",
      "Epoch 00048: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.2171 - acc: 0.6658 - val_loss: 9.3718 - val_acc: 0.2343\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2065 - acc: 0.6672\n",
      "Epoch 00049: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.2062 - acc: 0.6672 - val_loss: 10.4318 - val_acc: 0.2136\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1924 - acc: 0.6681\n",
      "Epoch 00050: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.1921 - acc: 0.6681 - val_loss: 8.7594 - val_acc: 0.2784\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1771 - acc: 0.6708\n",
      "Epoch 00051: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.1782 - acc: 0.6707 - val_loss: 9.1593 - val_acc: 0.2390\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1698 - acc: 0.6724\n",
      "Epoch 00052: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.1696 - acc: 0.6724 - val_loss: 9.0746 - val_acc: 0.2579\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1793 - acc: 0.6695\n",
      "Epoch 00053: val_loss did not improve from 8.33372\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.1795 - acc: 0.6694 - val_loss: 8.6908 - val_acc: 0.2765\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0540 - acc: 0.6857\n",
      "Epoch 00054: val_loss improved from 8.33372 to 8.25172, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/054-8.2517.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 4.0538 - acc: 0.6856 - val_loss: 8.2517 - val_acc: 0.2073\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0565 - acc: 0.7539\n",
      "Epoch 00055: val_loss improved from 8.25172 to 6.74712, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/055-6.7471.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.0573 - acc: 0.7539 - val_loss: 6.7471 - val_acc: 0.2874\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8178 - acc: 0.8024\n",
      "Epoch 00056: val_loss did not improve from 6.74712\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.8183 - acc: 0.8023 - val_loss: 6.8903 - val_acc: 0.2863\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5511 - acc: 0.8138\n",
      "Epoch 00057: val_loss improved from 6.74712 to 6.40504, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/057-6.4050.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.5512 - acc: 0.8138 - val_loss: 6.4050 - val_acc: 0.2805\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2469 - acc: 0.8344\n",
      "Epoch 00058: val_loss improved from 6.40504 to 6.29084, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/058-6.2908.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.2466 - acc: 0.8344 - val_loss: 6.2908 - val_acc: 0.3112\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1492 - acc: 0.8571\n",
      "Epoch 00059: val_loss improved from 6.29084 to 6.02396, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/059-6.0240.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1502 - acc: 0.8570 - val_loss: 6.0240 - val_acc: 0.3245\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1273 - acc: 0.8618\n",
      "Epoch 00060: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1270 - acc: 0.8618 - val_loss: 6.3147 - val_acc: 0.3054\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1194 - acc: 0.8629\n",
      "Epoch 00061: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1196 - acc: 0.8629 - val_loss: 6.3450 - val_acc: 0.3249\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1134 - acc: 0.8640\n",
      "Epoch 00062: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1136 - acc: 0.8640 - val_loss: 6.8193 - val_acc: 0.2860\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1105 - acc: 0.8642\n",
      "Epoch 00063: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1106 - acc: 0.8642 - val_loss: 6.5063 - val_acc: 0.3184\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1102 - acc: 0.8636\n",
      "Epoch 00064: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1108 - acc: 0.8636 - val_loss: 7.4283 - val_acc: 0.2879\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1063 - acc: 0.8645\n",
      "Epoch 00065: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.1065 - acc: 0.8645 - val_loss: 6.7470 - val_acc: 0.2956\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6514 - acc: 0.8740\n",
      "Epoch 00066: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.6521 - acc: 0.8740 - val_loss: 7.1602 - val_acc: 0.2632\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4904 - acc: 0.8969\n",
      "Epoch 00067: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4911 - acc: 0.8968 - val_loss: 6.2808 - val_acc: 0.2998\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4638 - acc: 0.9017\n",
      "Epoch 00068: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4639 - acc: 0.9017 - val_loss: 7.1083 - val_acc: 0.3019\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4479 - acc: 0.9039\n",
      "Epoch 00069: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4482 - acc: 0.9039 - val_loss: 7.1576 - val_acc: 0.2991\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4363 - acc: 0.9067\n",
      "Epoch 00070: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4362 - acc: 0.9068 - val_loss: 6.1541 - val_acc: 0.3222\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4326 - acc: 0.9067\n",
      "Epoch 00071: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4324 - acc: 0.9066 - val_loss: 7.2904 - val_acc: 0.2965\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4263 - acc: 0.9082\n",
      "Epoch 00072: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4261 - acc: 0.9082 - val_loss: 6.6596 - val_acc: 0.3070\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8247 - acc: 0.9280\n",
      "Epoch 00073: val_loss did not improve from 6.02396\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.8246 - acc: 0.9280 - val_loss: 6.8805 - val_acc: 0.2937\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9647\n",
      "Epoch 00074: val_loss improved from 6.02396 to 4.79218, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/074-4.7922.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.2856 - acc: 0.9647 - val_loss: 4.7922 - val_acc: 0.3748\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9771\n",
      "Epoch 00075: val_loss did not improve from 4.79218\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.2104 - acc: 0.9771 - val_loss: 5.9945 - val_acc: 0.2970\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9836\n",
      "Epoch 00076: val_loss improved from 4.79218 to 4.56245, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_1_conv_checkpoint/076-4.5625.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1757 - acc: 0.9836 - val_loss: 4.5625 - val_acc: 0.3832\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9850\n",
      "Epoch 00077: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1635 - acc: 0.9850 - val_loss: 5.0771 - val_acc: 0.3489\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9875\n",
      "Epoch 00078: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1496 - acc: 0.9875 - val_loss: 5.2714 - val_acc: 0.3615\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9877\n",
      "Epoch 00079: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1476 - acc: 0.9877 - val_loss: 4.9507 - val_acc: 0.3811\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9885\n",
      "Epoch 00080: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1428 - acc: 0.9885 - val_loss: 5.1245 - val_acc: 0.3832\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9890\n",
      "Epoch 00081: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1412 - acc: 0.9890 - val_loss: 5.2833 - val_acc: 0.3583\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9872\n",
      "Epoch 00082: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1487 - acc: 0.9872 - val_loss: 5.0191 - val_acc: 0.3767\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9893\n",
      "Epoch 00083: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1377 - acc: 0.9893 - val_loss: 5.4797 - val_acc: 0.3608\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9875\n",
      "Epoch 00084: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1448 - acc: 0.9875 - val_loss: 5.1683 - val_acc: 0.3638\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1374 - acc: 0.9886 - val_loss: 5.4423 - val_acc: 0.3489\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9895\n",
      "Epoch 00086: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1342 - acc: 0.9895 - val_loss: 5.1085 - val_acc: 0.3827\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9893\n",
      "Epoch 00087: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1349 - acc: 0.9893 - val_loss: 5.1458 - val_acc: 0.3820\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9890\n",
      "Epoch 00088: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1332 - acc: 0.9890 - val_loss: 6.7050 - val_acc: 0.3231\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9905\n",
      "Epoch 00089: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1277 - acc: 0.9905 - val_loss: 5.3261 - val_acc: 0.3785\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9895\n",
      "Epoch 00090: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1295 - acc: 0.9895 - val_loss: 5.4377 - val_acc: 0.3615\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9898\n",
      "Epoch 00091: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1302 - acc: 0.9898 - val_loss: 5.3145 - val_acc: 0.3701\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9894\n",
      "Epoch 00092: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1324 - acc: 0.9894 - val_loss: 5.3334 - val_acc: 0.3778\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9904\n",
      "Epoch 00093: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1258 - acc: 0.9904 - val_loss: 5.4381 - val_acc: 0.3627\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9904\n",
      "Epoch 00094: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1257 - acc: 0.9904 - val_loss: 5.3141 - val_acc: 0.3618\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9911\n",
      "Epoch 00095: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1215 - acc: 0.9911 - val_loss: 5.2295 - val_acc: 0.3860\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9905\n",
      "Epoch 00096: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1222 - acc: 0.9905 - val_loss: 5.2492 - val_acc: 0.3790\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9908\n",
      "Epoch 00097: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1221 - acc: 0.9908 - val_loss: 5.8681 - val_acc: 0.3627\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9910\n",
      "Epoch 00098: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1206 - acc: 0.9910 - val_loss: 5.5307 - val_acc: 0.3820\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9909\n",
      "Epoch 00099: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1207 - acc: 0.9909 - val_loss: 5.5092 - val_acc: 0.3657\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9908\n",
      "Epoch 00100: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1208 - acc: 0.9908 - val_loss: 6.2515 - val_acc: 0.3473\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9906\n",
      "Epoch 00101: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1212 - acc: 0.9906 - val_loss: 5.4682 - val_acc: 0.3753\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9906\n",
      "Epoch 00102: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1198 - acc: 0.9906 - val_loss: 6.1794 - val_acc: 0.3410\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9906\n",
      "Epoch 00103: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1198 - acc: 0.9906 - val_loss: 5.3439 - val_acc: 0.3795\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9890\n",
      "Epoch 00104: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1255 - acc: 0.9891 - val_loss: 5.3550 - val_acc: 0.3946\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9915\n",
      "Epoch 00105: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1171 - acc: 0.9915 - val_loss: 5.8182 - val_acc: 0.3687\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9910\n",
      "Epoch 00106: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1178 - acc: 0.9910 - val_loss: 5.3245 - val_acc: 0.3713\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9921\n",
      "Epoch 00107: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1142 - acc: 0.9921 - val_loss: 5.4655 - val_acc: 0.3788\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9919\n",
      "Epoch 00108: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1153 - acc: 0.9919 - val_loss: 6.3458 - val_acc: 0.3557\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9914\n",
      "Epoch 00109: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1167 - acc: 0.9914 - val_loss: 5.7663 - val_acc: 0.3657\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9916\n",
      "Epoch 00110: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1156 - acc: 0.9916 - val_loss: 5.4045 - val_acc: 0.3839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9910\n",
      "Epoch 00111: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1173 - acc: 0.9910 - val_loss: 5.4134 - val_acc: 0.3850\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9917\n",
      "Epoch 00112: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1161 - acc: 0.9917 - val_loss: 6.2360 - val_acc: 0.3436\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9918\n",
      "Epoch 00113: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1145 - acc: 0.9917 - val_loss: 5.4607 - val_acc: 0.3739\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9910\n",
      "Epoch 00114: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1164 - acc: 0.9910 - val_loss: 5.3725 - val_acc: 0.3718\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9914\n",
      "Epoch 00115: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1147 - acc: 0.9914 - val_loss: 6.0203 - val_acc: 0.3510\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9919\n",
      "Epoch 00116: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1126 - acc: 0.9919 - val_loss: 5.5293 - val_acc: 0.3857\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9912\n",
      "Epoch 00117: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1151 - acc: 0.9912 - val_loss: 5.3865 - val_acc: 0.4037\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9921\n",
      "Epoch 00118: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1133 - acc: 0.9921 - val_loss: 5.4518 - val_acc: 0.3864\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9916\n",
      "Epoch 00119: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1151 - acc: 0.9916 - val_loss: 6.2361 - val_acc: 0.3517\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9919\n",
      "Epoch 00120: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1141 - acc: 0.9919 - val_loss: 5.4473 - val_acc: 0.3832\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9919\n",
      "Epoch 00121: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1128 - acc: 0.9919 - val_loss: 5.6981 - val_acc: 0.3792\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9924\n",
      "Epoch 00122: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1110 - acc: 0.9924 - val_loss: 5.6957 - val_acc: 0.3687\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9910\n",
      "Epoch 00123: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1165 - acc: 0.9910 - val_loss: 5.6372 - val_acc: 0.3711\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9908\n",
      "Epoch 00124: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1150 - acc: 0.9908 - val_loss: 5.3832 - val_acc: 0.3811\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9926\n",
      "Epoch 00125: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1103 - acc: 0.9926 - val_loss: 5.7734 - val_acc: 0.3848\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9917\n",
      "Epoch 00126: val_loss did not improve from 4.56245\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1134 - acc: 0.9917 - val_loss: 5.4497 - val_acc: 0.3818\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 705us/sample - loss: 4.8689 - acc: 0.3574\n",
      "Loss: 4.868935505499597 Accuracy: 0.3574247\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.6607 - acc: 0.2915\n",
      "Epoch 00001: val_loss improved from inf to 4.93758, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_2_conv_checkpoint/001-4.9376.hdf5\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 4.6606 - acc: 0.2916 - val_loss: 4.9376 - val_acc: 0.2597\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8210 - acc: 0.4609\n",
      "Epoch 00002: val_loss improved from 4.93758 to 4.25920, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_2_conv_checkpoint/002-4.2592.hdf5\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 3.8210 - acc: 0.4609 - val_loss: 4.2592 - val_acc: 0.3853\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4506 - acc: 0.5613\n",
      "Epoch 00003: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 3.4506 - acc: 0.5614 - val_loss: 4.3511 - val_acc: 0.3734\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2051 - acc: 0.6329\n",
      "Epoch 00004: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 3.2060 - acc: 0.6329 - val_loss: 5.4244 - val_acc: 0.3042\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0031 - acc: 0.6895\n",
      "Epoch 00005: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 3.0027 - acc: 0.6895 - val_loss: 4.8967 - val_acc: 0.3937\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9041 - acc: 0.7210\n",
      "Epoch 00006: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.9043 - acc: 0.7209 - val_loss: 4.8007 - val_acc: 0.3997\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8320 - acc: 0.7442\n",
      "Epoch 00007: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.8317 - acc: 0.7442 - val_loss: 5.3673 - val_acc: 0.3650\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8229 - acc: 0.7496\n",
      "Epoch 00008: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.8225 - acc: 0.7497 - val_loss: 4.9650 - val_acc: 0.3925\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7477 - acc: 0.7723\n",
      "Epoch 00009: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.7474 - acc: 0.7723 - val_loss: 4.8790 - val_acc: 0.4307\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7614 - acc: 0.7713\n",
      "Epoch 00010: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.7615 - acc: 0.7713 - val_loss: 5.0085 - val_acc: 0.4111\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7350 - acc: 0.7793\n",
      "Epoch 00011: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.7351 - acc: 0.7793 - val_loss: 5.1443 - val_acc: 0.4177\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7047 - acc: 0.7866\n",
      "Epoch 00012: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.7056 - acc: 0.7865 - val_loss: 6.2444 - val_acc: 0.3571\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7073 - acc: 0.7882\n",
      "Epoch 00013: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.7070 - acc: 0.7882 - val_loss: 5.8494 - val_acc: 0.3953\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6696 - acc: 0.7989\n",
      "Epoch 00014: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6701 - acc: 0.7989 - val_loss: 5.3682 - val_acc: 0.4090\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6638 - acc: 0.8007\n",
      "Epoch 00015: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6635 - acc: 0.8007 - val_loss: 6.0717 - val_acc: 0.3804\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6669 - acc: 0.8014\n",
      "Epoch 00016: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6666 - acc: 0.8014 - val_loss: 5.9381 - val_acc: 0.3934\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6506 - acc: 0.8048\n",
      "Epoch 00017: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6505 - acc: 0.8048 - val_loss: 5.6870 - val_acc: 0.4090\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6491 - acc: 0.8057\n",
      "Epoch 00018: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6496 - acc: 0.8056 - val_loss: 5.7250 - val_acc: 0.4086\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6510 - acc: 0.8082\n",
      "Epoch 00019: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6513 - acc: 0.8081 - val_loss: 5.4569 - val_acc: 0.4288\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6464 - acc: 0.8075\n",
      "Epoch 00020: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6470 - acc: 0.8075 - val_loss: 6.1852 - val_acc: 0.3827\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6191 - acc: 0.8145\n",
      "Epoch 00021: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6192 - acc: 0.8145 - val_loss: 5.6776 - val_acc: 0.4293\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6288 - acc: 0.8130\n",
      "Epoch 00022: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6289 - acc: 0.8130 - val_loss: 5.8180 - val_acc: 0.4191\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6091 - acc: 0.8167\n",
      "Epoch 00023: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6092 - acc: 0.8167 - val_loss: 6.3654 - val_acc: 0.3925\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6072 - acc: 0.8179\n",
      "Epoch 00024: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6069 - acc: 0.8179 - val_loss: 5.8109 - val_acc: 0.4177\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6089 - acc: 0.8181\n",
      "Epoch 00025: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6086 - acc: 0.8181 - val_loss: 6.0610 - val_acc: 0.4116\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6088 - acc: 0.8175\n",
      "Epoch 00026: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6088 - acc: 0.8175 - val_loss: 6.0124 - val_acc: 0.4163\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6023 - acc: 0.8198\n",
      "Epoch 00027: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6024 - acc: 0.8198 - val_loss: 6.1583 - val_acc: 0.4067\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6109 - acc: 0.8186\n",
      "Epoch 00028: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.6110 - acc: 0.8186 - val_loss: 5.9075 - val_acc: 0.4125\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5955 - acc: 0.8210\n",
      "Epoch 00029: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5956 - acc: 0.8209 - val_loss: 5.9946 - val_acc: 0.4256\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5939 - acc: 0.8235\n",
      "Epoch 00030: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5935 - acc: 0.8235 - val_loss: 5.8038 - val_acc: 0.4314\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5959 - acc: 0.8228\n",
      "Epoch 00031: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5958 - acc: 0.8228 - val_loss: 6.1477 - val_acc: 0.3986\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5994 - acc: 0.8216\n",
      "Epoch 00032: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5991 - acc: 0.8216 - val_loss: 6.2088 - val_acc: 0.4116\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5808 - acc: 0.8265\n",
      "Epoch 00033: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5813 - acc: 0.8265 - val_loss: 5.9407 - val_acc: 0.4246\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5755 - acc: 0.8280\n",
      "Epoch 00034: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5756 - acc: 0.8280 - val_loss: 5.9555 - val_acc: 0.4316\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5799 - acc: 0.8268\n",
      "Epoch 00035: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5796 - acc: 0.8269 - val_loss: 7.0064 - val_acc: 0.3795\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5833 - acc: 0.8253\n",
      "Epoch 00036: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5834 - acc: 0.8253 - val_loss: 5.8669 - val_acc: 0.4335\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5771 - acc: 0.8268\n",
      "Epoch 00037: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5768 - acc: 0.8268 - val_loss: 5.7913 - val_acc: 0.4403\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5578 - acc: 0.8274\n",
      "Epoch 00038: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.5574 - acc: 0.8275 - val_loss: 6.3690 - val_acc: 0.3960\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0692 - acc: 0.8430\n",
      "Epoch 00039: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.0694 - acc: 0.8430 - val_loss: 5.8502 - val_acc: 0.4114\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9726 - acc: 0.8583\n",
      "Epoch 00040: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9724 - acc: 0.8583 - val_loss: 5.0408 - val_acc: 0.4687\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9342 - acc: 0.8662\n",
      "Epoch 00041: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9344 - acc: 0.8662 - val_loss: 5.3652 - val_acc: 0.4477\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9240 - acc: 0.8690\n",
      "Epoch 00042: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9238 - acc: 0.8690 - val_loss: 5.3006 - val_acc: 0.4580\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9231 - acc: 0.8685\n",
      "Epoch 00043: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9239 - acc: 0.8684 - val_loss: 5.6208 - val_acc: 0.4349\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9251 - acc: 0.8680\n",
      "Epoch 00044: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9252 - acc: 0.8680 - val_loss: 5.2817 - val_acc: 0.4596\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9094 - acc: 0.8726\n",
      "Epoch 00045: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9096 - acc: 0.8726 - val_loss: 6.8173 - val_acc: 0.3802\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9185 - acc: 0.8694\n",
      "Epoch 00046: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9191 - acc: 0.8694 - val_loss: 5.9138 - val_acc: 0.4212\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9108 - acc: 0.8715\n",
      "Epoch 00047: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9110 - acc: 0.8715 - val_loss: 6.0802 - val_acc: 0.4221\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9178 - acc: 0.8698\n",
      "Epoch 00048: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9180 - acc: 0.8698 - val_loss: 5.5135 - val_acc: 0.4563\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9189 - acc: 0.8699\n",
      "Epoch 00049: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9186 - acc: 0.8700 - val_loss: 5.7301 - val_acc: 0.4356\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9120 - acc: 0.8717\n",
      "Epoch 00050: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9122 - acc: 0.8717 - val_loss: 5.5457 - val_acc: 0.4396\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9120 - acc: 0.8720\n",
      "Epoch 00051: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.9118 - acc: 0.8721 - val_loss: 5.3681 - val_acc: 0.4584\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8932 - acc: 0.8721\n",
      "Epoch 00052: val_loss did not improve from 4.25920\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 1.8930 - acc: 0.8721 - val_loss: 5.0178 - val_acc: 0.4389\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 4.2525 - acc: 0.3533\n",
      "Loss: 4.252515628652286 Accuracy: 0.35327104\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3330 - acc: 0.3440\n",
      "Epoch 00001: val_loss improved from inf to 2.28626, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_3_conv_checkpoint/001-2.2863.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 2.3330 - acc: 0.3440 - val_loss: 2.2863 - val_acc: 0.3056\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5851 - acc: 0.5265\n",
      "Epoch 00002: val_loss improved from 2.28626 to 1.54006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_3_conv_checkpoint/002-1.5401.hdf5\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 1.5850 - acc: 0.5265 - val_loss: 1.5401 - val_acc: 0.5372\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2669 - acc: 0.6130\n",
      "Epoch 00003: val_loss did not improve from 1.54006\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 1.2669 - acc: 0.6130 - val_loss: 1.6911 - val_acc: 0.5246\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0274 - acc: 0.6818\n",
      "Epoch 00004: val_loss did not improve from 1.54006\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 1.0274 - acc: 0.6818 - val_loss: 1.7002 - val_acc: 0.4969\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7378\n",
      "Epoch 00005: val_loss improved from 1.54006 to 1.46817, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_3_conv_checkpoint/005-1.4682.hdf5\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.8316 - acc: 0.7378 - val_loss: 1.4682 - val_acc: 0.5849\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6903 - acc: 0.7820\n",
      "Epoch 00006: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.6905 - acc: 0.7819 - val_loss: 1.5676 - val_acc: 0.5805\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8198\n",
      "Epoch 00007: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.5604 - acc: 0.8197 - val_loss: 1.5318 - val_acc: 0.5917\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.8515\n",
      "Epoch 00008: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.4623 - acc: 0.8515 - val_loss: 1.7415 - val_acc: 0.5714\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8691\n",
      "Epoch 00009: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.4024 - acc: 0.8691 - val_loss: 1.7290 - val_acc: 0.5830\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8841\n",
      "Epoch 00010: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.3555 - acc: 0.8841 - val_loss: 1.7788 - val_acc: 0.5802\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9010\n",
      "Epoch 00011: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.3067 - acc: 0.9009 - val_loss: 1.7285 - val_acc: 0.6119\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9059\n",
      "Epoch 00012: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2882 - acc: 0.9059 - val_loss: 1.8009 - val_acc: 0.5970\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9185\n",
      "Epoch 00013: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2526 - acc: 0.9184 - val_loss: 1.6543 - val_acc: 0.6299\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9271\n",
      "Epoch 00014: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2298 - acc: 0.9271 - val_loss: 1.8207 - val_acc: 0.6054\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9318\n",
      "Epoch 00015: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2134 - acc: 0.9318 - val_loss: 1.8030 - val_acc: 0.6229\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9357\n",
      "Epoch 00016: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2009 - acc: 0.9357 - val_loss: 1.9005 - val_acc: 0.6175\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9420\n",
      "Epoch 00017: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1853 - acc: 0.9420 - val_loss: 1.9580 - val_acc: 0.6210\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9392\n",
      "Epoch 00018: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1894 - acc: 0.9392 - val_loss: 1.9029 - val_acc: 0.6212\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9501\n",
      "Epoch 00019: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1593 - acc: 0.9501 - val_loss: 1.8487 - val_acc: 0.6399\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9482\n",
      "Epoch 00020: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1669 - acc: 0.9481 - val_loss: 1.8571 - val_acc: 0.6403\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9508\n",
      "Epoch 00021: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1604 - acc: 0.9508 - val_loss: 1.9792 - val_acc: 0.6273\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9577\n",
      "Epoch 00022: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1379 - acc: 0.9576 - val_loss: 1.9748 - val_acc: 0.6198\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9547\n",
      "Epoch 00023: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1506 - acc: 0.9547 - val_loss: 2.0352 - val_acc: 0.6299\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9570\n",
      "Epoch 00024: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1392 - acc: 0.9570 - val_loss: 2.2400 - val_acc: 0.6159\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9570\n",
      "Epoch 00025: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1411 - acc: 0.9570 - val_loss: 1.8888 - val_acc: 0.6513\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9612\n",
      "Epoch 00026: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1252 - acc: 0.9612 - val_loss: 1.9460 - val_acc: 0.6506\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9623\n",
      "Epoch 00027: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1230 - acc: 0.9623 - val_loss: 1.9585 - val_acc: 0.6459\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9639\n",
      "Epoch 00028: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1184 - acc: 0.9639 - val_loss: 2.1032 - val_acc: 0.6313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9641\n",
      "Epoch 00029: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1197 - acc: 0.9641 - val_loss: 2.2182 - val_acc: 0.6138\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9657\n",
      "Epoch 00030: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1129 - acc: 0.9657 - val_loss: 2.0716 - val_acc: 0.6394\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9640\n",
      "Epoch 00031: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1150 - acc: 0.9639 - val_loss: 2.1316 - val_acc: 0.6422\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9662\n",
      "Epoch 00032: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1152 - acc: 0.9661 - val_loss: 2.1857 - val_acc: 0.6194\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9673\n",
      "Epoch 00033: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1065 - acc: 0.9673 - val_loss: 2.1321 - val_acc: 0.6380\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9668\n",
      "Epoch 00034: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1098 - acc: 0.9667 - val_loss: 2.2386 - val_acc: 0.6168\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9714\n",
      "Epoch 00035: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0985 - acc: 0.9714 - val_loss: 2.1258 - val_acc: 0.6392\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9740\n",
      "Epoch 00036: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0872 - acc: 0.9740 - val_loss: 1.9719 - val_acc: 0.6583\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9712\n",
      "Epoch 00037: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0982 - acc: 0.9713 - val_loss: 2.0972 - val_acc: 0.6410\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9715\n",
      "Epoch 00038: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0969 - acc: 0.9715 - val_loss: 2.0381 - val_acc: 0.6520\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9751\n",
      "Epoch 00039: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0859 - acc: 0.9751 - val_loss: 2.0985 - val_acc: 0.6520\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9750\n",
      "Epoch 00040: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0841 - acc: 0.9750 - val_loss: 2.0432 - val_acc: 0.6546\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9749\n",
      "Epoch 00041: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0846 - acc: 0.9749 - val_loss: 2.0334 - val_acc: 0.6608\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9742\n",
      "Epoch 00042: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0902 - acc: 0.9742 - val_loss: 2.0932 - val_acc: 0.6583\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9752\n",
      "Epoch 00043: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0865 - acc: 0.9752 - val_loss: 2.3263 - val_acc: 0.6271\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9771\n",
      "Epoch 00044: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0804 - acc: 0.9771 - val_loss: 2.2302 - val_acc: 0.6366\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9756\n",
      "Epoch 00045: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0819 - acc: 0.9756 - val_loss: 2.2643 - val_acc: 0.6539\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9767\n",
      "Epoch 00046: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0797 - acc: 0.9766 - val_loss: 2.2490 - val_acc: 0.6487\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9757\n",
      "Epoch 00047: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0835 - acc: 0.9757 - val_loss: 2.1529 - val_acc: 0.6518\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9783\n",
      "Epoch 00048: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0741 - acc: 0.9783 - val_loss: 2.2891 - val_acc: 0.6373\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9779\n",
      "Epoch 00049: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0749 - acc: 0.9779 - val_loss: 2.0683 - val_acc: 0.6597\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9803\n",
      "Epoch 00050: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0705 - acc: 0.9803 - val_loss: 2.0446 - val_acc: 0.6546\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9810\n",
      "Epoch 00051: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0683 - acc: 0.9810 - val_loss: 2.1491 - val_acc: 0.6573\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9762\n",
      "Epoch 00052: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0849 - acc: 0.9762 - val_loss: 2.1053 - val_acc: 0.6580\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9798\n",
      "Epoch 00053: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0710 - acc: 0.9798 - val_loss: 2.1644 - val_acc: 0.6522\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9807\n",
      "Epoch 00054: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0709 - acc: 0.9807 - val_loss: 2.1046 - val_acc: 0.6567\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9836\n",
      "Epoch 00055: val_loss did not improve from 1.46817\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0590 - acc: 0.9836 - val_loss: 2.1703 - val_acc: 0.6618\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 1.5907 - acc: 0.5454\n",
      "Loss: 1.590722113061669 Accuracy: 0.54537904\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3781 - acc: 0.3308\n",
      "Epoch 00001: val_loss improved from inf to 1.79391, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/001-1.7939.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 2.3780 - acc: 0.3308 - val_loss: 1.7939 - val_acc: 0.4365\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5243 - acc: 0.5360\n",
      "Epoch 00002: val_loss improved from 1.79391 to 1.27399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/002-1.2740.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.5243 - acc: 0.5360 - val_loss: 1.2740 - val_acc: 0.6177\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2408 - acc: 0.6247\n",
      "Epoch 00003: val_loss improved from 1.27399 to 1.16917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/003-1.1692.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.2409 - acc: 0.6246 - val_loss: 1.1692 - val_acc: 0.6518\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0613 - acc: 0.6778\n",
      "Epoch 00004: val_loss improved from 1.16917 to 0.95966, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/004-0.9597.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.0614 - acc: 0.6778 - val_loss: 0.9597 - val_acc: 0.7112\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9373 - acc: 0.7148\n",
      "Epoch 00005: val_loss improved from 0.95966 to 0.91161, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/005-0.9116.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.9373 - acc: 0.7148 - val_loss: 0.9116 - val_acc: 0.7282\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8492 - acc: 0.7434\n",
      "Epoch 00006: val_loss did not improve from 0.91161\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.8494 - acc: 0.7434 - val_loss: 0.9246 - val_acc: 0.7279\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.7663\n",
      "Epoch 00007: val_loss improved from 0.91161 to 0.86216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/007-0.8622.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.7698 - acc: 0.7663 - val_loss: 0.8622 - val_acc: 0.7570\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.7885\n",
      "Epoch 00008: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.6981 - acc: 0.7885 - val_loss: 0.8779 - val_acc: 0.7470\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.8080\n",
      "Epoch 00009: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.6280 - acc: 0.8081 - val_loss: 1.1260 - val_acc: 0.6811\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.8160\n",
      "Epoch 00010: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5910 - acc: 0.8159 - val_loss: 0.8680 - val_acc: 0.7598\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8368\n",
      "Epoch 00011: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5304 - acc: 0.8368 - val_loss: 0.8881 - val_acc: 0.7435\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8439\n",
      "Epoch 00012: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5023 - acc: 0.8438 - val_loss: 0.9224 - val_acc: 0.7524\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8569\n",
      "Epoch 00013: val_loss did not improve from 0.86216\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4600 - acc: 0.8569 - val_loss: 0.9173 - val_acc: 0.7512\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8703\n",
      "Epoch 00014: val_loss improved from 0.86216 to 0.85323, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_4_conv_checkpoint/014-0.8532.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4159 - acc: 0.8703 - val_loss: 0.8532 - val_acc: 0.7668\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8737\n",
      "Epoch 00015: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3980 - acc: 0.8737 - val_loss: 0.8851 - val_acc: 0.7673\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8861\n",
      "Epoch 00016: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3616 - acc: 0.8861 - val_loss: 0.8938 - val_acc: 0.7601\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8917\n",
      "Epoch 00017: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3376 - acc: 0.8917 - val_loss: 0.9807 - val_acc: 0.7435\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8974\n",
      "Epoch 00018: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3164 - acc: 0.8974 - val_loss: 0.9711 - val_acc: 0.7559\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8995\n",
      "Epoch 00019: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3083 - acc: 0.8995 - val_loss: 0.9374 - val_acc: 0.7594\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9107\n",
      "Epoch 00020: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2768 - acc: 0.9106 - val_loss: 0.8915 - val_acc: 0.7771\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9153\n",
      "Epoch 00021: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2638 - acc: 0.9153 - val_loss: 1.1108 - val_acc: 0.7275\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9169\n",
      "Epoch 00022: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2593 - acc: 0.9169 - val_loss: 0.9407 - val_acc: 0.7734\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9211\n",
      "Epoch 00023: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2404 - acc: 0.9211 - val_loss: 0.9466 - val_acc: 0.7729\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9259\n",
      "Epoch 00024: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2296 - acc: 0.9259 - val_loss: 0.9823 - val_acc: 0.7703\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9250\n",
      "Epoch 00025: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2278 - acc: 0.9250 - val_loss: 1.0358 - val_acc: 0.7517\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9325\n",
      "Epoch 00026: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2068 - acc: 0.9325 - val_loss: 1.0235 - val_acc: 0.7547\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9317\n",
      "Epoch 00027: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2100 - acc: 0.9317 - val_loss: 0.9688 - val_acc: 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9381\n",
      "Epoch 00028: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1899 - acc: 0.9381 - val_loss: 1.1034 - val_acc: 0.7419\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9368\n",
      "Epoch 00029: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1873 - acc: 0.9368 - val_loss: 1.0362 - val_acc: 0.7647\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9406\n",
      "Epoch 00030: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1831 - acc: 0.9406 - val_loss: 0.9475 - val_acc: 0.7799\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9419\n",
      "Epoch 00031: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1784 - acc: 0.9419 - val_loss: 1.0070 - val_acc: 0.7675\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9474\n",
      "Epoch 00032: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1652 - acc: 0.9474 - val_loss: 0.9527 - val_acc: 0.7880\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9479\n",
      "Epoch 00033: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1605 - acc: 0.9479 - val_loss: 1.0263 - val_acc: 0.7810\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9479\n",
      "Epoch 00034: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1607 - acc: 0.9479 - val_loss: 1.1303 - val_acc: 0.7664\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9485\n",
      "Epoch 00035: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1558 - acc: 0.9485 - val_loss: 1.2778 - val_acc: 0.7363\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9499\n",
      "Epoch 00036: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1543 - acc: 0.9499 - val_loss: 0.9723 - val_acc: 0.7890\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9520\n",
      "Epoch 00037: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1475 - acc: 0.9520 - val_loss: 1.0460 - val_acc: 0.7817\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9526\n",
      "Epoch 00038: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1473 - acc: 0.9525 - val_loss: 1.0478 - val_acc: 0.7752\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9529\n",
      "Epoch 00039: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1494 - acc: 0.9528 - val_loss: 1.0130 - val_acc: 0.7785\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9573\n",
      "Epoch 00040: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1346 - acc: 0.9573 - val_loss: 1.0062 - val_acc: 0.7820\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9555\n",
      "Epoch 00041: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1354 - acc: 0.9555 - val_loss: 1.0606 - val_acc: 0.7750\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9570\n",
      "Epoch 00042: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1343 - acc: 0.9569 - val_loss: 1.0160 - val_acc: 0.7808\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9574\n",
      "Epoch 00043: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1328 - acc: 0.9575 - val_loss: 1.0477 - val_acc: 0.7808\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9614\n",
      "Epoch 00044: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1196 - acc: 0.9613 - val_loss: 1.0182 - val_acc: 0.7883\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9587\n",
      "Epoch 00045: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1290 - acc: 0.9586 - val_loss: 1.0318 - val_acc: 0.7880\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9612\n",
      "Epoch 00046: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1210 - acc: 0.9612 - val_loss: 1.0414 - val_acc: 0.7857\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9620\n",
      "Epoch 00047: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1217 - acc: 0.9620 - val_loss: 1.0685 - val_acc: 0.7852\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9662\n",
      "Epoch 00048: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1060 - acc: 0.9662 - val_loss: 1.0206 - val_acc: 0.7901\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9645\n",
      "Epoch 00049: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1125 - acc: 0.9645 - val_loss: 1.0406 - val_acc: 0.7913\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9660\n",
      "Epoch 00050: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1100 - acc: 0.9660 - val_loss: 1.1069 - val_acc: 0.7789\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9665\n",
      "Epoch 00051: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1062 - acc: 0.9666 - val_loss: 1.0195 - val_acc: 0.7939\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9648\n",
      "Epoch 00052: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1124 - acc: 0.9648 - val_loss: 1.1262 - val_acc: 0.7787\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9667\n",
      "Epoch 00053: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1057 - acc: 0.9667 - val_loss: 1.0065 - val_acc: 0.7976\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9684\n",
      "Epoch 00054: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0988 - acc: 0.9684 - val_loss: 1.0171 - val_acc: 0.7966\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9673\n",
      "Epoch 00055: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1059 - acc: 0.9673 - val_loss: 1.0858 - val_acc: 0.7862\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9686\n",
      "Epoch 00056: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1006 - acc: 0.9686 - val_loss: 1.0316 - val_acc: 0.7922\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9695\n",
      "Epoch 00057: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0945 - acc: 0.9695 - val_loss: 1.1338 - val_acc: 0.7768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9676\n",
      "Epoch 00058: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1018 - acc: 0.9676 - val_loss: 1.0734 - val_acc: 0.7866\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9697\n",
      "Epoch 00059: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0976 - acc: 0.9697 - val_loss: 1.1052 - val_acc: 0.7796\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9707\n",
      "Epoch 00060: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0948 - acc: 0.9707 - val_loss: 1.0638 - val_acc: 0.7892\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9695\n",
      "Epoch 00061: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0972 - acc: 0.9695 - val_loss: 1.1353 - val_acc: 0.7759\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9708\n",
      "Epoch 00062: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0929 - acc: 0.9708 - val_loss: 1.0259 - val_acc: 0.7908\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9705\n",
      "Epoch 00063: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0932 - acc: 0.9705 - val_loss: 1.1194 - val_acc: 0.7857\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9706\n",
      "Epoch 00064: val_loss did not improve from 0.85323\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0942 - acc: 0.9706 - val_loss: 1.0454 - val_acc: 0.7945\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmSWTfSEkLAmQoCA7CWsQFxRUhApSBbVS9621Wn+2VmqrYuteW1usS9FK3REF61pFFAQUVDZZhLCELSFkgezbbOf3x8lkgSQEyDCQeT/Pc59JZu7cc+5kct6z3XOV1hohhBACwBLoDAghhDh5SFAQQghRR4KCEEKIOhIUhBBC1JGgIIQQoo4EBSGEEHUkKAghhKgjQUEIIUQdCQpCCCHq2AKdgaPVsWNHnZKSEuhsCCHEKWX16tWFWuuEI+13ygWFlJQUVq1aFehsCCHEKUUptbs1+0n3kRBCiDoSFIQQQtSRoCCEEKLOKTem0BSXy0V2djbV1dWBzsopKzQ0lOTkZOx2e6CzIoQIoHYRFLKzs4mKiiIlJQWlVKCzc8rRWnPgwAGys7NJTU0NdHaEEAHULrqPqquriY+Pl4BwjJRSxMfHS0tLCNE+ggIgAeE4yecnhIB2FBSOxOOppKYmB6/XFeisCCHESStogoLXW4PTmYvWbR8UiouLee65547pvRMmTKC4uLjV+8+cOZOnnnrqmNISQogjCZqgoJQVAK09bX7sloKC2+1u8b2ffPIJsbGxbZ4nIYQ4FkEUFMxEK61bLqSPxYwZM9ixYwdpaWncc889LFmyhLPPPptJkybRr18/AC699FKGDh1K//79mT17dt17U1JSKCwsZNeuXfTt25ebb76Z/v37c+GFF1JVVdViuuvWrSMjI4NBgwYxZcoUioqKAJg1axb9+vVj0KBBXHnllQB89dVXpKWlkZaWRnp6OmVlZW3+OQghTn3tYkpqQ9u23UV5+bomXvHi8VRgsYSi1NHNxY+MTKNXr783+/rjjz/Oxo0bWbfOpLtkyRLWrFnDxo0b66Z4vvzyy3To0IGqqiqGDx/OZZddRnx8/CF538Zbb73Fiy++yLRp05g/fz7Tp09vNt1rrrmGZ555hnPPPZcHHniAhx56iL///e88/vjj7Ny5E4fDUdc19dRTT/Hss88yevRoysvLCQ0NParPQAgRHIKmpQC+2TX6hKQ2YsSIRnP+Z82axeDBg8nIyGDv3r1s27btsPekpqaSlpYGwNChQ9m1a1ezxy8pKaG4uJhzzz0XgGuvvZalS5cCMGjQIK6++mpef/11bDYT90ePHs3dd9/NrFmzKC4urnteCCEaanclQ3M1eq015eWrCQnpgsOR5Pd8RERE1P28ZMkSFi1axIoVKwgPD2fMmDFNXhPgcDjqfrZarUfsPmrOxx9/zNKlS/nwww955JFH2LBhAzNmzGDixIl88sknjB49ms8++4w+ffoc0/GFEO1X0LQUzDx8m18GmqOiolrsoy8pKSEuLo7w8HC2bNnCypUrjzvNmJgY4uLiWLZsGQCvvfYa5557Ll6vl71793LeeefxxBNPUFJSQnl5OTt27GDgwIHce++9DB8+nC1bthx3HoQQ7U+7aym0RCmrXwaa4+PjGT16NAMGDODiiy9m4sSJjV4fP348L7zwAn379uWMM84gIyOjTdJ95ZVXuO2226isrKRnz57MmTMHj8fD9OnTKSkpQWvNnXfeSWxsLPfffz+LFy/GYrHQv39/Lr744jbJgxCifVFan5g+9rYybNgwfehNdjZv3kzfvn2P+N6Kih9Ryk54eC9/Ze+U1trPUQhx6lFKrdZaDzvSfkHTfQRmWqo/WgpCCNFeBFlQsAJtP6YghBDtRdAFBX8MNAshRHsRVEHBzD5yc6qNowghxIkSVEHBdB9pTtQFbEIIcaoJsqDgv/WPhBCiPQiyoOC/lVKPVmRk5FE9L4QQJ0KQBgVpKQghRFOCLCj4uo/atqUwY8YMnn322brffTfCKS8vZ+zYsQwZMoSBAwfy/vvvt/qYWmvuueceBgwYwMCBA3n77bcByM3N5ZxzziEtLY0BAwawbNkyPB4P1113Xd2+Tz/9dJuenxAieLS/ZS7uugvWNbV0Nli0lzCvWT6bo1k+Oy0N/t780tlXXHEFd911F7fffjsA8+bN47PPPiM0NJT33nuP6OhoCgsLycjIYNKkSa26H/KCBQtYt24dP/zwA4WFhQwfPpxzzjmHN998k4suuog//OEPeDweKisrWbduHTk5OWzcuBHgqO7kJoQQDbW/oNAS5Z/ls9PT08nPz2ffvn0UFBQQFxdHt27dcLlc3HfffSxduhSLxUJOTg55eXl07tz5iMdcvnw5V111FVarlU6dOnHuuefy/fffM3z4cG644QZcLheXXnopaWlp9OzZk6ysLO644w4mTpzIhRde2KbnJ4QIHu0vKLRQo0drqspXExLSFYeja5smO3XqVN59913279/PFVdcAcAbb7xBQUEBq1evxm63k5KS0uSS2UfjnHPOYenSpXz88cdcd9113H333VxzzTX88MMPfPbZZ7zwwgvMmzePl19+uS1OSwgRZIJsTEEBFr/MPrriiiuYO3cu7777LlOnTgXMktmJiYnY7XYWL17M7t27W328s88+m7fffhuPx0NBQQFLly5lxIgR7N69m06dOnHzzTdz0003sWbNGgoLC/F6vVx22WU8/PDDrFmzps3PTwgRHNpfS+EI/LUoXv/+/SkrKyMpKYkuXboAcPXVV3PJJZcwcOBAhg0bdlQ3tZkyZQorVqxg8ODBKKV48skn6dy5M6+88gp/+ctfsNvtREZG8uqrr5KTk8P111+P1+sF4LHHHmvz8xNCBIegWjoboKJiE0o5CA8/3R/ZO6XJ0tlCtF+ydHYzzLRUuU5BCCGaEoRBQVZKFUKI5gRdUAAJCkII0ZygCwpy9zUhhGheEAYFK+BFa2+gsyKEECcdvwUFpVQ3pdRipdSPSqlNSqlfN7GPUkrNUkptV0qtV0oN8Vd+6tP0z/pHQgjRHvizpeAGfqO17gdkALcrpfodss/FQK/a7RbgeT/mB/DP8tnFxcU899xzx/TeCRMmyFpFQoiTht+CgtY6V2u9pvbnMmAzkHTIbpOBV7WxEohVSnXxV56gPii05bTUloKC291yOp988gmxsbFtlhchhDgeJ2RMQSmVAqQD3x7yUhKwt8Hv2RweONpY23cfzZgxgx07dpCWlsY999zDkiVLOPvss5k0aRL9+pnG0aWXXsrQoUPp378/s2fPrntvSkoKhYWF7Nq1i759+3LzzTfTv39/LrzwQqqqqg5L68MPP2TkyJGkp6czbtw48vLyACgvL+f6669n4MCBDBo0iPnz5wPw6aefMmTIEAYPHszYsWPb7JyFEO2T35e5UEpFAvOBu7TWpcd4jFsw3Ut07969xX1bWDkbAK3D8HrPwGIJpRUrWANHXDmbxx9/nI0bN7KuNuElS5awZs0aNm7cSGpqKgAvv/wyHTp0oKqqiuHDh3PZZZcRHx/f6Djbtm3jrbfe4sUXX2TatGnMnz+f6dOnN9rnrLPOYuXKlSileOmll3jyySf561//yp///GdiYmLYsGEDAEVFRRQUFHDzzTezdOlSUlNTOXjwYOtOWAgRtPwaFJRSdkxAeENrvaCJXXKAbg1+T659rhGt9WxgNphlLo4zT76jHs9hjmjEiBF1AQFg1qxZvPfeewDs3buXbdu2HRYUUlNTSUtLA2Do0KHs2rXrsONmZ2dzxRVXkJubi9PprEtj0aJFzJ07t26/uLg4PvzwQ84555y6fTp06NCm5yiEaH/8FhSUKX3/DWzWWv+tmd0+AH6llJoLjARKtNa5x5NuSzV6AK2hvDyTkJAkHA7/DV9ERETU/bxkyRIWLVrEihUrCA8PZ8yYMU0uoe1wOOp+tlqtTXYf3XHHHdx9991MmjSJJUuWMHPmTL/kXwgRnPw5pjAa+DlwvlJqXe02QSl1m1Lqttp9PgGygO3Ai8Av/ZgfAJSyAKpNL2CLioqirKys2ddLSkqIi4sjPDycLVu2sHLlymNOq6SkhKQkM+zyyiuv1D1/wQUXNLolaFFRERkZGSxdupSdO3cCSPeREOKI/Dn7aLnWWmmtB2mt02q3T7TWL2itX6jdR2utb9dan6a1Hqi1XnWk47YFc61C2w00x8fHM3r0aAYMGMA999xz2Ovjx4/H7XbTt29fZsyYQUZGxjGnNXPmTKZOncrQoUPp2LFj3fN//OMfKSoqYsCAAQwePJjFixeTkJDA7Nmz+elPf8rgwYPrbv4jhBDNCbqlswEqKjZisYQRFnZaW2fvlCZLZwvRfsnS2S2S9Y+EEKIpQRkUZPlsIYRomgQFIYQQdYI0KEj3kRBCNCVIg4IV8HCqDbILIYS/BWlQkOWzhRCiKUEZFMC3UmrggkJkZGTA0hZCiOYEZVCov6eCjCsIIURDQRoU2rb7aMaMGY2WmJg5cyZPPfUU5eXljB07liFDhjBw4EDef//9Ix6ruSW2m1oCu7nlsoUQ4lj5fensE+2uT+9i3f4W1s4GtPbi9VZgsYTVBYiWpHVO4+/jm19p74orruCuu+7i9ttvB2DevHl89tlnhIaG8t577xEdHU1hYSEZGRlMmjSpwUqth2tqiW2v19vkEthNLZcthBDHo90FhdZo6+Wz09PTyc/PZ9++fRQUFBAXF0e3bt1wuVzcd999LF26FIvFQk5ODnl5eXTu3LnZYzW1xHZBQUGTS2A3tVy2EEIcj3YXFJqt0R88CFlZMGAA2mGnvHwtISHJOBzNF9BHY+rUqbz77rvs37+/buG5N954g4KCAlavXo3dbiclJaXJJbN9WrvEthBC+EvwjCnYauOfy0X9abfdQPMVV1zB3Llzeffdd5k6dSpglrlOTEzEbrezePFidu/e3eIxmltiu7klsJtaLlsIIY5H8ASFkBDz6HSilKq9qrntpqT279+fsrIykpKS6NLF3Lzn6quvZtWqVQwcOJBXX32VPn36tHiM5pbYbm4J7KaWyxZCiOMRPEtnezywdi0kJUGXLpSXb8BqjSAsrKcfc3tqkaWzhWi/ZOnsQ1mtZnO5AFn/SAghmhI8QQFMF5LTCchKqUII0ZR2ExRa1Q1mtzdoKVilpdDAqdaNKITwj3YRFEJDQzlw4MCRC7ZGLYW2vU/zqUxrzYEDBwgNDQ10VoQQAdYurlNITk4mOzubgoKClncsLoaSErDbcXuKcbtLCQ3dfGIyeZILDQ0lOTk50NkQQgRYuwgKdru97mrfFs2eDbfeCnv3ssf7EVlZ9zJwYBk2m6xYKoQQ0E66j1otKck85uRgs5klIdxuueBLCCF8giso+LpHsrOx2WIBCQpCCNFQcAWFJlsKxQHMkBBCnFyCKyjEx4PDAdnZ2O3SfSSEEIcKrqCglGktNGgpuFwSFIQQwie4ggKYcQUZUxBCiCYFX1CoaynEADKmIIQQDQVfUKhtKSgsWK0x0lIQQogGgi8oJCVBTQ0cPIjdHidBQQghGgi+oHDItQoy0CyEEPWCLygccq2CjCkIIUS94AsKjVoK0n0khBANBV9Q6NwZLJYGLQUJCkII4eO3oKCUelkpla+U2tjM62OUUiVKqXW12wP+yksjNpsJDLVjChIUhBCinj9bCv8Bxh9hn2Va67Ta7U9+zEtjtdcqOBzJeL3V1NTknrCkhRDiZOa3oKC1Xgoc9Nfxj0tSEmRnExU1BIDy8jUBzpAQQpwcAj2mMEop9YNS6n9Kqf7N7aSUukUptUopteqId1drjeRkyMkhMjINgLIyCQpCCAGBDQprgB5a68HAM8B/m9tRaz1baz1Maz0sISHh+FNOSoLiYmw1VsLCektLQQghagUsKGitS7XW5bU/fwLYlVIdT0jivmmpOTlERQ2hrGz1CUlWCCFOdgELCkqpzkopVfvziNq8HDghifsuYMvOJjJyKDU1e3E626BbSgghTnE2fx1YKfUWMAboqJTKBh4E7ABa6xeAy4FfKKXcQBVwpdZa+ys/jTS4gC0qvX6wuUOHi05I8kIIcbLyW1DQWl91hNf/CfzTX+m3qMFSF5GRkwAz2CxBQQgR7AI9+ygwwsMhLq72tpyxhIb2lHEFIYQgWIMC1F3ABhAVNVRmIAkhBMEcFGpvtgMQGTmE6uqduFwn57V2QghxogRvUDikpQBQXr42kDkSQoiAC96gkJwMeXngctUtdyHjCkKIYBe8QSEpCbSG3Fzs9ngcjh6y3IUQIugFb1BocK0CQFTUEBlsFkIEveANCg2uVQAzrlBVtQ23uySAmRJCiMAK3qBwSEshMtJ3ZfO6QOVICCECLniDQlwchIY2aCnIYLMQQrQqKCilfq2UilbGv5VSa5RSF/o7c36lVKNrFUJCOhESkiSDzUKIoNbalsINWutS4EIgDvg58LjfcnWiNLhWAXyDzdJSEEIEr9YGBVX7OAF4TWu9qcFzp64GLQUwg82VlZm43eUBzJQIGqtXQ1VVoHMhRCOtDQqrlVILMUHhM6VUFOD1X7ZOkNrbclJaCvgGm7UMNgv/27wZhg+H2bMDnRMhGmltULgRmAEM11pXYu6LcL3fcnWiXH45uFzwt78BDZe7kHEF4Wdz5piLJ1etCnROhGiktUFhFJCptS5WSk0H/gic+hP6hw0zgeGvf4X8fEJCumC3d5LBZuFfbje89pr5+YcfApsXIQ7R2qDwPFCplBoM/AbYAbzqt1ydSA8/bPp1H3kEpRRRUcMoLV0Z6FyJ9uzTT2H/fhg40HQj1dQEOkdC1GltUHDX3ipzMvBPrfWzQJT/snUCnXEG3HADPP887NxJhw4XUlWVSWXltkDnTLRXc+ZAQgL87nem1bB5c6BzJESd1gaFMqXU7zFTUT9WSlmovd9yu/Dgg2C1woMP0rHjZAAKC98PcKZEu1RYCB9+CNOnm+5LgPXrA5snIRpobVC4AqjBXK+wH0gG/uK3XJ1oSUlw553w+uuEbislMjKNwsL/BjpXoj16800zueH66+H0081V9TKuIE4irQoKtYHgDSBGKfUToFpr3T7GFHzuvRdiYuC+++jY8VJKS7/B6cwLdK5EezNnDgwdasYTbDYYMECCgjiptHaZi2nAd8BUYBrwrVLqcn9m7ITr0MEEho8+InFrD0Bz4MBHgc6VaE/WrTPb9Q1mcw8ebIKC1oHLlxANtLb76A+YaxSu1VpfA4wA7vdftgLkzjuhSxfCnnwdh6OHdCGJtjVnDoSEwFVX1T83eLAZZ8jNDVy+hGigtUHBorXOb/D7gaN476kjPBxuuAG1ZAmJjos5ePBzWfJCtA2nE954Ay691LRKfQYPNo/ShSROEq0t2D9VSn2mlLpOKXUd8DHwif+yFUAXXAAeD522JKN1DUVFCwOdI9EefPghHDjQuOsIzNgCyAykYHKSdxW2dqD5HmA2MKh2m621vtefGQuYjAwIDydiRS42W4cT04VUUQElp/4F4qIFc+aYWW4XXND4+bg46N5dWgrHIzcXqqv9d/y77oL/+z/wtsFyb08/Df37w8GDx38sP2l1F5DWer7W+u7a7T1/ZiqgHA445xzUoi+Ij/8JBw58hNfr8m+aN90EF13k3zRE4KxdCx9/DDfeaK6HOZRvsFkcvcJC6NsXfvUr/xx/6VL4xz/g7383geF4avmffw6//a25WLF2vbVmvfUWfBSYiS4tBgWlVJlSqrSJrUwpVXqiMnnCXXABbNlCovMs3O4iSkqW+y8trWHRIvjuOygr8186J4NHHjH/WIGWl+ffmuWhfv970yJo7twHD4bMzBObp1PB6tUwb17L+zz+uGllv/oq7N3btulrDffcY1p4t98Os2bBn//c9L5OJyxYAEVFTb++e7eZYNC3L0yaZAJNYWHT+27cCD//Ofz0p/D9921zLkdDa31KbUOHDtV+98MPWoP2vPwv/dVXoXrr1jv9l9bWrVqbr5/WX3zhv3QCzevVuksXre12rQ8eDFw+qqq07tRJ6/R0rcvL/Z/eF1+Yv+1TTzW/z7x5Zp9Vq/yfn1PFjz9qHR1tPpdly5reZ+9erR0OrS+6SGurVev/+7+2zYPv7/Lyy1p7PFpfe635/Zln6vfxerV+7z2tTz/dvNa9u9bffNP4OFVVWg8bZs4nM9Ocm8Wi9T33HJ6m16v1uedq3aGDOVZKSpv9vwCrdCvK2IAX8ke7nZCg4PFonZio9dVX6/XrL9HffNNDe71e/6T1yiv1QeHRR/2Txslgy5b683zllcDl4z//qc/H5Zebv7W/eL1aDx+udbdupmBoTmZmfeEjtC4o0LpnTxO8e/TQuk+fpj+/W281lYydO7WePl3riAitDxxofTrLl2t9yy1aFxUd/lpNjdannab1gAFau93mOZdL68mTzd/qjTe0XrdO6/POM7/366f1Cy9onZpqAtQTT9R/t26+2ezz3//WH3/6dK3DwrTOzW2c7ptvmn1feEHrFSu0ttlMmm1Q/khQOF5XXaV1p056X85LevFidGnpWv+kc+utpgZx+unmj99ePf+8+bpFR2t9ySWBy8eIEaaQefJJk58HH/RfWu+8Y9KYM6fl/dxurcPDtf71r/2Xl0DKzDQFamtUV2t99tmmBbBihdaffmo+w/vvb7zftm2mwPzVr8zv69eb/f70p9als2OHqY2D1kOHHh5MnnnGvPbJJ42fr6rSeswYU/ArpXV8vNbPPlt/fkVFprIBWo8fr/Vf/mJ+vu++xsfZutUc46676p8rLdW6a1eTH18gevpp8/6//rV159UCCQrH6+WXtQbtXLNEL16sdFbWA/5JZ+BArS+8UOtrrjE1I3+1SAJt6lStk5PNP0FIiNYlJSc+D99/b77ys2aZz/m668zvb7/d9mk5nVr36qV1//71/+AtGTnSFDb+VFNjCpnMTP+mo7X5fBcuNAU8mO94cfGR3+P7m7z5Zv3z06ebALBhQ/1zV199eE174kStO3bUuqKi5XTKykwLIC5O6+eeMwFo0CCt8/PN6yUl5jjnn9/0/2NJianA3X130107Xq+pBDkc5lzGjWv6O3D99Waf7Gzz+29/a/ZfubLxsaZMMee/YkXL53UEEhSO1+7d5uN5+mm9du35+ptvummPx9m2aZSUmNrGzJmmtgFa79rVtmmcDLxerRMStP75z7X++mtd1/w+0W64wdTIfYVTdbXWZ55pCpe27s9/4QVznh980Lr9b77ZFFKHFkIulylc331X69de0/rFF00t9r33jq4CUV1tWmi+1lpL+dq9W+v581vu8mqO12uOPWKESSs52dTmbTYTIFv6fj/xhG6y9VZQYArpkSNN4bp+vfm/mTGj8X7LlunD+vwP5fGYQtZiMZ+r1uYxLMx0Ae3bp/Uf/qDbZIxn3Tpz7gUFTb+elWU+l1/+UutNm8zPN954+H5FRaZbqls3rQsLjzk7EhTaQu/eWk+YoAsKPtCLF6P373/zyO85GgsXmj/BwoVar15tfp47t23TOBls2KAbDdh17Wr+MY9HSYkpKNevb11N/OBBrUNDTR9yQ3l5ZkCva1dTILSF8nKtO3fWevTo1hfc//yn+Yz27Kl/zus1LUjfGMih25gxZqzmSCorTVcGaP3ww6Z7ArR+6KHGYyplZVr/8Y/mcwJTCL30UvNdPy6X1ps3m26yBx/U+rLLTOEF5nH2bBOMtNZ60SKtY2JMa/i77xqf48qVpmBUSusrrmj6M3vtNV3Xyps82RyrqfGDM880g7PN5fmhh8xx/va3xs8vWWLGJE4/3QSIn/2s2Y+zTfnGRYYP1zo2tr61cqhVq0wL+9ZbjzmpgAcF4GUgH9jYzOsKmAVsB9YDQ1pz3BMaFH75S60jIrS3ukqvXHmG/v77IW074DxzpvlHKCkx3Q2hoW0/g+JkMGuW+art3Gl+v+MOc65lZcd2vIICM3vIVzhGRWk9dqwp0JprYv/1r2bfdesOf+2HH0xBcMklbdN99+c/m7SWL2/9e3y13A8/rH/upZfMc/fcY4Lf1q0maOTnmwI3NtYUFA8+2HytvqLCfDZKmeNpbYKEL9hMnmxqonPmmNlhYMbT5s+vr+2fcYYJwNXVpqX36KMmyERF1f8NLBZTiZoyxUwkcDbRqt60yRTYYWGmxTNzZv2sHYfD5Kmysunz8HrNLCNfwHr44ab3++AD3WxL9L33zGvXXNP03/nrr00rKiTE1OJPhD17THpgegta8umnx9XtejIEhXOAIS0EhQnA/2qDQwbwbWuOe0KDgu9LtHSpzsn5l168GH3w4OK2O/5FF5kxBZ/Ro7UeNartjn+ymDLFFAY+X32lj7kvf98+08wPDdX69de1fvVVE7zT0+sH//7zn8bv8XhM4TN6dPPH9Q0Ivvvu0eepoe++MzW/yy47uvcVF5v0H3nE/L5unTnH5vqjtdZ6/35TowVTIL/4otZffmkGUZ1OE3TPPdcU2IfO+PJ6tf7HP8xn5itoR45sPJ3S69V6wQKt+/Y1r1ut9UGgf3+tf/EL81mvXt18YX6ovDytMzLMMZQy/fYvv3zk8QatTaUiPNx0RTZXofB4zPdj0CBzzKVLTaXkhhtMS2DEiJa7xTZvNq2GE+nRR02FpDUt3uMQ8KBg8kBKC0HhX8BVDX7PBLoc6ZgnNCgUFZl/qAce0G53pV6+PEGvX/+Ttjm2x2NqJQ2bg7/5jakx1dS0TRonA4/H9JXfcEP9c2636UaYOvXojrVnjxm8jYjQevHiw18vKTGFqFKmgPT57LPma48+LpfWaWmmttxcAfX662a+ecMBz4YOHjTBr3v3Y+v7TUnReto0cx69epm85OUd+X0LF5rpkw27liwW8/2yWhsP2h5qyRLTknjjjean57rdJqj89rcmSDTXxdFalZVmeqZvgPVofP21mTDQkobTvH1bQoIpeHNyji3P7cCpEBQ+As5q8PsXwLBm9r0FWAWs6t69u38+seaMHFlXe8/KelAvXowuL998/Mf19bM3rNX6LpY50pf+VLJmjTmn115r/PwvfmFqfUeaKeKzfbuZsx4Tc/jFQQ017D9/7jnz3OTJplDw9W8357uyBd5/AAAgAElEQVTvTGH6y18e/tqCBeY13zTEtYdMUfZ6TaFjtzeePXI0Jk82XTXTppnC/KuvWv9el8t8RosWaf3vf2v9wAMmEB86pTIYOJ1a33uv6WL6+GMTCNrrrL6j0K6CQsPthLYUtDYzEaxWrYuLdU1Nnl6yxKG3bLnlyO87ktmzzce/dWv9c74ZTy3NnjiR3G4zv/q++0xtc8OGo2/F+PryD60V+q70nT//yMf44gszEBwfb7oqjqS6Wuuf/MQc//e/N4X573/fuvzeeacp+BsGns8/N/2+I0eabp1u3Uzrp2Hw9nU//eMfrUunKQ88UF+zffzxYz+OEE1obVBQZl//UEqlAB9prQc08dq/gCVa67dqf88ExmitW7zbyLBhw/SqVav8kNtmLFkC550H778PkyaRmXkL+/e/yqhRewgJSTz2415/vVnwKj8flDLPaW3WWRk7Fl57rU2yf1yeecbceMhiqV8h0mYz67fcc4+5+bwv78255BKzrs/WrY2fd7uhSxezztSbbzb93oMHTTovvwynnQb//a+5fWVrOJ1w5ZXw3nsmjzt3Qo8eR35fWRn06wexsbBmDaxaBePGmfSXLDH3Qti1C84/3yyF/emn4PHAmDHmXgnvvHPkz6Q5CxbAZZfBxInwwQfmcz/FaW3+1EVF5uMqLDSPJSVgt5tbVDsc5tFmM3+2mhqzVVeb21l7veYj9j3abGb/0FAICzPv93igqqrx5jtGdbX52ek06xHa7fWbxWLScDrrN5fL5Nntrv/Zaq1Pz7e53VBZaTZfmk5n4/f58hsSYtILCWk6TY/HfG18W8M/fcMi+uqr4bbbju1voZRarbUedqT9bMd2+DbxAfArpdRcYCRQcqSAEBCjRpmb77z6KkycSHLy3eTmvkhOznOkps489uN+8405dsMCRCkYORJWrjzubB+3PXvMQm4XX2wK1q1bzUJdGzeagvCaa+Cll+DZZ5svqN1u+Oqrxnca87HZYMoUsxpkdbX5j/PR2hSud9xhSpB774UHHzT/ia0VEgJvv22OERHRuoAAEBVlzmnyZPjFL2D+fOjaFRYurL85TkqKOa+xY01Qi4qC1FT497+PPSAAjB8PM2fi/sUdVFVY6gqa6urGhaLXW79pXf9YUQEFBabg9T1qXV/o+gpgj6e+kGxYKDU8vtNpCvKDB+u3mhqIjDSnGxlpNjDplpebraKivmD0HSvQLBZz7na7yZPLVR9sfBwO85UJCTFfTbvdPPq2hkGnuto82mymaAgPrw8UvsK/4THcbvPZ+AKOx1OfVkiIyZvF0ngQxOs9vGiAphfZbWt+aykopd4CxgAdgTzgQcAOoLV+QSmlgH8C44FK4Hqt9RGbACe8pQBw//3w8MOmJvj662zIupLS0pVkZOzBaj2KgsqnsBASEuCxx2DGjMavPfGEea6wEOLj2yb/R0trU1tduhQ2bTq8QPV6TQE4Y4ap8v361zBzpiktGvruOxPk3nrL1NoPtXChWTL8iSdMCyknB/btM/cx/uorc4P7l16CtDS/nWpTqqqg6LKbKP7fN9Qkdsf5rzk447tQU2MKvf37zRL+uTsqyP3vtxRXhOAZlIY3LLJRYd2Q7x/dV/D6NpervmbsqyW72mCldqVMDLNa62vKNTWN93E4zOYrvCyW+s1uN+/3bXFxZl9fACgrq1/U1xcgIiNN/HU4zPGs1vpCNTbWfJ19W2ysKSwb1uRdLlNI+vLly5vVajZf3jye+vf5Nqu1cS2+YSvCZms6Vvv+Hlbr8cXyU0XAWwpa6yaqh41e18Dt/kq/Tf35z6YQv+suGDOGbq/9nnWuj8jN/TfJycewjruvJXDmmYe/NnKkefzuO1NLD4S33oL//c8s79tUDdtigZtvNkv7/v735sYhc+fCu++a1o/P4sXm8bzzmk7nvPOgY0fTEvCJjITkZHjqKRNsbC1/Rb1eE0d27ICsLBNLLZbGhVJpqVlVee9e0wDKzjYFcMPams1mCruiIt8K1i+ZBPKBKU2nnZAQQZeUc+kQ4SQkMqxRwdVUIWOx1BduvoLOl76vpupwmMKsYe0zNLRxwegrxBqmpZR5T0KC2eLiDq9V+loAvs8mGArClvg+P9GYX8cU/CEgLQWfDz6Aq65Cd+xI5l87UdBpKyO6f4Vj1XZYvtyUSvffD8OOEIzvuw/+8hdTyw4Pb/xaeTnExMAf/wgPPdTycfbsgeeeM9WpTp2gc2fzeNpppsvjWPhuWnL66eacWtNe/fZb09m5dy/Mng3XXmueHz/ePLdpU92uWjfuN67ZsJWSrAMccHShUCVyoCq8rvsjP99sBQWmFwkaN+tdLrNMvdN55CzGxEC3bvVbWNjh/ciRkaYw9W2xsfVdLg0L7E6dzGa3H8PnK0SAtLalIEHhaK1eDT/5CbqijOqoCsL21T7va69arWa8oFev5o9x3nmm8G/uBhqDB5tB2E8/bfp1l8vUzh96yJRovnawj9Vqau2XXnr05/fzn5u++LVrzW0DW+vAAZg2jZIvV7Fm6uOsTr+JzAdeJz95KAVdBtUV7qWtvDVTZKSp8SYmmi0+3tRsGw4CWiyme79nz/qtU6f6Lhpfv3Z4OERHH/1HIUR7EvDuo3Zr6FD49lvU7bejnTvYPnkzHSc9Sez5vzbV1jPPNP3kK1aYEupQLpfpGrrxxubTGDnSDLR6vYe3b5ctMwOgmzaZwdB//MN0txQWmjuK5eWZ1srPfma6b3zdUa3xySfw+uvwwAONAkJ1dW0feq7pqtm/3/QtN5x5UVgYz5o9i9iGgneAdyCRCXTWDhIjYMQIU7jHxDTuM3Y4zHO+vuaOHc3j0YwpCyHajgSFY9G9O3z4IaFeJ0Wrh1LgnsVwy63YevUy9+I97zyYMMFMYTx08HX9elOSNjWe4DNyJLz4ImzbBmecYYLDN9/ACy/AG2+Yfv4PPjDTPX18fRpgWhqjRpnXV6ww3UmH2rTJtEQyM/FuzmTzJi/fFvViR/wL7Nt5E/suMgFg376W7zHu6/+OiYHBgxXXXgdD8z5h6HM3kuDZD6sKzFQDIcQpQYLCcbBYQujd+0XWrj2TnTv/QK9ez5gq8TvvmPuwXn45fPih6ZCurDRB4l//Mm9uKShkZJjHuXNNy+KNN8zc+PBwMyh7//1mmkdzEhNNrf/MM2HCBLzLvyHXGU9xMRTvLKLk2dcp/uxbMnUvVth/xrfeYZR6zPxCa7Gmy2JFly4mlpx1lpkY1KWLGabo2tUMXURF1U+lO9wEuGqBCWodJSIIcSqRMYU2sG3bHeTkPEt6+jfExNQW6HPmwA03mK4krc0Uy5oaU7W+9lozQNzc9A+Px4x0lpWZUnfcONPXf+ml9ZPDW+Bymeuulr6SxbIXfmS59RyK3Id3qlssmoEDFaNGmTg0apQZX5YZGUK0PzLQfAK53WV8/30/bLZYhg5djcUSYl545BEzi6hvXzMTZ/x4OOecxhdqNefNN834wJVXmmp6C0pLzSzX5cvN9u23pmEC0LtLKWfnzmOofQPxrlxiR5xB7G9vInZwD7p2bVWMEUK0AxIUTrDCwg/ZuHESPXr8kdTUP9e/UFJiOtzbwM6d8OOPpidp1y7z+7Zt5iJj35h0WhqMHm1iz1lnma4eZs0yXVEPPmhaLkKIoCNBIQA2b76OvLzXSE9fRkxMC2MGR6GoyMwQ/c9/TAvAx+Ew0zFTU80wxllnmS6gQ8e1hRACZEpqQPTqNYuSkq/YvPnnDBu2Dpvt6EvokhLYvt2sIfff/5pJRjU1Znmhv/zFtAJSU81YsvT9CyHamgSFNmSzRdOnz2usW3cu27ffRZ8+/25xf63NtXBvvWVmjm7fbi7w8omPh1tvNePS6emyLIEQwv8kKLSx2Niz6N59Bnv2PEp8/EQSEn562D5btphA8NZbZkzAbjczfy691Mz+8W19+pjZrCJ4eLwerJYTsBRmgGitKaou4kDlAWo8NTg9TlweF06PE4fNwcDEgYTZm75yMbs0my93folVWRnbcyydIzsfts/u4t288+M7fLztY6pcVdgstrotzB5G3459SeucRlrnNM6IP6Pusy6tKSW7NJuc0hzsVjujkkfhsDmO+1yr3dXNno9PYWUhuWW5JEYk0jG8Y6O/f5Wriq0HtrK5cDNbCreQkZzB+NPHH1e+jkSCgh+kpDzIwYOfkZl5C9HRo3A4ulBRAfPmmaWBVq40tf7zzoPf/c4soR8XF+hct09aa9QxNLEqnBVsyN9Arw69iA8/fLXaoqoi5m2ax6vrX2VzwWa6x3QnJTalbhuYOJAzu515xAKhoZfXvsztn9xOmC2M1LhUUmNrt7hUusd0r9tiHDF15+TVXspqyiipKaGkuoSi6iKKq4spri6mpLqEmNCYujwlRSXVFTiVrkpySnPYV7aPvIo8vLrxGteVrkp2Fe8iqyiLrKIsdhTtoLCyEACFQimFQhFiDSEiJIIIewQRIRGE28MJsYZgUZa6zfd55VXkkV+Rj9vrbvYzsFvspHVOY1TyKDKSMwi3h/PFzi/4POtzthRuabTvoE6DuKDnBZyfej6ZhZnM+3EeK7PNYpPpndNJjEjE7XXj9rqpcleRX5HPwh0LcXrMYllhtjCSopPIK8+jzFnW6NhhtjDGpIzhotMu4sLTLiTKEcWGvA1syDfbjwU/EmoLJSU2hdTYVFJiU+ga1ZU9JXvYlL+JjQUb2Zi/kcLKQnrH9+bs7mebrcfZdI7szPI9y1mUtYjPsz5n3f51delalIWE8AQ6RXairKaMXcW70Oi6z/2+s+/ze1CQgWY/qajYwurVQ9i//1qWLHmON95QlJaa2v9NN5lbDBzrmnVHy+lxsrdkLx7twau9dVtkSCTJ0cnYLG1TN9Baszp3Ne9tfo8FWxbg8XqYN3UeaZ2bXvq6xl3DzCUz2VSwqVEhYlEWwu3hRNgjiAyJJDIkktjQWMb1HEffhL6tykt2aTYvrXmJF9e8SLg9nOcmPMcFp13Q7P5e7WVN7ho+3/E5C7MW8vWer3F5zRrWveN7k5GcwajkUXQM78jbm97mg8wPcHqc9Evox9ndzyanLIddxbvYWbSTClcFAA6rgzO7ncm4nuMYmzqWYV2HNdsKeHrF09y98G7GpIyhX8d+ZBVnsbNoJ7uKd1HjabzmdVRIFDGhMZTWlFJWU1ZXaByJzWKja1RXSmtKKa4uPuL+FmWhe0x3esb1pGdsTzpFmivmtdZoNF7txelxUumqpMJVQYWzggpXBW6vu9H3zKu9dAjrQGJ4Ip0iO9XViENtoYRYQ7Bb7Nitdspqyvgu5ztWZK/g+33fU+ky86rD7eGc0+McxqWOY1zPcXi0h4U7FvJ51ucs37O8rpBP75zOtP7TmNZ/Gj3jejZ5Tk6Pky2FW1i3fx3r9q8juzSbLpFdSI5OrtuKq4tZuGMhC7MWsvXA1sOOkRSVRP/E/jg9TnYV76r732r49xmQOIABiQPoGtWV1bmrWb5ned1nblEWvNqL3WJndPfRXNDzAnp16EV+RT55FXnsL9/P/vL9hNvD6duxL30T+tKnYx96x/cm1NaK6ezNkNlHAVRVZVoFTz+zmx8K9hBS0Z1pE7pz6y2K0aNbNzawYPMCiqqKuLjXxXSNOvrosbNoJ5/t+IxPt3/KFzu/oNxZ3uR+VmWle0z3uprp8K7Dmdh7IsnRyU3u7/K42FK4hcLKwroaaXF1MTuKdvBB5gfsLd2LVVk5N+Vcth7YSkl1CQuuWMC4nuMaHSevPI+fzvsp3+z9hkGdBqFQdQWIR3uoclVR7iyn3FneqFAckDiAqf2mMrXf1EYBwqu9VDgrWJG9gudXPc+HmR/i1V4uOv0isoqy2HpgK9cMvoa/Xfi3RjX/nUU7+dfqfzFn3RzyK/IBSOucxgU9LyAjOYOtB7ayInsFK/auoKDSDPgkhCfws4E/45rB15DeOb1RS0RrzYGqA3yX8x1fZH3Bop2LWJ+3HoC+Hfvy2NjHmHTGpLr3aK3501d/YuZXM7m83+W88dM3CLHW9xl6tZf95fvZW7KXPSV72FtqHktqSohxxJgttP4xLjSO2NBYYkNjiXZEU1xdzK7iXewu2c2u4l3sKdlDtCOapKgkkqKTSIpKolNkp8MqBiHWELpFd8NuDcxSsG6vm/V566l0VTK86/Bmu3IqXZWs2LuC7jHd6RXfwiKUx2hX8S4+3/E5To+TgZ0GMiBxAB3COhyW15zSHHLKckiOTqZbdLfDWqde7WVT/iaW7VlGTmkOZ/cwLYeIkBZWJmhjEhQCYNs2szzRnFddFHX/D7axf8IdkQ1AXGgUgzsPZVDiIMb1HMclZ1zS7HGW7V7GmFfG1DXp0zunM7HXRC46/aK6QtnXx7j94HY0GrvFbmpdVjulNaVkFWUB0COmB+NPH8/IpJE4bI5GtfGS6hJ2Fu80W9FOdhTtaFQwTuw1kYtOu4jCykJWZq9kRfYKVu1bRZW76rA8h9pCufC0C5nSZwqX9L6E+PB4skuzmfDGBDYXbmbO5DlMHzQdgLW5a5k8dzKFlYW8cukrTO0/tcXP1eVxkVuey/tb3uedH99h+Z7laDTdY7rj8XoorSml3FleV2NOCE/gxvQbuWXoLaTGpVLtrubhpQ/zxNdPEBsay98v+jtRjiieX/U8n23/DKUUl/S+hGn9pzE2dWxdjbghrTU7i3eSU5pDRnLGURWW+RX5fLr9Ux5Z9ghbD2zlzG5n8sS4JxjdbTS/Wfgbnl75NNenXc/sS2a3WatNiENJUDiBtmwxq1jPfduLZdBcwic8SLljOxlJGdwx4jbWZs5ga0kx+3U/NhVkUuGq4JVLX+GawdccdqyS6hIGvzAYq8XKW5e9xZc7v+TjbR/zzd5vGvX7RoZE0qdjH3p16IXVYsXlceHyunB5XNitds7pfg7jTx9P7/jere5T11qzpXALH239iI+2fcTXe76uaxbbLXaGdBlCRnIGI5JG0CWyS12NNC4sjqiQqCa7RkqqS5jy9hQW71rM42Mfp2dcT67977V0DO/I+1e+T3qX9KP+vPeV7WP+j/P5JvsbIuwRRIVEEe2IJtoRTUpsCj/p/ZMma5Yb8jZw04c38V3OdwB0jerKzUNu5qYhNzXbMmpLbq+bOWvn8OCSB8ktz6VPxz5sKdzCnSPu5OnxT9f1vwvhDxIU/GTbgW18kPkBn+74lPySEnJyFAcKFRaLIrpLPsUqi0GdBvHI+Y8wsddElFJUVGxhzZrhREQMoP/ARUx4axLLdi/ji2u+4OweZzc6/vQF05m7cS7Lb1hORnJG3fMHqw7y1a6viHJE0adjH5Kiko5pAPVoFFUV8dXur0iMSGRIlyHH3J9Z467huvevY+7GuQCc2e1MFkxb0GSN3N88Xg9vb3qbMFsYP+n9k4B0j1S6KvnHyn/w9Mqn+eXwX/LguQ/6/W8phASFNrQpfxOv/vAqH2z9oG4GRJxrAEW7k7FYNN27a3qkaCLC7Fwz6Bqm9p96WK0vP/9dfvxxKl273k5Ctz8z6t+jTLfMTSs5vcPpALy54U2uXnA1D415iAfOfeCEnqO/ebWXh5c+THF1MY+Nfey4p/sJIY6OBIU28vWer7nw9Qtxepycl3IeKc5L+O8Tl3AwK4Vf/tLcovgI69XV2bHjHvbufYo+fV6jPCSDkS+NJCE8gRU3rqCkxnQbDUgcwFfXfSV9y0KINiXLXLSBtblrmfimmYmzYNISHruvCy++Ye5h89n35irjo5Ga+hilpd+zdestpKd/w3tXvMe4V8dx+TuX4/K40Frz+pTXJSAIIQJGRraasaVwCxe9fhExoTHM6LqIsSO68PbbMHOmuZvm0QYEAIvFRv/+b2OzdWDTpimM6tqflya9xJc7v2TZnmU8O+FZUuNS2/xchBCitaRK2oTdxbu54LULUEpxa9jn3Hh5N9M6+My0Eo5HSEgnBgxYwNq1Z/Pjj1cyfeD/KK4uJr8iv27KphBCBIoEhUPsL9/PuNfGUe4s5xb7V/zhtt6MHw/z55u7YbaF6OgR9O79PJmZN7Jz5x+4c+QTbXNgIYQ4TtJ9VEtrzZsb3iT9X+nkluVyhet/PPmbQUyZYpawbquA4NOlyw107foL9u59kvz8t9v24EIIcYwkKGDGD8a9No6rF1xNcnQy0yqW8q/7M5g+3SxX4fDT7MnTT/870dFnsmXLDZSXr/dPIkIIcRSCOihUu6v5wxd/YNDzg1iTu4bnJz7PRXtXMufRIdxyC7zyCtj82MFmsYTQv/+72Gwx/PDDWPLz3/VfYkII0QpBHRQeXvowjy5/lJ8N/BmZv8pkhOU2HnvEyjXXmDWMTsSdzRyOLgwe/AUORw9+/HEqmzZNxenM93/CQgjRhKC+eK3PP/vQLaYbn//8c9xuc4/j7GzYvPnE39/A63Wzd+9T7Nr1IFZrFL17P0tCwjRZ/kAI0SZae/Fa0LYUMgszyTyQyeQzJgPwz3+aW2POmhWYG95YLDZ69JjBsGFrCQs7jR9/vJKtW2/jVAvaQohTW9AGhfcz3wdg0hmT2LMH/vhHmDABpra8irPfRUT0Iz39a7p1+x25ubPJyro3sBkSQgSVoL1O4f3M90nvnE636O5ccjVoDc8+27ob4PibxWKjZ8/H8Xgq2Lv3L9jtHene/XeBzpYQIggEZUshrzyPFXtXMPmMybz7Lnz8MfzpT5CSEuic1VNK0avXLBITryIr615yc/8d6CwJIYJAULYUPtr6ERrN+UmTmXaVWcfo178OdK4Op5SFPn3+g8t1kMzMW7DZOpCQMCXQ2RJCtGN+bSkopcYrpTKVUtuVUjOaeP06pVSBUmpd7XaTP/Pj837m+/SI6cGXbw4mLw9efNG/1yMcD4slhAED5hMdPYIff7ySfftexFt7Q3khhGhrfgsKSikr8CxwMdAPuEop1a+JXd/WWqfVbi/5Kz8+Fc4KPs/6nElnTOJ//1OMHAlDh/o71eNjtUYwcODHREUNY+vWW/juuz7k5s6R4CCEaHP+bCmMALZrrbO01k5gLjDZj+m1yqKsRVS7qzmv6yS+/x4uvDDQOWodu70D6enLGTDgQ2y2ODIzb+C77/qSm/sfdO19lIUQ4nj5MygkAXsb/J5d+9yhLlNKrVdKvauU6ubH/ACm6yjGEYNz27l4vXDBBf5Ose0opejY8ScMHfo9AwZ8gM0WQ2bm9axff7FcBS2EaBOBnn30IZCitR4EfA680tROSqlblFKrlFKrCgoKjjkxj9fDR1s/YkKvCXz5uZ2oKBg58pgPFzAmOFzC0KGr6N17NsXFS1m1Kp3i4mWBzpoQ4hTnz6CQAzSs+SfXPldHa31Aa11T++tLQJO9+1rr2VrrYVrrYQkJCcecoRXZKyioLGDSGZNZuBDOPx/s9mM+XMAppeja9WaGDFmJ1RrOunXnsWfPk2jtDXTWhBCnKH8Ghe+BXkqpVKVUCHAl8EHDHZRSDW95PwnY7Mf88P6W97Fb7JxhvZhdu06d8YQjiYpKY+jQ1SQk/JSsrHvZuPFSXK6iQGdLCHEK8ltQ0Fq7gV8Bn2EK+3la601KqT8ppSbV7nanUmqTUuoH4E7gOj/mh/cz3+e81PNYsTgaOLXGE47EZoumX7+3Of30Zzh48FNWrx5OefmGQGdLCHGKCZpVUjcXbKbfc/14dsKzfP7oL1m3DrKyTo5lLdpaSck3bNp0OW53CWec8RKdOl0V6CwJIQJMVkk9xKaCTYTbw7m45yS+/NJ0HbXHgAAQE3MmQ4euISpqCJs3/4zt2++WaxqEEK0SNEHh8n6Xc+B3B9i3JZnS0vbVddQUh6Mzgwd/QVLSHWRnP83ataPJyXmBmpr9gc6aEOIkFjRBASDUFsrCheaOauefH+jc+J/FEkKvXrPo2/d13O5itm37BStWdGXNmrPYu/dv1NTkHPkgQoigEjRjCj5nngleL6xc2YaZOgVorams/JGCgvkUFCygouIHLJZwevZ8lKSkX2FWJRFCtFcyptCE4mL49tv233XUFKUUERH9SUl5gOHD1zFiRCaxseeyfftdrF17NhUVfp0NLIQ4RQRVUPjyS9NKaC/XJxyP8PDeDBz4MX36vEZlZSarVqWxe/ejMiAtRJALqqDw+ecQGQkZGYHOyclBKUXnztMZMWIzHTteys6df+Dbb3uxZ89TcvGbEEEqqILCwoVw3nmn9tIW/hASkkj//m8zcODHhIamkJV1DytWJJOZeRsVFZsCnT0hxAkUNEFhxw5zsZp0HTUvPn4C6elLGDZsHYmJV5GX9wrffz+AtWvPJjf3ZdzuskBnUQjhZ0ETFJYvN48SFI4sMnIwffq8REbGXnr2fAKns4DMzBv55psubN58HUVFi3G7ywOdTSGEHwTVlNSdOyElpf1eyewvWmtKS1ewf/8c8vPfxuMxLYaQkM6EhfUiLKwXkZGD6Nz5Bmy2qADnVgjRlNZOSQ2qoCCOn8dTwcGDC6ms3EJV1TaqqrZRWbkNlyuPkJDOpKY+TOfO18l1D0KcZFobFE7S29WLk5XVGkFCwpTDni8t/Zbt2/+PzMybyM5+htNP/xtxcUFw2bgQ7UzQjCkI/4qOHkl6+tf06zcXt7uYH34Yy7p1Y9mz5wlKSr7G66058kGEEAEnLQXRZpRSJCZeQXz8ZHJy/kFu7hyysmbUvuYgKmoYkZGDsNsTsNs71j1GRqYREtIxwLkXQoCMKQg/czoLKC39hpKS5ZSULKeychtu90Gg4ffOSlzcOBITp9Gx4xTs9rhAZVeIdksGmsVJS2sPLtdBXK4CnM48iooWkp8/j+rqLJSyExd3AbGxY4iKGkJkZDp2e4dAZ1mIU54EBXFK0VpTVraagoJ5FBTMp7o6q+41h6MHkZGDsdlisVgcWCyhWCyh2O0dSUi4jLCw0wKYcyFODRIUxCnN6SykvHwt5eVrKStbQ2XlJjyech4U3mEAAAwBSURBVLze6kYbQEzMOXTufB0JCZfLdRJCNEOCgmj3qquzyct7jf37/0NV1VYslghiYkbh8VTh8ZTgdpvN4UgiMfFndOp0NWFhqYHOthABIUFBBA1zxfVK9u+fQ3n5WqzWaGy2GGy2GKzWaCoq1lNcvASAmJiz6NRpOlFRw7BaI7FYIrBaI7FaI7BYZKVE0X7JxWsiaCiliIkZRUzMqGb3qa7eQ17em+TlvcbWrbc1uY/VGonNFl87XTa+btpsSEhC7fTZRByOJCIjB2OxhPjrdIQIKGkpiKCitaaiYgPV1bvweCrweMprH8twu4twuQpxuQ7UboW4XAV4PKWNjmGxhBIVNZKYmLOIjT2byMih2O0dUEquBRUnL+k+EqKNeDzVtQEin+rqnZSUfE1JyTLKytYCntq9rLUtikTs9gSs1kjMtRi+DazWaEJCErHbEwkJ6YTdnojVGo7F4kCpkLqZVQ5HElZrRGBOVrRb0n0kRBuxWkOxWpMJDU0mKmoICQmXAeB2l1NaupKKio24XAW4XPk4nfm4XPm4XIWA6doCVbv/Zlyu/LpVZltit3ckNDQFh6MHDkfX2gUGfcv7KrT24PVW4vVW4fGYx5CQzsTFjSU2diwOR2c/fBIiGEhLQYgTzOOprL1wL7+2YHfi9dagtROPp4Kamr1UV++munoX1dW7cTr3A97ad/v+Xy21rYwwLJZwrNYwqqqyaq8Wh4iIAbXBIQmt3WjtqX10o5QNi8WOUvbaFoodpRy1LZUQlHJgs8USFTUMmy0yEB+R8ANpKQhxkrJaw7FaexAa2qNNj6u1l/LydRQVLaKo6Atyc/9Vdy1HPQv1AeaIOSUyMo2YmLOIiTkLh6MrlZWbqajYVLeBt/aeGqfXPZquM09tIDLda5GR6YSFpbTZuQr/kZaCEO2U1+tCaydK2QArSllRSqG1RmsXWrvwep1o7WzQWqnB663B6cyrW7OqtPRbvN6quuNaLGGEh/cjIqI/Slkb3VOjJeHhfejQYTwdOownJuYs3O5Samr2UF29l5qaPbhcB7BYwuqmCJstGrs9DputA3Z7B2y2OLzeGmpq9lJTk137mIPVGoHD0Z3QUBNs7fbE2q474SMtBSGCnLnu4vBrL5RSKBUChLQ4oB0ffzEAXq+TsrI1uFyFRET0IzQ0pcmZVm53GVVVO/B6q2oDkA2lrHi9LkpKlnPw4Kfs2/cC2dl/byZFK/UD98dHKUdtgEip20JCEqmpyaW6emdt19xOXK6DhIQkEhLSucHWFYcjCYcjuXZLQmsXVVU7a99r3q+UtXYKc/1mtUZjtUZhs0VhtUZhsYTXjvuYWW5ebwVae+umPFut4W1yvm1JWgpCiBPG46mipGQppaXfYrd3rK3dd8fh6IbNFovW7rqpwl5vBW53ae1U4YO43QdxuQ5isYTgcHSr3ZJxOLri8VRSU7Ob6uo9VFfvrv3ZNy6zC5eroC4PISGdCQ1NJTQ0Fbu9Ay5XITU1uTid+3E69+PxlBzxPKzWKEDj8RzfvcotlrDa4BCBKYs14K1tzblrN1fdlpz8f6Sm/umY0pKWghDipGO1htGhw0V06HBRk68rZcdiicVujz2q41osIdjtsURGDm7ydY+nAqczn5CQzlitYS0eywz259R2T5lNKTuhoamEhZlgYrPFoZTC662pXfH3AG73AdzuMjwe31aOx1OJxRLaoDssArDUBrjCus3jqQBUbQtM1f5sq50MUD8xIDp65FF9LsdCgoIQot2zWiNave6V1RpBeHhvwsN7H3Ffi8WBw9EFh6PL8WbxpCGXYAoh/r+9+w2xozrjOP79tWlTTcT4ZytBpYlValPQ1UrqvxZNsKQitS8itVopRfBNBAXBGqqW+q5vGn0hrVJtUxtUTE2VIFpdJSBo4q6umj9NXTXiirrRxn8FbROfvjhnh+s1yV7Wzs4c7+8Dlztz7tzhOcm5+9yZufMcs4qTgpmZVWpNCpKWSdouaUzSNXt5fbaku/PrGyUtqDMeMzPbv9qSgtJ9+TcDPwAWAT+RtKhrs0uBXRFxLLAK+E1d8ZiZ2dTqPFJYDIxFxEsR8R/gLuD8rm3OB1bn5bXAUvmOEzOzxtSZFI4EXu1YH89te90mInYD7wKH1RiTmZntRxEXmiVdJmlY0vDOnTunfoOZmU1LnUnhNeDojvWjcttet1Eq0HIw8Hb3jiLi1og4JSJOGRgYqClcMzOr8+a1p4DjJC0k/fG/ELioa5v7gZ8BTwDLgUdjirobIyMjb0l6ZZoxHQ68Nc33tkXpfXD8zSu9D45/enoqy1tbUoiI3ZIuBx4iVbq6PSK2SLoBGI6I+4HbgDskjQH/IiWOqfY77UMFScO91P5os9L74PibV3ofHH+9ai1zEREPAA90tV3fsfwhcEGdMZiZWe+KuNBsZmYzo9+Swq1NB/B/UHofHH/zSu+D469RcfMpmJlZffrtSMHMzPajb5LCVMX52kjS7ZImJG3uaDtU0sOSXsjPhzQZ475IOlrSY5K2Stoi6YrcXkT8AJK+ImmTpGdzH36d2xfmAo5juaDjl5uOdX8kfVHSM5LW5/Vi4pe0Q9LzkkYlDee2YsYQgKR5ktZK+oekbZJOa3Mf+iIp9Ficr43+BCzrarsGGIqI44ChvN5Gu4GrImIRcCqwIv+blxI/wEfAkog4ERgElkk6lVS4cVUu5LiLVNixza4AtnWslxb/2REx2PEzzpLGEMBNwIMRcTxwIun/or19SHOBfr4fwGnAQx3rK4GVTcfVY+wLgM0d69uB+Xl5PrC96Rh77Md9wDkFx38g8DTwHdKNR7Ny+yfGVtsepEoCQ8ASYD1prseS4t8BHN7VVswYIlVpeJl8/baEPvTFkQK9FecrxRER8XpefgM4oslgepHnyTgJ2Ehh8edTL6PABPAw8CLwTqQCjtD+sXQjcDXwcV4/jLLiD+DvkkYkXZbbShpDC4GdwB/zKbw/SJpDi/vQL0nhcynS14xW/3xM0lzgr8CVEfFe52slxB8ReyJikPSNezFwfMMh9UzSecBERIw0HctncGZEnEw69btC0vc6XyxgDM0CTgZ+FxEnAf+m61RR2/rQL0mhl+J8pXhT0nyA/DzRcDz7JOlLpISwJiLuzc3FxN8pIt4BHiOdbpmXCzhCu8fSGcAPJe0gzWeyhHR+u5T4iYjX8vMEsI6UmEsaQ+PAeERszOtrSUmitX3ol6RQFefLv7S4kFSMr0STRQTJz/c1GMs+5cmSbgO2RcRvO14qIn4ASQOS5uXlA0jXRLaRksPyvFlr+xARKyPiqIhYQBrzj0bExRQSv6Q5kg6aXAa+D2ymoDEUEW8Ar0r6Rm5aCmylzX1o+qLGDF7wORf4J+mc8C+bjqfHmO8EXgf+S/rGcSnpnPAQ8ALwCHBo03HuI/YzSYfEzwGj+XFuKfHnPpwAPJP7sBm4PrcfA2wCxoB7gNlNx9pDX84C1pcUf47z2fzYMvm5LWkM5XgHgeE8jv4GHNLmPviOZjMzq/TL6SMzM+uBk4KZmVWcFMzMrOKkYGZmFScFMzOrOCmYzSBJZ01WKzVrIycFMzOrOCmY7YWkn+a5FEYl3ZIL430gaVWeW2FI0kDedlDSk5Kek7Rusja+pGMlPZLnY3ha0tfz7ud21Ndfk+/+NmsFJwWzLpK+CfwYOCNSMbw9wMXAHGA4Ir4FbAB+ld/yZ+AXEXEC8HxH+xrg5kjzMZxOujsdUsXYK0lzexxDqlFk1gqzpt7ErO8sBb4NPJW/xB9AKlj2MXB33uYvwL2SDgbmRcSG3L4auCfX7DkyItYBRMSHAHl/myJiPK+PkubMeLz+bplNzUnB7NMErI6IlZ9olK7r2m66NWI+6ljegz+H1iI+fWT2aUPAcklfhWpO4K+RPi+T1UUvAh6PiHeBXZK+m9svATZExPvAuKQf5X3MlnTgjPbCbBr8DcWsS0RslXQtacavL5Cq1K4gTZCyOL82QbruAKn08e/zH/2XgJ/n9kuAWyTdkPdxwQx2w2xaXCXVrEeSPoiIuU3HYVYnnz4yM7OKjxTMzKziIwUzM6s4KZiZWcVJwczMKk4KZmZWcVIwM7OKk4KZmVX+B6VSL6MeTMFSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 0.9283 - acc: 0.7346\n",
      "Loss: 0.9282862781859385 Accuracy: 0.73457944\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1927 - acc: 0.3536\n",
      "Epoch 00001: val_loss improved from inf to 1.58461, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/001-1.5846.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 2.1926 - acc: 0.3536 - val_loss: 1.5846 - val_acc: 0.4850\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3643 - acc: 0.5784\n",
      "Epoch 00002: val_loss improved from 1.58461 to 1.03792, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/002-1.0379.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 1.3643 - acc: 0.5784 - val_loss: 1.0379 - val_acc: 0.6832\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0720 - acc: 0.6695\n",
      "Epoch 00003: val_loss improved from 1.03792 to 0.98972, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/003-0.9897.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 1.0721 - acc: 0.6695 - val_loss: 0.9897 - val_acc: 0.7105\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7165\n",
      "Epoch 00004: val_loss improved from 0.98972 to 0.88004, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/004-0.8800.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.9317 - acc: 0.7165 - val_loss: 0.8800 - val_acc: 0.7410\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8296 - acc: 0.7476\n",
      "Epoch 00005: val_loss improved from 0.88004 to 0.80871, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/005-0.8087.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.8299 - acc: 0.7475 - val_loss: 0.8087 - val_acc: 0.7678\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7586 - acc: 0.7711\n",
      "Epoch 00006: val_loss improved from 0.80871 to 0.77382, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/006-0.7738.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.7585 - acc: 0.7711 - val_loss: 0.7738 - val_acc: 0.7827\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6972 - acc: 0.7899\n",
      "Epoch 00007: val_loss did not improve from 0.77382\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.6974 - acc: 0.7899 - val_loss: 0.8288 - val_acc: 0.7666\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.8050\n",
      "Epoch 00008: val_loss improved from 0.77382 to 0.64698, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/008-0.6470.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.6482 - acc: 0.8050 - val_loss: 0.6470 - val_acc: 0.8157\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8210\n",
      "Epoch 00009: val_loss did not improve from 0.64698\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.5960 - acc: 0.8210 - val_loss: 0.7233 - val_acc: 0.7943\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.8336\n",
      "Epoch 00010: val_loss did not improve from 0.64698\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.5516 - acc: 0.8335 - val_loss: 0.6587 - val_acc: 0.8074\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.8421\n",
      "Epoch 00011: val_loss improved from 0.64698 to 0.60936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_5_conv_checkpoint/011-0.6094.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.5194 - acc: 0.8421 - val_loss: 0.6094 - val_acc: 0.8253\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8505\n",
      "Epoch 00012: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.4922 - acc: 0.8505 - val_loss: 0.6778 - val_acc: 0.8130\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8586\n",
      "Epoch 00013: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.4605 - acc: 0.8585 - val_loss: 0.7552 - val_acc: 0.7806\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.8693\n",
      "Epoch 00014: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.4296 - acc: 0.8692 - val_loss: 0.6690 - val_acc: 0.8195\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8740\n",
      "Epoch 00015: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.4078 - acc: 0.8740 - val_loss: 0.6622 - val_acc: 0.8162\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8836\n",
      "Epoch 00016: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.3761 - acc: 0.8835 - val_loss: 0.6247 - val_acc: 0.8281\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8890\n",
      "Epoch 00017: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.3646 - acc: 0.8889 - val_loss: 0.6290 - val_acc: 0.8251\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8938\n",
      "Epoch 00018: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.3425 - acc: 0.8938 - val_loss: 0.6259 - val_acc: 0.8337\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8979\n",
      "Epoch 00019: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.3234 - acc: 0.8979 - val_loss: 0.6694 - val_acc: 0.8328\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9067\n",
      "Epoch 00020: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2969 - acc: 0.9067 - val_loss: 0.7039 - val_acc: 0.8211\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9101\n",
      "Epoch 00021: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2852 - acc: 0.9101 - val_loss: 0.6484 - val_acc: 0.8288\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9126\n",
      "Epoch 00022: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2731 - acc: 0.9125 - val_loss: 0.6943 - val_acc: 0.8211\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9159\n",
      "Epoch 00023: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2668 - acc: 0.9159 - val_loss: 0.6437 - val_acc: 0.8316\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9222\n",
      "Epoch 00024: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2427 - acc: 0.9222 - val_loss: 0.6293 - val_acc: 0.8353\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9255\n",
      "Epoch 00025: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2330 - acc: 0.9254 - val_loss: 0.6895 - val_acc: 0.8204\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9264\n",
      "Epoch 00026: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2296 - acc: 0.9264 - val_loss: 0.6635 - val_acc: 0.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9316\n",
      "Epoch 00027: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2122 - acc: 0.9316 - val_loss: 0.7778 - val_acc: 0.8071\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9319\n",
      "Epoch 00028: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.2110 - acc: 0.9319 - val_loss: 0.7185 - val_acc: 0.8304\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9374\n",
      "Epoch 00029: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1951 - acc: 0.9374 - val_loss: 0.8723 - val_acc: 0.7827\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9393\n",
      "Epoch 00030: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1909 - acc: 0.9393 - val_loss: 0.6731 - val_acc: 0.8269\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9402\n",
      "Epoch 00031: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1865 - acc: 0.9402 - val_loss: 0.7289 - val_acc: 0.8283\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9441\n",
      "Epoch 00032: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1785 - acc: 0.9441 - val_loss: 0.6813 - val_acc: 0.8318\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9448\n",
      "Epoch 00033: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1671 - acc: 0.9448 - val_loss: 0.7576 - val_acc: 0.8213\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9459\n",
      "Epoch 00034: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1707 - acc: 0.9459 - val_loss: 0.7013 - val_acc: 0.8404\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9483\n",
      "Epoch 00035: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1614 - acc: 0.9483 - val_loss: 0.6913 - val_acc: 0.8321\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9500\n",
      "Epoch 00036: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1587 - acc: 0.9500 - val_loss: 0.7164 - val_acc: 0.8348\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9514\n",
      "Epoch 00037: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1521 - acc: 0.9514 - val_loss: 0.6692 - val_acc: 0.8472\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9509\n",
      "Epoch 00038: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1506 - acc: 0.9509 - val_loss: 0.6621 - val_acc: 0.8542\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9529\n",
      "Epoch 00039: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1455 - acc: 0.9529 - val_loss: 0.7425 - val_acc: 0.8409\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9585\n",
      "Epoch 00040: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1319 - acc: 0.9585 - val_loss: 0.7263 - val_acc: 0.8360\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9561\n",
      "Epoch 00041: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1398 - acc: 0.9561 - val_loss: 0.6995 - val_acc: 0.8381\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9591\n",
      "Epoch 00042: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1314 - acc: 0.9591 - val_loss: 0.6691 - val_acc: 0.8521\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9580\n",
      "Epoch 00043: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1323 - acc: 0.9580 - val_loss: 0.7185 - val_acc: 0.8449\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9585\n",
      "Epoch 00044: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1297 - acc: 0.9585 - val_loss: 0.7107 - val_acc: 0.8346\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9587\n",
      "Epoch 00045: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1287 - acc: 0.9586 - val_loss: 0.7199 - val_acc: 0.8444\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9586\n",
      "Epoch 00046: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1319 - acc: 0.9586 - val_loss: 0.6829 - val_acc: 0.8474\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9616\n",
      "Epoch 00047: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1200 - acc: 0.9616 - val_loss: 0.6739 - val_acc: 0.8491\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9647\n",
      "Epoch 00048: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1115 - acc: 0.9647 - val_loss: 0.7012 - val_acc: 0.8498\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9644\n",
      "Epoch 00049: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1105 - acc: 0.9644 - val_loss: 0.6982 - val_acc: 0.8505\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9648\n",
      "Epoch 00050: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1120 - acc: 0.9648 - val_loss: 0.7559 - val_acc: 0.8404\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9671\n",
      "Epoch 00051: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1053 - acc: 0.9671 - val_loss: 0.8850 - val_acc: 0.8162\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9643\n",
      "Epoch 00052: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1146 - acc: 0.9642 - val_loss: 0.6900 - val_acc: 0.8495\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9639\n",
      "Epoch 00053: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1116 - acc: 0.9639 - val_loss: 0.7283 - val_acc: 0.8386\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9689\n",
      "Epoch 00054: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0998 - acc: 0.9689 - val_loss: 0.7536 - val_acc: 0.8416\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9666\n",
      "Epoch 00055: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1033 - acc: 0.9666 - val_loss: 0.6973 - val_acc: 0.8493\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9705\n",
      "Epoch 00056: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0965 - acc: 0.9705 - val_loss: 0.7093 - val_acc: 0.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9707\n",
      "Epoch 00057: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0950 - acc: 0.9707 - val_loss: 0.7017 - val_acc: 0.8446\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9711\n",
      "Epoch 00058: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0946 - acc: 0.9711 - val_loss: 0.7155 - val_acc: 0.8521\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9683\n",
      "Epoch 00059: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.1031 - acc: 0.9683 - val_loss: 0.7461 - val_acc: 0.8442\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9701\n",
      "Epoch 00060: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0988 - acc: 0.9700 - val_loss: 0.8131 - val_acc: 0.8402\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9707\n",
      "Epoch 00061: val_loss did not improve from 0.60936\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0958 - acc: 0.9707 - val_loss: 0.7952 - val_acc: 0.8414\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.7214 - acc: 0.7890\n",
      "Loss: 0.7214093419745456 Accuracy: 0.7889927\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3659 - acc: 0.2877\n",
      "Epoch 00001: val_loss improved from inf to 1.94201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/001-1.9420.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 2.3658 - acc: 0.2878 - val_loss: 1.9420 - val_acc: 0.3380\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4102 - acc: 0.5529\n",
      "Epoch 00002: val_loss improved from 1.94201 to 1.14365, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/002-1.1436.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 1.4102 - acc: 0.5529 - val_loss: 1.1436 - val_acc: 0.6553\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.6637\n",
      "Epoch 00003: val_loss improved from 1.14365 to 0.92763, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/003-0.9276.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 1.0874 - acc: 0.6637 - val_loss: 0.9276 - val_acc: 0.7235\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9089 - acc: 0.7225\n",
      "Epoch 00004: val_loss improved from 0.92763 to 0.80684, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/004-0.8068.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.9089 - acc: 0.7225 - val_loss: 0.8068 - val_acc: 0.7799\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7586\n",
      "Epoch 00005: val_loss improved from 0.80684 to 0.71465, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/005-0.7146.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.7960 - acc: 0.7586 - val_loss: 0.7146 - val_acc: 0.7948\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7212 - acc: 0.7849\n",
      "Epoch 00006: val_loss did not improve from 0.71465\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.7213 - acc: 0.7849 - val_loss: 0.9455 - val_acc: 0.7277\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.8069\n",
      "Epoch 00007: val_loss improved from 0.71465 to 0.60756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/007-0.6076.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.6450 - acc: 0.8068 - val_loss: 0.6076 - val_acc: 0.8239\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8272\n",
      "Epoch 00008: val_loss did not improve from 0.60756\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5830 - acc: 0.8272 - val_loss: 0.6386 - val_acc: 0.8183\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8424\n",
      "Epoch 00009: val_loss improved from 0.60756 to 0.53742, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/009-0.5374.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5304 - acc: 0.8424 - val_loss: 0.5374 - val_acc: 0.8470\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8550\n",
      "Epoch 00010: val_loss did not improve from 0.53742\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4868 - acc: 0.8550 - val_loss: 0.5563 - val_acc: 0.8491\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8644\n",
      "Epoch 00011: val_loss improved from 0.53742 to 0.50640, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/011-0.5064.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4540 - acc: 0.8644 - val_loss: 0.5064 - val_acc: 0.8644\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8752\n",
      "Epoch 00012: val_loss improved from 0.50640 to 0.45750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/012-0.4575.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4224 - acc: 0.8752 - val_loss: 0.4575 - val_acc: 0.8803\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8827\n",
      "Epoch 00013: val_loss improved from 0.45750 to 0.42304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/013-0.4230.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3903 - acc: 0.8828 - val_loss: 0.4230 - val_acc: 0.8817\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8901\n",
      "Epoch 00014: val_loss did not improve from 0.42304\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3638 - acc: 0.8901 - val_loss: 0.4569 - val_acc: 0.8719\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8983\n",
      "Epoch 00015: val_loss did not improve from 0.42304\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3407 - acc: 0.8984 - val_loss: 0.4827 - val_acc: 0.8730\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9027\n",
      "Epoch 00016: val_loss did not improve from 0.42304\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3215 - acc: 0.9026 - val_loss: 0.4546 - val_acc: 0.8807\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9084\n",
      "Epoch 00017: val_loss improved from 0.42304 to 0.40666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/017-0.4067.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3017 - acc: 0.9084 - val_loss: 0.4067 - val_acc: 0.8903\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9128\n",
      "Epoch 00018: val_loss did not improve from 0.40666\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2829 - acc: 0.9128 - val_loss: 0.4095 - val_acc: 0.8889\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9162\n",
      "Epoch 00019: val_loss did not improve from 0.40666\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2720 - acc: 0.9162 - val_loss: 0.4383 - val_acc: 0.8877\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9203\n",
      "Epoch 00020: val_loss did not improve from 0.40666\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2581 - acc: 0.9203 - val_loss: 0.4376 - val_acc: 0.8833\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9207\n",
      "Epoch 00021: val_loss improved from 0.40666 to 0.39704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/021-0.3970.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2527 - acc: 0.9207 - val_loss: 0.3970 - val_acc: 0.8961\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9292\n",
      "Epoch 00022: val_loss improved from 0.39704 to 0.36538, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/022-0.3654.hdf5\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.2251 - acc: 0.9292 - val_loss: 0.3654 - val_acc: 0.8945\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9307\n",
      "Epoch 00023: val_loss did not improve from 0.36538\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2255 - acc: 0.9307 - val_loss: 0.4011 - val_acc: 0.8852\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9360\n",
      "Epoch 00024: val_loss did not improve from 0.36538\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2078 - acc: 0.9359 - val_loss: 0.3829 - val_acc: 0.9045\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9341\n",
      "Epoch 00025: val_loss improved from 0.36538 to 0.35503, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_6_conv_checkpoint/025-0.3550.hdf5\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.2098 - acc: 0.9341 - val_loss: 0.3550 - val_acc: 0.9073\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9418\n",
      "Epoch 00026: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1867 - acc: 0.9418 - val_loss: 0.3977 - val_acc: 0.9010\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9413\n",
      "Epoch 00027: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1842 - acc: 0.9413 - val_loss: 0.3733 - val_acc: 0.9131\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9432\n",
      "Epoch 00028: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1764 - acc: 0.9431 - val_loss: 0.3901 - val_acc: 0.8963\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9457\n",
      "Epoch 00029: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1734 - acc: 0.9456 - val_loss: 0.3773 - val_acc: 0.9012\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9457\n",
      "Epoch 00030: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1725 - acc: 0.9457 - val_loss: 0.3862 - val_acc: 0.9066\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9522\n",
      "Epoch 00031: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1495 - acc: 0.9522 - val_loss: 0.3675 - val_acc: 0.9110\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9502\n",
      "Epoch 00032: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1539 - acc: 0.9503 - val_loss: 0.3576 - val_acc: 0.9106\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9550\n",
      "Epoch 00033: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1413 - acc: 0.9550 - val_loss: 0.3918 - val_acc: 0.9024\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9543\n",
      "Epoch 00034: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1421 - acc: 0.9542 - val_loss: 0.4215 - val_acc: 0.8949\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9569\n",
      "Epoch 00035: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1333 - acc: 0.9569 - val_loss: 0.3691 - val_acc: 0.9050\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9604\n",
      "Epoch 00036: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1253 - acc: 0.9603 - val_loss: 0.4321 - val_acc: 0.9001\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9585\n",
      "Epoch 00037: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1282 - acc: 0.9585 - val_loss: 0.4358 - val_acc: 0.8949\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9623\n",
      "Epoch 00038: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1175 - acc: 0.9623 - val_loss: 0.3587 - val_acc: 0.9106\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9645\n",
      "Epoch 00039: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1114 - acc: 0.9645 - val_loss: 0.3994 - val_acc: 0.9052\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9646\n",
      "Epoch 00040: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1120 - acc: 0.9646 - val_loss: 0.3866 - val_acc: 0.9078\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9657\n",
      "Epoch 00041: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1056 - acc: 0.9657 - val_loss: 0.4444 - val_acc: 0.8889\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9662\n",
      "Epoch 00042: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1059 - acc: 0.9662 - val_loss: 0.4128 - val_acc: 0.9036\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9672\n",
      "Epoch 00043: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1024 - acc: 0.9671 - val_loss: 0.3820 - val_acc: 0.9085\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9657\n",
      "Epoch 00044: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1090 - acc: 0.9656 - val_loss: 0.4269 - val_acc: 0.9047\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9699\n",
      "Epoch 00045: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0950 - acc: 0.9699 - val_loss: 0.3648 - val_acc: 0.9152\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9722\n",
      "Epoch 00046: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0861 - acc: 0.9722 - val_loss: 0.4483 - val_acc: 0.9012\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9706\n",
      "Epoch 00047: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0913 - acc: 0.9706 - val_loss: 0.3879 - val_acc: 0.9187\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9713\n",
      "Epoch 00048: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0878 - acc: 0.9712 - val_loss: 0.4588 - val_acc: 0.9017\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9699\n",
      "Epoch 00049: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0962 - acc: 0.9699 - val_loss: 0.3943 - val_acc: 0.9175\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9733\n",
      "Epoch 00050: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0833 - acc: 0.9733 - val_loss: 0.4071 - val_acc: 0.9122\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9755\n",
      "Epoch 00051: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0780 - acc: 0.9755 - val_loss: 0.4400 - val_acc: 0.9043\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9760\n",
      "Epoch 00052: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0765 - acc: 0.9759 - val_loss: 0.4145 - val_acc: 0.9015\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9732\n",
      "Epoch 00053: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0820 - acc: 0.9732 - val_loss: 0.3557 - val_acc: 0.9180\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9762\n",
      "Epoch 00054: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0757 - acc: 0.9762 - val_loss: 0.3772 - val_acc: 0.9168\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9767\n",
      "Epoch 00055: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0749 - acc: 0.9767 - val_loss: 0.4364 - val_acc: 0.9059\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9770\n",
      "Epoch 00056: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0722 - acc: 0.9770 - val_loss: 0.4875 - val_acc: 0.8931\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9786\n",
      "Epoch 00057: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0665 - acc: 0.9785 - val_loss: 0.4900 - val_acc: 0.8926\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9762\n",
      "Epoch 00058: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0772 - acc: 0.9762 - val_loss: 0.3730 - val_acc: 0.9187\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9807\n",
      "Epoch 00059: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0612 - acc: 0.9807 - val_loss: 0.4094 - val_acc: 0.9136\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9777\n",
      "Epoch 00060: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0708 - acc: 0.9777 - val_loss: 0.4137 - val_acc: 0.9082\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9803\n",
      "Epoch 00061: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0620 - acc: 0.9802 - val_loss: 0.5968 - val_acc: 0.8875\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9762\n",
      "Epoch 00062: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0769 - acc: 0.9762 - val_loss: 0.3891 - val_acc: 0.9131\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9807\n",
      "Epoch 00063: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0598 - acc: 0.9807 - val_loss: 0.3843 - val_acc: 0.9147\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9814\n",
      "Epoch 00064: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0568 - acc: 0.9814 - val_loss: 0.4581 - val_acc: 0.9043\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9824\n",
      "Epoch 00065: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0577 - acc: 0.9823 - val_loss: 0.4845 - val_acc: 0.9043\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9769\n",
      "Epoch 00066: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0702 - acc: 0.9769 - val_loss: 0.4089 - val_acc: 0.9178\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9830\n",
      "Epoch 00067: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0564 - acc: 0.9830 - val_loss: 0.4109 - val_acc: 0.9187\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9829\n",
      "Epoch 00068: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0546 - acc: 0.9829 - val_loss: 0.4625 - val_acc: 0.9075\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9771\n",
      "Epoch 00069: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0723 - acc: 0.9771 - val_loss: 0.4530 - val_acc: 0.8998\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9839\n",
      "Epoch 00070: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0507 - acc: 0.9839 - val_loss: 0.3674 - val_acc: 0.9276\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9850\n",
      "Epoch 00071: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0487 - acc: 0.9849 - val_loss: 0.5124 - val_acc: 0.8998\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9799\n",
      "Epoch 00072: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0627 - acc: 0.9799 - val_loss: 0.4373 - val_acc: 0.9059\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9826\n",
      "Epoch 00073: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0570 - acc: 0.9826 - val_loss: 0.4307 - val_acc: 0.9101\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9846\n",
      "Epoch 00074: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0483 - acc: 0.9846 - val_loss: 0.5607 - val_acc: 0.8970\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9862\n",
      "Epoch 00075: val_loss did not improve from 0.35503\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0467 - acc: 0.9862 - val_loss: 0.3771 - val_acc: 0.9217\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.4117 - acc: 0.8837\n",
      "Loss: 0.41169039384226935 Accuracy: 0.8836968\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6147 - acc: 0.2256\n",
      "Epoch 00001: val_loss improved from inf to 1.94653, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/001-1.9465.hdf5\n",
      "36805/36805 [==============================] - 200s 5ms/sample - loss: 2.6146 - acc: 0.2256 - val_loss: 1.9465 - val_acc: 0.3767\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6163 - acc: 0.4828\n",
      "Epoch 00002: val_loss improved from 1.94653 to 1.18696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/002-1.1870.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.6165 - acc: 0.4828 - val_loss: 1.1870 - val_acc: 0.6294\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1730 - acc: 0.6318\n",
      "Epoch 00003: val_loss improved from 1.18696 to 0.78035, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/003-0.7804.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.1729 - acc: 0.6318 - val_loss: 0.7804 - val_acc: 0.7829\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.7272\n",
      "Epoch 00004: val_loss improved from 0.78035 to 0.63445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/004-0.6344.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.8988 - acc: 0.7272 - val_loss: 0.6344 - val_acc: 0.8176\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.7814\n",
      "Epoch 00005: val_loss improved from 0.63445 to 0.52740, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/005-0.5274.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.7289 - acc: 0.7814 - val_loss: 0.5274 - val_acc: 0.8409\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8158\n",
      "Epoch 00006: val_loss improved from 0.52740 to 0.50521, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/006-0.5052.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6103 - acc: 0.8158 - val_loss: 0.5052 - val_acc: 0.8549\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8455\n",
      "Epoch 00007: val_loss improved from 0.50521 to 0.39348, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/007-0.3935.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5189 - acc: 0.8455 - val_loss: 0.3935 - val_acc: 0.8947\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8660\n",
      "Epoch 00008: val_loss improved from 0.39348 to 0.35866, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/008-0.3587.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4494 - acc: 0.8660 - val_loss: 0.3587 - val_acc: 0.8991\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8809\n",
      "Epoch 00009: val_loss improved from 0.35866 to 0.33937, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/009-0.3394.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3985 - acc: 0.8809 - val_loss: 0.3394 - val_acc: 0.9043\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8918\n",
      "Epoch 00010: val_loss improved from 0.33937 to 0.31915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/010-0.3192.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3565 - acc: 0.8918 - val_loss: 0.3192 - val_acc: 0.9085\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.9010\n",
      "Epoch 00011: val_loss improved from 0.31915 to 0.28524, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/011-0.2852.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3252 - acc: 0.9010 - val_loss: 0.2852 - val_acc: 0.9241\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9083\n",
      "Epoch 00012: val_loss improved from 0.28524 to 0.27561, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/012-0.2756.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2974 - acc: 0.9083 - val_loss: 0.2756 - val_acc: 0.9215\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9179\n",
      "Epoch 00013: val_loss improved from 0.27561 to 0.26226, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/013-0.2623.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2649 - acc: 0.9179 - val_loss: 0.2623 - val_acc: 0.9290\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9227\n",
      "Epoch 00014: val_loss did not improve from 0.26226\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2487 - acc: 0.9226 - val_loss: 0.2843 - val_acc: 0.9187\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9269\n",
      "Epoch 00015: val_loss did not improve from 0.26226\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2288 - acc: 0.9269 - val_loss: 0.2629 - val_acc: 0.9273\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9317\n",
      "Epoch 00016: val_loss improved from 0.26226 to 0.25431, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/016-0.2543.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2150 - acc: 0.9317 - val_loss: 0.2543 - val_acc: 0.9280\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9375\n",
      "Epoch 00017: val_loss did not improve from 0.25431\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1978 - acc: 0.9375 - val_loss: 0.3611 - val_acc: 0.9108\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9392\n",
      "Epoch 00018: val_loss did not improve from 0.25431\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1916 - acc: 0.9391 - val_loss: 0.2770 - val_acc: 0.9196\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9421\n",
      "Epoch 00019: val_loss did not improve from 0.25431\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1832 - acc: 0.9421 - val_loss: 0.2547 - val_acc: 0.9343\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9469\n",
      "Epoch 00020: val_loss improved from 0.25431 to 0.21423, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/020-0.2142.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1673 - acc: 0.9469 - val_loss: 0.2142 - val_acc: 0.9420\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9496\n",
      "Epoch 00021: val_loss did not improve from 0.21423\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1584 - acc: 0.9496 - val_loss: 0.2485 - val_acc: 0.9380\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9542\n",
      "Epoch 00022: val_loss improved from 0.21423 to 0.19667, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_7_conv_checkpoint/022-0.1967.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1460 - acc: 0.9542 - val_loss: 0.1967 - val_acc: 0.9467\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9539\n",
      "Epoch 00023: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1459 - acc: 0.9539 - val_loss: 0.2889 - val_acc: 0.9320\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9586\n",
      "Epoch 00024: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1304 - acc: 0.9586 - val_loss: 0.2621 - val_acc: 0.9338\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9599\n",
      "Epoch 00025: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1262 - acc: 0.9599 - val_loss: 0.2717 - val_acc: 0.9287\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9619\n",
      "Epoch 00026: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1216 - acc: 0.9619 - val_loss: 0.3168 - val_acc: 0.9210\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9636\n",
      "Epoch 00027: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1145 - acc: 0.9636 - val_loss: 0.2117 - val_acc: 0.9441\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9640\n",
      "Epoch 00028: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1098 - acc: 0.9640 - val_loss: 0.2915 - val_acc: 0.9154\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9670\n",
      "Epoch 00029: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1027 - acc: 0.9670 - val_loss: 0.2370 - val_acc: 0.9373\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9695\n",
      "Epoch 00030: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0960 - acc: 0.9694 - val_loss: 0.2393 - val_acc: 0.9387\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9658\n",
      "Epoch 00031: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1026 - acc: 0.9658 - val_loss: 0.2066 - val_acc: 0.9434\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9724\n",
      "Epoch 00032: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0873 - acc: 0.9724 - val_loss: 0.2289 - val_acc: 0.9439\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9690\n",
      "Epoch 00033: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0941 - acc: 0.9689 - val_loss: 0.2517 - val_acc: 0.9406\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9690\n",
      "Epoch 00034: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0939 - acc: 0.9690 - val_loss: 0.3038 - val_acc: 0.9301\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9754\n",
      "Epoch 00035: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0768 - acc: 0.9754 - val_loss: 0.2422 - val_acc: 0.9385\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9760\n",
      "Epoch 00036: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0743 - acc: 0.9759 - val_loss: 0.2363 - val_acc: 0.9432\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9756\n",
      "Epoch 00037: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0748 - acc: 0.9756 - val_loss: 0.2569 - val_acc: 0.9387\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9749\n",
      "Epoch 00038: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0761 - acc: 0.9748 - val_loss: 0.2530 - val_acc: 0.9397\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9760\n",
      "Epoch 00039: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0725 - acc: 0.9760 - val_loss: 0.2249 - val_acc: 0.9422\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9771\n",
      "Epoch 00040: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0709 - acc: 0.9771 - val_loss: 0.2560 - val_acc: 0.9394\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9803\n",
      "Epoch 00041: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0598 - acc: 0.9803 - val_loss: 0.2448 - val_acc: 0.9411\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9804\n",
      "Epoch 00042: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0616 - acc: 0.9804 - val_loss: 0.2443 - val_acc: 0.9406\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9811\n",
      "Epoch 00043: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0599 - acc: 0.9811 - val_loss: 0.2488 - val_acc: 0.9457\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9819\n",
      "Epoch 00044: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0570 - acc: 0.9819 - val_loss: 0.2148 - val_acc: 0.9483\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9788\n",
      "Epoch 00045: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0646 - acc: 0.9788 - val_loss: 0.2199 - val_acc: 0.9502\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9815\n",
      "Epoch 00046: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0599 - acc: 0.9815 - val_loss: 0.2262 - val_acc: 0.9425\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9851\n",
      "Epoch 00047: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0474 - acc: 0.9851 - val_loss: 0.2245 - val_acc: 0.9490\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9821\n",
      "Epoch 00048: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0540 - acc: 0.9821 - val_loss: 0.2604 - val_acc: 0.9369\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9856\n",
      "Epoch 00049: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0447 - acc: 0.9855 - val_loss: 0.2218 - val_acc: 0.9495\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9814\n",
      "Epoch 00050: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0590 - acc: 0.9814 - val_loss: 0.2359 - val_acc: 0.9481\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9863\n",
      "Epoch 00051: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0428 - acc: 0.9862 - val_loss: 0.2208 - val_acc: 0.9485\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9821\n",
      "Epoch 00052: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0539 - acc: 0.9821 - val_loss: 0.2853 - val_acc: 0.9397\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9862\n",
      "Epoch 00053: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0426 - acc: 0.9862 - val_loss: 0.2494 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9884\n",
      "Epoch 00054: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0382 - acc: 0.9884 - val_loss: 0.2717 - val_acc: 0.9427\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9879\n",
      "Epoch 00055: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0397 - acc: 0.9879 - val_loss: 0.2637 - val_acc: 0.9422\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9838\n",
      "Epoch 00056: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0507 - acc: 0.9838 - val_loss: 0.2279 - val_acc: 0.9474\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9872\n",
      "Epoch 00057: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0390 - acc: 0.9872 - val_loss: 0.2185 - val_acc: 0.9513\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 00058: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 0.2346 - val_acc: 0.9520\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9883\n",
      "Epoch 00059: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0369 - acc: 0.9883 - val_loss: 0.2671 - val_acc: 0.9432\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9839\n",
      "Epoch 00060: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0487 - acc: 0.9839 - val_loss: 0.2439 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9872\n",
      "Epoch 00061: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0401 - acc: 0.9872 - val_loss: 0.2193 - val_acc: 0.9448\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9881\n",
      "Epoch 00062: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0376 - acc: 0.9881 - val_loss: 0.2332 - val_acc: 0.9525\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9882\n",
      "Epoch 00063: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0361 - acc: 0.9882 - val_loss: 0.2308 - val_acc: 0.9464\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9868\n",
      "Epoch 00064: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0411 - acc: 0.9868 - val_loss: 0.2525 - val_acc: 0.9418\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9905\n",
      "Epoch 00065: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0297 - acc: 0.9905 - val_loss: 0.2939 - val_acc: 0.9411\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9868\n",
      "Epoch 00066: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0399 - acc: 0.9868 - val_loss: 0.2178 - val_acc: 0.9520\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9864\n",
      "Epoch 00067: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0412 - acc: 0.9864 - val_loss: 0.2331 - val_acc: 0.9511\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9918\n",
      "Epoch 00068: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.2576 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9916\n",
      "Epoch 00069: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0266 - acc: 0.9916 - val_loss: 0.2603 - val_acc: 0.9450\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9887\n",
      "Epoch 00070: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0341 - acc: 0.9887 - val_loss: 0.2422 - val_acc: 0.9546\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9914\n",
      "Epoch 00071: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0281 - acc: 0.9914 - val_loss: 0.2599 - val_acc: 0.9450\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9906\n",
      "Epoch 00072: val_loss did not improve from 0.19667\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0285 - acc: 0.9906 - val_loss: 0.2327 - val_acc: 0.9518\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2422 - acc: 0.9298\n",
      "Loss: 0.242210166550871 Accuracy: 0.9298027\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6316 - acc: 0.2246\n",
      "Epoch 00001: val_loss improved from inf to 1.91122, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/001-1.9112.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 2.6316 - acc: 0.2246 - val_loss: 1.9112 - val_acc: 0.3890\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5044 - acc: 0.5224\n",
      "Epoch 00002: val_loss improved from 1.91122 to 0.89534, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/002-0.8953.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 1.5045 - acc: 0.5224 - val_loss: 0.8953 - val_acc: 0.7487\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9557 - acc: 0.7030\n",
      "Epoch 00003: val_loss improved from 0.89534 to 0.57885, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/003-0.5788.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.9558 - acc: 0.7030 - val_loss: 0.5788 - val_acc: 0.8386\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.7908\n",
      "Epoch 00004: val_loss improved from 0.57885 to 0.43085, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/004-0.4308.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.6875 - acc: 0.7907 - val_loss: 0.4308 - val_acc: 0.8754\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.8342\n",
      "Epoch 00005: val_loss improved from 0.43085 to 0.37324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/005-0.3732.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5440 - acc: 0.8341 - val_loss: 0.3732 - val_acc: 0.8873\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.8629\n",
      "Epoch 00006: val_loss improved from 0.37324 to 0.28723, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/006-0.2872.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4453 - acc: 0.8629 - val_loss: 0.2872 - val_acc: 0.9150\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8833\n",
      "Epoch 00007: val_loss improved from 0.28723 to 0.26167, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/007-0.2617.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3763 - acc: 0.8832 - val_loss: 0.2617 - val_acc: 0.9241\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8974\n",
      "Epoch 00008: val_loss improved from 0.26167 to 0.24296, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/008-0.2430.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3323 - acc: 0.8974 - val_loss: 0.2430 - val_acc: 0.9290\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9105\n",
      "Epoch 00009: val_loss improved from 0.24296 to 0.22960, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/009-0.2296.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2931 - acc: 0.9105 - val_loss: 0.2296 - val_acc: 0.9334\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9189\n",
      "Epoch 00010: val_loss did not improve from 0.22960\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2631 - acc: 0.9189 - val_loss: 0.2351 - val_acc: 0.9322\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9267\n",
      "Epoch 00011: val_loss improved from 0.22960 to 0.19357, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/011-0.1936.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2371 - acc: 0.9267 - val_loss: 0.1936 - val_acc: 0.9443\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9334\n",
      "Epoch 00012: val_loss did not improve from 0.19357\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2118 - acc: 0.9334 - val_loss: 0.2178 - val_acc: 0.9359\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9370\n",
      "Epoch 00013: val_loss improved from 0.19357 to 0.18804, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/013-0.1880.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1991 - acc: 0.9370 - val_loss: 0.1880 - val_acc: 0.9413\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9441\n",
      "Epoch 00014: val_loss improved from 0.18804 to 0.17836, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/014-0.1784.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1770 - acc: 0.9441 - val_loss: 0.1784 - val_acc: 0.9464\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9484\n",
      "Epoch 00015: val_loss did not improve from 0.17836\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1649 - acc: 0.9484 - val_loss: 0.1832 - val_acc: 0.9453\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9524\n",
      "Epoch 00016: val_loss did not improve from 0.17836\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1517 - acc: 0.9524 - val_loss: 0.1795 - val_acc: 0.9471\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9562\n",
      "Epoch 00017: val_loss improved from 0.17836 to 0.16367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/017-0.1637.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1422 - acc: 0.9561 - val_loss: 0.1637 - val_acc: 0.9515\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9566\n",
      "Epoch 00018: val_loss improved from 0.16367 to 0.15241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/018-0.1524.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1368 - acc: 0.9566 - val_loss: 0.1524 - val_acc: 0.9534\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9614\n",
      "Epoch 00019: val_loss did not improve from 0.15241\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1230 - acc: 0.9614 - val_loss: 0.1945 - val_acc: 0.9443\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9651\n",
      "Epoch 00020: val_loss improved from 0.15241 to 0.14715, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/020-0.1471.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1115 - acc: 0.9651 - val_loss: 0.1471 - val_acc: 0.9567\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9658\n",
      "Epoch 00021: val_loss did not improve from 0.14715\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1081 - acc: 0.9658 - val_loss: 0.1933 - val_acc: 0.9490\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9677\n",
      "Epoch 00022: val_loss did not improve from 0.14715\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1022 - acc: 0.9677 - val_loss: 0.1623 - val_acc: 0.9515\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9670\n",
      "Epoch 00023: val_loss did not improve from 0.14715\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1031 - acc: 0.9670 - val_loss: 0.1618 - val_acc: 0.9532\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9699\n",
      "Epoch 00024: val_loss improved from 0.14715 to 0.12950, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/024-0.1295.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0951 - acc: 0.9699 - val_loss: 0.1295 - val_acc: 0.9620\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9740\n",
      "Epoch 00025: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0828 - acc: 0.9740 - val_loss: 0.1617 - val_acc: 0.9550\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9740\n",
      "Epoch 00026: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0792 - acc: 0.9740 - val_loss: 0.1628 - val_acc: 0.9546\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9768\n",
      "Epoch 00027: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0746 - acc: 0.9767 - val_loss: 0.1756 - val_acc: 0.9488\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9701\n",
      "Epoch 00028: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0939 - acc: 0.9701 - val_loss: 0.2022 - val_acc: 0.9488\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9765\n",
      "Epoch 00029: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0730 - acc: 0.9765 - val_loss: 0.1758 - val_acc: 0.9492\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9770\n",
      "Epoch 00030: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0722 - acc: 0.9770 - val_loss: 0.1538 - val_acc: 0.9611\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9804\n",
      "Epoch 00031: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0627 - acc: 0.9804 - val_loss: 0.1549 - val_acc: 0.9592\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9828\n",
      "Epoch 00032: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0551 - acc: 0.9828 - val_loss: 0.1881 - val_acc: 0.9483\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9821\n",
      "Epoch 00033: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0579 - acc: 0.9821 - val_loss: 0.2002 - val_acc: 0.9441\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9795\n",
      "Epoch 00034: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0633 - acc: 0.9795 - val_loss: 0.1709 - val_acc: 0.9513\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9821\n",
      "Epoch 00035: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0569 - acc: 0.9820 - val_loss: 0.1475 - val_acc: 0.9604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9843\n",
      "Epoch 00036: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0512 - acc: 0.9843 - val_loss: 0.1505 - val_acc: 0.9602\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9862\n",
      "Epoch 00037: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0445 - acc: 0.9862 - val_loss: 0.1376 - val_acc: 0.9618\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9835\n",
      "Epoch 00038: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0508 - acc: 0.9835 - val_loss: 0.1592 - val_acc: 0.9564\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9837\n",
      "Epoch 00039: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0515 - acc: 0.9837 - val_loss: 0.1348 - val_acc: 0.9609\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9880\n",
      "Epoch 00040: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0402 - acc: 0.9880 - val_loss: 0.1628 - val_acc: 0.9525\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9881\n",
      "Epoch 00041: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0388 - acc: 0.9881 - val_loss: 0.1697 - val_acc: 0.9595\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9881\n",
      "Epoch 00042: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0380 - acc: 0.9880 - val_loss: 0.1641 - val_acc: 0.9574\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9845\n",
      "Epoch 00043: val_loss did not improve from 0.12950\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0502 - acc: 0.9844 - val_loss: 0.1529 - val_acc: 0.9585\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9852\n",
      "Epoch 00044: val_loss improved from 0.12950 to 0.12892, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_8_conv_checkpoint/044-0.1289.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0471 - acc: 0.9852 - val_loss: 0.1289 - val_acc: 0.9653\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9909\n",
      "Epoch 00045: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0314 - acc: 0.9909 - val_loss: 0.1378 - val_acc: 0.9618\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9901\n",
      "Epoch 00046: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0313 - acc: 0.9901 - val_loss: 0.2096 - val_acc: 0.9474\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9873\n",
      "Epoch 00047: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0400 - acc: 0.9873 - val_loss: 0.1765 - val_acc: 0.9609\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9919\n",
      "Epoch 00048: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0287 - acc: 0.9919 - val_loss: 0.1720 - val_acc: 0.9569\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9896\n",
      "Epoch 00049: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0326 - acc: 0.9895 - val_loss: 0.1891 - val_acc: 0.9539\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9889\n",
      "Epoch 00050: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0367 - acc: 0.9889 - val_loss: 0.1771 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9915\n",
      "Epoch 00051: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0285 - acc: 0.9914 - val_loss: 0.1762 - val_acc: 0.9564\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9856\n",
      "Epoch 00052: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0463 - acc: 0.9856 - val_loss: 0.1390 - val_acc: 0.9632\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9923\n",
      "Epoch 00053: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0251 - acc: 0.9923 - val_loss: 0.1391 - val_acc: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9920\n",
      "Epoch 00054: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0267 - acc: 0.9920 - val_loss: 0.1667 - val_acc: 0.9618\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9894\n",
      "Epoch 00055: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0316 - acc: 0.9894 - val_loss: 0.1691 - val_acc: 0.9585\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 00056: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0354 - acc: 0.9889 - val_loss: 0.1797 - val_acc: 0.9602\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9904\n",
      "Epoch 00057: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0298 - acc: 0.9904 - val_loss: 0.1420 - val_acc: 0.9658\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9927\n",
      "Epoch 00058: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0243 - acc: 0.9927 - val_loss: 0.2090 - val_acc: 0.9522\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9895\n",
      "Epoch 00059: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0335 - acc: 0.9895 - val_loss: 0.1908 - val_acc: 0.9585\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9937\n",
      "Epoch 00060: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0207 - acc: 0.9937 - val_loss: 0.1423 - val_acc: 0.9625\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9939\n",
      "Epoch 00061: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0209 - acc: 0.9939 - val_loss: 0.1546 - val_acc: 0.9613\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9947\n",
      "Epoch 00062: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0183 - acc: 0.9947 - val_loss: 0.1817 - val_acc: 0.9604\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9929\n",
      "Epoch 00063: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0219 - acc: 0.9929 - val_loss: 0.1699 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9901\n",
      "Epoch 00064: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0327 - acc: 0.9901 - val_loss: 0.1522 - val_acc: 0.9632\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9922\n",
      "Epoch 00065: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0249 - acc: 0.9922 - val_loss: 0.1386 - val_acc: 0.9669\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00066: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0202 - acc: 0.9941 - val_loss: 0.2080 - val_acc: 0.9546\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9898\n",
      "Epoch 00067: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0321 - acc: 0.9898 - val_loss: 0.1757 - val_acc: 0.9564\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9936\n",
      "Epoch 00068: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0221 - acc: 0.9935 - val_loss: 0.1713 - val_acc: 0.9604\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 00069: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0272 - acc: 0.9916 - val_loss: 0.1604 - val_acc: 0.9609\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9955\n",
      "Epoch 00070: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.1942 - val_acc: 0.9578\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 00071: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0188 - acc: 0.9943 - val_loss: 0.1770 - val_acc: 0.9602\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9949\n",
      "Epoch 00072: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0162 - acc: 0.9949 - val_loss: 0.1900 - val_acc: 0.9567\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9949\n",
      "Epoch 00073: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.1557 - val_acc: 0.9637\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 00074: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0169 - acc: 0.9950 - val_loss: 0.1932 - val_acc: 0.9536\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9916\n",
      "Epoch 00075: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0248 - acc: 0.9916 - val_loss: 0.1410 - val_acc: 0.9660\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9948\n",
      "Epoch 00076: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.2369 - val_acc: 0.9467\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9946\n",
      "Epoch 00077: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0172 - acc: 0.9946 - val_loss: 0.1700 - val_acc: 0.9625\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9937\n",
      "Epoch 00078: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0196 - acc: 0.9937 - val_loss: 0.1672 - val_acc: 0.9667\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9965\n",
      "Epoch 00079: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0132 - acc: 0.9965 - val_loss: 0.2022 - val_acc: 0.9555\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9930\n",
      "Epoch 00080: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0238 - acc: 0.9930 - val_loss: 0.1694 - val_acc: 0.9606\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9952\n",
      "Epoch 00081: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0162 - acc: 0.9952 - val_loss: 0.2467 - val_acc: 0.9539\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9948\n",
      "Epoch 00082: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0167 - acc: 0.9948 - val_loss: 0.4051 - val_acc: 0.9276\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9934\n",
      "Epoch 00083: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0225 - acc: 0.9934 - val_loss: 0.2111 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9964\n",
      "Epoch 00084: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0119 - acc: 0.9964 - val_loss: 0.1498 - val_acc: 0.9660\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9950\n",
      "Epoch 00085: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0165 - acc: 0.9949 - val_loss: 0.2325 - val_acc: 0.9543\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9902\n",
      "Epoch 00086: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0320 - acc: 0.9902 - val_loss: 0.1693 - val_acc: 0.9623\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9922\n",
      "Epoch 00087: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0265 - acc: 0.9922 - val_loss: 0.1821 - val_acc: 0.9567\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9947\n",
      "Epoch 00088: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0183 - acc: 0.9947 - val_loss: 0.1487 - val_acc: 0.9658\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 00089: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0086 - acc: 0.9976 - val_loss: 0.1742 - val_acc: 0.9616\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9963\n",
      "Epoch 00090: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0119 - acc: 0.9963 - val_loss: 0.1612 - val_acc: 0.9609\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00091: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.1514 - val_acc: 0.9644\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9961\n",
      "Epoch 00092: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.1575 - val_acc: 0.9639\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9957\n",
      "Epoch 00093: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.1965 - val_acc: 0.9618\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9911\n",
      "Epoch 00094: val_loss did not improve from 0.12892\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0290 - acc: 0.9911 - val_loss: 0.1544 - val_acc: 0.9625\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1606 - acc: 0.9545\n",
      "Loss: 0.16056385195406922 Accuracy: 0.9545171\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2377 - acc: 0.3588\n",
      "Epoch 00001: val_loss improved from inf to 1.34844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/001-1.3484.hdf5\n",
      "36805/36805 [==============================] - 223s 6ms/sample - loss: 2.2375 - acc: 0.3588 - val_loss: 1.3484 - val_acc: 0.5518\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9646 - acc: 0.6978\n",
      "Epoch 00002: val_loss improved from 1.34844 to 0.52162, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/002-0.5216.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.9647 - acc: 0.6977 - val_loss: 0.5216 - val_acc: 0.8428\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.8151\n",
      "Epoch 00003: val_loss improved from 0.52162 to 0.39274, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/003-0.3927.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5960 - acc: 0.8151 - val_loss: 0.3927 - val_acc: 0.8847\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.8621\n",
      "Epoch 00004: val_loss improved from 0.39274 to 0.33022, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/004-0.3302.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4432 - acc: 0.8621 - val_loss: 0.3302 - val_acc: 0.8954\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8905\n",
      "Epoch 00005: val_loss improved from 0.33022 to 0.27985, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/005-0.2799.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3564 - acc: 0.8905 - val_loss: 0.2799 - val_acc: 0.9113\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9086\n",
      "Epoch 00006: val_loss improved from 0.27985 to 0.23600, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/006-0.2360.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2975 - acc: 0.9086 - val_loss: 0.2360 - val_acc: 0.9297\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9200\n",
      "Epoch 00007: val_loss did not improve from 0.23600\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.2557 - acc: 0.9200 - val_loss: 0.2884 - val_acc: 0.9080\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9303\n",
      "Epoch 00008: val_loss improved from 0.23600 to 0.19942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/008-0.1994.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2237 - acc: 0.9302 - val_loss: 0.1994 - val_acc: 0.9383\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9364\n",
      "Epoch 00009: val_loss did not improve from 0.19942\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.2033 - acc: 0.9363 - val_loss: 0.2180 - val_acc: 0.9317\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9419\n",
      "Epoch 00010: val_loss improved from 0.19942 to 0.18516, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/010-0.1852.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1837 - acc: 0.9419 - val_loss: 0.1852 - val_acc: 0.9448\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9505\n",
      "Epoch 00011: val_loss improved from 0.18516 to 0.18367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/011-0.1837.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1575 - acc: 0.9505 - val_loss: 0.1837 - val_acc: 0.9457\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9495\n",
      "Epoch 00012: val_loss did not improve from 0.18367\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1615 - acc: 0.9495 - val_loss: 0.1973 - val_acc: 0.9425\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9574\n",
      "Epoch 00013: val_loss did not improve from 0.18367\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1348 - acc: 0.9574 - val_loss: 0.1937 - val_acc: 0.9401\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9606\n",
      "Epoch 00014: val_loss did not improve from 0.18367\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1264 - acc: 0.9606 - val_loss: 0.2243 - val_acc: 0.9357\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9595\n",
      "Epoch 00015: val_loss improved from 0.18367 to 0.14801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/015-0.1480.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1268 - acc: 0.9595 - val_loss: 0.1480 - val_acc: 0.9560\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9667\n",
      "Epoch 00016: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1041 - acc: 0.9667 - val_loss: 0.1809 - val_acc: 0.9450\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9670\n",
      "Epoch 00017: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1016 - acc: 0.9670 - val_loss: 0.1980 - val_acc: 0.9443\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9720\n",
      "Epoch 00018: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0880 - acc: 0.9720 - val_loss: 0.1805 - val_acc: 0.9448\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9742\n",
      "Epoch 00019: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0830 - acc: 0.9742 - val_loss: 0.1580 - val_acc: 0.9541\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9760\n",
      "Epoch 00020: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0745 - acc: 0.9759 - val_loss: 0.1743 - val_acc: 0.9511\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9757\n",
      "Epoch 00021: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0752 - acc: 0.9757 - val_loss: 0.1707 - val_acc: 0.9504\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9743\n",
      "Epoch 00022: val_loss improved from 0.14801 to 0.14538, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/022-0.1454.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0787 - acc: 0.9743 - val_loss: 0.1454 - val_acc: 0.9602\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9810\n",
      "Epoch 00023: val_loss improved from 0.14538 to 0.13956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/023-0.1396.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0603 - acc: 0.9810 - val_loss: 0.1396 - val_acc: 0.9637\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9817\n",
      "Epoch 00024: val_loss did not improve from 0.13956\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0572 - acc: 0.9817 - val_loss: 0.1635 - val_acc: 0.9557\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9826\n",
      "Epoch 00025: val_loss did not improve from 0.13956\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0547 - acc: 0.9826 - val_loss: 0.1576 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9830\n",
      "Epoch 00026: val_loss did not improve from 0.13956\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0519 - acc: 0.9830 - val_loss: 0.1592 - val_acc: 0.9585\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9844\n",
      "Epoch 00027: val_loss improved from 0.13956 to 0.13171, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/027-0.1317.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0492 - acc: 0.9843 - val_loss: 0.1317 - val_acc: 0.9606\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9815\n",
      "Epoch 00028: val_loss improved from 0.13171 to 0.12028, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_BN_9_conv_checkpoint/028-0.1203.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0594 - acc: 0.9814 - val_loss: 0.1203 - val_acc: 0.9648\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9819\n",
      "Epoch 00029: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0570 - acc: 0.9819 - val_loss: 0.1474 - val_acc: 0.9618\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9868\n",
      "Epoch 00030: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0423 - acc: 0.9867 - val_loss: 0.1655 - val_acc: 0.9590\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9810\n",
      "Epoch 00031: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0586 - acc: 0.9810 - val_loss: 0.1357 - val_acc: 0.9632\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9891\n",
      "Epoch 00032: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0348 - acc: 0.9891 - val_loss: 0.1535 - val_acc: 0.9611\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9892\n",
      "Epoch 00033: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0343 - acc: 0.9892 - val_loss: 0.1564 - val_acc: 0.9616\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9899\n",
      "Epoch 00034: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0327 - acc: 0.9899 - val_loss: 0.1632 - val_acc: 0.9585\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9878\n",
      "Epoch 00035: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0396 - acc: 0.9877 - val_loss: 0.1532 - val_acc: 0.9632\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9823\n",
      "Epoch 00036: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0526 - acc: 0.9823 - val_loss: 0.1408 - val_acc: 0.9651\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9890\n",
      "Epoch 00037: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0363 - acc: 0.9890 - val_loss: 0.1627 - val_acc: 0.9606\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9923\n",
      "Epoch 00038: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0262 - acc: 0.9923 - val_loss: 0.1351 - val_acc: 0.9667\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9906\n",
      "Epoch 00039: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0305 - acc: 0.9905 - val_loss: 0.1759 - val_acc: 0.9576\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9889\n",
      "Epoch 00040: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0351 - acc: 0.9888 - val_loss: 0.1444 - val_acc: 0.9637\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9874\n",
      "Epoch 00041: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0419 - acc: 0.9874 - val_loss: 0.1598 - val_acc: 0.9574\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9936\n",
      "Epoch 00042: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0220 - acc: 0.9936 - val_loss: 0.1714 - val_acc: 0.9606\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9932\n",
      "Epoch 00043: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0227 - acc: 0.9932 - val_loss: 0.1615 - val_acc: 0.9597\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9927\n",
      "Epoch 00044: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0230 - acc: 0.9927 - val_loss: 0.1612 - val_acc: 0.9595\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9881\n",
      "Epoch 00045: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0408 - acc: 0.9881 - val_loss: 0.1560 - val_acc: 0.9641\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 00046: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0194 - acc: 0.9942 - val_loss: 0.1658 - val_acc: 0.9648\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 00047: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0179 - acc: 0.9946 - val_loss: 0.1370 - val_acc: 0.9704\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9935\n",
      "Epoch 00048: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0201 - acc: 0.9935 - val_loss: 0.1662 - val_acc: 0.9630\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.1780 - val_acc: 0.9599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9897\n",
      "Epoch 00050: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0331 - acc: 0.9897 - val_loss: 0.1443 - val_acc: 0.9679\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9947\n",
      "Epoch 00051: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0177 - acc: 0.9947 - val_loss: 0.1541 - val_acc: 0.9653\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9937\n",
      "Epoch 00052: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0203 - acc: 0.9938 - val_loss: 0.1534 - val_acc: 0.9641\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9944\n",
      "Epoch 00053: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.1779 - val_acc: 0.9620\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9918\n",
      "Epoch 00054: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0261 - acc: 0.9918 - val_loss: 0.1273 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9965\n",
      "Epoch 00055: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0132 - acc: 0.9965 - val_loss: 0.2301 - val_acc: 0.9506\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9950\n",
      "Epoch 00056: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0171 - acc: 0.9950 - val_loss: 0.1719 - val_acc: 0.9627\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00057: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0164 - acc: 0.9954 - val_loss: 0.1658 - val_acc: 0.9637\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9927\n",
      "Epoch 00058: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0225 - acc: 0.9927 - val_loss: 0.1700 - val_acc: 0.9590\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 00059: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.1601 - val_acc: 0.9639\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 00060: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 0.2286 - val_acc: 0.9518\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9910\n",
      "Epoch 00061: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0296 - acc: 0.9910 - val_loss: 0.1659 - val_acc: 0.9592\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9958\n",
      "Epoch 00062: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.1554 - val_acc: 0.9644\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9936\n",
      "Epoch 00063: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0212 - acc: 0.9936 - val_loss: 0.1485 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0099 - acc: 0.9970 - val_loss: 0.1561 - val_acc: 0.9648\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9958\n",
      "Epoch 00065: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.1624 - val_acc: 0.9641\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00066: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0116 - acc: 0.9965 - val_loss: 0.1403 - val_acc: 0.9688\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9954\n",
      "Epoch 00067: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0152 - acc: 0.9954 - val_loss: 0.1915 - val_acc: 0.9602\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9957\n",
      "Epoch 00068: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1562 - val_acc: 0.9679\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 00069: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0135 - acc: 0.9960 - val_loss: 0.1657 - val_acc: 0.9651\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9939\n",
      "Epoch 00070: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0208 - acc: 0.9939 - val_loss: 0.1590 - val_acc: 0.9674\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978\n",
      "Epoch 00071: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0085 - acc: 0.9978 - val_loss: 0.1564 - val_acc: 0.9667\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9969\n",
      "Epoch 00072: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.1513 - val_acc: 0.9637\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0128 - acc: 0.9961 - val_loss: 0.1436 - val_acc: 0.9700\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9937\n",
      "Epoch 00074: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0201 - acc: 0.9937 - val_loss: 0.1736 - val_acc: 0.9625\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00075: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1792 - val_acc: 0.9660\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1490 - val_acc: 0.9686\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9958\n",
      "Epoch 00077: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0132 - acc: 0.9958 - val_loss: 0.1580 - val_acc: 0.9683\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9969\n",
      "Epoch 00078: val_loss did not improve from 0.12028\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0103 - acc: 0.9969 - val_loss: 0.1671 - val_acc: 0.9669\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1700 - acc: 0.9518\n",
      "Loss: 0.17004611314828522 Accuracy: 0.9518172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_DO_BN'\n",
    "\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 4.8689 - acc: 0.3574\n",
      "Loss: 4.868935505499597 Accuracy: 0.3574247\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 4.2525 - acc: 0.3533\n",
      "Loss: 4.252515628652286 Accuracy: 0.35327104\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.5907 - acc: 0.5454\n",
      "Loss: 1.590722113061669 Accuracy: 0.54537904\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9283 - acc: 0.7346\n",
      "Loss: 0.9282862781859385 Accuracy: 0.73457944\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.7214 - acc: 0.7890\n",
      "Loss: 0.7214093419745456 Accuracy: 0.7889927\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4117 - acc: 0.8837\n",
      "Loss: 0.41169039384226935 Accuracy: 0.8836968\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2422 - acc: 0.9298\n",
      "Loss: 0.242210166550871 Accuracy: 0.9298027\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1606 - acc: 0.9545\n",
      "Loss: 0.16056385195406922 Accuracy: 0.9545171\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1700 - acc: 0.9518\n",
      "Loss: 0.17004611314828522 Accuracy: 0.9518172\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 5.6794 - acc: 0.3595\n",
      "Loss: 5.679400403625869 Accuracy: 0.35950157\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 5.2187 - acc: 0.4114\n",
      "Loss: 5.218717961197454 Accuracy: 0.41142264\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 2.3836 - acc: 0.6228\n",
      "Loss: 2.3835777024242364 Accuracy: 0.6228453\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.2241 - acc: 0.7643\n",
      "Loss: 1.2240855764872312 Accuracy: 0.7642783\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.9578 - acc: 0.8098\n",
      "Loss: 0.9577718736102896 Accuracy: 0.80976117\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.4791 - acc: 0.8935\n",
      "Loss: 0.47908884820413244 Accuracy: 0.89345795\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2946 - acc: 0.9348\n",
      "Loss: 0.2945919485997722 Accuracy: 0.9347871\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1914 - acc: 0.9566\n",
      "Loss: 0.19141455466081975 Accuracy: 0.956594\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2042 - acc: 0.9566\n",
      "Loss: 0.20422249990356744 Accuracy: 0.956594\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
