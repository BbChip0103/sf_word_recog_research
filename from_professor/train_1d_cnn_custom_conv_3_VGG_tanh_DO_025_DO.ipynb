{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9525 - acc: 0.3804\n",
      "Epoch 00001: val_loss improved from inf to 1.61049, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv_checkpoint/001-1.6105.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.9525 - acc: 0.3804 - val_loss: 1.6105 - val_acc: 0.4994\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3818 - acc: 0.5714\n",
      "Epoch 00002: val_loss improved from 1.61049 to 1.53026, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv_checkpoint/002-1.5303.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.3819 - acc: 0.5714 - val_loss: 1.5303 - val_acc: 0.5225\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1153 - acc: 0.6592\n",
      "Epoch 00003: val_loss improved from 1.53026 to 1.50362, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv_checkpoint/003-1.5036.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1153 - acc: 0.6592 - val_loss: 1.5036 - val_acc: 0.5425\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9077 - acc: 0.7279\n",
      "Epoch 00004: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9078 - acc: 0.7279 - val_loss: 1.5091 - val_acc: 0.5532\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7262 - acc: 0.7875\n",
      "Epoch 00005: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7262 - acc: 0.7875 - val_loss: 1.5203 - val_acc: 0.5700\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.8345\n",
      "Epoch 00006: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5802 - acc: 0.8345 - val_loss: 1.5251 - val_acc: 0.5728\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8791\n",
      "Epoch 00007: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4421 - acc: 0.8791 - val_loss: 1.5762 - val_acc: 0.5851\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.9112\n",
      "Epoch 00008: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3432 - acc: 0.9112 - val_loss: 1.6439 - val_acc: 0.5742\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9376\n",
      "Epoch 00009: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2682 - acc: 0.9376 - val_loss: 1.7173 - val_acc: 0.5835\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9556\n",
      "Epoch 00010: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2106 - acc: 0.9556 - val_loss: 1.7481 - val_acc: 0.5954\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9669\n",
      "Epoch 00011: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1679 - acc: 0.9669 - val_loss: 1.7951 - val_acc: 0.5896\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9750\n",
      "Epoch 00012: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1358 - acc: 0.9750 - val_loss: 1.8038 - val_acc: 0.6049\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9809\n",
      "Epoch 00013: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1162 - acc: 0.9809 - val_loss: 1.9138 - val_acc: 0.5858\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9852\n",
      "Epoch 00014: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0983 - acc: 0.9851 - val_loss: 1.9145 - val_acc: 0.5942\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9849\n",
      "Epoch 00015: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0918 - acc: 0.9849 - val_loss: 1.9145 - val_acc: 0.6031\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9913\n",
      "Epoch 00016: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0691 - acc: 0.9913 - val_loss: 1.9437 - val_acc: 0.6040\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9907\n",
      "Epoch 00017: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0675 - acc: 0.9907 - val_loss: 1.9770 - val_acc: 0.6087\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9922\n",
      "Epoch 00018: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0616 - acc: 0.9922 - val_loss: 2.0461 - val_acc: 0.5961\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9923\n",
      "Epoch 00019: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0573 - acc: 0.9923 - val_loss: 2.0822 - val_acc: 0.5998\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9939\n",
      "Epoch 00020: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0506 - acc: 0.9939 - val_loss: 2.1276 - val_acc: 0.6042\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9949\n",
      "Epoch 00021: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0437 - acc: 0.9949 - val_loss: 2.1298 - val_acc: 0.6096\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9933\n",
      "Epoch 00022: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0479 - acc: 0.9933 - val_loss: 2.1850 - val_acc: 0.6010\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9921\n",
      "Epoch 00023: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0530 - acc: 0.9921 - val_loss: 2.2751 - val_acc: 0.5982\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9903\n",
      "Epoch 00024: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0566 - acc: 0.9903 - val_loss: 2.2168 - val_acc: 0.6068\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9943\n",
      "Epoch 00025: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0421 - acc: 0.9943 - val_loss: 2.2092 - val_acc: 0.6082\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9967\n",
      "Epoch 00026: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0304 - acc: 0.9967 - val_loss: 2.3645 - val_acc: 0.5977\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9944\n",
      "Epoch 00027: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0378 - acc: 0.9944 - val_loss: 2.3600 - val_acc: 0.6038\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9961\n",
      "Epoch 00028: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0312 - acc: 0.9961 - val_loss: 2.2951 - val_acc: 0.6101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9962\n",
      "Epoch 00029: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0314 - acc: 0.9963 - val_loss: 2.4395 - val_acc: 0.5949\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9965\n",
      "Epoch 00030: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0285 - acc: 0.9965 - val_loss: 2.3643 - val_acc: 0.6049\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9948\n",
      "Epoch 00031: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0349 - acc: 0.9948 - val_loss: 2.5109 - val_acc: 0.5924\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9940\n",
      "Epoch 00032: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0361 - acc: 0.9940 - val_loss: 2.4198 - val_acc: 0.6112\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9957\n",
      "Epoch 00033: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0282 - acc: 0.9957 - val_loss: 2.4444 - val_acc: 0.6182\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9940\n",
      "Epoch 00034: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0341 - acc: 0.9940 - val_loss: 2.5594 - val_acc: 0.5849\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9962\n",
      "Epoch 00035: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0279 - acc: 0.9962 - val_loss: 2.5941 - val_acc: 0.5982\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9966\n",
      "Epoch 00036: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0267 - acc: 0.9966 - val_loss: 2.5287 - val_acc: 0.5973\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9964\n",
      "Epoch 00037: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0252 - acc: 0.9964 - val_loss: 2.5664 - val_acc: 0.6017\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9933\n",
      "Epoch 00038: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0375 - acc: 0.9933 - val_loss: 2.5514 - val_acc: 0.6003\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9949\n",
      "Epoch 00039: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0305 - acc: 0.9949 - val_loss: 2.5876 - val_acc: 0.6005\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9973\n",
      "Epoch 00040: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0218 - acc: 0.9973 - val_loss: 2.5606 - val_acc: 0.5982\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9969\n",
      "Epoch 00041: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0242 - acc: 0.9969 - val_loss: 2.6024 - val_acc: 0.6026\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9961\n",
      "Epoch 00042: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0253 - acc: 0.9961 - val_loss: 2.5633 - val_acc: 0.6010\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9970\n",
      "Epoch 00043: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0233 - acc: 0.9970 - val_loss: 2.6076 - val_acc: 0.6108\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9954\n",
      "Epoch 00044: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0309 - acc: 0.9954 - val_loss: 2.6651 - val_acc: 0.5993\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9952\n",
      "Epoch 00045: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0277 - acc: 0.9952 - val_loss: 2.6245 - val_acc: 0.6066\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9974\n",
      "Epoch 00046: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0211 - acc: 0.9974 - val_loss: 2.6767 - val_acc: 0.6056\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0175 - acc: 0.9982 - val_loss: 2.7124 - val_acc: 0.5996\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9945\n",
      "Epoch 00048: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0310 - acc: 0.9945 - val_loss: 2.7842 - val_acc: 0.6014\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0270 - acc: 0.9955 - val_loss: 2.7969 - val_acc: 0.6038\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9970\n",
      "Epoch 00050: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0216 - acc: 0.9970 - val_loss: 2.7176 - val_acc: 0.6138\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9977\n",
      "Epoch 00051: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0172 - acc: 0.9977 - val_loss: 2.6876 - val_acc: 0.6056\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9948\n",
      "Epoch 00052: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0298 - acc: 0.9948 - val_loss: 2.7760 - val_acc: 0.6035\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9965\n",
      "Epoch 00053: val_loss did not improve from 1.50362\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0226 - acc: 0.9964 - val_loss: 2.7366 - val_acc: 0.6140\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 894us/sample - loss: 1.5551 - acc: 0.5157\n",
      "Loss: 1.5551137644182484 Accuracy: 0.5156802\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7663 - acc: 0.4368\n",
      "Epoch 00001: val_loss improved from inf to 1.36412, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/001-1.3641.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.7663 - acc: 0.4369 - val_loss: 1.3641 - val_acc: 0.5770\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2532 - acc: 0.6173\n",
      "Epoch 00002: val_loss improved from 1.36412 to 1.23164, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/002-1.2316.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2532 - acc: 0.6173 - val_loss: 1.2316 - val_acc: 0.6259\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0737 - acc: 0.6770\n",
      "Epoch 00003: val_loss improved from 1.23164 to 1.17079, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/003-1.1708.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0737 - acc: 0.6770 - val_loss: 1.1708 - val_acc: 0.6441\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.7188\n",
      "Epoch 00004: val_loss improved from 1.17079 to 1.12577, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/004-1.1258.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9441 - acc: 0.7187 - val_loss: 1.1258 - val_acc: 0.6685\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7573\n",
      "Epoch 00005: val_loss improved from 1.12577 to 1.09921, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/005-1.0992.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8335 - acc: 0.7573 - val_loss: 1.0992 - val_acc: 0.6753\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7840\n",
      "Epoch 00006: val_loss improved from 1.09921 to 1.08216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv_checkpoint/006-1.0822.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7388 - acc: 0.7840 - val_loss: 1.0822 - val_acc: 0.6827\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6550 - acc: 0.8120\n",
      "Epoch 00007: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6550 - acc: 0.8120 - val_loss: 1.0966 - val_acc: 0.6823\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.8382\n",
      "Epoch 00008: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5778 - acc: 0.8382 - val_loss: 1.1060 - val_acc: 0.6844\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8602\n",
      "Epoch 00009: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5105 - acc: 0.8603 - val_loss: 1.1087 - val_acc: 0.6844\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8795\n",
      "Epoch 00010: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4491 - acc: 0.8796 - val_loss: 1.0835 - val_acc: 0.6921\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.9001\n",
      "Epoch 00011: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3943 - acc: 0.9001 - val_loss: 1.1150 - val_acc: 0.6911\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.9136\n",
      "Epoch 00012: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3452 - acc: 0.9136 - val_loss: 1.1386 - val_acc: 0.6862\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9281\n",
      "Epoch 00013: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3040 - acc: 0.9281 - val_loss: 1.1394 - val_acc: 0.6981\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9380\n",
      "Epoch 00014: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2669 - acc: 0.9380 - val_loss: 1.1427 - val_acc: 0.6979\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9492\n",
      "Epoch 00015: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2331 - acc: 0.9492 - val_loss: 1.1940 - val_acc: 0.6867\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9557\n",
      "Epoch 00016: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2098 - acc: 0.9557 - val_loss: 1.2069 - val_acc: 0.6937\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9649\n",
      "Epoch 00017: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1827 - acc: 0.9650 - val_loss: 1.1889 - val_acc: 0.7009\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9697\n",
      "Epoch 00018: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1613 - acc: 0.9697 - val_loss: 1.2173 - val_acc: 0.6981\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9746\n",
      "Epoch 00019: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1422 - acc: 0.9747 - val_loss: 1.2255 - val_acc: 0.6986\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9776\n",
      "Epoch 00020: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1287 - acc: 0.9776 - val_loss: 1.2371 - val_acc: 0.7002\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9808\n",
      "Epoch 00021: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1170 - acc: 0.9808 - val_loss: 1.2622 - val_acc: 0.6981\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9825\n",
      "Epoch 00022: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1078 - acc: 0.9825 - val_loss: 1.2544 - val_acc: 0.7002\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9861\n",
      "Epoch 00023: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0944 - acc: 0.9861 - val_loss: 1.2761 - val_acc: 0.6972\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9871\n",
      "Epoch 00024: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0852 - acc: 0.9871 - val_loss: 1.3209 - val_acc: 0.6976\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9891\n",
      "Epoch 00025: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0805 - acc: 0.9891 - val_loss: 1.3108 - val_acc: 0.7035\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9901\n",
      "Epoch 00026: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0715 - acc: 0.9901 - val_loss: 1.3631 - val_acc: 0.6939\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9900\n",
      "Epoch 00027: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0713 - acc: 0.9900 - val_loss: 1.3419 - val_acc: 0.7051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9911\n",
      "Epoch 00028: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0637 - acc: 0.9911 - val_loss: 1.3821 - val_acc: 0.7035\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9915\n",
      "Epoch 00029: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0591 - acc: 0.9915 - val_loss: 1.4133 - val_acc: 0.7002\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9926\n",
      "Epoch 00030: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0564 - acc: 0.9926 - val_loss: 1.3945 - val_acc: 0.7025\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9926\n",
      "Epoch 00031: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0525 - acc: 0.9926 - val_loss: 1.3958 - val_acc: 0.7039\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9942\n",
      "Epoch 00032: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0504 - acc: 0.9942 - val_loss: 1.4348 - val_acc: 0.6979\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9934\n",
      "Epoch 00033: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0474 - acc: 0.9934 - val_loss: 1.4150 - val_acc: 0.7065\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9941\n",
      "Epoch 00034: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0447 - acc: 0.9941 - val_loss: 1.4073 - val_acc: 0.7084\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9940\n",
      "Epoch 00035: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0434 - acc: 0.9940 - val_loss: 1.4900 - val_acc: 0.6995\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9951\n",
      "Epoch 00036: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0412 - acc: 0.9951 - val_loss: 1.4890 - val_acc: 0.7077\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9950\n",
      "Epoch 00037: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0382 - acc: 0.9950 - val_loss: 1.4687 - val_acc: 0.7051\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9951\n",
      "Epoch 00038: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0388 - acc: 0.9951 - val_loss: 1.4888 - val_acc: 0.7042\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9949\n",
      "Epoch 00039: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0375 - acc: 0.9949 - val_loss: 1.4962 - val_acc: 0.7018\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9955\n",
      "Epoch 00040: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0335 - acc: 0.9955 - val_loss: 1.4846 - val_acc: 0.7093\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9963\n",
      "Epoch 00041: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0310 - acc: 0.9963 - val_loss: 1.4973 - val_acc: 0.7035\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9953\n",
      "Epoch 00042: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0324 - acc: 0.9953 - val_loss: 1.5171 - val_acc: 0.7028\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9965\n",
      "Epoch 00043: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0293 - acc: 0.9965 - val_loss: 1.5433 - val_acc: 0.7072\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9961\n",
      "Epoch 00044: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0299 - acc: 0.9961 - val_loss: 1.5607 - val_acc: 0.7039\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9961\n",
      "Epoch 00045: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0302 - acc: 0.9961 - val_loss: 1.5471 - val_acc: 0.7065\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9958\n",
      "Epoch 00046: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0288 - acc: 0.9958 - val_loss: 1.5376 - val_acc: 0.7102\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9967\n",
      "Epoch 00047: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0275 - acc: 0.9967 - val_loss: 1.5485 - val_acc: 0.7070\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9958\n",
      "Epoch 00048: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0284 - acc: 0.9958 - val_loss: 1.5462 - val_acc: 0.7081\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9972\n",
      "Epoch 00049: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0247 - acc: 0.9972 - val_loss: 1.5556 - val_acc: 0.7079\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9972\n",
      "Epoch 00050: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0230 - acc: 0.9972 - val_loss: 1.5788 - val_acc: 0.7051\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0286 - acc: 0.9958 - val_loss: 1.6382 - val_acc: 0.7053\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9964\n",
      "Epoch 00052: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0260 - acc: 0.9964 - val_loss: 1.5722 - val_acc: 0.7088\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9971\n",
      "Epoch 00053: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0218 - acc: 0.9971 - val_loss: 1.6079 - val_acc: 0.7030\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9966\n",
      "Epoch 00054: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0231 - acc: 0.9966 - val_loss: 1.6026 - val_acc: 0.7119\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9962\n",
      "Epoch 00055: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0244 - acc: 0.9962 - val_loss: 1.5975 - val_acc: 0.7102\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9962\n",
      "Epoch 00056: val_loss did not improve from 1.08216\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0240 - acc: 0.9963 - val_loss: 1.6846 - val_acc: 0.6956\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 919us/sample - loss: 1.1546 - acc: 0.6474\n",
      "Loss: 1.1546156158328427 Accuracy: 0.64735204\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7738 - acc: 0.4355\n",
      "Epoch 00001: val_loss improved from inf to 1.42911, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/001-1.4291.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.7737 - acc: 0.4356 - val_loss: 1.4291 - val_acc: 0.5481\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3414 - acc: 0.5910\n",
      "Epoch 00002: val_loss improved from 1.42911 to 1.20719, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/002-1.2072.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3413 - acc: 0.5910 - val_loss: 1.2072 - val_acc: 0.6294\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1286 - acc: 0.6635\n",
      "Epoch 00003: val_loss improved from 1.20719 to 1.07825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/003-1.0783.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1287 - acc: 0.6634 - val_loss: 1.0783 - val_acc: 0.6737\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0022 - acc: 0.7039\n",
      "Epoch 00004: val_loss improved from 1.07825 to 0.98957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/004-0.9896.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0022 - acc: 0.7039 - val_loss: 0.9896 - val_acc: 0.7065\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9039 - acc: 0.7367\n",
      "Epoch 00005: val_loss improved from 0.98957 to 0.92141, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/005-0.9214.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9039 - acc: 0.7367 - val_loss: 0.9214 - val_acc: 0.7347\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8274 - acc: 0.7586\n",
      "Epoch 00006: val_loss did not improve from 0.92141\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8274 - acc: 0.7586 - val_loss: 0.9243 - val_acc: 0.7205\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7797\n",
      "Epoch 00007: val_loss improved from 0.92141 to 0.86378, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/007-0.8638.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7573 - acc: 0.7797 - val_loss: 0.8638 - val_acc: 0.7519\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7983\n",
      "Epoch 00008: val_loss improved from 0.86378 to 0.82589, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/008-0.8259.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6958 - acc: 0.7983 - val_loss: 0.8259 - val_acc: 0.7584\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6382 - acc: 0.8185\n",
      "Epoch 00009: val_loss improved from 0.82589 to 0.80070, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/009-0.8007.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6381 - acc: 0.8185 - val_loss: 0.8007 - val_acc: 0.7771\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8341\n",
      "Epoch 00010: val_loss did not improve from 0.80070\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5853 - acc: 0.8341 - val_loss: 0.8355 - val_acc: 0.7622\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8471\n",
      "Epoch 00011: val_loss improved from 0.80070 to 0.78217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/011-0.7822.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5386 - acc: 0.8471 - val_loss: 0.7822 - val_acc: 0.7731\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8617\n",
      "Epoch 00012: val_loss did not improve from 0.78217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4929 - acc: 0.8617 - val_loss: 0.7896 - val_acc: 0.7768\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8747\n",
      "Epoch 00013: val_loss did not improve from 0.78217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4547 - acc: 0.8747 - val_loss: 0.7893 - val_acc: 0.7773\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8854\n",
      "Epoch 00014: val_loss improved from 0.78217 to 0.78143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/014-0.7814.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4149 - acc: 0.8854 - val_loss: 0.7814 - val_acc: 0.7754\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8956\n",
      "Epoch 00015: val_loss did not improve from 0.78143\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3833 - acc: 0.8956 - val_loss: 0.7897 - val_acc: 0.7829\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.9062\n",
      "Epoch 00016: val_loss did not improve from 0.78143\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3494 - acc: 0.9062 - val_loss: 0.8157 - val_acc: 0.7727\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9153\n",
      "Epoch 00017: val_loss improved from 0.78143 to 0.76764, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv_checkpoint/017-0.7676.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3225 - acc: 0.9153 - val_loss: 0.7676 - val_acc: 0.7820\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9254\n",
      "Epoch 00018: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2876 - acc: 0.9254 - val_loss: 0.7726 - val_acc: 0.7820\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9336\n",
      "Epoch 00019: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2649 - acc: 0.9336 - val_loss: 0.7904 - val_acc: 0.7803\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9397\n",
      "Epoch 00020: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2444 - acc: 0.9397 - val_loss: 0.8027 - val_acc: 0.7801\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9458\n",
      "Epoch 00021: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2213 - acc: 0.9458 - val_loss: 0.8391 - val_acc: 0.7773\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9507\n",
      "Epoch 00022: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2033 - acc: 0.9507 - val_loss: 0.8029 - val_acc: 0.7817\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9579\n",
      "Epoch 00023: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1845 - acc: 0.9579 - val_loss: 0.8297 - val_acc: 0.7820\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9610\n",
      "Epoch 00024: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1703 - acc: 0.9610 - val_loss: 0.8356 - val_acc: 0.7822\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9663\n",
      "Epoch 00025: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1539 - acc: 0.9663 - val_loss: 0.8362 - val_acc: 0.7817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9702\n",
      "Epoch 00026: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1382 - acc: 0.9702 - val_loss: 0.8636 - val_acc: 0.7738\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9739\n",
      "Epoch 00027: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1275 - acc: 0.9739 - val_loss: 0.8795 - val_acc: 0.7780\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9769\n",
      "Epoch 00028: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1204 - acc: 0.9769 - val_loss: 0.8712 - val_acc: 0.7824\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9792\n",
      "Epoch 00029: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1077 - acc: 0.9792 - val_loss: 0.9216 - val_acc: 0.7745\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9778\n",
      "Epoch 00030: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1107 - acc: 0.9778 - val_loss: 0.8754 - val_acc: 0.7834\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9843\n",
      "Epoch 00031: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0909 - acc: 0.9843 - val_loss: 0.9102 - val_acc: 0.7778\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9860\n",
      "Epoch 00032: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0840 - acc: 0.9860 - val_loss: 0.9125 - val_acc: 0.7785\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0793 - acc: 0.9861 - val_loss: 0.9421 - val_acc: 0.7747\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9874\n",
      "Epoch 00034: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0772 - acc: 0.9873 - val_loss: 0.9020 - val_acc: 0.7859\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9869\n",
      "Epoch 00035: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0752 - acc: 0.9869 - val_loss: 0.9344 - val_acc: 0.7799\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9894\n",
      "Epoch 00036: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0663 - acc: 0.9894 - val_loss: 0.9461 - val_acc: 0.7801\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9904\n",
      "Epoch 00037: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0611 - acc: 0.9904 - val_loss: 0.9825 - val_acc: 0.7815\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9908\n",
      "Epoch 00038: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0588 - acc: 0.9908 - val_loss: 0.9442 - val_acc: 0.7787\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9913\n",
      "Epoch 00039: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0556 - acc: 0.9913 - val_loss: 0.9903 - val_acc: 0.7785\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9910\n",
      "Epoch 00040: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0552 - acc: 0.9910 - val_loss: 1.0155 - val_acc: 0.7789\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0518 - acc: 0.9919 - val_loss: 1.0128 - val_acc: 0.7736\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9923\n",
      "Epoch 00042: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0480 - acc: 0.9923 - val_loss: 0.9815 - val_acc: 0.7815\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9929\n",
      "Epoch 00043: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0463 - acc: 0.9929 - val_loss: 1.0281 - val_acc: 0.7771\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9918\n",
      "Epoch 00044: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0474 - acc: 0.9918 - val_loss: 0.9884 - val_acc: 0.7920\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9935\n",
      "Epoch 00045: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0430 - acc: 0.9935 - val_loss: 1.0063 - val_acc: 0.7862\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9942\n",
      "Epoch 00046: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0392 - acc: 0.9942 - val_loss: 1.0326 - val_acc: 0.7836\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9933\n",
      "Epoch 00047: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0421 - acc: 0.9933 - val_loss: 1.0349 - val_acc: 0.7817\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9940\n",
      "Epoch 00048: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0398 - acc: 0.9940 - val_loss: 1.0533 - val_acc: 0.7789\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9936\n",
      "Epoch 00049: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0394 - acc: 0.9936 - val_loss: 1.0141 - val_acc: 0.7843\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9942\n",
      "Epoch 00050: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0365 - acc: 0.9942 - val_loss: 1.0459 - val_acc: 0.7824\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0342 - acc: 0.9948 - val_loss: 1.0522 - val_acc: 0.7813\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9955\n",
      "Epoch 00052: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0316 - acc: 0.9955 - val_loss: 1.0481 - val_acc: 0.7864\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9942\n",
      "Epoch 00053: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0353 - acc: 0.9942 - val_loss: 1.0951 - val_acc: 0.7771\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0348 - acc: 0.9938 - val_loss: 1.0691 - val_acc: 0.7845\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9957\n",
      "Epoch 00055: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0306 - acc: 0.9957 - val_loss: 1.0667 - val_acc: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9953\n",
      "Epoch 00056: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0301 - acc: 0.9953 - val_loss: 1.1303 - val_acc: 0.7710\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9955\n",
      "Epoch 00057: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0296 - acc: 0.9955 - val_loss: 1.0929 - val_acc: 0.7838\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9960\n",
      "Epoch 00058: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0287 - acc: 0.9960 - val_loss: 1.0761 - val_acc: 0.7831\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0327 - acc: 0.9942 - val_loss: 1.1244 - val_acc: 0.7754\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9966\n",
      "Epoch 00060: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0252 - acc: 0.9966 - val_loss: 1.1222 - val_acc: 0.7792\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9960\n",
      "Epoch 00061: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0257 - acc: 0.9960 - val_loss: 1.1675 - val_acc: 0.7817\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9957\n",
      "Epoch 00062: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0270 - acc: 0.9957 - val_loss: 1.0918 - val_acc: 0.7843\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9962\n",
      "Epoch 00063: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0255 - acc: 0.9962 - val_loss: 1.1445 - val_acc: 0.7855\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9938\n",
      "Epoch 00064: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0309 - acc: 0.9938 - val_loss: 1.1669 - val_acc: 0.7794\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9968\n",
      "Epoch 00065: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0227 - acc: 0.9968 - val_loss: 1.1071 - val_acc: 0.7780\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9970\n",
      "Epoch 00066: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0227 - acc: 0.9970 - val_loss: 1.1353 - val_acc: 0.7827\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9968\n",
      "Epoch 00067: val_loss did not improve from 0.76764\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0227 - acc: 0.9968 - val_loss: 1.1923 - val_acc: 0.7741\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmSWTHZIQkkDAsK8hCQSMIopV3FCEKqJ1V9T6s7Zqq6XaKnWp1H2pu8XirlXRqiiKgqCyQ9jBsCeBhOz7NjPn98eZhAQSSCCTCcn7eZ77TOauZxK475ztvUprjRBCCHE0Fl8XQAghxIlBAoYQQohmkYAhhBCiWSRgCCGEaBYJGEIIIZpFAoYQQohmkYAhhBCiWSRgCCGEaBYJGEIIIZrF5usCtKZu3brpuLg4XxdDCCFOGKtXr87VWkc2Z98OFTDi4uJYtWqVr4shhBAnDKXUnubuK01SQgghmkUChhBCiGaRgCGEEKJZOlQfRmNqamrIyMigsrLS10U5Ifn7+xMbG4vdbvd1UYQQPtbhA0ZGRgYhISHExcWhlPJ1cU4oWmvy8vLIyMigT58+vi6OEMLHOnyTVGVlJRERERIsjoFSioiICKmdCSGAThAwAAkWx0F+d0KIWl4LGEqp2UqpA0qpjU1sv1splepZNiqlXEqpcM+23UqpDZ5tXp1YobWmqmofTmeRNy8jhBAnPG/WMP4DnNfURq3141rrRK11IvAX4AetdX69Xc70bE/2YhlRSlFdne21gFFYWMiLL754TMdecMEFFBYWNnv/mTNn8sQTTxzTtYQQ4mi8FjC01ouB/KPuaFwBvOetshyNUna0rvHKuY8UMJxO5xGPnTdvHl27dvVGsYQQosV83oehlArE1EQ+rrdaA98opVYrpW72dhksFu8FjBkzZrBjxw4SExO5++67WbRoEePGjWPSpEkMHToUgMmTJzNq1CiGDRvGq6++WndsXFwcubm57N69myFDhnDTTTcxbNgwzjnnHCoqKo543dTUVFJSUhgxYgRTpkyhoKAAgOeee46hQ4cyYsQILr/8cgB++OEHEhMTSUxMJCkpiZKSEq/8LoQQJ7b2MKz2IuCnQ5qjTtNaZyqlugPfKqW2emosh/EElJsBevfufcQLpaXdQWlp6mHr3e5KtHZhtQa1uPDBwYkMGPBMk9tnzZrFxo0bSU011120aBFr1qxh48aNdUNVZ8+eTXh4OBUVFYwePZpLLrmEiIiIQ8qexnvvvcdrr73GZZddxscff8xVV13V5HWvueYann/+ec444wzuv/9+/v73v/PMM88wa9Ysdu3ahcPhqGvueuKJJ3jhhRcYO3YspaWl+Pv7t/j3IITo+HxewwAu55DmKK11puf1ADAXGNPUwVrrV7XWyVrr5MjIZiVcbITCVGraxpgxYxrMa3juuedISEggJSWF9PR00tLSDjumT58+JCYmAjBq1Ch2797d5PmLioooLCzkjDPOAODaa69l8WITb0eMGMGVV17J22+/jc1mvi+MHTuWu+66i+eee47CwsK69UIIUZ9P7wxKqS7AGcBV9dYFARatdYnn53OAB1vjek3VBKqq9lNdnUlwcBJKWVvjUkcUFHSwJrNo0SIWLFjA0qVLCQwMZPz48Y3Oe3A4HHU/W63WozZJNeXLL79k8eLFfP755zzyyCNs2LCBGTNmMHHiRObNm8fYsWOZP38+gwcPPqbzCyE6Lq8FDKXUe8B4oJtSKgN4ALADaK1f9uw2BfhGa11W79AoYK5n/L8NeFdr/bW3ygmmD8OUq6bVA0ZISMgR+wSKiooICwsjMDCQrVu3smzZsuO+ZpcuXQgLC2PJkiWMGzeOt956izPOOAO32016ejpnnnkmp512Gu+//z6lpaXk5eURHx9PfHw8K1euZOvWrRIwhBCH8VrA0Fpf0Yx9/oMZflt/3U4gwTulapxSJmC43U4srdxIFxERwdixYxk+fDjnn38+EydObLD9vPPO4+WXX2bIkCEMGjSIlJSUVrnunDlz+O1vf0t5eTl9+/bljTfewOVycdVVV1FUVITWmt///vd07dqVv/3tbyxcuBCLxcKwYcM4//zzW6UMQoiORWnddm333pacnKwPfYDSli1bGDJkyBGPc7nKKS/fjL9/P+z2MG8W8YTUnN+hEOLEpJRa3dz5bu2h09vnamsY3hpaK4QQHYEEDEAp0zInAUMIIZomAQOTHkQpO263BAwhhGiKBAwPb6YHEUKIjkAChocEDCGEODIJGB4SMIQQ4sgkYHiYBIRO2sMw4+Dg4BatF0KItiABw8OMlNJofeSU40II0VlJwPDw1lyMGTNm8MILL9S9r33IUWlpKWeddRYjR44kPj6ezz77rNnn1Fpz9913M3z4cOLj4/nggw8A2L9/P6effjqJiYkMHz6cJUuW4HK5uO666+r2ffrpp1v18wkhOo/OlZb0jjsg9fD05gA27SLAXY7FEggtySeVmAjPNJ3efNq0adxxxx3cdtttAHz44YfMnz8ff39/5s6dS2hoKLm5uaSkpDBp0qRmPUP7k08+ITU1lXXr1pGbm8vo0aM5/fTTeffddzn33HO57777cLlclJeXk5qaSmZmJhs3mifltuQJfkIIUV/nChhHZG7UGjeK1ktAmJSUxIEDB9i3bx85OTmEhYXRq1cvampquPfee1m8eDEWi4XMzEyys7OJjo4+6jl//PFHrrjiCqxWK1FRUZxxxhmsXLmS0aNHc8MNN1BTU8PkyZNJTEykb9++7Ny5k9tvv52JEydyzjnntNpnE0J0Lp0rYByhJoB2UVG6Fj+/WByOo9+0W2Lq1Kl89NFHZGVlMW3aNADeeecdcnJyWL16NXa7nbi4uEbTmrfE6aefzuLFi/nyyy+57rrruOuuu7jmmmtYt24d8+fP5+WXX+bDDz9k9uzZrfGxhBCdjPRheJi05havDK2dNm0a77//Ph999BFTp04FTFrz7t27Y7fbWbhwIXv27Gn2+caNG8cHH3yAy+UiJyeHxYsXM2bMGPbs2UNUVBQ33XQT06dPZ82aNeTm5uJ2u7nkkkt4+OGHWbNmTat/PiFE59C5ahhH4a25GMOGDaOkpISePXsSExMDwJVXXslFF11EfHw8ycnJLXr+xJQpU1i6dCkJCQkopXjssceIjo5mzpw5PP7449jtdoKDg3nzzTfJzMzk+uuvx+12A/Doo4+2+ucTQnQOkt68nrKyrSilCAwc5I3inbAkvbkQHZekNz9GZvKezPYWQojGSMCoRzLWCiFE0yRg1GNme7vQ2u3rogghRLvjtYChlJqtlDqglNrYxPbxSqkipVSqZ7m/3rbzlFLblFLblVIzvFXGw8skT94TQoimeLOG8R/gvKPss0RrnehZHgRQZnzrC8D5wFDgCqXUUC+Ws47FIgFDCCGa4rWAobVeDOQfw6FjgO1a651a62rgfeDiVi1cE2prGG63JCAUQohD+boP4xSl1Dql1FdKqWGedT2B9Hr7ZHjWeZ03mqQKCwt58cUXj+nYCy64QHI/CSHaDV8GjDXASVrrBOB54NNjOYlS6mal1Cql1KqcnJzjKpDp9G67gOF0HrkmM2/ePLp27dpqZRFCiOPhs4ChtS7WWpd6fp4H2JVS3YBMoFe9XWM965o6z6ta62StdXJkZORxlUkpC0rZWjVgzJgxgx07dpCYmMjdd9/NokWLGDduHJMmTWLoUNM1M3nyZEaNGsWwYcN49dVX646Ni4sjNzeX3bt3M2TIEG666SaGDRvGOeecQ0VFxWHX+vzzzzn55JNJSkri7LPPJjs7G4DS0lKuv/564uPjGTFiBB9//DEAX3/9NSNHjiQhIYGzzjqr1T6zEKJj8llqEKVUNJCttdZKqTGY4JUHFAIDlFJ9MIHicuA3rXHNI2Q3r+NyDUApC5ZmhtKjZDdn1qxZbNy4kVTPhRctWsSaNWvYuHEjffr0AWD27NmEh4dTUVHB6NGjueSSS4iIiGhwnrS0NN577z1ee+01LrvsMj7++GOuuuqqBvucdtppLFu2DKUUr7/+Oo899hhPPvkkDz30EF26dGHDhg0AFBQUkJOTw0033cTixYvp06cP+fnH0t0khOhMvBYwlFLvAeOBbkqpDOABwA6gtX4ZuBS4VSnlBCqAy7XJU+JUSv0OmA9Ygdla603eKmcj5fb6PIwxY8bUBQuA5557jrlz5wKQnp5OWlraYQGjT58+JCYmAjBq1Ch279592HkzMjKYNm0a+/fvp7q6uu4aCxYs4P3336/bLywsjM8//5zTTz+9bp/w8PBW/YxCiI7HawFDa33FUbb/C/hXE9vmAfNau0xHqgnUqqjIxuUqITh4RGtfvk5QUFDdz4sWLWLBggUsXbqUwMBAxo8f32iac4fDUfez1WpttEnq9ttv56677mLSpEksWrSImTNneqX8QojOydejpNqd2oy1rZWUMSQkhJKSkia3FxUVERYWRmBgIFu3bmXZsmXHfK2ioiJ69jQDyubMmVO3fsKECQ0eE1tQUEBKSgqLFy9m165dANIkJYQ4KgkYh7BYbIBGa1ernC8iIoKxY8cyfPhw7r777sO2n3feeTidToYMGcKMGTNISUk55mvNnDmTqVOnMmrUKLp161a3/q9//SsFBQUMHz6chIQEFi5cSGRkJK+++iq//vWvSUhIqHuwkxBCNEXSm7vdkJEBISEQFkZNTR6VlbsIDByG1Rrg5RKfGCS9uRAdl6Q3bwmLBfLzwTNBTvJJCSFE4yRgAAQGgqcTWQKGEEI0TgIGHAwYbne9gCH5pIQQoj4JGAABAaA1VFVhkuUqeZCSEEIcQgIGmBoGQHk5Sqm6obVCCCEOkoAB4HCAUg36MSRgCCFEQxIwwIyUCgiA8nLA9wEjODjYZ9cWQoimSMCoVS9gWCxSwxBCiENJwKgVGAhOJ9TUeGoYzlZJQjhjxowGaTlmzpzJE088QWlpKWeddRYjR44kPj6ezz777KjnaioNemNpyptKaS6EEMfKZ+nNfeGOr+8gNauJ/OYul6lhpAagrRq3uxKrNRhQRzxnYnQiz5zXdFbDadOmcccdd3DbbbcB8OGHHzJ//nz8/f2ZO3cuoaGh5ObmkpKSwqRJk1Cq6es1lgbd7XY3mqa8sZTmQghxPDpVwDii2gdguN1gNT9r7fYMsz12SUlJHDhwgH379pGTk0NYWBi9evWipqaGe++9l8WLF2OxWMjMzCQ7O5vo6Ogmz9VYGvScnJxG05Q3ltJcCCGOR6cKGEeqCQCwfj0EB+OO60VZ2Tocjlj8/Jq+gTfX1KlT+eijj8jKyqpL8vfOO++Qk5PD6tWrsdvtxMXFNZrWvFZz06ALIYS3SB9GfQEBUFGBxWJHKTsuV3mrnHbatGm8//77fPTRR0ydOhUwqci7d++O3W5n4cKF7Nmz54jnaCoNelNpyhtLaS6EEMdDAkZ99VKEWCyBuN2tEzCGDRtGSUkJPXv2JCYmBoArr7ySVatWER8fz5tvvsngwYOPeI6m0qA3laa8sZTmQghxPCS9eX35+bBzJwwZQpWtkOrq/QQHJx13P8aJTtKbC9FxSXrzY1WbIqSiAovF/Ox2H/4oVCGE6IwkYNTncJjRUuXlWK0mYLRWP4YQQpzovBYwlFKzlVIHlFIbm9h+pVJqvVJqg1LqZ6VUQr1tuz3rU5VSqxo7viWa3eymVF3Ht1J+gLXT1zA6UpOlEOL4eLOG8R/gvCNs3wWcobWOBx4CXj1k+5la68Tmtq01xd/fn7y8vObf+AIDTdZawGoNxOUqO57Ln9C01uTl5eHv7+/roggh2gGvzcPQWi9WSsUdYfvP9d4uA2K9UY7Y2FgyMjLIyclp3gElJabze+NGanQJLleJJ5ntkWd8d1T+/v7ExnrlTyOEOMG0l4l7NwJf1XuvgW+UUhp4RWt9aO2jjlLqZuBmgN69ex+23W63182Cbpaff4bzz4f//Y/sMcVs2XIVycnrCQ6Ob/45hBCiA/J5p7dS6kxMwPhzvdWnaa1HAucDtymlTm/qeK31q1rrZK11cmRk5PEXKN4TGNatIzg4CYDS0rXHf14hhDjB+TRgKKVGAK8DF2ut82rXa60zPa8HgLnAmDYrVEgI9O8P69YRGDgIiyVAAoYQQuDDgKGU6g18Alyttf6l3vogpVRI7c/AOUCjI628JiEBUlNRykpwcAIlJWva9PJCCNEeea0PQyn1HjAe6KaUygAeAOwAWuuXgfuBCOBFT4ey0zMiKgqY61lnA97VWn/trXI2KiEBPvkESksJDk4iO/sdT+Zan7fgCSGEz3hzlNQVR9k+HZjeyPqdQMLhR7ShMWNAa1iyhOCEJPbte4nKyl0EBPTzabGEEOIwTqe5X9ntXr+UfGVuzPjxpi9j7ty6ju+SEunHEEK0Qw8/DKedBqWlXr+UBIzGOBxwwQXw6acE+Q8BrNLxLYRof378ER56CAYNguBgr19OAkZTfv1ryMnBunwNQUHDKC2Vjm8hRDO5XN6/RmEhXHUVxMXBv/7l/eshAaNp559vahqeZilpkhJCNMubb0JEhPn231w7dsBll0Hv3nCUh6kBps/i1lshIwPefRdCQ4+9vC0gAaMpISFw9tkwdy4hwYnU1GRTVbXf16USQrRnX30FN9wARUXwpz+ZG/uR5ObCH/4AQ4bAl19CXh5cfz243Uc+7q234P334e9/h5NPbr3yH4UEjCOZMgV276bL7hBAZnwLIY5g+XK49FIzLP+ZZ8z7Tz5pfF+t4emnzSThf/3LBInt2+HZZ2HhwiM3MW3fDrfdBqefDjNmeOezNEECxpFMmgQWC0HfpAESMIQQTdi2DSZOhOhomDfP3NCHDoV774WamsP3f/FFuOsuOPVU2LABXnkFYmLgxhtNc/if/2zOeajycrjySrDZTC3D2rZPA5WAcSSRkXDaaVg+/ZKAgP4y41uIjmL5cjMUdfnylh3ndpsbfFoaZGdDZSXs2wfnnmsevjZ/PkRFmRv6o4/CL7/A7NkNz7FsGdx5pwkwX3xhAkstpeD1181zea691syxqLVyJSQlwYoV8Nprpr+jrWmtO8wyatQo3eqeflpr0GlfTdZLloRpl6u69a8hhGg7FRVaDx6sNWgdFKT1N9807ziXS+tf/9ocd+gSFKT1ypUN93e7tR47VuvoaK1LS826Awe07tlT6z59tM7Pb/pa771nzvvII1pXV2v9wANaW61a9+ql9XffHdPHbgqwSjfzHis1jKOZPBmA6KWhOJ0FFBYu8m15hBDH55FHYOtW+M9/oF8/803/gw+Ofty995o+iRkzTHPQCy+YWsS995p+h+RDnvWmFDz2GGRlmT4NlwuuuMJ0bH/8MYSFNX2tyy83o6ZmzjSZJ/7+d/jNb2D9evjVr47n0x+f5kaWE2HxSg1Da62TkrQ75WT9ww9BeuvWW7xzDSGE961fr7XNpvXVV5v3BQVajxuntVJav/BC08fNnm2+8d9yi6k5tMTkyVqHhGh9663mHLNnN++4nBxTO4mI0Pqjj1p2zRagBTUMn9/kW3PxWsB48EGtQW9deKH+8cfu2u12euc6QoiW2bhR66QkrR991DTdHInTqfWYMVpHRmqdm3twfXm51pMmmdvhbbdpvWtXw+MWLdLabtf67LOPfo3GbN6stcVizj99esuOzc4+ctNVK2hJwJAmqeaYMgWAmBXdqak5QFHRz0c5QAjhdVlZJoXPli3wl7/A6NGmY7gpzz9vOoyffdZMrKsVEGCaiG69FV56Cfr2hQsvNPMitm0zWR/69YP//vfYEvwNGWLKd845pgwt0b37kZuu2pgEjOYYNgz69yd4XhpKOcjJ+djXJRKicysrMzf1vDz46SeYOxdyciAlxYxAOjQR3+7dcN99pr/i8ssPP5/NZoa67tpl9lu1ypx/6FDTF/HFF9C167GX9+GHzQgqf/9jP0c7IAGjOZSC6dOx/LCEHtknk5v7iWnPE0K0vdrO47VrzWznkSPN4JTNm+GWW0wHc7du0KOH+XafkmI6ii0WExTMs3Ya17u3Sea3d6/pCJ8yBT7/3NQwBKoj3fiSk5P1qlWrvHPywkLo1YuKs4ex/A/LGTlyBaGho71zLSE6Eq0hMxNiY1t+XGWlaTKqv+72280IpRdegP/7v8OPW7rUjGYqLDQpOgoLTY3krrvgkkuO77N0QEqp1do8vO6ovPYApQ6na1e45Rb8n3kG/2lWcnI+loAhxJE4nebG/cQTpm/hj3+Exx9v+hv+Dz/Azz+bfoOtW81SVGTSdsfEmBqDv79p2vnjHxsPFgCnnGIW0eqkhtESGRnQpw85U2PY+QcHY8b8gjpS9VaIzqa6GvLzTXPO00+bzKsDBsDw4aaf4dZbTZ4ki6XhMX/608EO4R49YPBgs/Tsafom9u07uEyYYJqWLNKi3hqkhuEtsbFw5ZVEfPge26ZVU1a2keDgeF+XSojj53KZNBkpKc2/ETud8NRTJvVFQYGpDVRVHdx+2mlmRNJFF5laxYwZZiJbRYVJf2G1mqaqqVNNM9Idd5gJam2Uqlu0nFcDhlJqNnAhcEBrPbyR7Qp4FrgAKAeu01qv8Wy7FvirZ9eHtdZzvFnWZvvTn7DMmUPPzyBnxMcSMMSJT2uTLO+VV+Caa0wAOFpSuzVrYPp00/H8q1/BmWeaG33tkpJihrnWN2uWaV66/36TRG/6dPMAoLIyUyO57DLvfUbROpo7YeNYFuB0YCSwsYntFwBfAQpIAZZ71ocDOz2vYZ6fw452Pa9N3DvUxIm6uqtNr1w8rG2uJ0RzuN0mB9HgwVr366d1crLWEyZoPXWq1vffr3VZWePH/eMfZlLZqaea18svb3qCWlmZ1vfcY/IaRUcf2wzkJ57QdTmYBg82E9uEz9BeJu5prRcD+UfY5WLgTU+5lwFdlVIxwLnAt1rrfK11AfAtcJ43y9oi99yDvdBJ6CebKC9P83VphDDZU8891ww3DQgwD9WJjITiYli3Dh580HQEpx3y7/Wdd0wupCuugCVL4J//NENVL7/c9C3UKi01fQ/Dh5tmpeuuM8NYj2XU0R//CG+8YWo1K1aYoa/ihODrPoyeQHq99xmedU2tbx/GjcOdnEivD1PJum0Offo/7OsSiQ5Ca/P4hOpqszidpnuhyaWiGj37Dayvv4LFz4blr2+jrrgcp7bWnaemBmp+XI7rkVm4Eu/BfdefcKWMxbV2Pc6Zn+EcOhPnuffifteC/aR78LthKI7ZL+J3+oPw299S/tm3lM9fQlmForz3/1Hz20nUxA2k5gVTPrfbTICuXfz8zGdxOg8uLpfpGjm4XAdx1+F68eD2+q/1l5qahj9brWawVO3icJh1Vqs5t9VqzlNWZuJc7avb3XAfi8X8vt3ug69utzm2/qu/PwQFQWCgebXZTBwuLjbdNsXFpmw2mzlv7WvtNWqvV/s7qf2Mbrc5Z2godOliFofDdAfl5R1cKioapsatpdTBJTISFizw/r9Pr4+SUkrFAV/oxvswvgBmaa1/9Lz/DvgzMB7w11o/7Fn/N6BCa/1EI+e4GbgZoHfv3qP2NOd5uK3h44/h0kv55f4u9H/gABaLX9tcV7Q5pxN27jQZKDIzTfN7/aX2kQW1/6HdbjN9oKrKvFZWmnX1b1RKmZtYYeHB6QKlpY0/a6czsdkaLlbrwUBUu67291t/cbkOP5efn+kyCQ42N3qL5fCAUPu3qP966M2+stIEnfJy8+p0Huyq6dLFvNpshwe9QwOQ1g0/l8VizllUdDDwuFymvBERB5fAwIbBQanD86t37WqS7x6LE2mUVCbQq977WM+6TEzQqL9+UWMn0Fq/CrwKZlitNwrZqMmTcSb056QXt5N37XtE9r22zS4tmqeqyjzj5sAB8+jkvDwz4jMv7+Cgnvr/WbU2rTn+/ubVYoEdO0wrTv3WmVoOh9nPZjs4taD2ptPYN+BDv7mGhECvXqaVp2tXc6NwOMyNzm/7Zuxvz8bWIwrr7f+HNTSo7kZmtYJVubE+8yTWVctRd92B+5TT6m5Qbrcpk5/fwW/8dd96XdVYX3wey0cfYOsWhu3t/2DrFVP3GWprJVVVUPXV97BzJ0HTLiSwbzRBQebzOhzmfLU3caXMZ6qtzVRXm3WHfuOuDaaN3UBttuMfJav1wXMrdWxpn3xJaxNo2nO5fV3DmAj8DtP5fTLwnNZ6jFIqHFiN6TAHWAOM0lofqT/E+/MwDqGX/Yw6ZSzZV8cS9Wb60Q8QrcLlMsPxd+0yw/z37z+4ZGWZJTvbfHNvSv1mgNpviRaLqf5XVppXpxP69DFN7IMHm9eTTjrYPOG1p2O+/rpJcTF4sHl+86BB8M035vGfcHBU00svmX6F225r+TW+/x7i4kyiPdGptZsahlLqPUxNoZtSKgN4ALADaK1fBuZhgsV2zLDa6z3b8pVSDwG1qScfPFqw8AWVciql00YT+e5Kyn7/GUHJF/u6SCc8rSE93TwFc8cOUzOov6SnmyBxaNNNUJCZDBwdDfHxcPbZ5ueoKLN062aq9+HhJvmnzdd168ZobTqnZ86E884z2VGXLYOLL4Zx40wj9UknmY7pl16Ce+45tmABvn0IjzhhNauGoZT6A/AGUAK8DiQBM7TW33i3eC3T1jUMgOrMraghQ6gZ2oPApRlHTmwmgIM1hD17TI63vXtNMtGNG81SVHRwX6XMTb62PTc21nzrr13i4szE4JAQX32aVuJ0mlQXr71mRiC9+urBtomlS00a7+BgM1P6vvvM09feektmO4vj1pIaRnMDxjqtdYJS6lzgFuBvwFta65FHObRN+SJgAOx/IIWYB5fjemc21t9c3+bXb4+0NjWCtDSTGuiXX8zrtm2mleXQPoGICNPkEx9/cBk0yAQLrzX9eMOqVSZ/0r33mhv8keTnw7ffwldfwddfm3a0++4z2VIP/eKxbp15nsKBA2aS3Fdfmc4EIY6TNwLGeq31CKXUs8AirfVcpdRarXXS8Ra2NfkqYBQXLEWdfCqBxV2wpqV3gK+7LVNeDqmpsHq1WbZsMQGifh+C3W4yRA8aZJZ+/UzrSu/eZgkK8l35W4XbDU8+aQKF0wljx8LS0+IzAAAgAElEQVS8eY2nudiyxdQUliwxx4WFmWBwxRWm+akpaWlmFvaMGabjRYhW4I2A8QZmHkQfIAGwYgLHqOMpaGvzVcDQWrP1P0MYcsM29DXXoE4/3Xx7zM837StXX92hsmfu22cSiy5aZFpLNm0y9z0w/QXx8Sbf3MCBZhkwwDQftct+g1q7d5uoFRnZ8mP374drrzW1hUsuMQ/puflmSE42NYf6N/dPPjH7Bgaaju3zz4cxY06wapToSLwRMCxAIrBTa13oGcUUq7Vef3xFbV2+ChgA+/e/gb7pBnp8WW9l7XjB4GBzV+3e3SdlOx5ut8kyvWyZyTz9ww+mSQnMl+dTTzUpg5KTYdQo059wwnXjbNliZkaHh5sIGBPT9L6lpQ3H4qalmecslJWZRHvTp5tfwKefmtxICQlmhFNoKPztb/Doo+ZaH39sMrEK4WMtCRjNzQk1Fgjy/HwV8BRwUnPzj7TV0ma5pBrhdJbpH7/vqtM+Okfr3bu1LikxuX02bdLa4dB6yhTzvp2rrtb6p5+0fughrc89V+uuXQ9ODwoL03rSJK2ffFLrVau0djp9XdpWUFCg9YABWkdGah0UpHVSktbFxYfvV1xsPvzhc6a0HjHC/J0P9fnnWvv5mXOee67Z96abtK6s9P7nEqKZaEEuqeY2ErwEJCilEoA/YkZKvQmc0aJQ1oFZrYFE955OunqaHpHVBAZ6OjyHDjWdmPfcA+++C1de6duCHsLlgvXrYeFC+O47WLzYfIlWykwou+yyg8+jGTCggw3KcbnMaKPdu828hJISk4p76lTzWM7aUUp795rnO2/ebPoP+vZtOJFj1KiD+TDqu/BC+Owz8/jQTZvMyKebbmrTjyhEq2pOVAHWeF7vB26sv649Lb6sYWitdWXlfv3DD4F648bLGm5wOrU+5RTzFT0z0zeF83C5tF67Vuunn9b64otNkWq/KA8YoPVvf6v1f/+rdW6uT4vZNv78Z/PBX3nl4Lp//9usu+46UyNcudJkZQ0N1Xr+/GO7Tmqq1hs2tE6ZhWhleKGGUaKU+gtwNTDO06fRjiew+4bDEU2vXnexZ8/DlJTcQ0iIZ0yA1WoSvSQmms7Qzz9vs4Z+rU2fw3ffmWXhQpMaA8xIpV//GsaPhzPOMGkqOpwtW8wEuIgI09GSkGDydbz3npkAd+ut5m9S64YbzOzAmTNNv8QXX5i+pwULYNiwYytDQkKrfBQhfK45UQWIBu4Cxnne9wauaW5UaqvF1zUMrbWuqSnSS5ZE6NTUCYdvfOYZ8+119myvlsHlMv0Qf/qTeSxCbQ0iNlbra6/Ves4crffu9WoRfKu0VOs33tB67NjD+xtsNtOnEBCg9bhxWldVHX682631jTea/VNStM7KavOPIERboQU1jGbnklJKRQG1j9BaobU+0OrR6zj5cpRUfenpT7Njx10kJCwgLOysgxvcbjPpKjUVfvrJdBK0ErfbnPK998wAnf37TRP8WWfBpEkmVUb//ifgCKaWqKw0/UXPP2/6IwYNMqOWrr7azBRctergUlpqnjHd1Mg1p9PMo5gwwWTcE6KD8saw2suAxzEZYxUwDrhba/3RcZSz1bWXgOFyVbJixUD8/KIYOXIFqv5detcu86zjigpzQ0pJOa5rbdkCb79tnoOzZ48Z3n/BBTBlipkO0Gnmd61cefChPtOmmRxLp53WwSOkEMfPG8kH7wNG19YqlFKRwAKgXQWM9sJq9Scu7kG2bbuenJyP6N596sGNffrAjz+ab65nn22+5U6Y0Oxza23yLc2da+aArVtnRi6dcw488oiZKHy0jBQdSlWVSdj3z3+abIPz5pnJcEKIVtfcGsYGrXV8vfcWYF39de1Be6lhAGjtYuXKEWhdw+jRm7BYDhkjkJVlHqm5ZYsZbnvppUc8X+oH23j3pSI+yRjNjh0KpcxQ16lTzdM0azNft3sFBaYGUFFhJq8daTKjy2WqTdu3mwly27cffLxZ7ePX1q83Caquvx6eeso8WEII0WzeqGF8rZSaD7zneT8Nk5pcNEEpK337/oONGyeTlTWbHj1uabhDdLTJrXHhhWayw7PPmmaUehMd8vJMLJn9dCGpuwZhp5pf9dvO3S8P4OKLT6AgUSs93aTtTkszI8dOPdWkzujfv+F+RUXw5z+b5z7Xz1IYGGhmY9d/6k6XLvDll6YdTgjhVS3p9L4EM+MbYInWeq7XSnWM2lMNA8wItNTU0ykv38aYMb9gtzfy7beszASMefPMMw9eeonUmmHMmgVz52qqqxUjWc0NJ33PFUlbCf90dts0u2zYYG7Q/fq13vnOP990Rn/6qelIvvBC08fw5ZcmnxKYLKw332wSVt14o0mjMWCACSoxMdInIUQra/XUICfK0h6G1R6quHi1XrhQ6bS0O5veyeXS+t//1qtCz9ST1GcatA4Ndes/DF+gUxmh9dSpWpeXa11WZtJQhIdrvWePtwqs9e23a62USZexc+fxn3PhQjPxrUcPrdevP7h+2zat+/QxQ1zfeUfra64xQ1mHDdN6+fLjv64Q4qhowbDao82/KAGKG1lKgOLmXqStlvYYMLTWeuvW6XrRIpsuLd3c6PZly7SeONH8Nbr6leq/8zdd4IgyK/7yFxNQav3yi9YhIVqPGdP4HILj8fnnWvfqZYLF9OlmGviQISbfUkvl5ZkgcPnlJp/S0KGNB7msLK2Tk81ntVq1/utfJdeSEG2oJQHD68/0bkvtrUmqVnX1AZYvH0hoaAojRnyFUgq320wifvxxM2gqPNwkPb39dghdt8TMNL76ajNU9FAff2w6yW+/HZ57zqwrLDTNPtu2mQ5hi8U03yhlnreQlGRGaB3apJOfbx5i8frr8OGHZjbza6+ZHvWFC83wqzPPNM1GR3s6fWEhzJljhm/99JPptI6MNLmU/vlPU47GlJaaDuuLLjLlFD63NXcrb69/m/TidPqF9WNA+AD6h/dnQMQAuji6NBwq3oacbicWZcGiOlJSM99q9XkYJ4r2GjAA0tOfYceOOxk48HPmzbuQJ580acNPOgnuvNNkpGjRc5fuugueftrMzNu+3YwmqkcDbgUuC1g02NyYDuKkJBgxAlfWPrb/soz11Rmsi4asUAsnJ13E+JsfoX/U0IM3hDfeYP/vb+CnGyew5sJRVLkaPirParESWFxB4LLVZimrxtazF5bEJFTSSCz9+2Oz+tHVvythAWGEB4QT5h+G0+0koziDjOIMMksy2Veyj2pXNW7tRmttXtGHvVcorBYrVmXFarHib/NnaORQRsaMpH94/8NuJG7t5kDZATKLM+uulVGcQXlNOcO7D2dkzEiGRQ7DYTNPr8stz2VF5gqWZSxjU84mooKi6BfWj37h/egX1o/uQd1xaRc1rhqcbidOt5Mu/l2IDIzEajn4TIsDZQdYsHMB3+z4hm93fktFTQUDIwYyMGIgA8IH0C+8Hw6ro8FndLqdVDgrqKipoMJZQaWzkgBbgPmdeX53XRxdsFqsKBQWZan7O7ncLlzahdPtRGtNj5AexITEHPb7KKwsJDUrldSsVABiQ2PrFrvFzn83/5e31r/FiswVWJSFmOAYMksyG5zDbrHT1b9r3dLFvwvBfsEE2YMI9gsm2C+YLo4uDf7ewX7BZJZksrNgJ7sKdrGzcCeFlYV1+9ce62/zx9/mj8PqwGFzoLVmb/FedhfuZnfhbtKL0gm0B5ISm8KpvU5lbK+xjIwZSWZJJuuy1rEuex3rs9eTWZKJv82fAFsAAfYAAmwBjIgawYUDL2R0j9EN/lZl1WV8s+MbPt32KbsKdmG32rFZbNgsNhxWB/3D+5MUnURidCIDIwbWHVvlrCK7LJvs0mzyK/IprCw8fKk6+HNJVQlVrioqnZVUOauodlUTExJj/l2Em38bcV3jCPILalDu4qpi9hbtJb04nfSidHLKcxjcbTBjeo5hZMxIAu2BLbhxHK7dBAyl1HnAs5gHLr2utZ51yPangTM9bwOB7lrrrp5tLmCDZ9terfWko12vPQeM6uoaHnpoJq+/fitZWbEkJcHdd5thsfUfLJRZnMlzy59jzro5DOo2iGtGXMOlQy+li//BGXgut4u1GSv57sHrSavJIqt7INkhiixbFbnuEqrdNbi1u8H1/bET5LYSXOkmoKKGvaFQbjd/eysWuvh3Ib+yAICeIT0ZHzcegJ/Sf2J34W4AbFjwtweCdoNbg9uN011DpaXhtY6FQuFn9UMpz40QVfdz/fdaa1zahcvtwq3dVLmq6j5rsF8wSdFJdAvsxr6SfWSWZJJVmoXT7WxwrdobQVlNGWBugEMjh1JWU8b2fPOwD4uy0D+8PzllORR4fi9HK3+3wG5EB5uhaxsOmH+64QHhTOg7gTD/MNLy0/gl7xfSi9OP+/fVHA6rg7iucfQN60uAPYDUrFR2Fuw86nEJUQlck3ANVwy/gpiQGCpqKthZsJO0/DR25O8gtzz3sJthWXUZZTVllFaXUlpdSnlNeZPnjwqKok9YH8IDwimvKae0upSyanNspbPS3FBd5oaqUPQI6UFc1zjiusZxUpeTKKgs4Of0n1mfvR5Nw/uXw+pgePfhnNT1JKqcVXWBt6SqhM05m3FpF5GBkVww4AJGxozk+13f882Ob6hwVhDmH0Z8VDxu7a77QlDhrGB7/naqPV+UAmwB9AztWfc7aIrdYicsIKxBYK0NiA6rA3+bPzaLjcySTH7J+4W0vDSqXFVH/dtYlZWwgDByy3Pr3g/vPpyU2BRenPjiMdW82kXAUEpZgV+ACUAGsBK4Qmu9uYn9bweStNY3eN6Xaq1bNAWtPQYMt9u09DzwgHls6aBBK/nLX3ZyzTXTGrQObTywkSd+foJ3N7yLS7uYOGAi2/K28UveL/jb/Ll40MWcEnsKP6b/yPe7vie/Ih+AHiE9iAqKIio4iujgaCIDI/Gz+mFVVizKgtVixeV2UVZTZv5T1pj/zLEhsSREJ5AQlcCQyCE4rA5+yfuFRbsXsWjPIn7Y/QNKKU7tdSqn9jyFsS99QeJ7C/FzHfIBo6Jw33oLlTdeR3l4CGXVZeZbbr1vzjXuGgorC8mvyKegooD8inxsFhuxobH0DO1JbGgsUUFRDb71NVe1q5rNOZtZu38ta/avYU3WGgorC+kZ0pMeIT3qXutfq3uQmfuxs2Ana/avYe3+tazNWlv3zfXkniczqscogv3MP7+CigJ2FOyou1narXbsFvMt1GqxUlhZSHZpNlmlWWSXZVPprGRc73Gc2/9ckqKTDvtc5TXl7C7cXde8UhsMrcpa960ywB6Av82fipoKCioL6n53RVVFuLW7Qc0LaFDrAvPFY1fhLnYW7GRnwU5Kq0tJiE5gZPRIRsaMJCkmCZvFVlfLyyjOoKiyiHP7n8uIqBEt/jscqsZV729eWUBJVUndjT/Ir3nP4639nDZL46P/i6uKWZ6xnNSsVGJDzb/ngREDm9w/vyKf+dvn80XaF3yV9hUFlQX0Cu3F5MGTmTx4MuN6j8NuPbzZtcZVw9bcrXU1s4ySDLoHdq/7PxcVFEW3wG4NgoO/zb9FzXYut4uM4gz2FO2pq2HWvgb7BdMrtBe9u/QmOjgaq8VKdmk2K/etZEXmClZkrqC0upQfb/ix2derr70EjFOAmVrrcz3v/wKgtX60if1/Bh7QWn/reX9CBwytzejXe+81c8vi402ao7i4i1m371ss0U+yo+gAW3K3sDlnMxsObCDQHsgNiTdw5yl30jesL1prVu5byZvr3uT9je+TV5FHz5CeTOg3gQl9J3BWn7OICo5qmw9UXm76SywWiIszbWlxcWbinQx1FScYp9tJelE6cV3jfNYf0160l4BxKXCe1nq65/3VwMla6981su9JwDLMY19dnnVOIBVwArO01p8e7ZrtJWAsXWrmnS1ZYqYPPPSQmWpR6Srnlv9dw9sbPwZMM0afsD4M6TaEsb3GcvOom4kIjGj0nNWuavaX7Kd3l96d/h+4EKL1eGOmt7ddDnxUGyw8TtJaZyql+gLfe9KT7Dj0QKXUzcDNAL17926b0jZha1o1f7g3m28+6kVUFLz4okmWarfD9vztXPrhpazLXsdtCWeTaFvAWYmv0Ce2eU9g87P6cVLXk7z8CYQQomneHJuWCdR/JE+sZ11jLudg2hEAtNaZntedmCy5jY631Fq/qrVO1lonR0ZGHm+Zj9m/Pl7NsOcT+WZ4b6JmjuDWdx/lvMt3YbfDp1s/ZdSro0gvTmfeb+bx/MXzGdVzHBm77qaqqqlfiRBCtC/eDBgrgQFKqT5KKT9MUPjfoTsppQYDYcDSeuvClFIOz8/dMClJGu0s97Ual5NzHnmI29elYAko5o6EB+nXK4SZS+6l73N9iX8pnikfTGFgxEDW3LyG8wecj1IWBg36N1pXs23bLXirWVAIIVqT15qktNZOpdTvgPmYYbWztdablFIPYmYW1gaPy4H3dcO75hDgFaWUGxPUZjU1usqXUtN/4awXriY/YAW9S37DT3/9F7ERYcDf2F24mw82fsBn2z7jjpPvYNbZs+rG+gMEBg6gT59/sGPHnWRnv0109NW++yBCCNEMMnHvGL3y0/v839c34q5xcEXIy7xz72UtHiyktYu1a8+gvHwTo0dvxuGI8U5hhRCiCS3p9Jb59S3kcru47dM/89sFV0BWEm+mbOTd+1oeLMCkQB88eDZudyW//CJNU0KI9k0CRgvkV+Rz1uwLeHHdY9jW3so3V37P1ZN7HNc5AwMH0qfPo+Tlfc7+/a+1UkmFEKL1ScBopk0HNjHy5TH8sHchjvmvsvBPL3LWeL9WOXds7O8JCzub7dvvpLx8W6ucUwghWpsEjGbYmruVcbPPICO7jMAPfuD7J27itNNa7/xKWRg8eA4Wiz+bN1+J21199IOEEKKNScA4ir1Fe5nw1gRKiq0EvLeE7+acwqmntv51HI4eDBr0OqWlq9m9e2brX0AIIY6TBIwjyCnL4Zy3ziGvpBjnG/N56ZH+pKR473qRkVOIjr6RvXtnUVi42HsXEkKIYyABowklVSVc8O4F7CnaQ9D/vmB0r0R+8xvvX7d//2cICOjHli1XUVPTdPpkIYRoaxIwGlHlrGLyB5NZu38tU5z/JXf1OJ5+2iRq9TabLZghQ96huno/27bdIENthRDthgSMRjy59Em+3/U9T57+Bp89diFTp8LYsW13/dDQMfTt+xi5uXNJT3+y7S4shBBHIAHjEMVVxTzx8xNMHDCRtf+5GqcTZs06+nGtLTb2DiIjL2XnzhkUFv7Q9gUQQohDSMA4xPPLn6egsoBpUTOZMwfuuAP69m37ciilGDTo3wQE9GfTpmlUVe1v+0IIIUQ9EjDqKaos4smlT3LRwIt4/cFkunUzT8zzFZstlOHDP8blKmHz5stwu2t8VxghRKcnAaOe55Y/R0FlAec6HmDxYnjwQejSxbdlCgoaxqBBr1FU9CM7d87wbWGEEJ1ae3nins8VVRbx1LKnmDRoEtsWjSIgAG64wdelMqKifkNx8VIyMp4iKGgYMTHtpGBCiE5Fahgezy5/lsLKQmaeMZPvvoNx48DhOPpxbaVfv6cICzuHbdtuJj9/vq+LI4TohCRgAIWVhTy19CkuHnQx0SSxeTOcfbavS9WQxWJn2LD/EhQ0nE2bLqW0dJ2viySE6GQkYADPLnuWoqoiZo6fyfffm3VnneXbMjXGZgtlxIgvsdm6sn79RCorM3xdJCFEJ9LpA0ZRZRFPL3uaKYOnkBidyIIFEB4OiYm+LlnjHI6exMfPw+UqYcOGC3A6i3xdJCFEJ9HpA0aoI5R3fv0OD//qYbSG776DX/2qbdKAHKvg4HiGD/+E8vItbNz4a0mHLoRoE169LSqlzlNKbVNKbVdKHTYmVCl1nVIqRymV6lmm19t2rVIqzbNc68UyMnHgRIZGDmX7dkhPb5/NUYcKCzuLQYNmU1j4PVu3XofWbl8XSQjRwXltWK1Sygq8AEwAMoCVSqn/aa03H7LrB1rr3x1ybDjwAJAMaGC159gCb5UXTO0CToyAARAdfTXV1fvYuXMGfn496N//CV8XSQjRgXmzhjEG2K613qm1rgbeBy5u5rHnAt9qrfM9QeJb4DwvlbPOggXQqxf07+/tK7WeXr3uoWfP28nIeJL09Kd8XRwhRAfmzYDRE0iv9z7Ds+5Qlyil1iulPlJK9Wrhsa3G7YaFC81wWqW8eaXWpZSif/+niYy8lB07/kh29nu+LpIQooPyddfu50Cc1noEphYxp6UnUErdrJRapZRalZOTc8wFSU2F/PwTpzmqPqWsDB78Fl26nM7WrdeSm/u5r4skhOiAvBkwMoFe9d7HetbV0Vrnaa2rPG9fB0Y199h653hVa52stU6OjIw85sLW9l/86lfHfAqfslr9GT78M4KDE9m4cQpZWW/7ukhCiA7GmwFjJTBAKdVHKeUHXA78r/4OSqmYem8nAVs8P88HzlFKhSmlwoBzPOu8ZsECGDoUYmKOvm97Zbd3JSHhO7p2HcfWrVeTmfmir4skhOhAvBYwtNZO4HeYG/0W4EOt9Sal1INKqUme3X6vlNqklFoH/B64znNsPvAQJuisBB70rPOKqipYsqT9pQM5FjZbCPHx84iIuIi0tNvYs+cf8phXIUSrUB3pZpKcnKxXrVrV4uN++AHGj4fPPoNJk466+wnB7a5h69brOXDgHXr1upu+ff+JOpF684UQbUIptVprndycfSW9OaY5ymKBM87wdUlaj8ViZ8iQN7HZupCe/jhudyX9+z8rQUMIccwkYGA6vMeM8f3DklqbUhYGDPgXFos/GRlP4XZXMXDgSyjl68FxQogTUacPGJWVsH69eXZ3R6SUol+/J7BYHOzd+yhaVzNo0OuYifhCCNF8nT5g+PtDTo4JHB2VUoo+fR7BYnGwe/dM3O5qBg+eg8XS6f/8QogWkDsGEBBglo5MKUVc3AMoZWfXrvuoqcllyJB38PPr5uuiCSFOENKY3cmcdNK9DBz4GoWFi1i9eiTFxSt8XSQhxAlCAkYn1KPHdJKSfgIsrF17GpmZL8lcDSHEUUnA6KRCQ5NJTl5DWNjZpKX9H1u3XovLVeHrYgkh2jEJGJ2Y3R5OfPwXxMX9nezst0lNPYOqqn2+LpYQop2SgNHJKWUhLu5+hg+fS1nZZlavHk1xcctnywshOj4JGAKAbt0uZuTIn1HKRmrq6Rw48KGviySEaGckYIg6wcEjGDVqJcHBI9m8eRo7dszA7a46+oFCiE5BAoZowM+vO4mJ3xETM5309H+yatVIiouX+7pYQoh2QAKGOIzF4mDQoNeIj5+Hy1XMmjWnsmPH3TKKSohOTgKGaFJExPmMHr3RU9t4glWrEigtXefrYgkhfEQChjgim60Lgwa9QkLCAlyuctauPY28vC99XSwhhA9IwBDNEhZ2FqNGLScgYAAbNkwiI+N5XxdJCNHGJGCIZnM4epKYuJiIiAvZvv33pKX9Hq1dvi6WEKKNSMAQLWKzBTN8+CfExt5JZubzrF9/PuXlab4ulhCiDUjAEC2mlJX+/Z9i4MCXKSr6mZUrh5KW9geqq3N9XTQhhBd5NWAopc5TSm1TSm1XSs1oZPtdSqnNSqn1SqnvlFIn1dvmUkqlepb/ebOc4tj06HELJ5+8nejoG8jM/BfLl/dn797Hcbk68NOohOjEvBYwlHkG6AvA+cBQ4Aql1NBDdlsLJGutRwAfAY/V21ahtU70LJO8VU5xfByOaAYNeoXRo9fTpctp7Nx5D8uX9ycz8wUJHEJ0MN6sYYwBtmutd2qtq4H3gYvr76C1Xqi1Lve8XQbEerE8wouCgoYxYsQXJCQsJCCgL2lpv5PAIUQH482A0RNIr/c+w7OuKTcCX9V776+UWqWUWqaUmtzUQUqpmz37rcrJyTm+EovjFhY2nsTEH0hI+I6AgD6kpf2OFSsGkJX1Jlq7fV08IcRxaBed3kqpq4Bk4PF6q0/SWicDvwGeUUr1a+xYrfWrWutkrXVyZGRkG5RWHI1SirCwX5GYuJiEhAX4+cWwdeu1rF49hsLCJb4unhDiGHkzYGQCveq9j/Wsa0ApdTZwHzBJa12XGlVrnel53QksApK8WFbhBSZwnMXIkcsYMuRtamqySU09nY0bL6W8fLuviyeEaCFvBoyVwAClVB+llB9wOdBgtJNSKgl4BRMsDtRbH6aUcnh+7gaMBTZ7sazCi5SyEBV1JWPGbCMu7kHy879ixYqBbNw4hcLCxfI8cSFOEF4LGFprJ/A7YD6wBfhQa71JKfWgUqp21NPjQDDw30OGzw4BViml1gELgVlaawkYJzirNZC4uL9x8sk76N37XgoLl5CaegarV48iK+st3O5qXxdRCHEEqiN9u0tOTtarVsnjRU8ULlc52dlvk5HxDOXlW3A4YomNvZOYmJuw2UJ8XTwhOgWl1GpPf/FRtYtOb9E5Wa2B9OhxM6NHbyI+fh4BAQPYseOPLF3ai50776WqKsvXRRRC1CMBQ/icUoqIiPNJTPyekSNXEB4+gb17Z7FsWS/Wr7+ArKw3cTqLfV1MITo9m68LIER9oaGjGTbsv5SXb2f//tc5cOB9tm69FovFn/DwiURHX0N4+AVYLPJPV4i2Jn0Yol3TWlNcvJQDB97nwIEPqKk5gMMRS3T0jcTETMffX5IDCHE8WtKHIQFDnDDc7hry8r5g375XKCj4BlBEREwkJuZGT63D7usiCnHCaUnAkHq9OGFYLHYiI6cQGTmFiopd7N//OllZs8nL+xy7vTvR0dcQHX09QUGH5rgUQrQGqWGIE5rb7SQ//+u6wKG1k+DgRMLCJhAWdhZdupyG1Rrk62IK0W5Jk5TolKqrD5Cd/Ta5uZ9RXLwUrWtQyk5o6CmEhIwmODieoKB4AgOHYrX6+7q4QrQLEjBEp+dyla8w+0EAAAxGSURBVFFU9CMFBd9RWLiQsrKNuN21adYtBAUNJTx8It26TSY0dAxKyQhz0TlJwBDiEG63k4qK7ZSVbaCsbANFRT9TVPQDWjvx84smIuIi/P1PoqYmj5qafJzOPNzuSrp3v4KoqKuwWPx8/RGE8AoJGEI0Q01NAfn5X5Gb+yn5+V/hcpVisQRht4djt0fgclVQUbENh6M3vXvfQ3T0DVitAb4uthCtSgKGEC3kdtegtatB34bWmvz8r9mz52GKi3/Gzy+a6OgbCQlJIihoOP7+/WQCoTjhybBaIVrIzOFoOI+jNmVJePh5FBb+wJ49D7N37z8A7dnuIDBwEAEB/XA4euPv3wuHozcOR0+s1lBstlCs1hCs1hAJLKJDkH/FQhyFeRDUeMLCxuNylVNevoWyso2eZRPl5VvJz/8Gt7usyXNYLP6e4BHsWUIJDo4nNDSF0NAUAgIGSMe7aPckYAjRAlZrICEhowgJGdVgvdYap7OQqqp0qqr24XIV43KV4HSaV5er1LOYn2tq8snOfod9+14GwGbrSnDwSPz9+xAQ0Ad//zj8/ftgt0dis3XBag2VocDC5yRgCNEKlFLY7WHY7WEEB49o1jFauygv30px8XKKi5dRWrqevLwvqKnJbuIafthsXbDbuzVYrNZglLICFpSyoJQNmy0Muz0Suz0SP7/u2O3d8fPrLqO9xHGRgCGEjyhlJShoGEFBw4iJuaFuvctVTmXlHiord1FTk4fLVYzTWYTTWYzTWYjTmUdNTS4VFWkUFf2M212G1m60dgFuzMMuGx/MYrNF4HDE4OcXjc3WFYslCKs1EKs1CIvF39P5X4XbbRaLxYHD0ROHI7Zu8fOLwWoNQSnV6DVMOZQ0sXVAEjCEaGes1kCCgoYQFDTkmI43zWNF1NTk1C3V1dlUV2d5lv1UV2dRVZWJy1WGy1WG212O212BUn5YLA4sFsf/t3d3MXKVdRzHv79523e6bFvLCoQWWkWItAWCIGgQoqnEiBcYUCTEYLipERITpfGdK70RMRKFIAraCIKgDRciFIIhUWALhbZUpJYathR27Zvdzb7Mmfl78Ty7Dgt0zy6dnTO7/09yMuc8c2b2N5sz85/znDPnQWqhWh0hSQ687W/kcq2USidQKp1APn8cSXKQcnk/SXKAJDkUX0dX7E5bRKFwHFIxFpEcUp5croVCoYdisWfyNqy7KJ4wMDHfHR+fr3mNVcrlA5TLA5TLg5hZ3LvKA3ny+XZKpV6KxSXvWtjczHnBcG6eCd1j3RSL3cCq1I8LH7pv/3CtVEYYG9vL+Phexsb6awpPmJLkEIVCD21tqygWF1Mo9AChaFUqhyf3jqAS94TKQJVyeZChoW0kyQEqlSPT5gtnnnVjVo5FIknxvyhRKvXGM9c6JguKNDEVJm9De4FcrhSLWxEpT7m8PxbZMFWr43R0nEln5xo6O1fT0bGaYnFx3LOqYJZglsS9tNE4jWBm5PNt5HLt5HJt5HItjI+/yejobkZGdjM6+ipJcpjOzg/T1XUunZ3n0NZ2WqYKXl0LhqR1wK1AHrjTzH445f4W4B7gHGA/cKWZ7Yn3bQCuAyrA18zskXpmdW6he7cPpny+jfb2lbS3r6zb365WyyTJwVhcDtd0w00UnUMkySHK5YNIeUqlZZRKyygWl1EqLSV8xFQwC1OlMsT4+D7Gxl6Phe51KpUjk/eHKXnLY8IHfRmzcuyaK2OWUCz2UCr1Uir10tl5NlKe4eFt7Nt311HPjJsJqUhr63Ly+U76+3+K2TgA+fwiWlp6gaMXjWJxCWvX/vWYZDmauhUMhVJ+G/BJoB94VtImM3upZrXrgINmtlLSVcCPgCslnQFcBZwJvB94TNIHLJRw59w8k8sVKZXCgflmYVZlZGQ3w8MvkCRHpuyx5MnlWuOeRCu5XDjDrVodoVIZiV2Ao5RKy2htXUFLy4mTXW7V6jjDwzs4cmQLQ0NbKJf3T5ulUFhU19c6+Xfq+NznAbvMbDeApHuBy4HagnE58P04/wDwM4WvOZcD95rZGPCqpF3x+f5Wx7zOOZealKvLnlcuV6Kray1dXWuBrxzT536v6nkaw4nAazXL/bHtHdexsH94GFic8rEASLpeUp+kvsHBwWMU3Tnn3FRNf96bmd1hZuea2blLly5tdBznnJu36lkw9gIn1yyfFNvecR2F0xQWEQ5+p3msc865OVTPgvEssErSCkklwkHsTVPW2QRcG+evAB63cPncTcBVklokrSCcG/hMHbM655ybRt0OeptZIumrwCOEc97uMrMdkm4G+sxsE/BL4DfxoPYBQlEhrvd7wgHyBFjvZ0g551xj+XgYzjm3gM1kPIymP+jtnHNubnjBcM45l8q86pKSNAj8e5YPXwL85xjGmSuee2557rnluevvFDNL9ZuEeVUw3gtJfWn78bLEc88tzz23PHe2eJeUc865VLxgOOecS8ULxv/d0egAs+S555bnnlueO0P8GIZzzrlUfA/DOedcKgu+YEhaJ+llSbsk3dToPEcj6S5JA5K217T1SHpU0ivx9vhGZpxK0smSnpD0kqQdkm6I7ZnODSCpVdIzkl6I2X8Q21dIejpuM/fFa6VliqS8pOclPRyXM58ZQNIeSdskbZXUF9uaYVvplvSApH9I2inpgmbIPVMLumDUjAr4aeAM4AtxtL+s+jWwbkrbTcBmM1sFbI7LWZIAXzezM4DzgfXxf5z13ABjwCVmthpYA6yTdD5hZMhbzGwlcJAwcmTW3ADsrFluhswTPmFma2pOS22GbeVW4M9mdjqwmvC/b4bcM2NmC3YCLgAeqVneAGxodK5pMi8Httcsvwz0xvle4OVGZ5wm/58Iw/Y2W+524DngI4QfZBXeaRvKwkQYDmAzcAnwMGFA6Exnrsm+B1gypS3T2wphWIZXiceEmyX3bKYFvYfBDEb2y7BlZrYvzr8BLGtkmKORtBxYCzxNk+SOXTtbgQHgUeBfwCELI0RCNreZnwDfAKpxeTHZzzzBgL9I2iLp+tiW9W1lBTAI/Cp2A94pqYPs556xhV4w5hULX2UyedqbpE7gD8CNZvbf2vuynNvMKma2hvCt/Tzg9AZHOipJnwEGzGxLo7PM0kVmdjahm3i9pI/X3pnRbaUAnA383MzWAsNM6X7KaO4ZW+gFYz6M7PempF6AeDvQ4DxvI6lIKBYbzezB2Jz53LXM7BDwBKE7pzuOEAnZ22YuBD4raQ9wL6Fb6laynXmSme2NtwPAQ4QinfVtpR/oN7On4/IDhAKS9dwzttALRppRAbOudtTCawnHCDJDkggDZe00sx/X3JXp3ACSlkrqjvNthGMvOwmF44q4Wqaym9kGMzvJzJYTtufHzexqMpx5gqQOSV0T88CngO1kfFsxszeA1yR9MDZdShj8LdO5Z6XRB1EaPQGXAf8k9E1/q9F5psn6O2AfUCZ8q7mO0D+9GXgFeAzoaXTOKZkvIuyKvwhsjdNlWc8ds58FPB+zbwe+G9tPJQwZvAu4H2hpdNZ3yX8x8HCzZI4ZX4jTjon3Y5NsK2uAvrit/BE4vhlyz3TyX3o755xLZaF3STnnnEvJC4ZzzrlUvGA455xLxQuGc865VLxgOOecS8ULhnMZIOniiSvLOpdVXjCcc86l4gXDuRmQ9KU4RsZWSbfHixMOSboljpmxWdLSuO4aSX+X9KKkhybGQ5C0UtJjcZyN5ySdFp++s2ZMhY3xV/LOZYYXDOdSkvQh4ErgQgsXJKwAVwMdQJ+ZnQk8CXwvPuQe4JtmdhawraZ9I3CbhXE2Pkr49T6EK/neSBib5VTCdaGcy4zC9Ks456JLgXOAZ+OX/zbCBeWqwH1xnd8CD0paBHSb2ZOx/W7g/nitpBPN7CEAMxsFiM/3jJn1x+WthLFPnqr/y3IuHS8YzqUn4G4z2/CWRuk7U9ab7fV2xmrmK/j702WMd0k5l95m4ApJ74PJsaZPIbyPJq4E+0XgKTM7DByU9LHYfg3wpJkdAfolfS4+R4uk9jl9Fc7Nkn+DcS4lM3tJ0rcJI8LlCFcNXk8YMOe8eN8A4TgHhEta/yIWhN3Al2P7NcDtkm6Oz/H5OXwZzs2aX63WufdI0pCZdTY6h3P15l1SzjnnUvE9DOecc6n4HoZzzrlUvGA455xLxQuGc865VLxgOOecS8ULhnPOuVS8YDjnnEvlf/11H1QatcQjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 977us/sample - loss: 0.8536 - acc: 0.7564\n",
      "Loss: 0.853619275348085 Accuracy: 0.7563863\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7897 - acc: 0.4245\n",
      "Epoch 00001: val_loss improved from inf to 1.38553, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/001-1.3855.hdf5\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 1.7896 - acc: 0.4246 - val_loss: 1.3855 - val_acc: 0.5805\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3461 - acc: 0.5902\n",
      "Epoch 00002: val_loss improved from 1.38553 to 1.17599, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/002-1.1760.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3461 - acc: 0.5901 - val_loss: 1.1760 - val_acc: 0.6424\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1266 - acc: 0.6648\n",
      "Epoch 00003: val_loss improved from 1.17599 to 0.99787, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/003-0.9979.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1266 - acc: 0.6647 - val_loss: 0.9979 - val_acc: 0.7018\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9907 - acc: 0.7080\n",
      "Epoch 00004: val_loss improved from 0.99787 to 0.91198, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/004-0.9120.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9906 - acc: 0.7080 - val_loss: 0.9120 - val_acc: 0.7345\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7358\n",
      "Epoch 00005: val_loss improved from 0.91198 to 0.79337, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/005-0.7934.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9027 - acc: 0.7357 - val_loss: 0.7934 - val_acc: 0.7757\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8363 - acc: 0.7552\n",
      "Epoch 00006: val_loss improved from 0.79337 to 0.75603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/006-0.7560.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8363 - acc: 0.7552 - val_loss: 0.7560 - val_acc: 0.7887\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.7742\n",
      "Epoch 00007: val_loss improved from 0.75603 to 0.72700, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/007-0.7270.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7732 - acc: 0.7742 - val_loss: 0.7270 - val_acc: 0.7966\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.7891\n",
      "Epoch 00008: val_loss improved from 0.72700 to 0.70459, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/008-0.7046.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7247 - acc: 0.7891 - val_loss: 0.7046 - val_acc: 0.7985\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.8012\n",
      "Epoch 00009: val_loss improved from 0.70459 to 0.66491, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/009-0.6649.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6826 - acc: 0.8012 - val_loss: 0.6649 - val_acc: 0.8097\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.8127\n",
      "Epoch 00010: val_loss improved from 0.66491 to 0.64565, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/010-0.6456.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6478 - acc: 0.8127 - val_loss: 0.6456 - val_acc: 0.8162\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6163 - acc: 0.8209\n",
      "Epoch 00011: val_loss improved from 0.64565 to 0.61665, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/011-0.6166.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6163 - acc: 0.8209 - val_loss: 0.6166 - val_acc: 0.8267\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.8302\n",
      "Epoch 00012: val_loss improved from 0.61665 to 0.60836, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/012-0.6084.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5885 - acc: 0.8302 - val_loss: 0.6084 - val_acc: 0.8274\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8401\n",
      "Epoch 00013: val_loss improved from 0.60836 to 0.56378, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/013-0.5638.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5621 - acc: 0.8401 - val_loss: 0.5638 - val_acc: 0.8355\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8464\n",
      "Epoch 00014: val_loss improved from 0.56378 to 0.56095, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/014-0.5609.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5383 - acc: 0.8464 - val_loss: 0.5609 - val_acc: 0.8437\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8518\n",
      "Epoch 00015: val_loss improved from 0.56095 to 0.53392, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/015-0.5339.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5168 - acc: 0.8518 - val_loss: 0.5339 - val_acc: 0.8481\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8568\n",
      "Epoch 00016: val_loss improved from 0.53392 to 0.53148, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/016-0.5315.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4942 - acc: 0.8568 - val_loss: 0.5315 - val_acc: 0.8530\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8644\n",
      "Epoch 00017: val_loss did not improve from 0.53148\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4761 - acc: 0.8644 - val_loss: 0.5420 - val_acc: 0.8477\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8712\n",
      "Epoch 00018: val_loss improved from 0.53148 to 0.50483, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/018-0.5048.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4543 - acc: 0.8712 - val_loss: 0.5048 - val_acc: 0.8526\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8756\n",
      "Epoch 00019: val_loss did not improve from 0.50483\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4365 - acc: 0.8756 - val_loss: 0.5224 - val_acc: 0.8502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8780\n",
      "Epoch 00020: val_loss improved from 0.50483 to 0.49890, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/020-0.4989.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4223 - acc: 0.8780 - val_loss: 0.4989 - val_acc: 0.8558\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8851\n",
      "Epoch 00021: val_loss did not improve from 0.49890\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4023 - acc: 0.8851 - val_loss: 0.5173 - val_acc: 0.8560\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8874\n",
      "Epoch 00022: val_loss improved from 0.49890 to 0.49382, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/022-0.4938.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3915 - acc: 0.8874 - val_loss: 0.4938 - val_acc: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8918\n",
      "Epoch 00023: val_loss improved from 0.49382 to 0.48042, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/023-0.4804.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3746 - acc: 0.8917 - val_loss: 0.4804 - val_acc: 0.8663\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8940\n",
      "Epoch 00024: val_loss did not improve from 0.48042\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3670 - acc: 0.8940 - val_loss: 0.4820 - val_acc: 0.8588\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.9021\n",
      "Epoch 00025: val_loss improved from 0.48042 to 0.45443, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/025-0.4544.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3459 - acc: 0.9021 - val_loss: 0.4544 - val_acc: 0.8721\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.9020\n",
      "Epoch 00026: val_loss did not improve from 0.45443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3387 - acc: 0.9020 - val_loss: 0.4560 - val_acc: 0.8703\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.9080\n",
      "Epoch 00027: val_loss improved from 0.45443 to 0.45141, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/027-0.4514.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3218 - acc: 0.9081 - val_loss: 0.4514 - val_acc: 0.8682\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9107\n",
      "Epoch 00028: val_loss did not improve from 0.45141\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3104 - acc: 0.9107 - val_loss: 0.4602 - val_acc: 0.8656\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9146\n",
      "Epoch 00029: val_loss improved from 0.45141 to 0.45035, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/029-0.4504.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3000 - acc: 0.9146 - val_loss: 0.4504 - val_acc: 0.8735\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9163\n",
      "Epoch 00030: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2914 - acc: 0.9163 - val_loss: 0.4513 - val_acc: 0.8724\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9196\n",
      "Epoch 00031: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2790 - acc: 0.9196 - val_loss: 0.4511 - val_acc: 0.8740\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9217\n",
      "Epoch 00032: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2680 - acc: 0.9217 - val_loss: 0.4770 - val_acc: 0.8740\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9250\n",
      "Epoch 00033: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2617 - acc: 0.9250 - val_loss: 0.4555 - val_acc: 0.8719\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9283\n",
      "Epoch 00034: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2505 - acc: 0.9283 - val_loss: 0.4837 - val_acc: 0.8691\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9296\n",
      "Epoch 00035: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2454 - acc: 0.9296 - val_loss: 0.4681 - val_acc: 0.8728\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9330\n",
      "Epoch 00036: val_loss did not improve from 0.45035\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2368 - acc: 0.9330 - val_loss: 0.4599 - val_acc: 0.8700\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9340\n",
      "Epoch 00037: val_loss improved from 0.45035 to 0.42857, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/037-0.4286.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2321 - acc: 0.9339 - val_loss: 0.4286 - val_acc: 0.8821\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9377\n",
      "Epoch 00038: val_loss did not improve from 0.42857\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2177 - acc: 0.9377 - val_loss: 0.4439 - val_acc: 0.8775\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9396\n",
      "Epoch 00039: val_loss did not improve from 0.42857\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2111 - acc: 0.9396 - val_loss: 0.4519 - val_acc: 0.8770\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9416\n",
      "Epoch 00040: val_loss did not improve from 0.42857\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2036 - acc: 0.9416 - val_loss: 0.4322 - val_acc: 0.8842\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9458\n",
      "Epoch 00041: val_loss did not improve from 0.42857\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1919 - acc: 0.9458 - val_loss: 0.4558 - val_acc: 0.8817\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9452\n",
      "Epoch 00042: val_loss improved from 0.42857 to 0.42533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/042-0.4253.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1889 - acc: 0.9452 - val_loss: 0.4253 - val_acc: 0.8863\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9467\n",
      "Epoch 00043: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1843 - acc: 0.9467 - val_loss: 0.4326 - val_acc: 0.8854\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9490\n",
      "Epoch 00044: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1762 - acc: 0.9490 - val_loss: 0.4410 - val_acc: 0.8770\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9502\n",
      "Epoch 00045: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1754 - acc: 0.9502 - val_loss: 0.4516 - val_acc: 0.8845\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9519\n",
      "Epoch 00046: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1668 - acc: 0.9519 - val_loss: 0.4508 - val_acc: 0.8782\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9543\n",
      "Epoch 00047: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1583 - acc: 0.9543 - val_loss: 0.4370 - val_acc: 0.8810\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9564\n",
      "Epoch 00048: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1514 - acc: 0.9564 - val_loss: 0.4290 - val_acc: 0.8882\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9552\n",
      "Epoch 00049: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1546 - acc: 0.9552 - val_loss: 0.4361 - val_acc: 0.8800\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9601\n",
      "Epoch 00050: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1436 - acc: 0.9601 - val_loss: 0.4475 - val_acc: 0.8817\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9592\n",
      "Epoch 00051: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1410 - acc: 0.9592 - val_loss: 0.4371 - val_acc: 0.8831\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9615\n",
      "Epoch 00052: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1352 - acc: 0.9615 - val_loss: 0.4664 - val_acc: 0.8814\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9620\n",
      "Epoch 00053: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1343 - acc: 0.9620 - val_loss: 0.4320 - val_acc: 0.8859\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9625\n",
      "Epoch 00054: val_loss did not improve from 0.42533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1331 - acc: 0.9625 - val_loss: 0.4499 - val_acc: 0.8873\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9665\n",
      "Epoch 00055: val_loss improved from 0.42533 to 0.42422, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/055-0.4242.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1221 - acc: 0.9665 - val_loss: 0.4242 - val_acc: 0.8880\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9685\n",
      "Epoch 00056: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1169 - acc: 0.9685 - val_loss: 0.4415 - val_acc: 0.8805\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9704\n",
      "Epoch 00057: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1096 - acc: 0.9704 - val_loss: 0.4443 - val_acc: 0.8852\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9671\n",
      "Epoch 00058: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1137 - acc: 0.9671 - val_loss: 0.4272 - val_acc: 0.8868\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9690\n",
      "Epoch 00059: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1109 - acc: 0.9690 - val_loss: 0.4493 - val_acc: 0.8861\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9727\n",
      "Epoch 00060: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1021 - acc: 0.9726 - val_loss: 0.4335 - val_acc: 0.8894\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9695\n",
      "Epoch 00061: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1082 - acc: 0.9695 - val_loss: 0.4487 - val_acc: 0.8870\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9757\n",
      "Epoch 00062: val_loss did not improve from 0.42422\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0927 - acc: 0.9757 - val_loss: 0.4483 - val_acc: 0.8889\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9748\n",
      "Epoch 00063: val_loss improved from 0.42422 to 0.42397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv_checkpoint/063-0.4240.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0926 - acc: 0.9748 - val_loss: 0.4240 - val_acc: 0.8898\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9746\n",
      "Epoch 00064: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0929 - acc: 0.9746 - val_loss: 0.4378 - val_acc: 0.8903\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9760\n",
      "Epoch 00065: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0889 - acc: 0.9760 - val_loss: 0.4697 - val_acc: 0.8831\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9768\n",
      "Epoch 00066: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0874 - acc: 0.9768 - val_loss: 0.4353 - val_acc: 0.8884\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9782\n",
      "Epoch 00067: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0839 - acc: 0.9782 - val_loss: 0.4452 - val_acc: 0.8884\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9787\n",
      "Epoch 00068: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0795 - acc: 0.9787 - val_loss: 0.4736 - val_acc: 0.8859\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9805\n",
      "Epoch 00069: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0764 - acc: 0.9805 - val_loss: 0.4508 - val_acc: 0.8868\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9796\n",
      "Epoch 00070: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0777 - acc: 0.9796 - val_loss: 0.4393 - val_acc: 0.8908\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9799\n",
      "Epoch 00071: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0776 - acc: 0.9799 - val_loss: 0.4677 - val_acc: 0.8880\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9823\n",
      "Epoch 00072: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0719 - acc: 0.9823 - val_loss: 0.4839 - val_acc: 0.8896\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9804\n",
      "Epoch 00073: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0767 - acc: 0.9804 - val_loss: 0.4541 - val_acc: 0.8898\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9828\n",
      "Epoch 00074: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0669 - acc: 0.9828 - val_loss: 0.4770 - val_acc: 0.8868\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9845\n",
      "Epoch 00075: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0658 - acc: 0.9845 - val_loss: 0.4526 - val_acc: 0.8926\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9836\n",
      "Epoch 00076: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0645 - acc: 0.9836 - val_loss: 0.4764 - val_acc: 0.8882\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9809\n",
      "Epoch 00077: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0716 - acc: 0.9809 - val_loss: 0.4701 - val_acc: 0.8912\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9840\n",
      "Epoch 00078: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0614 - acc: 0.9840 - val_loss: 0.4766 - val_acc: 0.8884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9820\n",
      "Epoch 00079: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0702 - acc: 0.9820 - val_loss: 0.4687 - val_acc: 0.8875\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9842\n",
      "Epoch 00080: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0613 - acc: 0.9842 - val_loss: 0.4773 - val_acc: 0.8910\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9863\n",
      "Epoch 00081: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0556 - acc: 0.9863 - val_loss: 0.4644 - val_acc: 0.8928\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9865\n",
      "Epoch 00082: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0555 - acc: 0.9865 - val_loss: 0.4677 - val_acc: 0.8919\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9868\n",
      "Epoch 00083: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0527 - acc: 0.9868 - val_loss: 0.4780 - val_acc: 0.8866\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9843\n",
      "Epoch 00084: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0584 - acc: 0.9843 - val_loss: 0.4823 - val_acc: 0.8912\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9874\n",
      "Epoch 00085: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0524 - acc: 0.9874 - val_loss: 0.4751 - val_acc: 0.8921\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9871\n",
      "Epoch 00086: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0527 - acc: 0.9871 - val_loss: 0.5293 - val_acc: 0.8761\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9867\n",
      "Epoch 00087: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0528 - acc: 0.9866 - val_loss: 0.4788 - val_acc: 0.8942\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9870\n",
      "Epoch 00088: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0527 - acc: 0.9870 - val_loss: 0.4621 - val_acc: 0.8921\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9889\n",
      "Epoch 00089: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0472 - acc: 0.9889 - val_loss: 0.4724 - val_acc: 0.8849\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9901\n",
      "Epoch 00090: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0446 - acc: 0.9901 - val_loss: 0.4922 - val_acc: 0.8891\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9885\n",
      "Epoch 00091: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0471 - acc: 0.9885 - val_loss: 0.4663 - val_acc: 0.8970\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9880\n",
      "Epoch 00092: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0499 - acc: 0.9880 - val_loss: 0.4879 - val_acc: 0.8940\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9890\n",
      "Epoch 00093: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0454 - acc: 0.9891 - val_loss: 0.5297 - val_acc: 0.8840\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9903\n",
      "Epoch 00094: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0435 - acc: 0.9903 - val_loss: 0.4975 - val_acc: 0.8935\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9885\n",
      "Epoch 00095: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0471 - acc: 0.9885 - val_loss: 0.5022 - val_acc: 0.8896\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9903\n",
      "Epoch 00096: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0431 - acc: 0.9903 - val_loss: 0.5207 - val_acc: 0.8838\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9909\n",
      "Epoch 00097: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0410 - acc: 0.9909 - val_loss: 0.5053 - val_acc: 0.8898\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9889\n",
      "Epoch 00098: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0456 - acc: 0.9889 - val_loss: 0.4878 - val_acc: 0.8935\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9914\n",
      "Epoch 00099: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0379 - acc: 0.9914 - val_loss: 0.5125 - val_acc: 0.8905\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9910\n",
      "Epoch 00100: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0392 - acc: 0.9910 - val_loss: 0.5350 - val_acc: 0.8833\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9901\n",
      "Epoch 00101: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0428 - acc: 0.9901 - val_loss: 0.4880 - val_acc: 0.8959\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9905\n",
      "Epoch 00102: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0398 - acc: 0.9905 - val_loss: 0.5082 - val_acc: 0.8919\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9904\n",
      "Epoch 00103: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0399 - acc: 0.9904 - val_loss: 0.5210 - val_acc: 0.8863\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9908\n",
      "Epoch 00104: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0394 - acc: 0.9908 - val_loss: 0.5018 - val_acc: 0.8915\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9924\n",
      "Epoch 00105: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0345 - acc: 0.9924 - val_loss: 0.4985 - val_acc: 0.8952\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9908\n",
      "Epoch 00106: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0385 - acc: 0.9908 - val_loss: 0.4854 - val_acc: 0.8940\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9914\n",
      "Epoch 00107: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0373 - acc: 0.9914 - val_loss: 0.5167 - val_acc: 0.8903\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9939\n",
      "Epoch 00108: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0302 - acc: 0.9939 - val_loss: 0.4990 - val_acc: 0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9923\n",
      "Epoch 00109: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0326 - acc: 0.9923 - val_loss: 0.5154 - val_acc: 0.8912\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9897\n",
      "Epoch 00110: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0408 - acc: 0.9897 - val_loss: 0.5283 - val_acc: 0.8880\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9924\n",
      "Epoch 00111: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0331 - acc: 0.9924 - val_loss: 0.4970 - val_acc: 0.8926\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9928\n",
      "Epoch 00112: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0324 - acc: 0.9928 - val_loss: 0.5150 - val_acc: 0.8921\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9929\n",
      "Epoch 00113: val_loss did not improve from 0.42397\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0314 - acc: 0.9929 - val_loss: 0.5320 - val_acc: 0.8931\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 985us/sample - loss: 0.5095 - acc: 0.8677\n",
      "Loss: 0.5095143529237369 Accuracy: 0.8677051\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8417 - acc: 0.4108\n",
      "Epoch 00001: val_loss improved from inf to 1.43938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/001-1.4394.hdf5\n",
      "36805/36805 [==============================] - 98s 3ms/sample - loss: 1.8417 - acc: 0.4108 - val_loss: 1.4394 - val_acc: 0.5535\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4147 - acc: 0.5600\n",
      "Epoch 00002: val_loss improved from 1.43938 to 1.20139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/002-1.2014.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4146 - acc: 0.5600 - val_loss: 1.2014 - val_acc: 0.6504\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1596 - acc: 0.6558\n",
      "Epoch 00003: val_loss improved from 1.20139 to 1.02217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/003-1.0222.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1596 - acc: 0.6557 - val_loss: 1.0222 - val_acc: 0.6951\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9888 - acc: 0.7086\n",
      "Epoch 00004: val_loss improved from 1.02217 to 0.84979, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/004-0.8498.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9889 - acc: 0.7086 - val_loss: 0.8498 - val_acc: 0.7531\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8641 - acc: 0.7463\n",
      "Epoch 00005: val_loss improved from 0.84979 to 0.76682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/005-0.7668.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8641 - acc: 0.7463 - val_loss: 0.7668 - val_acc: 0.7850\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7651 - acc: 0.7789\n",
      "Epoch 00006: val_loss improved from 0.76682 to 0.66456, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/006-0.6646.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7650 - acc: 0.7789 - val_loss: 0.6646 - val_acc: 0.8109\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.8028\n",
      "Epoch 00007: val_loss improved from 0.66456 to 0.59253, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/007-0.5925.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6839 - acc: 0.8028 - val_loss: 0.5925 - val_acc: 0.8304\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6176 - acc: 0.8255\n",
      "Epoch 00008: val_loss improved from 0.59253 to 0.54755, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/008-0.5475.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6175 - acc: 0.8255 - val_loss: 0.5475 - val_acc: 0.8449\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8382\n",
      "Epoch 00009: val_loss improved from 0.54755 to 0.50329, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/009-0.5033.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5700 - acc: 0.8381 - val_loss: 0.5033 - val_acc: 0.8621\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8522\n",
      "Epoch 00010: val_loss improved from 0.50329 to 0.46324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/010-0.4632.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5222 - acc: 0.8522 - val_loss: 0.4632 - val_acc: 0.8740\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.8614\n",
      "Epoch 00011: val_loss improved from 0.46324 to 0.44292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/011-0.4429.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4830 - acc: 0.8614 - val_loss: 0.4429 - val_acc: 0.8772\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8696\n",
      "Epoch 00012: val_loss improved from 0.44292 to 0.41718, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/012-0.4172.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4556 - acc: 0.8696 - val_loss: 0.4172 - val_acc: 0.8891\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8781\n",
      "Epoch 00013: val_loss improved from 0.41718 to 0.38820, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/013-0.3882.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4251 - acc: 0.8781 - val_loss: 0.3882 - val_acc: 0.8903\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8849\n",
      "Epoch 00014: val_loss improved from 0.38820 to 0.36824, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/014-0.3682.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4012 - acc: 0.8849 - val_loss: 0.3682 - val_acc: 0.8942\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.8920\n",
      "Epoch 00015: val_loss improved from 0.36824 to 0.33892, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/015-0.3389.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3834 - acc: 0.8919 - val_loss: 0.3389 - val_acc: 0.9038\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8973\n",
      "Epoch 00016: val_loss improved from 0.33892 to 0.33681, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/016-0.3368.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3614 - acc: 0.8973 - val_loss: 0.3368 - val_acc: 0.9061\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8999\n",
      "Epoch 00017: val_loss did not improve from 0.33681\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3516 - acc: 0.8999 - val_loss: 0.3414 - val_acc: 0.9019\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9053\n",
      "Epoch 00018: val_loss improved from 0.33681 to 0.31407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/018-0.3141.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3321 - acc: 0.9053 - val_loss: 0.3141 - val_acc: 0.9126\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9096\n",
      "Epoch 00019: val_loss improved from 0.31407 to 0.30119, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/019-0.3012.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3180 - acc: 0.9096 - val_loss: 0.3012 - val_acc: 0.9143\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9132\n",
      "Epoch 00020: val_loss improved from 0.30119 to 0.29971, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/020-0.2997.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3072 - acc: 0.9131 - val_loss: 0.2997 - val_acc: 0.9175\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9136\n",
      "Epoch 00021: val_loss did not improve from 0.29971\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2993 - acc: 0.9136 - val_loss: 0.3007 - val_acc: 0.9199\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9183\n",
      "Epoch 00022: val_loss improved from 0.29971 to 0.29939, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/022-0.2994.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2820 - acc: 0.9183 - val_loss: 0.2994 - val_acc: 0.9122\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9226\n",
      "Epoch 00023: val_loss improved from 0.29939 to 0.28552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/023-0.2855.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2714 - acc: 0.9225 - val_loss: 0.2855 - val_acc: 0.9180\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9244\n",
      "Epoch 00024: val_loss improved from 0.28552 to 0.27229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/024-0.2723.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2653 - acc: 0.9244 - val_loss: 0.2723 - val_acc: 0.9252\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9285\n",
      "Epoch 00025: val_loss did not improve from 0.27229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2515 - acc: 0.9285 - val_loss: 0.2864 - val_acc: 0.9168\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9288\n",
      "Epoch 00026: val_loss did not improve from 0.27229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2437 - acc: 0.9288 - val_loss: 0.2859 - val_acc: 0.9187\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9336\n",
      "Epoch 00027: val_loss improved from 0.27229 to 0.26009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/027-0.2601.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2333 - acc: 0.9337 - val_loss: 0.2601 - val_acc: 0.9266\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9344\n",
      "Epoch 00028: val_loss did not improve from 0.26009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2259 - acc: 0.9344 - val_loss: 0.2604 - val_acc: 0.9276\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9377\n",
      "Epoch 00029: val_loss did not improve from 0.26009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2186 - acc: 0.9377 - val_loss: 0.2886 - val_acc: 0.9203\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9373\n",
      "Epoch 00030: val_loss improved from 0.26009 to 0.24561, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/030-0.2456.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2141 - acc: 0.9373 - val_loss: 0.2456 - val_acc: 0.9308\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9413\n",
      "Epoch 00031: val_loss did not improve from 0.24561\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2036 - acc: 0.9413 - val_loss: 0.2495 - val_acc: 0.9315\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9433\n",
      "Epoch 00032: val_loss improved from 0.24561 to 0.23803, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/032-0.2380.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1977 - acc: 0.9433 - val_loss: 0.2380 - val_acc: 0.9331\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9455\n",
      "Epoch 00033: val_loss improved from 0.23803 to 0.23750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/033-0.2375.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1913 - acc: 0.9455 - val_loss: 0.2375 - val_acc: 0.9376\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9474\n",
      "Epoch 00034: val_loss improved from 0.23750 to 0.22678, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/034-0.2268.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1846 - acc: 0.9475 - val_loss: 0.2268 - val_acc: 0.9385\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9485\n",
      "Epoch 00035: val_loss did not improve from 0.22678\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1796 - acc: 0.9485 - val_loss: 0.2442 - val_acc: 0.9338\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9490\n",
      "Epoch 00036: val_loss did not improve from 0.22678\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1756 - acc: 0.9490 - val_loss: 0.2366 - val_acc: 0.9345\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9511\n",
      "Epoch 00037: val_loss did not improve from 0.22678\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1678 - acc: 0.9511 - val_loss: 0.2607 - val_acc: 0.9345\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9530\n",
      "Epoch 00038: val_loss did not improve from 0.22678\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1630 - acc: 0.9530 - val_loss: 0.2348 - val_acc: 0.9345\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9535\n",
      "Epoch 00039: val_loss did not improve from 0.22678\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1574 - acc: 0.9535 - val_loss: 0.2334 - val_acc: 0.9385\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9540\n",
      "Epoch 00040: val_loss improved from 0.22678 to 0.21827, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/040-0.2183.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1552 - acc: 0.9540 - val_loss: 0.2183 - val_acc: 0.9406\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9565\n",
      "Epoch 00041: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1500 - acc: 0.9565 - val_loss: 0.2306 - val_acc: 0.9355\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9573\n",
      "Epoch 00042: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1450 - acc: 0.9573 - val_loss: 0.2191 - val_acc: 0.9399\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9582\n",
      "Epoch 00043: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1457 - acc: 0.9582 - val_loss: 0.2336 - val_acc: 0.9376\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9608\n",
      "Epoch 00044: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1352 - acc: 0.9608 - val_loss: 0.2341 - val_acc: 0.9366\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9630\n",
      "Epoch 00045: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1281 - acc: 0.9630 - val_loss: 0.2231 - val_acc: 0.9425\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9626\n",
      "Epoch 00046: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1273 - acc: 0.9626 - val_loss: 0.2495 - val_acc: 0.9331\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9650\n",
      "Epoch 00047: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1215 - acc: 0.9650 - val_loss: 0.2318 - val_acc: 0.9366\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9646\n",
      "Epoch 00048: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1195 - acc: 0.9646 - val_loss: 0.2341 - val_acc: 0.9390\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9670\n",
      "Epoch 00049: val_loss did not improve from 0.21827\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1138 - acc: 0.9670 - val_loss: 0.2219 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9678\n",
      "Epoch 00050: val_loss improved from 0.21827 to 0.19611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/050-0.1961.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1133 - acc: 0.9678 - val_loss: 0.1961 - val_acc: 0.9464\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9683\n",
      "Epoch 00051: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1075 - acc: 0.9683 - val_loss: 0.2202 - val_acc: 0.9422\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9696\n",
      "Epoch 00052: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1040 - acc: 0.9696 - val_loss: 0.2224 - val_acc: 0.9408\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9693\n",
      "Epoch 00053: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1051 - acc: 0.9693 - val_loss: 0.2073 - val_acc: 0.9478\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9712\n",
      "Epoch 00054: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0977 - acc: 0.9712 - val_loss: 0.2234 - val_acc: 0.9432\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9714\n",
      "Epoch 00055: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0987 - acc: 0.9714 - val_loss: 0.2591 - val_acc: 0.9331\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9727\n",
      "Epoch 00056: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0933 - acc: 0.9727 - val_loss: 0.2297 - val_acc: 0.9366\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9741\n",
      "Epoch 00057: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0894 - acc: 0.9741 - val_loss: 0.2178 - val_acc: 0.9411\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9743\n",
      "Epoch 00058: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0880 - acc: 0.9743 - val_loss: 0.2143 - val_acc: 0.9436\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9759\n",
      "Epoch 00059: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0843 - acc: 0.9758 - val_loss: 0.2251 - val_acc: 0.9427\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9752\n",
      "Epoch 00060: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0871 - acc: 0.9752 - val_loss: 0.2006 - val_acc: 0.9488\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9783\n",
      "Epoch 00061: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0773 - acc: 0.9783 - val_loss: 0.2193 - val_acc: 0.9418\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9773\n",
      "Epoch 00062: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0787 - acc: 0.9773 - val_loss: 0.2069 - val_acc: 0.9455\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9780\n",
      "Epoch 00063: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0756 - acc: 0.9780 - val_loss: 0.2141 - val_acc: 0.9432\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9798\n",
      "Epoch 00064: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0710 - acc: 0.9798 - val_loss: 0.2084 - val_acc: 0.9464\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9784\n",
      "Epoch 00065: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0746 - acc: 0.9784 - val_loss: 0.2235 - val_acc: 0.9420\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9822\n",
      "Epoch 00066: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0664 - acc: 0.9822 - val_loss: 0.2198 - val_acc: 0.9429\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9798\n",
      "Epoch 00067: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0711 - acc: 0.9798 - val_loss: 0.2237 - val_acc: 0.9434\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9814\n",
      "Epoch 00068: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0644 - acc: 0.9814 - val_loss: 0.1996 - val_acc: 0.9502\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9816\n",
      "Epoch 00069: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0635 - acc: 0.9816 - val_loss: 0.2302 - val_acc: 0.9406\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9835\n",
      "Epoch 00070: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0590 - acc: 0.9835 - val_loss: 0.2074 - val_acc: 0.9471\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9833\n",
      "Epoch 00071: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0589 - acc: 0.9833 - val_loss: 0.2184 - val_acc: 0.9420\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9836\n",
      "Epoch 00072: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0562 - acc: 0.9836 - val_loss: 0.2342 - val_acc: 0.9383\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9843\n",
      "Epoch 00073: val_loss did not improve from 0.19611\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0563 - acc: 0.9843 - val_loss: 0.2227 - val_acc: 0.9462\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9845\n",
      "Epoch 00074: val_loss improved from 0.19611 to 0.19454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv_checkpoint/074-0.1945.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0558 - acc: 0.9845 - val_loss: 0.1945 - val_acc: 0.9499\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9834\n",
      "Epoch 00075: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0588 - acc: 0.9834 - val_loss: 0.2195 - val_acc: 0.9469\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9869\n",
      "Epoch 00076: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0485 - acc: 0.9869 - val_loss: 0.2282 - val_acc: 0.9441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9860\n",
      "Epoch 00077: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0510 - acc: 0.9860 - val_loss: 0.2248 - val_acc: 0.9427\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9863\n",
      "Epoch 00078: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0476 - acc: 0.9863 - val_loss: 0.2316 - val_acc: 0.9436\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9872\n",
      "Epoch 00079: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0479 - acc: 0.9872 - val_loss: 0.2178 - val_acc: 0.9464\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9893\n",
      "Epoch 00080: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0416 - acc: 0.9893 - val_loss: 0.2139 - val_acc: 0.9504\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9871\n",
      "Epoch 00081: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0455 - acc: 0.9871 - val_loss: 0.2244 - val_acc: 0.9425\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9843\n",
      "Epoch 00082: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0566 - acc: 0.9843 - val_loss: 0.2309 - val_acc: 0.9436\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9899\n",
      "Epoch 00083: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0390 - acc: 0.9899 - val_loss: 0.2310 - val_acc: 0.9460\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9890\n",
      "Epoch 00084: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0415 - acc: 0.9890 - val_loss: 0.2238 - val_acc: 0.9464\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9884\n",
      "Epoch 00085: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0412 - acc: 0.9884 - val_loss: 0.2522 - val_acc: 0.9383\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9906\n",
      "Epoch 00086: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0374 - acc: 0.9906 - val_loss: 0.2109 - val_acc: 0.9490\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9893\n",
      "Epoch 00087: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0388 - acc: 0.9893 - val_loss: 0.2394 - val_acc: 0.9425\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9887\n",
      "Epoch 00088: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0406 - acc: 0.9887 - val_loss: 0.2159 - val_acc: 0.9488\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9912\n",
      "Epoch 00089: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0347 - acc: 0.9912 - val_loss: 0.2291 - val_acc: 0.9481\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9914\n",
      "Epoch 00090: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0329 - acc: 0.9914 - val_loss: 0.2230 - val_acc: 0.9485\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9896\n",
      "Epoch 00091: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0376 - acc: 0.9896 - val_loss: 0.2401 - val_acc: 0.9441\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9913\n",
      "Epoch 00092: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0335 - acc: 0.9913 - val_loss: 0.2252 - val_acc: 0.9483\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9918\n",
      "Epoch 00093: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0321 - acc: 0.9918 - val_loss: 0.2274 - val_acc: 0.9499\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9913\n",
      "Epoch 00094: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0334 - acc: 0.9913 - val_loss: 0.2464 - val_acc: 0.9401\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9937\n",
      "Epoch 00095: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0274 - acc: 0.9937 - val_loss: 0.2355 - val_acc: 0.9460\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9913\n",
      "Epoch 00096: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0308 - acc: 0.9913 - val_loss: 0.2518 - val_acc: 0.9432\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9904\n",
      "Epoch 00097: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0340 - acc: 0.9904 - val_loss: 0.2217 - val_acc: 0.9474\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9952\n",
      "Epoch 00098: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0217 - acc: 0.9952 - val_loss: 0.2262 - val_acc: 0.9492\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9936\n",
      "Epoch 00099: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0251 - acc: 0.9936 - val_loss: 0.2380 - val_acc: 0.9460\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9918\n",
      "Epoch 00100: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0303 - acc: 0.9918 - val_loss: 0.2593 - val_acc: 0.9420\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9917\n",
      "Epoch 00101: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0326 - acc: 0.9917 - val_loss: 0.2535 - val_acc: 0.9408\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9940\n",
      "Epoch 00102: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9940 - val_loss: 0.2674 - val_acc: 0.9441\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9914\n",
      "Epoch 00103: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0317 - acc: 0.9914 - val_loss: 0.2408 - val_acc: 0.9429\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9944\n",
      "Epoch 00104: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0221 - acc: 0.9944 - val_loss: 0.2625 - val_acc: 0.9446\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9930\n",
      "Epoch 00105: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0270 - acc: 0.9930 - val_loss: 0.2405 - val_acc: 0.9462\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9938\n",
      "Epoch 00106: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0247 - acc: 0.9938 - val_loss: 0.2401 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9947\n",
      "Epoch 00107: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0221 - acc: 0.9947 - val_loss: 0.2710 - val_acc: 0.9413\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9912\n",
      "Epoch 00108: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0320 - acc: 0.9912 - val_loss: 0.2508 - val_acc: 0.9425\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9945\n",
      "Epoch 00109: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0214 - acc: 0.9945 - val_loss: 0.2349 - val_acc: 0.9467\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9944\n",
      "Epoch 00110: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0215 - acc: 0.9944 - val_loss: 0.2517 - val_acc: 0.9429\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9934\n",
      "Epoch 00111: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0236 - acc: 0.9934 - val_loss: 0.2381 - val_acc: 0.9488\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9938\n",
      "Epoch 00112: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0234 - acc: 0.9938 - val_loss: 0.2746 - val_acc: 0.9413\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9947\n",
      "Epoch 00113: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0211 - acc: 0.9947 - val_loss: 0.2691 - val_acc: 0.9369\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9936\n",
      "Epoch 00114: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0245 - acc: 0.9936 - val_loss: 0.2430 - val_acc: 0.9478\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9936\n",
      "Epoch 00115: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0251 - acc: 0.9936 - val_loss: 0.2597 - val_acc: 0.9441\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9951\n",
      "Epoch 00116: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0193 - acc: 0.9951 - val_loss: 0.2460 - val_acc: 0.9481\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9945\n",
      "Epoch 00117: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0209 - acc: 0.9945 - val_loss: 0.2437 - val_acc: 0.9492\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9968\n",
      "Epoch 00118: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0148 - acc: 0.9968 - val_loss: 0.2400 - val_acc: 0.9492\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9941\n",
      "Epoch 00119: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0226 - acc: 0.9941 - val_loss: 0.2993 - val_acc: 0.9355\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9958\n",
      "Epoch 00120: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0180 - acc: 0.9958 - val_loss: 0.2623 - val_acc: 0.9446\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9949\n",
      "Epoch 00121: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0199 - acc: 0.9949 - val_loss: 0.2503 - val_acc: 0.9485\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9927\n",
      "Epoch 00122: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0246 - acc: 0.9927 - val_loss: 0.2382 - val_acc: 0.9483\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9964\n",
      "Epoch 00123: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0163 - acc: 0.9964 - val_loss: 0.2981 - val_acc: 0.9404\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9957\n",
      "Epoch 00124: val_loss did not improve from 0.19454\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0182 - acc: 0.9957 - val_loss: 0.2740 - val_acc: 0.9420\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2530 - acc: 0.9277\n",
      "Loss: 0.2529973657084155 Accuracy: 0.92772585\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8411 - acc: 0.4154\n",
      "Epoch 00001: val_loss improved from inf to 1.34986, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/001-1.3499.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.8411 - acc: 0.4154 - val_loss: 1.3499 - val_acc: 0.6028\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2672 - acc: 0.6083\n",
      "Epoch 00002: val_loss improved from 1.34986 to 0.99530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/002-0.9953.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.2671 - acc: 0.6084 - val_loss: 0.9953 - val_acc: 0.7177\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9557 - acc: 0.7157\n",
      "Epoch 00003: val_loss improved from 0.99530 to 0.71115, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/003-0.7111.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9557 - acc: 0.7157 - val_loss: 0.7111 - val_acc: 0.8004\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7241 - acc: 0.7896\n",
      "Epoch 00004: val_loss improved from 0.71115 to 0.63288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/004-0.6329.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7241 - acc: 0.7896 - val_loss: 0.6329 - val_acc: 0.8137\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5898 - acc: 0.8297\n",
      "Epoch 00005: val_loss improved from 0.63288 to 0.43565, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/005-0.4356.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5898 - acc: 0.8298 - val_loss: 0.4356 - val_acc: 0.8770\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8570\n",
      "Epoch 00006: val_loss improved from 0.43565 to 0.39220, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/006-0.3922.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4988 - acc: 0.8570 - val_loss: 0.3922 - val_acc: 0.8954\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8762\n",
      "Epoch 00007: val_loss improved from 0.39220 to 0.32728, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/007-0.3273.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4310 - acc: 0.8762 - val_loss: 0.3273 - val_acc: 0.9131\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8903\n",
      "Epoch 00008: val_loss improved from 0.32728 to 0.28062, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/008-0.2806.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3877 - acc: 0.8903 - val_loss: 0.2806 - val_acc: 0.9213\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.9006\n",
      "Epoch 00009: val_loss improved from 0.28062 to 0.27547, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/009-0.2755.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3474 - acc: 0.9005 - val_loss: 0.2755 - val_acc: 0.9266\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9082\n",
      "Epoch 00010: val_loss improved from 0.27547 to 0.26639, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/010-0.2664.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3162 - acc: 0.9082 - val_loss: 0.2664 - val_acc: 0.9245\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9184\n",
      "Epoch 00011: val_loss improved from 0.26639 to 0.22591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/011-0.2259.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2885 - acc: 0.9184 - val_loss: 0.2259 - val_acc: 0.9355\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9210\n",
      "Epoch 00012: val_loss did not improve from 0.22591\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2712 - acc: 0.9210 - val_loss: 0.2337 - val_acc: 0.9364\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9258\n",
      "Epoch 00013: val_loss improved from 0.22591 to 0.20373, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/013-0.2037.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2513 - acc: 0.9257 - val_loss: 0.2037 - val_acc: 0.9464\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9307\n",
      "Epoch 00014: val_loss improved from 0.20373 to 0.20239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/014-0.2024.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2376 - acc: 0.9307 - val_loss: 0.2024 - val_acc: 0.9453\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9339\n",
      "Epoch 00015: val_loss improved from 0.20239 to 0.19216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/015-0.1922.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2261 - acc: 0.9339 - val_loss: 0.1922 - val_acc: 0.9478\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9395\n",
      "Epoch 00016: val_loss improved from 0.19216 to 0.18993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/016-0.1899.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2080 - acc: 0.9395 - val_loss: 0.1899 - val_acc: 0.9483\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9408\n",
      "Epoch 00017: val_loss did not improve from 0.18993\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2005 - acc: 0.9408 - val_loss: 0.1930 - val_acc: 0.9448\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9444\n",
      "Epoch 00018: val_loss improved from 0.18993 to 0.18271, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/018-0.1827.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1887 - acc: 0.9444 - val_loss: 0.1827 - val_acc: 0.9488\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9471\n",
      "Epoch 00019: val_loss improved from 0.18271 to 0.16107, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/019-0.1611.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1797 - acc: 0.9471 - val_loss: 0.1611 - val_acc: 0.9543\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9504\n",
      "Epoch 00020: val_loss did not improve from 0.16107\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1692 - acc: 0.9504 - val_loss: 0.1743 - val_acc: 0.9478\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9527\n",
      "Epoch 00021: val_loss improved from 0.16107 to 0.15984, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/021-0.1598.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1628 - acc: 0.9527 - val_loss: 0.1598 - val_acc: 0.9546\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9534\n",
      "Epoch 00022: val_loss did not improve from 0.15984\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1590 - acc: 0.9534 - val_loss: 0.1676 - val_acc: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9550\n",
      "Epoch 00023: val_loss improved from 0.15984 to 0.15003, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/023-0.1500.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1514 - acc: 0.9550 - val_loss: 0.1500 - val_acc: 0.9541\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9582\n",
      "Epoch 00024: val_loss did not improve from 0.15003\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1422 - acc: 0.9582 - val_loss: 0.1606 - val_acc: 0.9546\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9571\n",
      "Epoch 00025: val_loss did not improve from 0.15003\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1404 - acc: 0.9571 - val_loss: 0.1567 - val_acc: 0.9518\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9589\n",
      "Epoch 00026: val_loss did not improve from 0.15003\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1363 - acc: 0.9589 - val_loss: 0.1670 - val_acc: 0.9534\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9626\n",
      "Epoch 00027: val_loss did not improve from 0.15003\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1250 - acc: 0.9626 - val_loss: 0.1505 - val_acc: 0.9578\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9630\n",
      "Epoch 00028: val_loss improved from 0.15003 to 0.14690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/028-0.1469.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1226 - acc: 0.9630 - val_loss: 0.1469 - val_acc: 0.9576\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9639\n",
      "Epoch 00029: val_loss did not improve from 0.14690\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1186 - acc: 0.9639 - val_loss: 0.1578 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9663\n",
      "Epoch 00030: val_loss did not improve from 0.14690\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1104 - acc: 0.9663 - val_loss: 0.1685 - val_acc: 0.9562\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9661\n",
      "Epoch 00031: val_loss did not improve from 0.14690\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1089 - acc: 0.9661 - val_loss: 0.1541 - val_acc: 0.9562\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9693\n",
      "Epoch 00032: val_loss did not improve from 0.14690\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1020 - acc: 0.9694 - val_loss: 0.1548 - val_acc: 0.9564\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9708\n",
      "Epoch 00033: val_loss improved from 0.14690 to 0.14684, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/033-0.1468.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0984 - acc: 0.9708 - val_loss: 0.1468 - val_acc: 0.9567\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9710\n",
      "Epoch 00034: val_loss did not improve from 0.14684\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0965 - acc: 0.9710 - val_loss: 0.1557 - val_acc: 0.9583\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9715\n",
      "Epoch 00035: val_loss did not improve from 0.14684\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0914 - acc: 0.9715 - val_loss: 0.1569 - val_acc: 0.9567\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9736\n",
      "Epoch 00036: val_loss did not improve from 0.14684\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0890 - acc: 0.9736 - val_loss: 0.1698 - val_acc: 0.9522\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9732\n",
      "Epoch 00037: val_loss did not improve from 0.14684\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0886 - acc: 0.9731 - val_loss: 0.1721 - val_acc: 0.9509\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9757\n",
      "Epoch 00038: val_loss did not improve from 0.14684\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0830 - acc: 0.9757 - val_loss: 0.1595 - val_acc: 0.9588\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9750\n",
      "Epoch 00039: val_loss improved from 0.14684 to 0.13731, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv_checkpoint/039-0.1373.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0815 - acc: 0.9750 - val_loss: 0.1373 - val_acc: 0.9623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9783\n",
      "Epoch 00040: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0734 - acc: 0.9783 - val_loss: 0.1574 - val_acc: 0.9588\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9782\n",
      "Epoch 00041: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0720 - acc: 0.9782 - val_loss: 0.1498 - val_acc: 0.9613\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9789\n",
      "Epoch 00042: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0696 - acc: 0.9789 - val_loss: 0.1522 - val_acc: 0.9557\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9788\n",
      "Epoch 00043: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0701 - acc: 0.9788 - val_loss: 0.1529 - val_acc: 0.9592\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9789\n",
      "Epoch 00044: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0668 - acc: 0.9789 - val_loss: 0.1479 - val_acc: 0.9583\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9831\n",
      "Epoch 00045: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0589 - acc: 0.9831 - val_loss: 0.1687 - val_acc: 0.9557\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9816\n",
      "Epoch 00046: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0618 - acc: 0.9816 - val_loss: 0.1728 - val_acc: 0.9490\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9841\n",
      "Epoch 00047: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0548 - acc: 0.9841 - val_loss: 0.1579 - val_acc: 0.9602\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9848\n",
      "Epoch 00048: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0532 - acc: 0.9848 - val_loss: 0.1447 - val_acc: 0.9609\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9840\n",
      "Epoch 00049: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0540 - acc: 0.9840 - val_loss: 0.1661 - val_acc: 0.9578\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9855\n",
      "Epoch 00050: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0492 - acc: 0.9855 - val_loss: 0.1551 - val_acc: 0.9599\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9860\n",
      "Epoch 00051: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0462 - acc: 0.9860 - val_loss: 0.1558 - val_acc: 0.9590\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9854\n",
      "Epoch 00052: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0485 - acc: 0.9854 - val_loss: 0.1621 - val_acc: 0.9550\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9873\n",
      "Epoch 00053: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0433 - acc: 0.9873 - val_loss: 0.1404 - val_acc: 0.9639\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9885\n",
      "Epoch 00054: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0415 - acc: 0.9885 - val_loss: 0.1501 - val_acc: 0.9630\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9879\n",
      "Epoch 00055: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0428 - acc: 0.9879 - val_loss: 0.1712 - val_acc: 0.9571\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9889\n",
      "Epoch 00056: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0381 - acc: 0.9889 - val_loss: 0.1470 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9902\n",
      "Epoch 00057: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0363 - acc: 0.9902 - val_loss: 0.1532 - val_acc: 0.9595\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9904\n",
      "Epoch 00058: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0351 - acc: 0.9904 - val_loss: 0.1498 - val_acc: 0.9616\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9901\n",
      "Epoch 00059: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0340 - acc: 0.9901 - val_loss: 0.1592 - val_acc: 0.9592\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9910\n",
      "Epoch 00060: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0322 - acc: 0.9910 - val_loss: 0.1739 - val_acc: 0.9567\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9917\n",
      "Epoch 00061: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0307 - acc: 0.9917 - val_loss: 0.1569 - val_acc: 0.9592\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9911\n",
      "Epoch 00062: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0313 - acc: 0.9911 - val_loss: 0.1581 - val_acc: 0.9592\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9910\n",
      "Epoch 00063: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0306 - acc: 0.9910 - val_loss: 0.1658 - val_acc: 0.9599\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9933\n",
      "Epoch 00064: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0250 - acc: 0.9933 - val_loss: 0.1747 - val_acc: 0.9599\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9922\n",
      "Epoch 00065: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0274 - acc: 0.9922 - val_loss: 0.1658 - val_acc: 0.9609\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9929\n",
      "Epoch 00066: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0263 - acc: 0.9929 - val_loss: 0.1604 - val_acc: 0.9630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9924\n",
      "Epoch 00067: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0275 - acc: 0.9924 - val_loss: 0.1542 - val_acc: 0.9618\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9909\n",
      "Epoch 00068: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0304 - acc: 0.9909 - val_loss: 0.1601 - val_acc: 0.9604\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9931\n",
      "Epoch 00069: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0237 - acc: 0.9931 - val_loss: 0.1945 - val_acc: 0.9571\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9942\n",
      "Epoch 00070: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0216 - acc: 0.9942 - val_loss: 0.1729 - val_acc: 0.9590\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9935\n",
      "Epoch 00071: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0247 - acc: 0.9935 - val_loss: 0.1660 - val_acc: 0.9609\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9964\n",
      "Epoch 00072: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0163 - acc: 0.9964 - val_loss: 0.1716 - val_acc: 0.9571\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9948\n",
      "Epoch 00073: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0209 - acc: 0.9948 - val_loss: 0.2013 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9944\n",
      "Epoch 00074: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0195 - acc: 0.9944 - val_loss: 0.1747 - val_acc: 0.9602\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9947\n",
      "Epoch 00075: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0197 - acc: 0.9947 - val_loss: 0.1686 - val_acc: 0.9648\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9936\n",
      "Epoch 00076: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0218 - acc: 0.9936 - val_loss: 0.1640 - val_acc: 0.9595\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9964\n",
      "Epoch 00077: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0145 - acc: 0.9964 - val_loss: 0.1739 - val_acc: 0.9590\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9940\n",
      "Epoch 00078: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0209 - acc: 0.9940 - val_loss: 0.1820 - val_acc: 0.9581\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00079: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 0.1716 - val_acc: 0.9606\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9951\n",
      "Epoch 00080: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0188 - acc: 0.9951 - val_loss: 0.1718 - val_acc: 0.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9958\n",
      "Epoch 00081: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0161 - acc: 0.9958 - val_loss: 0.1706 - val_acc: 0.9637\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9950\n",
      "Epoch 00082: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0189 - acc: 0.9950 - val_loss: 0.1651 - val_acc: 0.9651\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9956\n",
      "Epoch 00083: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0171 - acc: 0.9956 - val_loss: 0.1642 - val_acc: 0.9620\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967\n",
      "Epoch 00084: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.1577 - val_acc: 0.9627\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9964\n",
      "Epoch 00085: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0137 - acc: 0.9964 - val_loss: 0.1746 - val_acc: 0.9613\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9948\n",
      "Epoch 00086: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0189 - acc: 0.9948 - val_loss: 0.1746 - val_acc: 0.9616\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9983\n",
      "Epoch 00087: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0083 - acc: 0.9983 - val_loss: 0.1827 - val_acc: 0.9590\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9945\n",
      "Epoch 00088: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0190 - acc: 0.9945 - val_loss: 0.1998 - val_acc: 0.9546\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9964\n",
      "Epoch 00089: val_loss did not improve from 0.13731\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0132 - acc: 0.9964 - val_loss: 0.1679 - val_acc: 0.9613\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1670 - acc: 0.9487\n",
      "Loss: 0.166974950755868 Accuracy: 0.948702\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4279 - acc: 0.5553\n",
      "Epoch 00001: val_loss improved from inf to 0.82447, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/001-0.8245.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.4278 - acc: 0.5553 - val_loss: 0.8245 - val_acc: 0.7503\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7520 - acc: 0.7740\n",
      "Epoch 00002: val_loss improved from 0.82447 to 0.50486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/002-0.5049.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.7519 - acc: 0.7740 - val_loss: 0.5049 - val_acc: 0.8488\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8462\n",
      "Epoch 00003: val_loss improved from 0.50486 to 0.37257, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/003-0.3726.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.5227 - acc: 0.8462 - val_loss: 0.3726 - val_acc: 0.8921\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8821\n",
      "Epoch 00004: val_loss improved from 0.37257 to 0.28718, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/004-0.2872.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.4001 - acc: 0.8821 - val_loss: 0.2872 - val_acc: 0.9185\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9051\n",
      "Epoch 00005: val_loss improved from 0.28718 to 0.26770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/005-0.2677.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.3225 - acc: 0.9051 - val_loss: 0.2677 - val_acc: 0.9224\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9179\n",
      "Epoch 00006: val_loss improved from 0.26770 to 0.21784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/006-0.2178.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2744 - acc: 0.9179 - val_loss: 0.2178 - val_acc: 0.9392\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9265\n",
      "Epoch 00007: val_loss improved from 0.21784 to 0.18244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/007-0.1824.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2419 - acc: 0.9266 - val_loss: 0.1824 - val_acc: 0.9448\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9356\n",
      "Epoch 00008: val_loss did not improve from 0.18244\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2152 - acc: 0.9356 - val_loss: 0.1845 - val_acc: 0.9460\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9402\n",
      "Epoch 00009: val_loss improved from 0.18244 to 0.16456, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/009-0.1646.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1949 - acc: 0.9403 - val_loss: 0.1646 - val_acc: 0.9495\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9452\n",
      "Epoch 00010: val_loss improved from 0.16456 to 0.14835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/010-0.1483.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1774 - acc: 0.9452 - val_loss: 0.1483 - val_acc: 0.9571\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9498\n",
      "Epoch 00011: val_loss did not improve from 0.14835\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1654 - acc: 0.9498 - val_loss: 0.1484 - val_acc: 0.9539\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9554\n",
      "Epoch 00012: val_loss improved from 0.14835 to 0.13707, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/012-0.1371.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1497 - acc: 0.9554 - val_loss: 0.1371 - val_acc: 0.9576\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9567\n",
      "Epoch 00013: val_loss did not improve from 0.13707\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1407 - acc: 0.9567 - val_loss: 0.1496 - val_acc: 0.9567\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9593\n",
      "Epoch 00014: val_loss improved from 0.13707 to 0.13671, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/014-0.1367.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1304 - acc: 0.9593 - val_loss: 0.1367 - val_acc: 0.9571\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9624\n",
      "Epoch 00015: val_loss did not improve from 0.13671\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1218 - acc: 0.9624 - val_loss: 0.1657 - val_acc: 0.9483\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9658\n",
      "Epoch 00016: val_loss improved from 0.13671 to 0.12499, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/016-0.1250.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1110 - acc: 0.9658 - val_loss: 0.1250 - val_acc: 0.9616\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9688\n",
      "Epoch 00017: val_loss did not improve from 0.12499\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1042 - acc: 0.9688 - val_loss: 0.1365 - val_acc: 0.9602\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9701\n",
      "Epoch 00018: val_loss did not improve from 0.12499\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0961 - acc: 0.9701 - val_loss: 0.1266 - val_acc: 0.9606\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9720\n",
      "Epoch 00019: val_loss improved from 0.12499 to 0.12358, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/019-0.1236.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0905 - acc: 0.9720 - val_loss: 0.1236 - val_acc: 0.9625\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9734\n",
      "Epoch 00020: val_loss did not improve from 0.12358\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0837 - acc: 0.9734 - val_loss: 0.1409 - val_acc: 0.9569\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9759\n",
      "Epoch 00021: val_loss did not improve from 0.12358\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0781 - acc: 0.9759 - val_loss: 0.1289 - val_acc: 0.9611\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9778\n",
      "Epoch 00022: val_loss improved from 0.12358 to 0.12072, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv_checkpoint/022-0.1207.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0720 - acc: 0.9778 - val_loss: 0.1207 - val_acc: 0.9637\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9774\n",
      "Epoch 00023: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0718 - acc: 0.9774 - val_loss: 0.1238 - val_acc: 0.9644\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9810\n",
      "Epoch 00024: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0631 - acc: 0.9810 - val_loss: 0.1388 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9820\n",
      "Epoch 00025: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0597 - acc: 0.9820 - val_loss: 0.1364 - val_acc: 0.9595\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9828\n",
      "Epoch 00026: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0562 - acc: 0.9828 - val_loss: 0.1345 - val_acc: 0.9590\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9839\n",
      "Epoch 00027: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0536 - acc: 0.9839 - val_loss: 0.1346 - val_acc: 0.9627\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9855\n",
      "Epoch 00028: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0501 - acc: 0.9855 - val_loss: 0.1360 - val_acc: 0.9604\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9858\n",
      "Epoch 00029: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0482 - acc: 0.9858 - val_loss: 0.1294 - val_acc: 0.9620\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9883\n",
      "Epoch 00030: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0393 - acc: 0.9883 - val_loss: 0.1379 - val_acc: 0.9609\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9879\n",
      "Epoch 00031: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0404 - acc: 0.9879 - val_loss: 0.1285 - val_acc: 0.9627\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9885\n",
      "Epoch 00032: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0385 - acc: 0.9885 - val_loss: 0.1429 - val_acc: 0.9623\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9897\n",
      "Epoch 00033: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0362 - acc: 0.9897 - val_loss: 0.1264 - val_acc: 0.9632\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9898\n",
      "Epoch 00034: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0354 - acc: 0.9898 - val_loss: 0.1493 - val_acc: 0.9595\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9902\n",
      "Epoch 00035: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0339 - acc: 0.9902 - val_loss: 0.1305 - val_acc: 0.9609\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9918\n",
      "Epoch 00036: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0293 - acc: 0.9918 - val_loss: 0.1431 - val_acc: 0.9613\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9925\n",
      "Epoch 00037: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0263 - acc: 0.9925 - val_loss: 0.1434 - val_acc: 0.9592\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9921\n",
      "Epoch 00038: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0268 - acc: 0.9921 - val_loss: 0.1427 - val_acc: 0.9630\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9914\n",
      "Epoch 00039: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0270 - acc: 0.9914 - val_loss: 0.1426 - val_acc: 0.9611\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9932\n",
      "Epoch 00040: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0246 - acc: 0.9932 - val_loss: 0.1322 - val_acc: 0.9644\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9935\n",
      "Epoch 00041: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0229 - acc: 0.9935 - val_loss: 0.1417 - val_acc: 0.9639\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9930\n",
      "Epoch 00042: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0239 - acc: 0.9930 - val_loss: 0.1283 - val_acc: 0.9634\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9942\n",
      "Epoch 00043: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0204 - acc: 0.9942 - val_loss: 0.1440 - val_acc: 0.9637\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00044: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.1529 - val_acc: 0.9585\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9951\n",
      "Epoch 00045: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0173 - acc: 0.9951 - val_loss: 0.1417 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9954\n",
      "Epoch 00046: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0170 - acc: 0.9954 - val_loss: 0.1475 - val_acc: 0.9646\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9949\n",
      "Epoch 00047: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0184 - acc: 0.9949 - val_loss: 0.1404 - val_acc: 0.9639\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9957\n",
      "Epoch 00048: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0143 - acc: 0.9957 - val_loss: 0.1728 - val_acc: 0.9557\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9949\n",
      "Epoch 00049: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0180 - acc: 0.9949 - val_loss: 0.1461 - val_acc: 0.9641\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9976\n",
      "Epoch 00050: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0100 - acc: 0.9976 - val_loss: 0.1893 - val_acc: 0.9550\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9952\n",
      "Epoch 00051: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0171 - acc: 0.9952 - val_loss: 0.1366 - val_acc: 0.9630\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 00052: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.1767 - val_acc: 0.9576\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9968\n",
      "Epoch 00053: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0122 - acc: 0.9968 - val_loss: 0.1511 - val_acc: 0.9634\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9960\n",
      "Epoch 00054: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0139 - acc: 0.9960 - val_loss: 0.1595 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9965\n",
      "Epoch 00055: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0130 - acc: 0.9965 - val_loss: 0.1473 - val_acc: 0.9644\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9974\n",
      "Epoch 00056: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0095 - acc: 0.9974 - val_loss: 0.1562 - val_acc: 0.9637\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
      "Epoch 00057: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.1834 - val_acc: 0.9595\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0166 - acc: 0.9948 - val_loss: 0.1827 - val_acc: 0.9574\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9972\n",
      "Epoch 00059: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.1428 - val_acc: 0.9660\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00060: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0066 - acc: 0.9985 - val_loss: 0.1733 - val_acc: 0.9632\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 00061: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0145 - acc: 0.9954 - val_loss: 0.1645 - val_acc: 0.9625\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9974\n",
      "Epoch 00062: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0094 - acc: 0.9974 - val_loss: 0.1620 - val_acc: 0.9630\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 00063: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.1481 - val_acc: 0.9641\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9955\n",
      "Epoch 00064: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.1921 - val_acc: 0.9527\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.1464 - val_acc: 0.9651\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 00066: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1517 - val_acc: 0.9616\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9966\n",
      "Epoch 00067: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0124 - acc: 0.9966 - val_loss: 0.1412 - val_acc: 0.9651\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 00068: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 0.1582 - val_acc: 0.9623\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00069: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1540 - val_acc: 0.9658\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9942\n",
      "Epoch 00070: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0179 - acc: 0.9942 - val_loss: 0.1924 - val_acc: 0.9590\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9969\n",
      "Epoch 00071: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0103 - acc: 0.9969 - val_loss: 0.1561 - val_acc: 0.9648\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00072: val_loss did not improve from 0.12072\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1504 - val_acc: 0.9660\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1817 - acc: 0.9475\n",
      "Loss: 0.18166653738276856 Accuracy: 0.9474559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 992us/sample - loss: 1.5551 - acc: 0.5157\n",
      "Loss: 1.5551137644182484 Accuracy: 0.5156802\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.1546 - acc: 0.6474\n",
      "Loss: 1.1546156158328427 Accuracy: 0.64735204\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8536 - acc: 0.7564\n",
      "Loss: 0.853619275348085 Accuracy: 0.7563863\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5095 - acc: 0.8677\n",
      "Loss: 0.5095143529237369 Accuracy: 0.8677051\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2530 - acc: 0.9277\n",
      "Loss: 0.2529973657084155 Accuracy: 0.92772585\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1670 - acc: 0.9487\n",
      "Loss: 0.166974950755868 Accuracy: 0.948702\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1817 - acc: 0.9475\n",
      "Loss: 0.18166653738276856 Accuracy: 0.9474559\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 3.0533 - acc: 0.5653\n",
      "Loss: 3.0532921957944907 Accuracy: 0.56531674\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.8126 - acc: 0.6723\n",
      "Loss: 1.8126494708214718 Accuracy: 0.6722742\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.3754 - acc: 0.7406\n",
      "Loss: 1.3754152800311428 Accuracy: 0.74060225\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6758 - acc: 0.8573\n",
      "Loss: 0.6757808298089175 Accuracy: 0.85732085\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3292 - acc: 0.9227\n",
      "Loss: 0.3291676991964426 Accuracy: 0.9227414\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1866 - acc: 0.9562\n",
      "Loss: 0.18658491095270757 Accuracy: 0.9561786\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2015 - acc: 0.9566\n",
      "Loss: 0.20154722960741256 Accuracy: 0.956594\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
