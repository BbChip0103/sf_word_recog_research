{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO_BN(conv_num=1):\n",
    "    kernel_size = 64\n",
    "    filter_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3*kernel_size, filters=filter_size, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        target_kernel_size = 3 * (kernel_size//(2**(i+1)))\n",
    "        model.add(Conv1D (kernel_size=target_kernel_size if target_kernel_size != 0 else 3, \n",
    "                          filters=filter_size*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 1,063,920\n",
      "Trainable params: 1,063,728\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 481,936\n",
      "Trainable params: 481,680\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 405,456\n",
      "Trainable params: 405,072\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 295,184\n",
      "Trainable params: 294,672\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 262,736\n",
      "Trainable params: 262,096\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 261,008\n",
      "Trainable params: 260,240\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 283,152\n",
      "Trainable params: 282,128\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2231 - acc: 0.3293\n",
      "Epoch 00001: val_loss improved from inf to 1.74101, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/001-1.7410.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 2.2229 - acc: 0.3294 - val_loss: 1.7410 - val_acc: 0.4428\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4168 - acc: 0.5598\n",
      "Epoch 00002: val_loss improved from 1.74101 to 1.52017, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/002-1.5202.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 1.4168 - acc: 0.5597 - val_loss: 1.5202 - val_acc: 0.5274\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1906 - acc: 0.6314\n",
      "Epoch 00003: val_loss improved from 1.52017 to 1.15086, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/003-1.1509.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 1.1906 - acc: 0.6314 - val_loss: 1.1509 - val_acc: 0.6355\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0419 - acc: 0.6791\n",
      "Epoch 00004: val_loss improved from 1.15086 to 1.02449, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/004-1.0245.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 1.0420 - acc: 0.6791 - val_loss: 1.0245 - val_acc: 0.6825\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.7141\n",
      "Epoch 00005: val_loss did not improve from 1.02449\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.9296 - acc: 0.7141 - val_loss: 1.4856 - val_acc: 0.5849\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7437\n",
      "Epoch 00006: val_loss improved from 1.02449 to 0.89854, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/006-0.8985.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.8342 - acc: 0.7437 - val_loss: 0.8985 - val_acc: 0.7345\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.7675\n",
      "Epoch 00007: val_loss did not improve from 0.89854\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.7563 - acc: 0.7675 - val_loss: 0.9099 - val_acc: 0.7379\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6898 - acc: 0.7845\n",
      "Epoch 00008: val_loss did not improve from 0.89854\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.6898 - acc: 0.7845 - val_loss: 0.9278 - val_acc: 0.7296\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.8056\n",
      "Epoch 00009: val_loss did not improve from 0.89854\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.6276 - acc: 0.8056 - val_loss: 0.9418 - val_acc: 0.7296\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.8209\n",
      "Epoch 00010: val_loss did not improve from 0.89854\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.5731 - acc: 0.8209 - val_loss: 0.9492 - val_acc: 0.7317\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8349\n",
      "Epoch 00011: val_loss did not improve from 0.89854\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.5192 - acc: 0.8349 - val_loss: 1.0019 - val_acc: 0.7205\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.8485\n",
      "Epoch 00012: val_loss improved from 0.89854 to 0.80211, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv_checkpoint/012-0.8021.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.4809 - acc: 0.8484 - val_loss: 0.8021 - val_acc: 0.7729\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8619\n",
      "Epoch 00013: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.4370 - acc: 0.8619 - val_loss: 1.0938 - val_acc: 0.7121\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8692\n",
      "Epoch 00014: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.4131 - acc: 0.8692 - val_loss: 0.9510 - val_acc: 0.7454\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8817\n",
      "Epoch 00015: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.3721 - acc: 0.8816 - val_loss: 0.9452 - val_acc: 0.7517\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8887\n",
      "Epoch 00016: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.3514 - acc: 0.8887 - val_loss: 0.8604 - val_acc: 0.7789\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8986\n",
      "Epoch 00017: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.3221 - acc: 0.8986 - val_loss: 0.8585 - val_acc: 0.7773\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9057\n",
      "Epoch 00018: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2923 - acc: 0.9057 - val_loss: 0.9890 - val_acc: 0.7526\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9087\n",
      "Epoch 00019: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2850 - acc: 0.9087 - val_loss: 1.1335 - val_acc: 0.7223\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9133\n",
      "Epoch 00020: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2658 - acc: 0.9133 - val_loss: 0.9671 - val_acc: 0.7612\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9177\n",
      "Epoch 00021: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2494 - acc: 0.9177 - val_loss: 0.9278 - val_acc: 0.7729\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9254\n",
      "Epoch 00022: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2301 - acc: 0.9253 - val_loss: 1.0248 - val_acc: 0.7549\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9215\n",
      "Epoch 00023: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2402 - acc: 0.9215 - val_loss: 1.3873 - val_acc: 0.6867\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9292\n",
      "Epoch 00024: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2156 - acc: 0.9292 - val_loss: 0.9502 - val_acc: 0.7626\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9355\n",
      "Epoch 00025: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2028 - acc: 0.9355 - val_loss: 1.1596 - val_acc: 0.7466\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9313\n",
      "Epoch 00026: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.2158 - acc: 0.9313 - val_loss: 0.8427 - val_acc: 0.7964\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9434\n",
      "Epoch 00027: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1748 - acc: 0.9434 - val_loss: 0.9608 - val_acc: 0.7736\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9453\n",
      "Epoch 00028: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1694 - acc: 0.9453 - val_loss: 0.9148 - val_acc: 0.7934\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9442\n",
      "Epoch 00029: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1710 - acc: 0.9442 - val_loss: 1.2848 - val_acc: 0.7263\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9476\n",
      "Epoch 00030: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1620 - acc: 0.9476 - val_loss: 0.9767 - val_acc: 0.7876\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9503\n",
      "Epoch 00031: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1569 - acc: 0.9503 - val_loss: 0.9292 - val_acc: 0.7885\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9507\n",
      "Epoch 00032: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1537 - acc: 0.9507 - val_loss: 0.9649 - val_acc: 0.7829\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9537\n",
      "Epoch 00033: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1494 - acc: 0.9537 - val_loss: 1.3768 - val_acc: 0.7277\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9541\n",
      "Epoch 00034: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1421 - acc: 0.9541 - val_loss: 1.1410 - val_acc: 0.7554\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9557\n",
      "Epoch 00035: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1373 - acc: 0.9557 - val_loss: 0.9789 - val_acc: 0.7878\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9528\n",
      "Epoch 00036: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1493 - acc: 0.9528 - val_loss: 0.9438 - val_acc: 0.7980\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9588\n",
      "Epoch 00037: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1287 - acc: 0.9588 - val_loss: 1.0431 - val_acc: 0.7873\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9643\n",
      "Epoch 00038: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1137 - acc: 0.9643 - val_loss: 1.1103 - val_acc: 0.7633\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9626\n",
      "Epoch 00039: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1198 - acc: 0.9625 - val_loss: 1.1549 - val_acc: 0.7622\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9608\n",
      "Epoch 00040: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1213 - acc: 0.9608 - val_loss: 1.3978 - val_acc: 0.7179\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9627\n",
      "Epoch 00041: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1194 - acc: 0.9627 - val_loss: 1.0858 - val_acc: 0.7747\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9655\n",
      "Epoch 00042: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1111 - acc: 0.9655 - val_loss: 1.0386 - val_acc: 0.7927\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9629\n",
      "Epoch 00043: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1267 - acc: 0.9629 - val_loss: 1.0389 - val_acc: 0.7862\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9616\n",
      "Epoch 00044: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1249 - acc: 0.9616 - val_loss: 1.2823 - val_acc: 0.7515\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9706\n",
      "Epoch 00045: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0912 - acc: 0.9706 - val_loss: 1.1022 - val_acc: 0.7871\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9709\n",
      "Epoch 00046: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0926 - acc: 0.9709 - val_loss: 1.1690 - val_acc: 0.7640\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9689\n",
      "Epoch 00047: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1007 - acc: 0.9689 - val_loss: 1.2714 - val_acc: 0.7531\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9699\n",
      "Epoch 00048: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0980 - acc: 0.9699 - val_loss: 1.1678 - val_acc: 0.7692\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9645\n",
      "Epoch 00049: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.1189 - acc: 0.9645 - val_loss: 1.3265 - val_acc: 0.7608\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9723\n",
      "Epoch 00050: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0885 - acc: 0.9723 - val_loss: 0.9856 - val_acc: 0.8029\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9726\n",
      "Epoch 00051: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0906 - acc: 0.9725 - val_loss: 1.0038 - val_acc: 0.8060\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9718\n",
      "Epoch 00052: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0920 - acc: 0.9719 - val_loss: 1.1625 - val_acc: 0.7638\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9723\n",
      "Epoch 00053: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0896 - acc: 0.9723 - val_loss: 1.1382 - val_acc: 0.7834\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9719\n",
      "Epoch 00054: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0912 - acc: 0.9719 - val_loss: 1.1217 - val_acc: 0.7941\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9755\n",
      "Epoch 00055: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0818 - acc: 0.9755 - val_loss: 1.1404 - val_acc: 0.7717\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9718\n",
      "Epoch 00056: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0915 - acc: 0.9718 - val_loss: 1.0191 - val_acc: 0.7992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9740\n",
      "Epoch 00057: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0833 - acc: 0.9739 - val_loss: 1.0406 - val_acc: 0.8076\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9724\n",
      "Epoch 00058: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0926 - acc: 0.9724 - val_loss: 1.0593 - val_acc: 0.7932\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9760\n",
      "Epoch 00059: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0774 - acc: 0.9760 - val_loss: 1.1815 - val_acc: 0.7808\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9774\n",
      "Epoch 00060: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0740 - acc: 0.9774 - val_loss: 1.1427 - val_acc: 0.7817\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9773\n",
      "Epoch 00061: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0770 - acc: 0.9773 - val_loss: 1.4215 - val_acc: 0.7498\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9765\n",
      "Epoch 00062: val_loss did not improve from 0.80211\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 0.0789 - acc: 0.9765 - val_loss: 1.0076 - val_acc: 0.8057\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 727us/sample - loss: 0.8692 - acc: 0.7506\n",
      "Loss: 0.8692441447130245 Accuracy: 0.75057113\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2533 - acc: 0.3220\n",
      "Epoch 00001: val_loss improved from inf to 1.47805, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/001-1.4780.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.2533 - acc: 0.3220 - val_loss: 1.4780 - val_acc: 0.5183\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3859 - acc: 0.5664\n",
      "Epoch 00002: val_loss improved from 1.47805 to 1.12839, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/002-1.1284.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.3858 - acc: 0.5664 - val_loss: 1.1284 - val_acc: 0.6713\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1191 - acc: 0.6514\n",
      "Epoch 00003: val_loss improved from 1.12839 to 1.03718, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/003-1.0372.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.1190 - acc: 0.6514 - val_loss: 1.0372 - val_acc: 0.6865\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9551 - acc: 0.7092\n",
      "Epoch 00004: val_loss improved from 1.03718 to 0.92498, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/004-0.9250.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9552 - acc: 0.7092 - val_loss: 0.9250 - val_acc: 0.7167\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7358\n",
      "Epoch 00005: val_loss did not improve from 0.92498\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8635 - acc: 0.7358 - val_loss: 1.2070 - val_acc: 0.6506\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7805 - acc: 0.7617\n",
      "Epoch 00006: val_loss improved from 0.92498 to 0.71978, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/006-0.7198.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7805 - acc: 0.7616 - val_loss: 0.7198 - val_acc: 0.7973\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7103 - acc: 0.7850\n",
      "Epoch 00007: val_loss did not improve from 0.71978\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7104 - acc: 0.7850 - val_loss: 0.7857 - val_acc: 0.7855\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6618 - acc: 0.8005\n",
      "Epoch 00008: val_loss improved from 0.71978 to 0.69267, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/008-0.6927.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6619 - acc: 0.8005 - val_loss: 0.6927 - val_acc: 0.8006\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6154 - acc: 0.8144\n",
      "Epoch 00009: val_loss did not improve from 0.69267\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6154 - acc: 0.8144 - val_loss: 0.7503 - val_acc: 0.7834\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5809 - acc: 0.8252\n",
      "Epoch 00010: val_loss did not improve from 0.69267\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5810 - acc: 0.8252 - val_loss: 0.8825 - val_acc: 0.7538\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8358\n",
      "Epoch 00011: val_loss did not improve from 0.69267\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5466 - acc: 0.8358 - val_loss: 1.1485 - val_acc: 0.6979\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8462\n",
      "Epoch 00012: val_loss improved from 0.69267 to 0.69187, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/012-0.6919.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5092 - acc: 0.8462 - val_loss: 0.6919 - val_acc: 0.8218\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8455\n",
      "Epoch 00013: val_loss improved from 0.69187 to 0.64565, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/013-0.6456.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5050 - acc: 0.8456 - val_loss: 0.6456 - val_acc: 0.8153\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8601\n",
      "Epoch 00014: val_loss did not improve from 0.64565\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4583 - acc: 0.8601 - val_loss: 0.7041 - val_acc: 0.7962\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8682\n",
      "Epoch 00015: val_loss improved from 0.64565 to 0.52691, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/015-0.5269.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4375 - acc: 0.8682 - val_loss: 0.5269 - val_acc: 0.8649\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8731\n",
      "Epoch 00016: val_loss did not improve from 0.52691\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4185 - acc: 0.8731 - val_loss: 0.6560 - val_acc: 0.8239\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8780\n",
      "Epoch 00017: val_loss did not improve from 0.52691\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3990 - acc: 0.8780 - val_loss: 0.5445 - val_acc: 0.8570\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8870\n",
      "Epoch 00018: val_loss did not improve from 0.52691\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3794 - acc: 0.8870 - val_loss: 0.6165 - val_acc: 0.8374\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8855\n",
      "Epoch 00019: val_loss did not improve from 0.52691\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3754 - acc: 0.8855 - val_loss: 0.6714 - val_acc: 0.8125\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8948\n",
      "Epoch 00020: val_loss improved from 0.52691 to 0.50545, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/020-0.5054.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3439 - acc: 0.8947 - val_loss: 0.5054 - val_acc: 0.8658\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8924\n",
      "Epoch 00021: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3522 - acc: 0.8924 - val_loss: 0.5475 - val_acc: 0.8507\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8990\n",
      "Epoch 00022: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3244 - acc: 0.8991 - val_loss: 0.6780 - val_acc: 0.8134\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9019\n",
      "Epoch 00023: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3165 - acc: 0.9019 - val_loss: 0.5283 - val_acc: 0.8544\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9063\n",
      "Epoch 00024: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3036 - acc: 0.9063 - val_loss: 0.7305 - val_acc: 0.8116\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9086\n",
      "Epoch 00025: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.2927 - acc: 0.9086 - val_loss: 0.5081 - val_acc: 0.8700\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9130\n",
      "Epoch 00026: val_loss did not improve from 0.50545\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.2814 - acc: 0.9130 - val_loss: 0.5608 - val_acc: 0.8619\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9147\n",
      "Epoch 00027: val_loss improved from 0.50545 to 0.48753, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/027-0.4875.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.2728 - acc: 0.9147 - val_loss: 0.4875 - val_acc: 0.8710\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9170\n",
      "Epoch 00028: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2673 - acc: 0.9170 - val_loss: 0.6554 - val_acc: 0.8344\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9209\n",
      "Epoch 00029: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2527 - acc: 0.9209 - val_loss: 0.5074 - val_acc: 0.8658\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9216\n",
      "Epoch 00030: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2499 - acc: 0.9216 - val_loss: 0.5786 - val_acc: 0.8505\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9244\n",
      "Epoch 00031: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.2378 - acc: 0.9244 - val_loss: 0.6807 - val_acc: 0.8092\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9265\n",
      "Epoch 00032: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2376 - acc: 0.9265 - val_loss: 0.7532 - val_acc: 0.8092\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9294\n",
      "Epoch 00033: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2241 - acc: 0.9294 - val_loss: 0.5219 - val_acc: 0.8635\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9305\n",
      "Epoch 00034: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2179 - acc: 0.9304 - val_loss: 0.5247 - val_acc: 0.8658\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9324\n",
      "Epoch 00035: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2164 - acc: 0.9324 - val_loss: 0.5147 - val_acc: 0.8672\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9362\n",
      "Epoch 00036: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1999 - acc: 0.9362 - val_loss: 0.6088 - val_acc: 0.8477\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9351\n",
      "Epoch 00037: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2035 - acc: 0.9351 - val_loss: 0.6482 - val_acc: 0.8479\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9380\n",
      "Epoch 00038: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1952 - acc: 0.9380 - val_loss: 0.6405 - val_acc: 0.8442\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9379\n",
      "Epoch 00039: val_loss did not improve from 0.48753\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1919 - acc: 0.9379 - val_loss: 0.7189 - val_acc: 0.8255\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9408\n",
      "Epoch 00040: val_loss improved from 0.48753 to 0.46336, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/040-0.4634.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1874 - acc: 0.9408 - val_loss: 0.4634 - val_acc: 0.8924\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9426\n",
      "Epoch 00041: val_loss did not improve from 0.46336\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1845 - acc: 0.9426 - val_loss: 0.4735 - val_acc: 0.8810\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9432\n",
      "Epoch 00042: val_loss improved from 0.46336 to 0.44988, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv_checkpoint/042-0.4499.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1838 - acc: 0.9432 - val_loss: 0.4499 - val_acc: 0.8824\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9479\n",
      "Epoch 00043: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1674 - acc: 0.9478 - val_loss: 0.6025 - val_acc: 0.8514\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9472\n",
      "Epoch 00044: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1712 - acc: 0.9472 - val_loss: 0.4857 - val_acc: 0.8840\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9511\n",
      "Epoch 00045: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1555 - acc: 0.9511 - val_loss: 0.5723 - val_acc: 0.8516\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9511\n",
      "Epoch 00046: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1598 - acc: 0.9511 - val_loss: 3.0476 - val_acc: 0.5171\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9488\n",
      "Epoch 00047: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1580 - acc: 0.9488 - val_loss: 0.5645 - val_acc: 0.8588\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9491\n",
      "Epoch 00048: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1578 - acc: 0.9491 - val_loss: 0.6981 - val_acc: 0.8365\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9540\n",
      "Epoch 00049: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1441 - acc: 0.9540 - val_loss: 0.5001 - val_acc: 0.8756\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9542\n",
      "Epoch 00050: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1425 - acc: 0.9542 - val_loss: 0.5443 - val_acc: 0.8744\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9540\n",
      "Epoch 00051: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1447 - acc: 0.9539 - val_loss: 0.5039 - val_acc: 0.8812\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9550\n",
      "Epoch 00052: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1422 - acc: 0.9550 - val_loss: 0.4746 - val_acc: 0.8889\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9554\n",
      "Epoch 00053: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1397 - acc: 0.9554 - val_loss: 0.9115 - val_acc: 0.8008\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9593\n",
      "Epoch 00054: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1280 - acc: 0.9594 - val_loss: 0.6205 - val_acc: 0.8586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9604\n",
      "Epoch 00055: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1292 - acc: 0.9604 - val_loss: 0.6248 - val_acc: 0.8463\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9599\n",
      "Epoch 00056: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1283 - acc: 0.9599 - val_loss: 0.7393 - val_acc: 0.8321\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9609\n",
      "Epoch 00057: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1238 - acc: 0.9609 - val_loss: 0.5620 - val_acc: 0.8758\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9610\n",
      "Epoch 00058: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1246 - acc: 0.9610 - val_loss: 1.0554 - val_acc: 0.7885\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9619\n",
      "Epoch 00059: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1222 - acc: 0.9619 - val_loss: 0.7909 - val_acc: 0.8328\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9643\n",
      "Epoch 00060: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1148 - acc: 0.9642 - val_loss: 0.5044 - val_acc: 0.8912\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9621\n",
      "Epoch 00061: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1232 - acc: 0.9622 - val_loss: 0.5897 - val_acc: 0.8563\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9643\n",
      "Epoch 00062: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1135 - acc: 0.9643 - val_loss: 0.6539 - val_acc: 0.8458\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9648\n",
      "Epoch 00063: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1120 - acc: 0.9648 - val_loss: 0.6110 - val_acc: 0.8658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9649\n",
      "Epoch 00064: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1140 - acc: 0.9650 - val_loss: 0.5089 - val_acc: 0.8838\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9650\n",
      "Epoch 00065: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1109 - acc: 0.9650 - val_loss: 0.8116 - val_acc: 0.8139\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9630\n",
      "Epoch 00066: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1185 - acc: 0.9630 - val_loss: 0.5143 - val_acc: 0.8761\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9692\n",
      "Epoch 00067: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1025 - acc: 0.9692 - val_loss: 0.5073 - val_acc: 0.8758\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9660\n",
      "Epoch 00068: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1064 - acc: 0.9660 - val_loss: 0.5338 - val_acc: 0.8861\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9677\n",
      "Epoch 00069: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1028 - acc: 0.9677 - val_loss: 0.5697 - val_acc: 0.8800\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9678\n",
      "Epoch 00070: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1006 - acc: 0.9678 - val_loss: 0.5429 - val_acc: 0.8826\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9692\n",
      "Epoch 00071: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0994 - acc: 0.9692 - val_loss: 0.4792 - val_acc: 0.8915\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9685\n",
      "Epoch 00072: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1009 - acc: 0.9685 - val_loss: 0.7393 - val_acc: 0.8362\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9654\n",
      "Epoch 00073: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1083 - acc: 0.9654 - val_loss: 0.6539 - val_acc: 0.8689\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9719\n",
      "Epoch 00074: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0894 - acc: 0.9719 - val_loss: 0.6288 - val_acc: 0.8665\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9715\n",
      "Epoch 00075: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0903 - acc: 0.9715 - val_loss: 0.4692 - val_acc: 0.8975\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9695\n",
      "Epoch 00076: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0965 - acc: 0.9695 - val_loss: 0.5487 - val_acc: 0.8817\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9723\n",
      "Epoch 00077: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0887 - acc: 0.9723 - val_loss: 0.6857 - val_acc: 0.8544\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9746\n",
      "Epoch 00078: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0823 - acc: 0.9746 - val_loss: 0.7888 - val_acc: 0.8423\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9730\n",
      "Epoch 00079: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0857 - acc: 0.9730 - val_loss: 0.6696 - val_acc: 0.8616\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9707\n",
      "Epoch 00080: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0940 - acc: 0.9707 - val_loss: 0.5202 - val_acc: 0.8952\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9714\n",
      "Epoch 00081: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0866 - acc: 0.9714 - val_loss: 0.6613 - val_acc: 0.8672\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9735\n",
      "Epoch 00082: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0873 - acc: 0.9735 - val_loss: 0.8138 - val_acc: 0.8276\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9706\n",
      "Epoch 00083: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0940 - acc: 0.9705 - val_loss: 0.6150 - val_acc: 0.8658\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9693\n",
      "Epoch 00084: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0979 - acc: 0.9693 - val_loss: 0.5366 - val_acc: 0.8873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9761\n",
      "Epoch 00085: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0759 - acc: 0.9761 - val_loss: 0.5659 - val_acc: 0.8877\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9755\n",
      "Epoch 00086: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0762 - acc: 0.9754 - val_loss: 0.6548 - val_acc: 0.8710\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9723\n",
      "Epoch 00087: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0874 - acc: 0.9723 - val_loss: 0.4902 - val_acc: 0.8940\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9760\n",
      "Epoch 00088: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0770 - acc: 0.9760 - val_loss: 0.5057 - val_acc: 0.8898\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9755\n",
      "Epoch 00089: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0800 - acc: 0.9755 - val_loss: 1.3741 - val_acc: 0.7587\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9733\n",
      "Epoch 00090: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0874 - acc: 0.9732 - val_loss: 0.6447 - val_acc: 0.8640\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9754\n",
      "Epoch 00091: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0767 - acc: 0.9754 - val_loss: 0.7446 - val_acc: 0.8528\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9793\n",
      "Epoch 00092: val_loss did not improve from 0.44988\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0699 - acc: 0.9793 - val_loss: 0.6699 - val_acc: 0.8628\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 740us/sample - loss: 0.5532 - acc: 0.8424\n",
      "Loss: 0.5531512016324116 Accuracy: 0.8423676\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2263 - acc: 0.3166\n",
      "Epoch 00001: val_loss improved from inf to 1.46377, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/001-1.4638.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.2262 - acc: 0.3166 - val_loss: 1.4638 - val_acc: 0.5271\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4200 - acc: 0.5439\n",
      "Epoch 00002: val_loss improved from 1.46377 to 1.05125, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/002-1.0512.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.4200 - acc: 0.5439 - val_loss: 1.0512 - val_acc: 0.6683\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1422 - acc: 0.6415\n",
      "Epoch 00003: val_loss improved from 1.05125 to 1.02266, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/003-1.0227.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.1422 - acc: 0.6415 - val_loss: 1.0227 - val_acc: 0.6865\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9595 - acc: 0.7018\n",
      "Epoch 00004: val_loss improved from 1.02266 to 0.89654, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/004-0.8965.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9596 - acc: 0.7018 - val_loss: 0.8965 - val_acc: 0.7379\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8287 - acc: 0.7474\n",
      "Epoch 00005: val_loss did not improve from 0.89654\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8286 - acc: 0.7475 - val_loss: 0.9040 - val_acc: 0.7356\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7310 - acc: 0.7815\n",
      "Epoch 00006: val_loss improved from 0.89654 to 0.84235, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/006-0.8423.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7309 - acc: 0.7815 - val_loss: 0.8423 - val_acc: 0.7482\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6487 - acc: 0.8048\n",
      "Epoch 00007: val_loss improved from 0.84235 to 0.81000, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/007-0.8100.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6488 - acc: 0.8048 - val_loss: 0.8100 - val_acc: 0.7643\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8233\n",
      "Epoch 00008: val_loss improved from 0.81000 to 0.66555, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/008-0.6656.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5906 - acc: 0.8233 - val_loss: 0.6656 - val_acc: 0.8104\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.8359\n",
      "Epoch 00009: val_loss improved from 0.66555 to 0.53757, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/009-0.5376.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5453 - acc: 0.8359 - val_loss: 0.5376 - val_acc: 0.8493\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8504\n",
      "Epoch 00010: val_loss did not improve from 0.53757\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4969 - acc: 0.8504 - val_loss: 0.5634 - val_acc: 0.8323\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8619\n",
      "Epoch 00011: val_loss did not improve from 0.53757\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4624 - acc: 0.8619 - val_loss: 0.6425 - val_acc: 0.8262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8669\n",
      "Epoch 00012: val_loss did not improve from 0.53757\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4415 - acc: 0.8669 - val_loss: 0.6223 - val_acc: 0.8330\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8733\n",
      "Epoch 00013: val_loss improved from 0.53757 to 0.52990, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/013-0.5299.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4162 - acc: 0.8732 - val_loss: 0.5299 - val_acc: 0.8498\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8806\n",
      "Epoch 00014: val_loss improved from 0.52990 to 0.46617, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/014-0.4662.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3877 - acc: 0.8806 - val_loss: 0.4662 - val_acc: 0.8721\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8866\n",
      "Epoch 00015: val_loss improved from 0.46617 to 0.42666, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/015-0.4267.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3728 - acc: 0.8866 - val_loss: 0.4267 - val_acc: 0.8803\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8909\n",
      "Epoch 00016: val_loss did not improve from 0.42666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3578 - acc: 0.8909 - val_loss: 0.4468 - val_acc: 0.8742\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8984\n",
      "Epoch 00017: val_loss improved from 0.42666 to 0.39487, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/017-0.3949.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3329 - acc: 0.8984 - val_loss: 0.3949 - val_acc: 0.8954\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9011\n",
      "Epoch 00018: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3223 - acc: 0.9012 - val_loss: 0.7521 - val_acc: 0.7843\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9062\n",
      "Epoch 00019: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3087 - acc: 0.9062 - val_loss: 0.6238 - val_acc: 0.8393\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9100\n",
      "Epoch 00020: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2942 - acc: 0.9100 - val_loss: 0.4601 - val_acc: 0.8807\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9133\n",
      "Epoch 00021: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2810 - acc: 0.9133 - val_loss: 0.5783 - val_acc: 0.8484\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9144\n",
      "Epoch 00022: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2758 - acc: 0.9144 - val_loss: 0.4450 - val_acc: 0.8800\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9162\n",
      "Epoch 00023: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2694 - acc: 0.9162 - val_loss: 0.5022 - val_acc: 0.8595\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9214\n",
      "Epoch 00024: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2530 - acc: 0.9214 - val_loss: 0.5105 - val_acc: 0.8770\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9234\n",
      "Epoch 00025: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2439 - acc: 0.9234 - val_loss: 0.4021 - val_acc: 0.8980\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9256\n",
      "Epoch 00026: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2357 - acc: 0.9256 - val_loss: 0.5561 - val_acc: 0.8549\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9230\n",
      "Epoch 00027: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2432 - acc: 0.9231 - val_loss: 0.4099 - val_acc: 0.8980\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9275\n",
      "Epoch 00028: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2259 - acc: 0.9275 - val_loss: 0.4725 - val_acc: 0.8873\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9310\n",
      "Epoch 00029: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2155 - acc: 0.9310 - val_loss: 0.5002 - val_acc: 0.8724\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9321\n",
      "Epoch 00030: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2135 - acc: 0.9321 - val_loss: 0.4190 - val_acc: 0.8928\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9343\n",
      "Epoch 00031: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2023 - acc: 0.9343 - val_loss: 0.3953 - val_acc: 0.9003\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9365\n",
      "Epoch 00032: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1994 - acc: 0.9366 - val_loss: 0.3989 - val_acc: 0.9005\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9373\n",
      "Epoch 00033: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1972 - acc: 0.9373 - val_loss: 0.3999 - val_acc: 0.8952\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9399\n",
      "Epoch 00034: val_loss did not improve from 0.39487\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1875 - acc: 0.9399 - val_loss: 0.4586 - val_acc: 0.8686\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9413\n",
      "Epoch 00035: val_loss improved from 0.39487 to 0.36981, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/035-0.3698.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1800 - acc: 0.9413 - val_loss: 0.3698 - val_acc: 0.9012\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9421\n",
      "Epoch 00036: val_loss did not improve from 0.36981\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1797 - acc: 0.9421 - val_loss: 0.6168 - val_acc: 0.8344\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9443\n",
      "Epoch 00037: val_loss did not improve from 0.36981\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1747 - acc: 0.9443 - val_loss: 0.6457 - val_acc: 0.8425\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9414\n",
      "Epoch 00038: val_loss did not improve from 0.36981\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1819 - acc: 0.9413 - val_loss: 0.4738 - val_acc: 0.8782\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9479\n",
      "Epoch 00039: val_loss improved from 0.36981 to 0.36688, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv_checkpoint/039-0.3669.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1618 - acc: 0.9478 - val_loss: 0.3669 - val_acc: 0.9033\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9488\n",
      "Epoch 00040: val_loss did not improve from 0.36688\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1615 - acc: 0.9488 - val_loss: 0.7367 - val_acc: 0.8283\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9500\n",
      "Epoch 00041: val_loss did not improve from 0.36688\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1511 - acc: 0.9500 - val_loss: 0.4907 - val_acc: 0.8777\n",
      "Epoch 42/500\n",
      "29248/36805 [======================>.......] - ETA: 16s - loss: 0.1502 - acc: 0.9513"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_kernel_192_ch_32_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 1,063,920\n",
      "Trainable params: 1,063,728\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 933us/sample - loss: 0.8692 - acc: 0.7506\n",
      "Loss: 0.8692441447130245 Accuracy: 0.75057113\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 481,936\n",
      "Trainable params: 481,680\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 965us/sample - loss: 0.5532 - acc: 0.8424\n",
      "Loss: 0.5531512016324116 Accuracy: 0.8423676\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 405,456\n",
      "Trainable params: 405,072\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 997us/sample - loss: 0.4510 - acc: 0.8856\n",
      "Loss: 0.45099152171846985 Accuracy: 0.88556594\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 295,184\n",
      "Trainable params: 294,672\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2946 - acc: 0.9302\n",
      "Loss: 0.29462155802011 Accuracy: 0.93021804\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 262,736\n",
      "Trainable params: 262,096\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2384 - acc: 0.9385\n",
      "Loss: 0.23838554100091894 Accuracy: 0.93852544\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 261,008\n",
      "Trainable params: 260,240\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1890 - acc: 0.9468\n",
      "Loss: 0.1889980023461834 Accuracy: 0.94683284\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 283,152\n",
      "Trainable params: 282,128\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1922 - acc: 0.9398\n",
      "Loss: 0.1922173076971731 Accuracy: 0.93977153\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_kernel_192_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
