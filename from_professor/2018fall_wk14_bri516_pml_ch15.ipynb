{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n",
    "\n",
    "Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n",
    "\n",
    "Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n",
    "\n",
    "Prepared by JH Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Classifying Images with Deep Convolutional Neural Networks\n",
    "\n",
    "\n",
    "In this chapter, we'll learn about __Convolutional Neural Networks (CNNs)__ and how to implement CNNs in TensorFlow (TF).\n",
    "\n",
    "* Understanding convolution operations in one and two dimensions\n",
    "* Learning about the building blocks of CNN architectures\n",
    "* Implementing deep CNNs in TF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Building blocks of convolutional neural networks](#Building-blocks-of-convolutional-neural-networks)\n",
    "  - [Understanding CNNs and learning feature hierarchies](#Understanding-CNNs-and-learning-feature-hierarchies)\n",
    "  - [Performing discrete convolutions](#Performing-discrete-convolutions)\n",
    "    - [Performing a discrete convolution in one dimension](#Performing-a-discrete-convolution-in-one-dimension)\n",
    "    - [The effect of zero-padding in convolution](#The-effect-of-zero-padding-in-convolution)\n",
    "    - [Determining the size of the convolution output](#Determining-the-size-of-the-convolution-output)\n",
    "    - [Performing a discrete convolution in 2D](#Performing-a-discrete-convolution-in-2D)\n",
    "    - [Sub-sampling](#Sub-sampling)\n",
    "  - [Putting everything together to build a CNN](#Putting-everything-together-to-build-a-CNN)\n",
    "    - [The multilayer CNN architecture](#The-multilayer-CNN-architecture)\n",
    "    - [Loading and preprocessing the data](#Loading-and-preprocessing-the-data)\n",
    "    - [Implementing a CNN in TensorFlow low-level API](#Implementing-a-CNN-in-TensorFlow-low-level-API)\n",
    "    - [Implementing a CNN in the TensorFlow layers API](#Implementing-a-CNN-in-the-TensorFlow-layers-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Python 2.7 compatibility\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of convolutional neural networks \n",
    "\n",
    "\n",
    "Convolutional neural networks, or (CNNs) are a family of models that were inspired by how the visual cortex of human brain works when recognizing objects.\n",
    "* Yann LeCun and his colleagues propoesd a novel neural network architecture for classifying handwritten digits from images (Handwritten Digit Recognition with a Back-Propagation Network, Y LeCun, and others, 1989, published at Neural Information Processing Systems.(NIPS) conference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CNNs and learning feature hierarchies\n",
    "\n",
    "Successfully extracting __salient (relevant) features__ is key to the performance of any machine learning algorithm.\n",
    "* traditional machine learning models rely on input features that may come from a domain expert, or are based on computational feature extraction techniques\n",
    "* neural networks are able to automatically learn the features from raw data that are most useful for a particular task\n",
    "    * for this reason, it's common to consider a neural network as a feature extraction engine: the early layers (those right after the input layer) extract __low-level features__ (e.g., edges and blobs) \n",
    "    * high-level features (e.g., object shapes like a building, a car, or a dog)\n",
    "* multilayer neural networks and deep CNNs construct a so-called __feature hierarchy__ by combining the low-level features in a layer-wise fashion to form high-level features\n",
    "\n",
    "A CNN computes __feature maps__ from an input image, where each element comes from a local patch of pixels in the input image:\n",
    "\n",
    "<img src='images/15_01.png' width=700> \n",
    "\n",
    "This local patch of pixels is referred to as the __local receptive field__. \n",
    "\n",
    "CNNs will usually perform very well for image-related tasks largely due to two important ideas:\n",
    "* __Sparse-connectivity__: a single element in the feature map is connected to only a small patch of pixels (cf. connecting to the whole input image)\n",
    "* __Parmeter-sharing__: the same weights are used for different patches of the input image\n",
    "\n",
    "The number of weights (parameters) in the network decreases dramatically and ability to capture __salient__ feaures is improved. \n",
    "\n",
    "Typically, CNNs are composed of several __Convolutional (conv)__ layers and subsampling (a.k.a. __Pooling (P)__) layers that are followed by one or more __Fully Connected (FC)__ layers at the end.\n",
    "* there is no weights or bias in the pooling layers, however, both convolution and fully connected layers have weights and biases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing discrete convolutions  \n",
    "\n",
    "A __discrete convolution__ (or simply __convolution__) is a fundamental operation in a CNN.\n",
    "* in this section, the mathematical definition and discuss some of the __naive__ algorithms to compute convolutions of two one-dimensional vectors or two two-dimensional matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Performing a discrete convolution in one dimension\n",
    "\n",
    "A discrete convolution is defined as follows:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x} * \\mathbf{w} \\longrightarrow y[i] = \\sum^{+\\infty}_{k = -\\infty} x[i-k] w[k] $$\n",
    "\n",
    "In practical situations, the bounded feature vector $\\mathbf{x}$ is padded with a finite number of zeros, which is called a __zero-padding__ or simply __padding__.\n",
    "\n",
    "The number of zeros padded on each side is denoted by $p$. \n",
    "e.g.,\n",
    "<img src='images/15_02.png' width=700>\n",
    "\n",
    "Let's assume that the original input $\\mathbf{x}$ and filter $\\mathbf{w}$ have $n$ and $m$ elements, respectively, where $m \\le n$. Thus, the padded vector $\\mathbf{x}^p$ has size $n+2p$. \n",
    "\n",
    "The practical formula for computing a discrete convolution will change to the following:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x} * \\mathbf{w} \\longrightarrow y[i] = \\sum^{k=m-1}_{k=0} x^p [i+m-k] w[k] $$\n",
    "\n",
    "Let's assume we flip the filter $\\mathbf{w}$ to get the rotated filter $\\mathbf{w}^r$. Then, the dot product $\\mathbf{x}[i:i+m] \\cdot \\mathbf{w}^r$ is computed to get one element $\\mathbf{y}[i]$, where $\\mathbf{x} [i:i+m]$ is a patch of $\\mathbf{x}$ with size $m$.\n",
    "\n",
    "This operation is repeated like in a sliding window approach to get all the output elements, as exemplified below:\n",
    "\n",
    "<img src='images/15_03.png' width=700> \n",
    "* padding size is zero ($p=0$)\n",
    "* __shift__ of the rotated filter $\\mathbf{w}^r$ is another hyperparameter of a convolution, the __stride__ $s$ (e.g., $s = 2$ in this example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of zero-padding in a convolution\n",
    "\n",
    "Depending on the choice of $p$, boundary cells may be treated differently than the cells located in the middle of $\\mathbf{x}$.\n",
    "e.g., an example where $n = 5, m = 3$. Then, $p = 0$, $\\mathbf{x}[0]$ is only used in computing one output element (i.e., y[0]), whereas $\\mathbf{x}[1]$ is used in the computation of two output elements (i.e., y[0] and y[1]).\n",
    "* put more emphsis on x[2] than x[1] \n",
    "\n",
    "The size of the output $\\mathbf{y}$ also depends on the choice of the padding strategy we use. \n",
    "\n",
    "Three modes of padding that are commonly used in practice: __full__, __same__, and __valid__\n",
    "* in the __full__ mode, $p = m - 1$. increased the dimension of the output, and thus rarely used in CNN architecture\n",
    "* in the __same__ padding, the size of the output is the same as the size of the input vector $\\mathbf{x}$\n",
    "* in the __valid__ mode, $p = 0$ (no padding)\n",
    "\n",
    "In the following figure, the three different padding modes were examplified for a simple $5 \\times 5$ pixel input with a kernel size of $3 \\times 3$ and a stride of 1:\n",
    "\n",
    "<img src='images/15_11.png' width=700> \n",
    "* __same__ padding is commonly used in CNN since the height and width of the input images or tensors are preserved and designing a network architecture more convenient\n",
    "* __full__ padding is usually used in signal processing applications where it is important to minimize boundary effects, however, in deep learning context, bounday effect is not usually an issue, so full padding is rarely seen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the size of the convolution output\n",
    "\n",
    "The output of a convolution is determined by the total number of times that we shift the filter $\\mathbf{w}$ along the input vector. The size of the output resulting from $\\mathbf{x} * \\mathbf{w}$ with padding $p$ and stride $s$ is determined as follows:\n",
    "\n",
    "$$ o = \\left\\lfloor \\frac{n + 2p - m}{s} + 1 \\right\\rfloor, $$\n",
    "where $\\lfloor \\cdot \\rfloor$ denotes the floor operation.\n",
    "\n",
    "\n",
    "Finally, a naive implementation is shown in the following code block, and the results are compared with the `numpy.convolve` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation:  [  5.  14.  16.  26.  24.  34.  19.  22.]\n",
      "Numpy Results:          [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s),s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "print('Conv1d Implementation: ', \n",
    "      conv1d(x, w, p=2, s=1))\n",
    "print('Numpy Results:         ', \n",
    "      np.convolve(x, w, mode='same'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a discrete convolution in 2D\n",
    "\n",
    "The concepts in the previous sections are easily extendible to two dimensions.\n",
    "\n",
    "When we deal with 2D input, such as a matrix $\\mathbf{X}_{n_1 \\times n_2}$ and the filter matrix $\\mathbf{W}_{m_1 \\times m_2}$, where $m_1 \\le n_1$ and $m_2 \\le n_2$, then the matrix $\\mathbf{Y} = \\mathbf{X} * \\mathbf{W}$ is the result of 2D convolution of $\\mathbf{X}$ with $\\mathbf{W}$ defined as follows:\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{X} * \\mathbf{W} \\longrightarrow \\mathbf{Y}[i,j] = \\sum^{+\\infty}_{k_1=-\\infty} \\sum^{+\\infty}_{k_2=-\\infty} \\mathbf{X}[i-k_1, j-k_2] \\mathbf{W}[k_1, k_2] $$\n",
    "\n",
    "The following example illustrates the computation of a 2D convolution between an input matrix $\\mathbf{X}_{3\\times3}$, a kernel matrix $\\mathbf{W}_{3\\times3}$, a padding $p=(1,1)$, and stride $s=(2,2)$.\n",
    "\n",
    "First, the zero padded matrix $\\mathbf{X}^{padded}_{5\\times5}$:\n",
    "\n",
    "<img src='images/15_04.png' width=500> \n",
    "\n",
    "The rotated filter will be:\n",
    "\n",
    "$$ \\mathbf{W}^r = \\begin{bmatrix} 0.5 & 1 & 0.5 \\\\ 0.1 & 0.4 & 0.3 \\\\ 0.4 & 0.7 & 0.5 \\end{bmatrix} $$\n",
    "\n",
    "Next, shifting the rotated filter matrix along the padded input matrix $\\mathbf{X}^{padded}$ like a sliding window and compute the sum of the element-wise product ($\\odot$):\n",
    "\n",
    "<img src='images/15_05.png' width=700> \n",
    "\n",
    "Let's implement 2D convolution according to the naive algorithm described. \n",
    "\n",
    "The `scipy.signal` package provides a way to compute 2D convolution via the `scipy.signal.convolve2d` function:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation: \n",
      " [[ 11.  25.  32.  13.]\n",
      " [ 19.  25.  24.  13.]\n",
      " [ 13.  28.  25.  17.]\n",
      " [ 11.  17.  14.   9.]]\n",
      "Scipy Results:         \n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def conv2d(X, W, p=(0,0), s=(1,1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1,n2))\n",
    "    X_padded[p[0]:p[0] + X_orig.shape[0], \n",
    "             p[1]:p[1] + X_orig.shape[1]] = X_orig\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - \n",
    "                           W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - \n",
    "                               W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return(np.array(res))\n",
    "    \n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1 , 2,0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "print('Conv2d Implementation: \\n', \n",
    "      conv2d(X, W, p=(1,1), s=(1,1)))\n",
    "\n",
    "print('Scipy Results:         \\n', \n",
    "      scipy.signal.convolve2d(X, W, mode='same'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "Subsampling is typically applied in two forms of pooling operations in CNNs: __max-pooling__ and __mean-pooling__ (a.k.a. __average-pooling__).\n",
    "\n",
    "The pooling layer is usually denoted by $\\mathbf{P}_{n_1 \\times n_2}$, where the subscript determines the size of the neighborhood that the max or mean operation is performed (i.e., __pooling size__).\n",
    "\n",
    "The operation is described in the following figure:\n",
    "\n",
    "<img src='images/15_06.png' width=500> \n",
    "\n",
    "The advantage of pooling is twofold:\n",
    "* introduces some sort of local invariance\n",
    "    * small changes in a local neighborhood do not change the result of max-pooling\n",
    "    * helps to generate features more robust to noise in the input data\n",
    "    <img src='images/15_X1X2.png' width=500>\n",
    "* decrease the size of features\n",
    "    * results in higher computational efficiency\n",
    "    * reduce the number of features and may reduce the degree of overfitting as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together to build a CNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with multiple input or color channels\n",
    "\n",
    "\n",
    "An input sample to a convolutional layer may contain one or more 2D arrays or matrices with dimensions $N_1 \\times N_2$ (e.g., the image height and width in pixels).\n",
    "\n",
    "These $N_1 \\times N_2$ matrices are called __channels__. Therefore, rank-3 tensors or 3D array, $\\mathbf{X}_{N_1 \\times N_2 \\times C_{in}}$, where $C_{in}$ is the number of input channels, is required as input to convolutional layer (e.g., $C_{in} = 3$ for color image, $C_{in} = 1$ for grayscale image).\n",
    "\n",
    "The convolution operation is performed for each channel separately and then add the results together using the matrix summation.\n",
    "\n",
    "The total pre-activation result is computed in the following formula:\n",
    "\n",
    "<img src='images/15_FM_Cin.png' width=500>\n",
    "* the final result, $\\mathbf{h}$ is called a __feature map__\n",
    "\n",
    "If we use multiple feature maps, the kernel tensor becomes 4D: $width \\times height \\times C_{in} \\times C_{out}$.\n",
    "\n",
    "Now, let's include the number of output feature maps:\n",
    "\n",
    "<img src='images/15_FM_CinCout.png' width=500>\n",
    "\n",
    "To conclude our discussion of computing convolutions in the context of neural networks, let's look at the example in the following figure that shows a convolutional layer followed by a pooling layer:\n",
    "\n",
    "<img src='images/15_07.png' width=800> \n",
    "\n",
    "* three input channels\n",
    "* kernel tensor is 4D ($m_1 \\times m_2$ for each kernel), and there are three of them, one for each input channel\n",
    "* five such kernels, accounting for five output feature maps\n",
    "* finally, a polling layer for subsampling the feature maps\n",
    "\n",
    "Q: how many trainable parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing a neural network with dropout\n",
    "\n",
    "The __capacity__ of a network refer to the level of complexity of the function that can be learned.\n",
    "* small networks with a relatively small number of parameters, have a low capacity and are therefore likely to be __underfit__\n",
    "* very large networks may more easily result in __overfitting__\n",
    "    * memorize the training data and do extremely well on the training set while achieving poor performance on the held-out test set\n",
    "    \n",
    "One way to address this problem is to build a network with a relatively large capacity (in practice, that is slightly larger than necessary) to do well on the training set\n",
    "* then, one or multiple regularization schemes to achieve good generalization performance on new data such as the held-out test set (e.g., L2 regularization) \n",
    "\n",
    "In recent years, another popular regularization technique called __dropout__ has emerged.\n",
    "* intuitively, dropout can be considered as the consensus (averaging) of an ensemble of models \n",
    "* usually applied to the hidden units of higher layers\n",
    "* during the training phase of a neural network, a fraction of the hidden units is __randomly__ dropped at every iteration with probability $p_{drop}$ (or the keep probability $p_{keep} = 1 - p_{drop}$)\n",
    "    * when dropping a certain fraction of input neurons, the weights associated with the remaining neurons are rescaled to account for the missing (dropped) neurons\n",
    "    * the network cannot rely on an activation of any set of hidden units since they may be turned off at any time during trianing and is forced to learn more general and robust patterns from the data\n",
    " \n",
    "<img src='images/15_08.png' width=800> \n",
    "* to ensure that the overall activations are on the same scale during training and prediction, the activations of the active neurons have to be scaled appropriately (e.g., by halving the activation if the dropout probability was set to 0.5)\n",
    "    * in practice, TF and other tools scale the activations during training (e.g., by doubling the activations if the dropout probability was set to p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep convolutional neural network using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multilayer CNN architecture \n",
    "\n",
    "The architecture of the network to implement is shown in the following figure:\n",
    "\n",
    "<img src='images/15_09.png' width=800> \n",
    "* the input is $28 \\times 28$ grayscale image\n",
    "    * input tensor's dimension will be $batchsize \\times 28 \\times 28 \\times 1$\n",
    "* a kernel size is $5\\times5$ and 32 output feature maps from the first convolution layer and 64 output feature maps from the second convolution layer\n",
    "* a subsampling layer in the form of a max-pooloing operation is following\n",
    "* then, a fully-connected layer passes the output to a second fully-connected layer (which is a _softmax_ output layer)\n",
    "\n",
    "The dimensions of the tensors in each layer are as follows:\n",
    "* __Input__: [$batchsize \\times 28 \\times 28 \\times 1$]\n",
    "* __Conv_1__: [$batchsize \\times 24 \\times 24 \\times 32$]\n",
    "* __Pooling_1__: [$batchsize \\times 12 \\times 12 \\times 32$]\n",
    "* __Conv_2__: [$batchsize \\times 8 \\times 8 \\times 64$]\n",
    "* __Pooling_2__: [$batchsize \\times 4 \\times 4 \\times 64$]\n",
    "* __FC_1__: [$batchsize \\times 1024$]\n",
    "* __FC_2 and softmax layer__: [$batchsize \\times 10$]\n",
    "\n",
    "This network will be implemented using two APIs:\n",
    "* the low-level TF API\n",
    "* the TF Layers API\n",
    "\n",
    "First, let's define some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "The `load_mnist` function to read the MNIST handwritten digit dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzips mnist\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "if (sys.version_info > (3, 0)):\n",
    "    writemode = 'wb'\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir('./')\n",
    "                if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode='rb') as decompressed, open(z[:-3], writemode) as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n",
      "Training:    (50000, 784) (50000,)\n",
      "Validation:  (10000, 784) (10000,)\n",
      "Test Set:    (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                                % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_data, y_data = load_mnist('./', kind='train')\n",
    "print('Rows: %d,  Columns: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train, y_train = X_data[:50000,:], y_data[:50000]\n",
    "X_valid, y_valid = X_data[50000:,:], y_data[50000:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Set:   ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function for iterating through mini-batches of data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=64, \n",
    "                    shuffle=False, random_seed=None):\n",
    "    \n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this function returns a generator with a tuple for a match of samples\n",
    "    * data $X$ and lables $y$\n",
    "\n",
    "Then, we need to normalize the data (mean centering and division by the standard deviation) for better training performance and convergence.\n",
    "* compute standard deviation from the `X_train` array using `np.std` without specifying an `axis` argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_valid_centered = (X_valid - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_data, y_data, X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in TensorFlow low-level API\n",
    "\n",
    "To implement a CNN in a TF, first we define two wrapper functions to make the process of building the network simpler.\n",
    "* a wrapper function for convolutional layer\n",
    "* a wrapper function for a fully connected layer\n",
    "\n",
    "First, a convolution layer is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/net_pre-activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## wrapper functions \n",
    "\n",
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels, \n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ##   input tensor shape: \n",
    "        ##   [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1] \n",
    "\n",
    "        weights_shape = (list(kernel_size) + \n",
    "                         [n_input_channels, n_output_channels])\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_channels]))\n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input=input_tensor, \n",
    "                            filter=weights,\n",
    "                            strides=strides, \n",
    "                            padding=padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases, \n",
    "                              name='net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name='activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "\n",
    "## testing\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name='convtest', kernel_size=(3, 3), n_output_channels=32)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* defining the weights, biases, initializing them, and the convolution operation using the `tf.nn.conv2d` function with the following required arguments\n",
    "    * `input_tensor`: the tensor given as input to the convolutional layer\n",
    "    * `name`: the name of the layer, which is used as the scope name\n",
    "    * `kernel_size`: the dimensions of the kernel tensor provided as a tuple or list\n",
    "    * `n_output_channels`: the number of output feature maps\n",
    "* this function was tested with a simple input by defining a placeholder\n",
    "\n",
    "<br>\n",
    "\n",
    "The next wrapper function is for defining the fully connected layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def fc_layer(input_tensor, name, \n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor, \n",
    "                                      shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                              name='net_pre-activation')\n",
    "        print(layer)\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        \n",
    "        layer = activation_fn(layer, name='activation')\n",
    "        print(layer)\n",
    "        return layer\n",
    "\n",
    "    \n",
    "## testing:\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, \n",
    "                       shape=[None, 28, 28, 1])\n",
    "    fc_layer(x, name='fctest', n_output_units=32, \n",
    "             activation_fn=tf.nn.relu)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fc1/_weights:0' shape=(784, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc1/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc1/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc1/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc1/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "<tf.Variable 'fc2/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc2/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc2/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc2/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc2/activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    fc_layer1 = fc_layer(x, name='fc1', n_output_units=1024, activation_fn=tf.nn.relu)\n",
    "    fc_layer2 = fc_layer(fc_layer1, name='fc2', n_output_units=10, activation_fn=tf.nn.sigmoid)\n",
    "    \n",
    "del g2, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* builds the weights and biases, initializes them similar to the `conv_layer` function, and then perform a matrix multiplication using the `tf.matmul` function\n",
    "* the requied arguments are\n",
    "    * `input_tensor`: the input tensor\n",
    "    * `name`: the name of the layer, a scope name\n",
    "    * `n_output_units`: the number of output units\n",
    "* the `fc_layer` function was tested for a simple input tensor \n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we can utilize these wrapper functions to build the whole convolutional network.\n",
    "* define a function called `build_cnn` to handle the building of the CNN model as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, 784],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    h1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5), \n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling\n",
    "    h1_pool = tf.nn.max_pool(h1, \n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], \n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    h2 = conv_layer(h1_pool, name='conv_2', \n",
    "                    kernel_size=(5,5), \n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    h2_pool = tf.nn.max_pool(h2, \n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], \n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Fully Connected\n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    h3 = fc_layer(h2_pool, name='fc_3',\n",
    "                  n_output_units=1024, \n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, \n",
    "                            name='dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    h4 = fc_layer(h3_drop, name='fc_4',\n",
    "                  n_output_units=10, \n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(h4, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(h4, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "    \n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=h4, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'], \n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n",
    "\n",
    "    \n",
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' % path)\n",
    "    saver.save(sess, os.path.join(path,'cnn-model.ckpt'),\n",
    "               global_step=epoch)\n",
    "\n",
    "    \n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "            path, 'cnn-model.ckpt-%d' % epoch))\n",
    "\n",
    "    \n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=0.5, random_seed=None):\n",
    "\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "\n",
    "    ## initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed) # for shuflling in batch_generator\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(\n",
    "                        X_data, y_data, \n",
    "                        shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0': batch_x, \n",
    "                    'tf_y:0': batch_y, \n",
    "                    'fc_keep_prob:0': dropout}\n",
    "            loss, _ = sess.run(\n",
    "                    ['cross_entropy_loss:0', 'train_op'],\n",
    "                    feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "\n",
    "        training_loss.append(avg_loss / (i+1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f' % (\n",
    "            epoch, avg_loss), end=' ')\n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0': validation_set[0],\n",
    "                    'tf_y:0': validation_set[1],\n",
    "                    'fc_keep_prob:0':1.0}\n",
    "            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "            \n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_x:0': X_test, \n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the following figure shows the TF graph related to our multilayer CNN as visualized by TensorBoard:\n",
    "<img src='images/15_10.png' width=800> \n",
    "* also, defined four other functions\n",
    "    * `save`, `load`, `train` for training the model using `training_set`, and `predict` to get prediction probabilities or prediction labels of the test data\n",
    "\n",
    "Now, we can create a TF graph object, set the graph-level random seed, and build the CNN model in that graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer: \n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "## create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to train our CNN model.\n",
    "* need to create a TF session to launch the graph\n",
    "    * then, call the `train` function\n",
    "    * to train the model for the first time, need to initialize all the variables in the network\n",
    "        * `initialize` argument to `True` will execute `tf.global_variables_initializer` through `session.run`\n",
    "            * this initialization step should be avoided in case you want to train additional epochs (e.g., restoring an already trained model and train further for additional epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training Avg. Loss: 272.307  Validation Acc:   0.972\n",
      "Epoch 02 Training Avg. Loss:  75.711  Validation Acc:   0.982\n",
      "Epoch 03 Training Avg. Loss:  51.187  Validation Acc:   0.985\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "## crearte a TF session \n",
    "## and train the CNN model\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    train(sess, \n",
    "          training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid), \n",
    "          initialize=True,\n",
    "          random_seed=123, epochs=3)\n",
    "    save(saver, sess, epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* after the 20 epochs, the trained model was saved for future use\n",
    "* the following code shows how to restore a saved model\n",
    "    * delte the graph `g`, then create a new graph `g2`, and reload the trained model to do prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer: \n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-3\n",
      "Test Accuracy: 98.630%\n"
     ]
    }
   ],
   "source": [
    "### Calculate prediction accuracy\n",
    "### on test set\n",
    "### restoring the saved model\n",
    "\n",
    "del g\n",
    "\n",
    "## create a new graph \n",
    "## and build the model\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=3, path='./model/')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, \n",
    "                    return_proba=False)\n",
    "\n",
    "    print('Test Accuracy: %.3f%%' % (100*\n",
    "                np.sum(preds == y_test)/len(y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the prediction accuracy on the test set is already better than what we obtained using multilayer perceptron in Chapter 13\n",
    "* `X_test_centered` as test data not `X_test` \n",
    "* now, let's look at the predicted labels as well as their probabilities on the first 10 test samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[[ 0.    0.    0.    0.    0.    0.    0.    1.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 1.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.99  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.99]\n",
      " [ 0.    0.    0.    0.    0.    0.99  0.01  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "## run the prediction on \n",
    "##  some test samples\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=20, path='./model/')\n",
    "        \n",
    "    print(predict(sess, X_test_centered[:10], \n",
    "              return_proba=False))\n",
    "    \n",
    "    print(predict(sess, X_test_centered[:10], \n",
    "                  return_proba=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how we can train the model further to reach a total of 40 epochs.\n",
    "* restoring the already trained model and continue training for 20 additional epochs\n",
    "    * call the `train` function again with `initialize = False` to avoid the initialization step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-3\n",
      "Epoch 01 Training Avg. Loss:  40.560  Validation Acc:   0.986\n",
      "Epoch 02 Training Avg. Loss:  32.581  Validation Acc:   0.988\n",
      "Epoch 03 Training Avg. Loss:  27.416  Validation Acc:   0.988\n",
      "Saving model in ./model/\n",
      "Test Accuracy: 99.100%\n"
     ]
    }
   ],
   "source": [
    "## continue training for 20 more epochs\n",
    "## without re-initializing :: initialize=False\n",
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=3, path='./model/')\n",
    "    \n",
    "    train(sess,\n",
    "          training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid),\n",
    "          initialize=False,\n",
    "          epochs=3,\n",
    "          random_seed=123)\n",
    "        \n",
    "    save(saver, sess, epoch=6, path='./model/')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, \n",
    "                    return_proba=False)\n",
    "    \n",
    "    print('Test Accuracy: %.3f%%' % (100*\n",
    "                np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the result shows that training for 20 additional epochs slightly improved the performance to get 99.37% prediction accuracy on the test set\n",
    "\n",
    "In the next section, we'll now implement the CNN using the TF Layers API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in the TensorFlow Layers API\n",
    "\n",
    "We can implement the model in a new class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConvNN(object):\n",
    "    def __init__(self, batchsize=64,\n",
    "                 epochs=20, learning_rate=1e-4, \n",
    "                 dropout_rate=0.5,\n",
    "                 shuffle=True, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "                \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            ## set random-seed:\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            ## build the network:\n",
    "            self.build()\n",
    "\n",
    "            ## initializer\n",
    "            self.init_op = \\\n",
    "                tf.global_variables_initializer()\n",
    "\n",
    "            ## saver\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "        ## create a session\n",
    "        self.sess = tf.Session(graph=g)\n",
    "                \n",
    "    def build(self):\n",
    "        \n",
    "        ## Placeholders for X and y:\n",
    "        tf_x = tf.placeholder(tf.float32, \n",
    "                              shape=[None, 784],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[None],\n",
    "                              name='tf_y')\n",
    "        is_train = tf.placeholder(tf.bool, \n",
    "                              shape=(),\n",
    "                              name='is_train')\n",
    "\n",
    "        ## reshape x to a 4D tensor: \n",
    "        ##  [batchsize, width, height, 1]\n",
    "        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                              name='input_x_2dimages')\n",
    "        ## One-hot encoding:\n",
    "        tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                              dtype=tf.float32,\n",
    "                              name='input_y_onehot')\n",
    "\n",
    "        ## 1st layer: Conv_1\n",
    "        h1 = tf.layers.conv2d(tf_x_image, \n",
    "                              kernel_size=(5, 5), \n",
    "                              filters=32, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling\n",
    "        h1_pool = tf.layers.max_pooling2d(h1, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "        ## 2n layer: Conv_2\n",
    "        h2 = tf.layers.conv2d(h1_pool, kernel_size=(5,5), \n",
    "                              filters=64, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling \n",
    "        h2_pool = tf.layers.max_pooling2d(h2, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "\n",
    "        ## 3rd layer: Fully Connected\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool, \n",
    "                              shape=[-1, n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat, 1024, \n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "        ## Dropout\n",
    "        h3_drop = tf.layers.dropout(h3, \n",
    "                              rate=self.dropout_rate,\n",
    "                              training=is_train)\n",
    "        \n",
    "        ## 4th layer: Fully Connected (linear activation)\n",
    "        h4 = tf.layers.dense(h3_drop, 10, \n",
    "                              activation=None)\n",
    "\n",
    "        ## Prediction\n",
    "        predictions = {\n",
    "            'probabilities': tf.nn.softmax(h4, \n",
    "                              name='probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(h4, axis=1), \n",
    "                              tf.int32, name='labels')}\n",
    "        \n",
    "        ## Loss Function and Optimization\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=h4, labels=tf_y_onehot),\n",
    "            name='cross_entropy_loss')\n",
    "        \n",
    "        ## Optimizer:\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                              name='train_op')\n",
    "\n",
    "        ## Finding accuracy\n",
    "        correct_predictions = tf.equal(\n",
    "            predictions['labels'], \n",
    "            tf_y, name='correct_preds')\n",
    "        \n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "\n",
    "    def save(self, epoch, path='./tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s' % path)\n",
    "        self.saver.save(self.sess, \n",
    "                        os.path.join(path, 'model.ckpt'),\n",
    "                        global_step=epoch)\n",
    "        \n",
    "    def load(self, epoch, path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess, \n",
    "             os.path.join(path, 'model.ckpt-%d' % epoch))\n",
    "        \n",
    "    def train(self, training_set, \n",
    "              validation_set=None,\n",
    "              initialize=True):\n",
    "        ## initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            batch_gen = \\\n",
    "                batch_generator(X_data, y_data, \n",
    "                                 shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x,batch_y) in \\\n",
    "                enumerate(batch_gen):\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': True} ## for dropout\n",
    "                loss, _ = self.sess.run(\n",
    "                        ['cross_entropy_loss:0', 'train_op'], \n",
    "                        feed_dict=feed)\n",
    "                avg_loss += loss\n",
    "                \n",
    "            print('Epoch %02d: Training Avg. Loss: '\n",
    "                  '%7.3f' % (epoch, avg_loss), end=' ')\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': False} ## for dropout\n",
    "                valid_acc = self.sess.run('accuracy:0',\n",
    "                                          feed_dict=feed)\n",
    "                print('Validation Acc: %7.3f' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "                    \n",
    "    def predict(self, X_test, return_proba = False):\n",
    "        feed = {'tf_x:0': X_test,\n",
    "                'is_train:0': False} ## for dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',\n",
    "                                 feed_dict=feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',\n",
    "                                 feed_dict=feed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the structure of this class is very similar to the previous section with the low-level TF API\n",
    "    * the class has a constructor that sets the training parameters, creates a graph `g`, and builds the model\n",
    "    * besides the constructor, there are five major methods \n",
    "        * `.build`: builds the model\n",
    "        * `.save`: to save a trained model\n",
    "        * `.load`: to restore a saved model\n",
    "        * `.train`: trains the model\n",
    "        * `.predict`: to do prediction on a test set\n",
    "* a dropout layer after the first fully connected layer (`tf.nn.dropout` in low-level API and `tf.layers.dropout` in the high-level Layers API)\n",
    "    * `tf.nn.dropout`: :an argument, `keep_prob` indicates the probability of keeping the units, whereas `tf.layers.dropout` has a `rate` parameter, which is the rate of dropping units (i.e., `rate = 1 - keep_prob`)\n",
    "    * in the `tf.nn.dropout` function, we fed the `keep_prob` parameter using a placeholder\n",
    "        * during the training, `keep_prob = 0.5`\n",
    "        * during the inference (or prediction), `keep_prob = 1`\n",
    "    * in the `tf.layers.dropout`, the value `rate` is provided upon the creation of the dropout layer in the graph\n",
    "        * we cannot change it during the training or the inference modes\n",
    "            * instead, a Boolean argument called `training` to determine whether we need to apply dropout or not (`tf.bool=True` during the training mode and `tf.bool=False` during the inference mode)\n",
    "            \n",
    "\n",
    "We can create an instance of the `ConvNN` class, train it for 20 epochs, and save the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss: 269.172 Validation Acc:   1.000\n",
      "Epoch 02: Training Avg. Loss:  74.298 Validation Acc:   0.938\n",
      "Epoch 03: Training Avg. Loss:  50.655 Validation Acc:   1.000\n",
      "Saving model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNN(random_seed=123, epochs=3)\n",
    "cnn.train(training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid))\n",
    "\n",
    "#cnn.save(epoch=20)\n",
    "cnn.save(epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* after the training is finished, the model can be used to do prediction on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./tflayers-model/\n",
      "INFO:tensorflow:Restoring parameters from ./tflayers-model/model.ckpt-3\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "del cnn\n",
    "\n",
    "cnn2 = ConvNN(random_seed=123)\n",
    "\n",
    "#cnn2.load(epoch=20, path='./tflayers-model/')\n",
    "cnn2.load(epoch=3, path='./tflayers-model/')\n",
    "\n",
    "print(cnn2.predict(X_test_centered[:10,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can measure the accuracy of the test dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.76%\n"
     ]
    }
   ],
   "source": [
    "preds = cnn2.predict(X_test_centered)\n",
    "\n",
    "print('Test Accuracy: %.2f%%' % (100*\n",
    "      np.sum(y_test == preds)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the obtained accuracy is 98.76%, which means there are 124 misclassified test samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "In this chapter, we learned about CNNs and explored the building blocks that form different CNN architectures.\n",
    "* started by defining the convolution operation\n",
    "* covered subsampling by discussing two forms of pooling operations\n",
    "    * max-pooling \n",
    "    * average-pooling\n",
    "\n",
    "Then, putting all these blocks together, a deep CNN was built using the TF core API as well as TF __Layers__ API for image classification.\n",
    "\n",
    "In the next chapter, we'll move on to __Recurrent Neural Networks (RNN)__.\n",
    "* used for learning the structure of sequence data \n",
    "* applications including language translation and image captioning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
