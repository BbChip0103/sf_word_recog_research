{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 256)          1024        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 256)          1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 384)          1536        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 512)          2048        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 512)          2048        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 512)          2048        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6156 - acc: 0.5109\n",
      "Epoch 00001: val_loss improved from inf to 1.87797, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/001-1.8780.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.6156 - acc: 0.5109 - val_loss: 1.8780 - val_acc: 0.4277\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2910 - acc: 0.6175\n",
      "Epoch 00002: val_loss did not improve from 1.87797\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2910 - acc: 0.6175 - val_loss: 1.9198 - val_acc: 0.3739\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1707 - acc: 0.6551\n",
      "Epoch 00003: val_loss improved from 1.87797 to 1.61310, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/003-1.6131.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1707 - acc: 0.6551 - val_loss: 1.6131 - val_acc: 0.4775\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.6798\n",
      "Epoch 00004: val_loss improved from 1.61310 to 1.44549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/004-1.4455.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0889 - acc: 0.6797 - val_loss: 1.4455 - val_acc: 0.5413\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0361 - acc: 0.6976\n",
      "Epoch 00005: val_loss did not improve from 1.44549\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0363 - acc: 0.6975 - val_loss: 1.4560 - val_acc: 0.5323\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9959 - acc: 0.7092\n",
      "Epoch 00006: val_loss improved from 1.44549 to 1.36216, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/006-1.3622.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9960 - acc: 0.7091 - val_loss: 1.3622 - val_acc: 0.5721\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9615 - acc: 0.7211\n",
      "Epoch 00007: val_loss improved from 1.36216 to 1.16882, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/007-1.1688.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.9615 - acc: 0.7210 - val_loss: 1.1688 - val_acc: 0.6266\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.7277\n",
      "Epoch 00008: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9321 - acc: 0.7276 - val_loss: 1.7403 - val_acc: 0.4836\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9112 - acc: 0.7337\n",
      "Epoch 00009: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9112 - acc: 0.7337 - val_loss: 1.4634 - val_acc: 0.5437\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.7418\n",
      "Epoch 00010: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8894 - acc: 0.7418 - val_loss: 1.3111 - val_acc: 0.5993\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8647 - acc: 0.7496\n",
      "Epoch 00011: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8647 - acc: 0.7496 - val_loss: 1.1766 - val_acc: 0.6382\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8499 - acc: 0.7511\n",
      "Epoch 00012: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8499 - acc: 0.7511 - val_loss: 1.6526 - val_acc: 0.5029\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.7555\n",
      "Epoch 00013: val_loss did not improve from 1.16882\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8387 - acc: 0.7555 - val_loss: 1.2704 - val_acc: 0.6056\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8209 - acc: 0.7614\n",
      "Epoch 00014: val_loss improved from 1.16882 to 0.95458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/014-0.9546.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8210 - acc: 0.7613 - val_loss: 0.9546 - val_acc: 0.7303\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8102 - acc: 0.7648\n",
      "Epoch 00015: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8101 - acc: 0.7648 - val_loss: 1.0209 - val_acc: 0.7037\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7933 - acc: 0.7697\n",
      "Epoch 00016: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7936 - acc: 0.7696 - val_loss: 1.5005 - val_acc: 0.5514\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7841 - acc: 0.7718\n",
      "Epoch 00017: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7841 - acc: 0.7718 - val_loss: 1.0316 - val_acc: 0.6788\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7806 - acc: 0.7756\n",
      "Epoch 00018: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7805 - acc: 0.7756 - val_loss: 1.1552 - val_acc: 0.6466\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7775\n",
      "Epoch 00019: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7662 - acc: 0.7775 - val_loss: 0.9702 - val_acc: 0.7070\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7805\n",
      "Epoch 00020: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7576 - acc: 0.7805 - val_loss: 1.6953 - val_acc: 0.5311\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7801\n",
      "Epoch 00021: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7490 - acc: 0.7801 - val_loss: 1.5266 - val_acc: 0.5472\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7431 - acc: 0.7849\n",
      "Epoch 00022: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7432 - acc: 0.7849 - val_loss: 1.2783 - val_acc: 0.5961\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7866\n",
      "Epoch 00023: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7319 - acc: 0.7866 - val_loss: 1.4265 - val_acc: 0.5733\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7879\n",
      "Epoch 00024: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7239 - acc: 0.7879 - val_loss: 1.1907 - val_acc: 0.6399\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7147 - acc: 0.7915\n",
      "Epoch 00025: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7147 - acc: 0.7915 - val_loss: 1.8652 - val_acc: 0.4815\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7925\n",
      "Epoch 00026: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7117 - acc: 0.7925 - val_loss: 1.1131 - val_acc: 0.6534\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.7942\n",
      "Epoch 00027: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.7075 - acc: 0.7942 - val_loss: 1.2163 - val_acc: 0.6506\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7957\n",
      "Epoch 00028: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6965 - acc: 0.7957 - val_loss: 1.0083 - val_acc: 0.6762\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7980\n",
      "Epoch 00029: val_loss did not improve from 0.95458\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6927 - acc: 0.7980 - val_loss: 1.3884 - val_acc: 0.6252\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.7993\n",
      "Epoch 00030: val_loss improved from 0.95458 to 0.90804, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/030-0.9080.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6875 - acc: 0.7993 - val_loss: 0.9080 - val_acc: 0.7342\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.8028\n",
      "Epoch 00031: val_loss did not improve from 0.90804\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6794 - acc: 0.8027 - val_loss: 1.4436 - val_acc: 0.5933\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6704 - acc: 0.8062\n",
      "Epoch 00032: val_loss did not improve from 0.90804\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6705 - acc: 0.8062 - val_loss: 1.0743 - val_acc: 0.6590\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.8037\n",
      "Epoch 00033: val_loss improved from 0.90804 to 0.81417, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/033-0.8142.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6773 - acc: 0.8037 - val_loss: 0.8142 - val_acc: 0.7582\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.8103\n",
      "Epoch 00034: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6564 - acc: 0.8103 - val_loss: 1.0082 - val_acc: 0.6788\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.8080\n",
      "Epoch 00035: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6584 - acc: 0.8080 - val_loss: 0.8873 - val_acc: 0.7219\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.8106\n",
      "Epoch 00036: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6534 - acc: 0.8107 - val_loss: 1.1445 - val_acc: 0.6492\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6444 - acc: 0.8120\n",
      "Epoch 00037: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6444 - acc: 0.8120 - val_loss: 1.0508 - val_acc: 0.6662\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.8144\n",
      "Epoch 00038: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6410 - acc: 0.8144 - val_loss: 1.3313 - val_acc: 0.5903\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6396 - acc: 0.8136\n",
      "Epoch 00039: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6396 - acc: 0.8136 - val_loss: 0.9723 - val_acc: 0.6956\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.8159\n",
      "Epoch 00040: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6322 - acc: 0.8159 - val_loss: 0.9480 - val_acc: 0.7151\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8192\n",
      "Epoch 00041: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6258 - acc: 0.8192 - val_loss: 0.8881 - val_acc: 0.7435\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6221 - acc: 0.8195\n",
      "Epoch 00042: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6221 - acc: 0.8195 - val_loss: 1.0143 - val_acc: 0.7186\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.8226\n",
      "Epoch 00043: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6147 - acc: 0.8226 - val_loss: 1.2658 - val_acc: 0.6224\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6135 - acc: 0.8209\n",
      "Epoch 00044: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6135 - acc: 0.8209 - val_loss: 0.8756 - val_acc: 0.7456\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.8241\n",
      "Epoch 00045: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6065 - acc: 0.8241 - val_loss: 0.9540 - val_acc: 0.7130\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.8274\n",
      "Epoch 00046: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6026 - acc: 0.8273 - val_loss: 1.4019 - val_acc: 0.5719\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8254\n",
      "Epoch 00047: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6047 - acc: 0.8254 - val_loss: 1.8413 - val_acc: 0.5558\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.8261\n",
      "Epoch 00048: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5959 - acc: 0.8262 - val_loss: 1.3090 - val_acc: 0.5900\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8280\n",
      "Epoch 00049: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5922 - acc: 0.8280 - val_loss: 1.0348 - val_acc: 0.6925\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8283\n",
      "Epoch 00050: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5880 - acc: 0.8283 - val_loss: 0.9714 - val_acc: 0.7123\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5879 - acc: 0.8297\n",
      "Epoch 00051: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5880 - acc: 0.8297 - val_loss: 2.1316 - val_acc: 0.4766\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.8316\n",
      "Epoch 00052: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5783 - acc: 0.8316 - val_loss: 0.8759 - val_acc: 0.7300\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8323\n",
      "Epoch 00053: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5752 - acc: 0.8322 - val_loss: 0.9915 - val_acc: 0.7044\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8328\n",
      "Epoch 00054: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5740 - acc: 0.8327 - val_loss: 1.2080 - val_acc: 0.6417\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8364\n",
      "Epoch 00055: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5701 - acc: 0.8364 - val_loss: 1.2902 - val_acc: 0.6301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.8365\n",
      "Epoch 00056: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5654 - acc: 0.8365 - val_loss: 2.4520 - val_acc: 0.4218\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.8384\n",
      "Epoch 00057: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5609 - acc: 0.8384 - val_loss: 0.9160 - val_acc: 0.7277\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8366\n",
      "Epoch 00058: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5631 - acc: 0.8366 - val_loss: 2.1178 - val_acc: 0.5083\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.8371\n",
      "Epoch 00059: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5572 - acc: 0.8371 - val_loss: 1.0576 - val_acc: 0.6914\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.8401\n",
      "Epoch 00060: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5529 - acc: 0.8400 - val_loss: 1.0984 - val_acc: 0.6532\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.8383\n",
      "Epoch 00061: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5567 - acc: 0.8383 - val_loss: 1.4686 - val_acc: 0.5921\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.8439\n",
      "Epoch 00062: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5477 - acc: 0.8439 - val_loss: 1.3189 - val_acc: 0.6056\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8424\n",
      "Epoch 00063: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5429 - acc: 0.8425 - val_loss: 0.9486 - val_acc: 0.7119\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8432\n",
      "Epoch 00064: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5437 - acc: 0.8432 - val_loss: 2.0008 - val_acc: 0.5528\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8422\n",
      "Epoch 00065: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5416 - acc: 0.8422 - val_loss: 1.5801 - val_acc: 0.5609\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8458\n",
      "Epoch 00066: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5322 - acc: 0.8457 - val_loss: 0.8876 - val_acc: 0.7428\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8457\n",
      "Epoch 00067: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5306 - acc: 0.8458 - val_loss: 0.8452 - val_acc: 0.7559\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8451\n",
      "Epoch 00068: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5325 - acc: 0.8451 - val_loss: 1.5256 - val_acc: 0.5544\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.8479\n",
      "Epoch 00069: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5258 - acc: 0.8478 - val_loss: 0.8551 - val_acc: 0.7508\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.8480\n",
      "Epoch 00070: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5256 - acc: 0.8480 - val_loss: 1.1160 - val_acc: 0.6755\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8479\n",
      "Epoch 00071: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5188 - acc: 0.8480 - val_loss: 1.2967 - val_acc: 0.6341\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.8497\n",
      "Epoch 00072: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5175 - acc: 0.8496 - val_loss: 1.4141 - val_acc: 0.5966\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8530\n",
      "Epoch 00073: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5163 - acc: 0.8530 - val_loss: 1.5772 - val_acc: 0.5861\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8525\n",
      "Epoch 00074: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5075 - acc: 0.8525 - val_loss: 0.9635 - val_acc: 0.7072\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8530\n",
      "Epoch 00075: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5071 - acc: 0.8529 - val_loss: 1.2069 - val_acc: 0.6611\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8538\n",
      "Epoch 00076: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5029 - acc: 0.8538 - val_loss: 1.1533 - val_acc: 0.6737\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8547\n",
      "Epoch 00077: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5027 - acc: 0.8547 - val_loss: 1.3186 - val_acc: 0.6375\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8530\n",
      "Epoch 00078: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.5016 - acc: 0.8530 - val_loss: 0.8503 - val_acc: 0.7526\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8568\n",
      "Epoch 00079: val_loss did not improve from 0.81417\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4970 - acc: 0.8568 - val_loss: 1.2109 - val_acc: 0.6618\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8575\n",
      "Epoch 00080: val_loss improved from 0.81417 to 0.78755, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/080-0.7875.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4917 - acc: 0.8575 - val_loss: 0.7875 - val_acc: 0.7806\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8579\n",
      "Epoch 00081: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4925 - acc: 0.8578 - val_loss: 1.8084 - val_acc: 0.5320\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8551\n",
      "Epoch 00082: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4910 - acc: 0.8551 - val_loss: 1.1403 - val_acc: 0.6599\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8596\n",
      "Epoch 00083: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4865 - acc: 0.8596 - val_loss: 0.8276 - val_acc: 0.7591\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8572\n",
      "Epoch 00084: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4903 - acc: 0.8571 - val_loss: 1.0605 - val_acc: 0.6918\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4843 - acc: 0.8601\n",
      "Epoch 00085: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4843 - acc: 0.8601 - val_loss: 1.2753 - val_acc: 0.6401\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8585\n",
      "Epoch 00086: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4801 - acc: 0.8584 - val_loss: 1.3881 - val_acc: 0.6191\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8603\n",
      "Epoch 00087: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4779 - acc: 0.8603 - val_loss: 0.9362 - val_acc: 0.7372\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8610\n",
      "Epoch 00088: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4759 - acc: 0.8609 - val_loss: 2.2204 - val_acc: 0.4575\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8614\n",
      "Epoch 00089: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4784 - acc: 0.8614 - val_loss: 1.3394 - val_acc: 0.6546\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.8647\n",
      "Epoch 00090: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4690 - acc: 0.8647 - val_loss: 1.5739 - val_acc: 0.5735\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8629\n",
      "Epoch 00091: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4761 - acc: 0.8629 - val_loss: 0.9358 - val_acc: 0.7128\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8641\n",
      "Epoch 00092: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4631 - acc: 0.8641 - val_loss: 0.9827 - val_acc: 0.7025\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8672\n",
      "Epoch 00093: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4589 - acc: 0.8672 - val_loss: 1.6713 - val_acc: 0.5931\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.8687\n",
      "Epoch 00094: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4572 - acc: 0.8687 - val_loss: 1.5217 - val_acc: 0.6021\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8660\n",
      "Epoch 00095: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4643 - acc: 0.8659 - val_loss: 2.0353 - val_acc: 0.5085\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8666\n",
      "Epoch 00096: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4579 - acc: 0.8665 - val_loss: 1.2543 - val_acc: 0.6508\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8694\n",
      "Epoch 00097: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4543 - acc: 0.8694 - val_loss: 1.4249 - val_acc: 0.6070\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8685\n",
      "Epoch 00098: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4508 - acc: 0.8684 - val_loss: 1.0083 - val_acc: 0.7272\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8703\n",
      "Epoch 00099: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4494 - acc: 0.8703 - val_loss: 1.0090 - val_acc: 0.6916\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8711\n",
      "Epoch 00100: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4469 - acc: 0.8711 - val_loss: 1.1143 - val_acc: 0.7060\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8729\n",
      "Epoch 00101: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4437 - acc: 0.8728 - val_loss: 1.4948 - val_acc: 0.6005\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8713\n",
      "Epoch 00102: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4467 - acc: 0.8713 - val_loss: 3.0097 - val_acc: 0.4125\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8740\n",
      "Epoch 00103: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4394 - acc: 0.8739 - val_loss: 1.0911 - val_acc: 0.7291\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8724\n",
      "Epoch 00104: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4435 - acc: 0.8725 - val_loss: 0.9795 - val_acc: 0.7389\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.8756\n",
      "Epoch 00105: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4298 - acc: 0.8755 - val_loss: 0.9217 - val_acc: 0.7160\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4336 - acc: 0.8753\n",
      "Epoch 00106: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4338 - acc: 0.8753 - val_loss: 1.1695 - val_acc: 0.6662\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.8751\n",
      "Epoch 00107: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4308 - acc: 0.8750 - val_loss: 1.2086 - val_acc: 0.6818\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8751\n",
      "Epoch 00108: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4295 - acc: 0.8750 - val_loss: 1.0692 - val_acc: 0.7116\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8784\n",
      "Epoch 00109: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4249 - acc: 0.8783 - val_loss: 0.9299 - val_acc: 0.7333\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8747\n",
      "Epoch 00110: val_loss did not improve from 0.78755\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4305 - acc: 0.8747 - val_loss: 1.9315 - val_acc: 0.5362\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.8755\n",
      "Epoch 00111: val_loss improved from 0.78755 to 0.69096, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv_checkpoint/111-0.6910.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4268 - acc: 0.8755 - val_loss: 0.6910 - val_acc: 0.8171\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8793\n",
      "Epoch 00112: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4199 - acc: 0.8792 - val_loss: 1.4159 - val_acc: 0.6525\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8793\n",
      "Epoch 00113: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4174 - acc: 0.8793 - val_loss: 1.3710 - val_acc: 0.6362\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8795\n",
      "Epoch 00114: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4185 - acc: 0.8795 - val_loss: 1.1645 - val_acc: 0.6855\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8817\n",
      "Epoch 00115: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4184 - acc: 0.8816 - val_loss: 1.4936 - val_acc: 0.6154\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8793\n",
      "Epoch 00116: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4125 - acc: 0.8793 - val_loss: 2.8652 - val_acc: 0.4819\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8824\n",
      "Epoch 00117: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4085 - acc: 0.8824 - val_loss: 1.0424 - val_acc: 0.7154\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8802\n",
      "Epoch 00118: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4108 - acc: 0.8802 - val_loss: 2.5419 - val_acc: 0.4934\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8839\n",
      "Epoch 00119: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4054 - acc: 0.8838 - val_loss: 1.8986 - val_acc: 0.5635\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8845\n",
      "Epoch 00120: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4032 - acc: 0.8846 - val_loss: 1.7722 - val_acc: 0.5514\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8831\n",
      "Epoch 00121: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4024 - acc: 0.8831 - val_loss: 2.3624 - val_acc: 0.4694\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8851\n",
      "Epoch 00122: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4004 - acc: 0.8851 - val_loss: 1.1055 - val_acc: 0.6883\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8843\n",
      "Epoch 00123: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3982 - acc: 0.8842 - val_loss: 1.2256 - val_acc: 0.6739\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8830\n",
      "Epoch 00124: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4009 - acc: 0.8830 - val_loss: 1.0604 - val_acc: 0.7070\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8847\n",
      "Epoch 00125: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3940 - acc: 0.8847 - val_loss: 1.4687 - val_acc: 0.6222\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8842\n",
      "Epoch 00126: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3984 - acc: 0.8842 - val_loss: 1.2103 - val_acc: 0.6587\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8883\n",
      "Epoch 00127: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3906 - acc: 0.8883 - val_loss: 1.0604 - val_acc: 0.6869\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8884\n",
      "Epoch 00128: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3856 - acc: 0.8884 - val_loss: 1.4761 - val_acc: 0.6233\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8883\n",
      "Epoch 00129: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3889 - acc: 0.8882 - val_loss: 2.2326 - val_acc: 0.5162\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8877\n",
      "Epoch 00130: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3853 - acc: 0.8877 - val_loss: 1.3502 - val_acc: 0.6660\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8879\n",
      "Epoch 00131: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3850 - acc: 0.8879 - val_loss: 1.0316 - val_acc: 0.7063\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8903\n",
      "Epoch 00132: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3783 - acc: 0.8902 - val_loss: 1.0304 - val_acc: 0.7202\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8879\n",
      "Epoch 00133: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3875 - acc: 0.8879 - val_loss: 0.8501 - val_acc: 0.7582\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8913\n",
      "Epoch 00134: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3769 - acc: 0.8912 - val_loss: 0.9943 - val_acc: 0.7417\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8899\n",
      "Epoch 00135: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3802 - acc: 0.8899 - val_loss: 2.2261 - val_acc: 0.5201\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8913\n",
      "Epoch 00136: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3747 - acc: 0.8912 - val_loss: 1.2103 - val_acc: 0.6713\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8926\n",
      "Epoch 00137: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3730 - acc: 0.8926 - val_loss: 1.4081 - val_acc: 0.6355\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8936\n",
      "Epoch 00138: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3707 - acc: 0.8936 - val_loss: 0.7942 - val_acc: 0.7710\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8932\n",
      "Epoch 00139: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3691 - acc: 0.8932 - val_loss: 5.6332 - val_acc: 0.3331\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8919\n",
      "Epoch 00140: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3682 - acc: 0.8919 - val_loss: 1.7023 - val_acc: 0.6080\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8965\n",
      "Epoch 00141: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3610 - acc: 0.8965 - val_loss: 1.5788 - val_acc: 0.6233\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8960\n",
      "Epoch 00142: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3630 - acc: 0.8960 - val_loss: 2.0708 - val_acc: 0.5183\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8932\n",
      "Epoch 00143: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3650 - acc: 0.8932 - val_loss: 1.4351 - val_acc: 0.6536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8953\n",
      "Epoch 00144: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3636 - acc: 0.8952 - val_loss: 1.4682 - val_acc: 0.6336\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8962\n",
      "Epoch 00145: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3588 - acc: 0.8962 - val_loss: 2.0248 - val_acc: 0.5600\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8961\n",
      "Epoch 00146: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3565 - acc: 0.8960 - val_loss: 1.6316 - val_acc: 0.5973\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8968\n",
      "Epoch 00147: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3575 - acc: 0.8969 - val_loss: 2.4041 - val_acc: 0.4987\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8980\n",
      "Epoch 00148: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3531 - acc: 0.8980 - val_loss: 3.3464 - val_acc: 0.4160\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8979\n",
      "Epoch 00149: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3547 - acc: 0.8979 - val_loss: 1.5830 - val_acc: 0.5977\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8958\n",
      "Epoch 00150: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3562 - acc: 0.8957 - val_loss: 1.0456 - val_acc: 0.7095\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8984\n",
      "Epoch 00151: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3522 - acc: 0.8984 - val_loss: 1.7086 - val_acc: 0.6014\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.9000\n",
      "Epoch 00152: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3449 - acc: 0.9000 - val_loss: 0.9384 - val_acc: 0.7296\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.9008\n",
      "Epoch 00153: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3443 - acc: 0.9008 - val_loss: 1.5320 - val_acc: 0.6282\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.9004\n",
      "Epoch 00154: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3437 - acc: 0.9004 - val_loss: 1.2969 - val_acc: 0.6501\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.9013\n",
      "Epoch 00155: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3397 - acc: 0.9012 - val_loss: 1.1556 - val_acc: 0.6655\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.9022\n",
      "Epoch 00156: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3402 - acc: 0.9022 - val_loss: 1.2766 - val_acc: 0.6587\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.9028\n",
      "Epoch 00157: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3390 - acc: 0.9028 - val_loss: 1.6206 - val_acc: 0.5863\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.9026\n",
      "Epoch 00158: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3340 - acc: 0.9026 - val_loss: 2.0979 - val_acc: 0.5290\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.9048\n",
      "Epoch 00159: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3324 - acc: 0.9048 - val_loss: 1.2056 - val_acc: 0.6960\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.9055\n",
      "Epoch 00160: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3322 - acc: 0.9056 - val_loss: 1.0732 - val_acc: 0.7154\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9052\n",
      "Epoch 00161: val_loss did not improve from 0.69096\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3305 - acc: 0.9052 - val_loss: 1.3445 - val_acc: 0.6385\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 996us/sample - loss: 0.8053 - acc: 0.7711\n",
      "Loss: 0.8053225175118768 Accuracy: 0.7711319\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4705 - acc: 0.5520\n",
      "Epoch 00001: val_loss improved from inf to 1.65542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/001-1.6554.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4705 - acc: 0.5520 - val_loss: 1.6554 - val_acc: 0.4899\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6745\n",
      "Epoch 00002: val_loss improved from 1.65542 to 1.13865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/002-1.1387.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0949 - acc: 0.6745 - val_loss: 1.1387 - val_acc: 0.6580\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9646 - acc: 0.7211\n",
      "Epoch 00003: val_loss did not improve from 1.13865\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9645 - acc: 0.7211 - val_loss: 1.3577 - val_acc: 0.5928\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8870 - acc: 0.7437\n",
      "Epoch 00004: val_loss improved from 1.13865 to 1.02821, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/004-1.0282.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8870 - acc: 0.7437 - val_loss: 1.0282 - val_acc: 0.6865\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8299 - acc: 0.7608\n",
      "Epoch 00005: val_loss did not improve from 1.02821\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8302 - acc: 0.7608 - val_loss: 1.3211 - val_acc: 0.5816\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7917 - acc: 0.7725\n",
      "Epoch 00006: val_loss did not improve from 1.02821\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7916 - acc: 0.7725 - val_loss: 1.2686 - val_acc: 0.5870\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7521 - acc: 0.7826\n",
      "Epoch 00007: val_loss improved from 1.02821 to 1.00576, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/007-1.0058.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7522 - acc: 0.7826 - val_loss: 1.0058 - val_acc: 0.6839\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7229 - acc: 0.7924\n",
      "Epoch 00008: val_loss improved from 1.00576 to 0.90036, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/008-0.9004.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7231 - acc: 0.7923 - val_loss: 0.9004 - val_acc: 0.7328\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.7993\n",
      "Epoch 00009: val_loss did not improve from 0.90036\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6992 - acc: 0.7993 - val_loss: 0.9440 - val_acc: 0.7112\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.8076\n",
      "Epoch 00010: val_loss did not improve from 0.90036\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6801 - acc: 0.8076 - val_loss: 1.1583 - val_acc: 0.6380\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.8147\n",
      "Epoch 00011: val_loss improved from 0.90036 to 0.85623, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/011-0.8562.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6505 - acc: 0.8146 - val_loss: 0.8562 - val_acc: 0.7517\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.8186\n",
      "Epoch 00012: val_loss did not improve from 0.85623\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6375 - acc: 0.8186 - val_loss: 1.2913 - val_acc: 0.6098\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8228\n",
      "Epoch 00013: val_loss did not improve from 0.85623\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6189 - acc: 0.8228 - val_loss: 0.9521 - val_acc: 0.7167\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.8282\n",
      "Epoch 00014: val_loss did not improve from 0.85623\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5988 - acc: 0.8282 - val_loss: 0.9693 - val_acc: 0.6965\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8347\n",
      "Epoch 00015: val_loss improved from 0.85623 to 0.81392, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/015-0.8139.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5836 - acc: 0.8347 - val_loss: 0.8139 - val_acc: 0.7622\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8389\n",
      "Epoch 00016: val_loss improved from 0.81392 to 0.78634, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/016-0.7863.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5700 - acc: 0.8389 - val_loss: 0.7863 - val_acc: 0.7699\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.8416\n",
      "Epoch 00017: val_loss did not improve from 0.78634\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5603 - acc: 0.8416 - val_loss: 0.9198 - val_acc: 0.7326\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.8461\n",
      "Epoch 00018: val_loss did not improve from 0.78634\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5437 - acc: 0.8460 - val_loss: 0.7896 - val_acc: 0.7629\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.8490\n",
      "Epoch 00019: val_loss did not improve from 0.78634\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5360 - acc: 0.8489 - val_loss: 1.0196 - val_acc: 0.7014\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8545\n",
      "Epoch 00020: val_loss did not improve from 0.78634\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5193 - acc: 0.8544 - val_loss: 1.0157 - val_acc: 0.6925\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8553\n",
      "Epoch 00021: val_loss improved from 0.78634 to 0.76469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/021-0.7647.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5101 - acc: 0.8553 - val_loss: 0.7647 - val_acc: 0.7517\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8615\n",
      "Epoch 00022: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4972 - acc: 0.8615 - val_loss: 0.8443 - val_acc: 0.7470\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8638\n",
      "Epoch 00023: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4861 - acc: 0.8637 - val_loss: 0.9092 - val_acc: 0.7298\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8651\n",
      "Epoch 00024: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4800 - acc: 0.8651 - val_loss: 0.9611 - val_acc: 0.7193\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8672\n",
      "Epoch 00025: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4692 - acc: 0.8672 - val_loss: 1.1514 - val_acc: 0.6562\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4594 - acc: 0.8720\n",
      "Epoch 00026: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4596 - acc: 0.8719 - val_loss: 1.0755 - val_acc: 0.6897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8724\n",
      "Epoch 00027: val_loss did not improve from 0.76469\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4562 - acc: 0.8724 - val_loss: 1.1508 - val_acc: 0.6529\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8780\n",
      "Epoch 00028: val_loss improved from 0.76469 to 0.72785, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/028-0.7279.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4413 - acc: 0.8780 - val_loss: 0.7279 - val_acc: 0.8039\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8783\n",
      "Epoch 00029: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4343 - acc: 0.8783 - val_loss: 1.0979 - val_acc: 0.6646\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8792\n",
      "Epoch 00030: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4282 - acc: 0.8792 - val_loss: 0.9149 - val_acc: 0.7291\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8839\n",
      "Epoch 00031: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4143 - acc: 0.8838 - val_loss: 1.2909 - val_acc: 0.6166\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8858\n",
      "Epoch 00032: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4092 - acc: 0.8858 - val_loss: 0.8654 - val_acc: 0.7552\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8870\n",
      "Epoch 00033: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4028 - acc: 0.8870 - val_loss: 0.8218 - val_acc: 0.7694\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8906\n",
      "Epoch 00034: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3945 - acc: 0.8906 - val_loss: 1.2736 - val_acc: 0.6424\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8936\n",
      "Epoch 00035: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3859 - acc: 0.8936 - val_loss: 0.9322 - val_acc: 0.7114\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8952\n",
      "Epoch 00036: val_loss did not improve from 0.72785\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3800 - acc: 0.8951 - val_loss: 0.9298 - val_acc: 0.7214\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8962\n",
      "Epoch 00037: val_loss improved from 0.72785 to 0.65605, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/037-0.6561.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3750 - acc: 0.8962 - val_loss: 0.6561 - val_acc: 0.8160\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8999\n",
      "Epoch 00038: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3665 - acc: 0.8998 - val_loss: 0.8956 - val_acc: 0.7421\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8988\n",
      "Epoch 00039: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3610 - acc: 0.8988 - val_loss: 1.2018 - val_acc: 0.6608\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8990\n",
      "Epoch 00040: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3672 - acc: 0.8990 - val_loss: 0.6971 - val_acc: 0.7950\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.9051\n",
      "Epoch 00041: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3467 - acc: 0.9051 - val_loss: 0.8213 - val_acc: 0.7678\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.9056\n",
      "Epoch 00042: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3421 - acc: 0.9055 - val_loss: 1.0166 - val_acc: 0.6962\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.9036\n",
      "Epoch 00043: val_loss did not improve from 0.65605\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3537 - acc: 0.9035 - val_loss: 0.7560 - val_acc: 0.7869\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.9082\n",
      "Epoch 00044: val_loss improved from 0.65605 to 0.58315, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv_checkpoint/044-0.5832.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3306 - acc: 0.9082 - val_loss: 0.5832 - val_acc: 0.8376\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9089\n",
      "Epoch 00045: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3268 - acc: 0.9090 - val_loss: 0.7001 - val_acc: 0.8083\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.9109\n",
      "Epoch 00046: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3188 - acc: 0.9109 - val_loss: 0.9038 - val_acc: 0.7426\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.9139\n",
      "Epoch 00047: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3124 - acc: 0.9138 - val_loss: 0.6971 - val_acc: 0.7999\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9151\n",
      "Epoch 00048: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3091 - acc: 0.9151 - val_loss: 0.9475 - val_acc: 0.7489\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9154\n",
      "Epoch 00049: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3066 - acc: 0.9154 - val_loss: 0.6265 - val_acc: 0.8120\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9158\n",
      "Epoch 00050: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3043 - acc: 0.9158 - val_loss: 0.8096 - val_acc: 0.7612\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9194\n",
      "Epoch 00051: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2923 - acc: 0.9194 - val_loss: 0.7821 - val_acc: 0.7563\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9207\n",
      "Epoch 00052: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2896 - acc: 0.9207 - val_loss: 0.8150 - val_acc: 0.7652\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9224\n",
      "Epoch 00053: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2833 - acc: 0.9223 - val_loss: 0.8391 - val_acc: 0.7559\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9231\n",
      "Epoch 00054: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2810 - acc: 0.9231 - val_loss: 0.8426 - val_acc: 0.7456\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9245\n",
      "Epoch 00055: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2785 - acc: 0.9245 - val_loss: 0.9257 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9268\n",
      "Epoch 00056: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2658 - acc: 0.9268 - val_loss: 0.8678 - val_acc: 0.7531\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9282\n",
      "Epoch 00057: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2628 - acc: 0.9282 - val_loss: 0.7862 - val_acc: 0.7885\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9306\n",
      "Epoch 00058: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2585 - acc: 0.9306 - val_loss: 1.3267 - val_acc: 0.6622\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9289\n",
      "Epoch 00059: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2565 - acc: 0.9289 - val_loss: 1.2368 - val_acc: 0.6839\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9315\n",
      "Epoch 00060: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2512 - acc: 0.9315 - val_loss: 0.8165 - val_acc: 0.7659\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9329\n",
      "Epoch 00061: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2456 - acc: 0.9329 - val_loss: 0.7598 - val_acc: 0.7845\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9326\n",
      "Epoch 00062: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2432 - acc: 0.9325 - val_loss: 0.7799 - val_acc: 0.7834\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9348\n",
      "Epoch 00063: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2372 - acc: 0.9348 - val_loss: 0.9065 - val_acc: 0.7305\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9388\n",
      "Epoch 00064: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2321 - acc: 0.9388 - val_loss: 1.4230 - val_acc: 0.6341\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9390\n",
      "Epoch 00065: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2270 - acc: 0.9390 - val_loss: 0.8308 - val_acc: 0.7757\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9374\n",
      "Epoch 00066: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2344 - acc: 0.9374 - val_loss: 0.7909 - val_acc: 0.7743\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9386\n",
      "Epoch 00067: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2244 - acc: 0.9386 - val_loss: 0.8736 - val_acc: 0.7706\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9401\n",
      "Epoch 00068: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2150 - acc: 0.9400 - val_loss: 1.2404 - val_acc: 0.6886\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9414\n",
      "Epoch 00069: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2142 - acc: 0.9414 - val_loss: 1.5430 - val_acc: 0.6222\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9433\n",
      "Epoch 00070: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2081 - acc: 0.9434 - val_loss: 0.8955 - val_acc: 0.7685\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9436\n",
      "Epoch 00071: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2118 - acc: 0.9436 - val_loss: 0.8344 - val_acc: 0.7841\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9434\n",
      "Epoch 00072: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2024 - acc: 0.9434 - val_loss: 1.5880 - val_acc: 0.6070\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9447\n",
      "Epoch 00073: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2038 - acc: 0.9447 - val_loss: 0.9241 - val_acc: 0.7556\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9478\n",
      "Epoch 00074: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1973 - acc: 0.9477 - val_loss: 0.6504 - val_acc: 0.8211\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9470\n",
      "Epoch 00075: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1957 - acc: 0.9470 - val_loss: 0.9563 - val_acc: 0.7517\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9493\n",
      "Epoch 00076: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1902 - acc: 0.9493 - val_loss: 1.0917 - val_acc: 0.7354\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9503\n",
      "Epoch 00077: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1827 - acc: 0.9503 - val_loss: 0.9651 - val_acc: 0.7647\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9512\n",
      "Epoch 00078: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1795 - acc: 0.9512 - val_loss: 0.7577 - val_acc: 0.8125\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9508\n",
      "Epoch 00079: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1832 - acc: 0.9508 - val_loss: 0.5855 - val_acc: 0.8446\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9542\n",
      "Epoch 00080: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1721 - acc: 0.9542 - val_loss: 1.0054 - val_acc: 0.7496\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9545\n",
      "Epoch 00081: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1697 - acc: 0.9545 - val_loss: 0.9236 - val_acc: 0.7484\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9555\n",
      "Epoch 00082: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1681 - acc: 0.9555 - val_loss: 0.8887 - val_acc: 0.7715\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9541\n",
      "Epoch 00083: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1699 - acc: 0.9541 - val_loss: 1.1175 - val_acc: 0.7025\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9565\n",
      "Epoch 00084: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1632 - acc: 0.9565 - val_loss: 1.3139 - val_acc: 0.6909\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9544\n",
      "Epoch 00085: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1723 - acc: 0.9544 - val_loss: 0.9908 - val_acc: 0.7547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9570\n",
      "Epoch 00086: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1611 - acc: 0.9570 - val_loss: 0.6118 - val_acc: 0.8460\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9595\n",
      "Epoch 00087: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1563 - acc: 0.9595 - val_loss: 1.0600 - val_acc: 0.7400\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9613\n",
      "Epoch 00088: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1516 - acc: 0.9613 - val_loss: 1.5011 - val_acc: 0.6553\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9604\n",
      "Epoch 00089: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1484 - acc: 0.9604 - val_loss: 0.8146 - val_acc: 0.7869\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9633\n",
      "Epoch 00090: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1457 - acc: 0.9632 - val_loss: 0.7018 - val_acc: 0.8190\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9572\n",
      "Epoch 00091: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1622 - acc: 0.9572 - val_loss: 0.8725 - val_acc: 0.7570\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9629\n",
      "Epoch 00092: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1429 - acc: 0.9629 - val_loss: 0.7597 - val_acc: 0.8225\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9610\n",
      "Epoch 00093: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1446 - acc: 0.9610 - val_loss: 1.0961 - val_acc: 0.7526\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9648\n",
      "Epoch 00094: val_loss did not improve from 0.58315\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1340 - acc: 0.9648 - val_loss: 0.8408 - val_acc: 0.7955\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.6988 - acc: 0.8019\n",
      "Loss: 0.6988116537299112 Accuracy: 0.80186915\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2644 - acc: 0.6194\n",
      "Epoch 00001: val_loss improved from inf to 1.46668, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/001-1.4667.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.2643 - acc: 0.6195 - val_loss: 1.4667 - val_acc: 0.5714\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8239 - acc: 0.7677\n",
      "Epoch 00002: val_loss improved from 1.46668 to 0.84374, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/002-0.8437.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8238 - acc: 0.7677 - val_loss: 0.8437 - val_acc: 0.7659\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.8132\n",
      "Epoch 00003: val_loss did not improve from 0.84374\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6666 - acc: 0.8132 - val_loss: 0.9323 - val_acc: 0.7144\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8438\n",
      "Epoch 00004: val_loss improved from 0.84374 to 0.76711, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/004-0.7671.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5737 - acc: 0.8437 - val_loss: 0.7671 - val_acc: 0.7745\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8604\n",
      "Epoch 00005: val_loss improved from 0.76711 to 0.74822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/005-0.7482.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5094 - acc: 0.8604 - val_loss: 0.7482 - val_acc: 0.7678\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8759\n",
      "Epoch 00006: val_loss improved from 0.74822 to 0.58764, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/006-0.5876.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4593 - acc: 0.8759 - val_loss: 0.5876 - val_acc: 0.8244\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8847\n",
      "Epoch 00007: val_loss improved from 0.58764 to 0.55609, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/007-0.5561.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4228 - acc: 0.8847 - val_loss: 0.5561 - val_acc: 0.8397\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8966\n",
      "Epoch 00008: val_loss improved from 0.55609 to 0.54420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/008-0.5442.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3882 - acc: 0.8966 - val_loss: 0.5442 - val_acc: 0.8446\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8989\n",
      "Epoch 00009: val_loss did not improve from 0.54420\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3696 - acc: 0.8989 - val_loss: 0.5634 - val_acc: 0.8386\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.9046\n",
      "Epoch 00010: val_loss did not improve from 0.54420\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3469 - acc: 0.9047 - val_loss: 0.6332 - val_acc: 0.8202\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.9100\n",
      "Epoch 00011: val_loss did not improve from 0.54420\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3246 - acc: 0.9100 - val_loss: 0.5708 - val_acc: 0.8206\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9155\n",
      "Epoch 00012: val_loss improved from 0.54420 to 0.43349, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/012-0.4335.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3067 - acc: 0.9154 - val_loss: 0.4335 - val_acc: 0.8775\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9192\n",
      "Epoch 00013: val_loss did not improve from 0.43349\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2940 - acc: 0.9191 - val_loss: 0.7909 - val_acc: 0.7799\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9230\n",
      "Epoch 00014: val_loss did not improve from 0.43349\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2802 - acc: 0.9230 - val_loss: 0.5636 - val_acc: 0.8458\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9265\n",
      "Epoch 00015: val_loss improved from 0.43349 to 0.39046, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/015-0.3905.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2650 - acc: 0.9265 - val_loss: 0.3905 - val_acc: 0.8921\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9297\n",
      "Epoch 00016: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2542 - acc: 0.9297 - val_loss: 0.6021 - val_acc: 0.8258\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9340\n",
      "Epoch 00017: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2379 - acc: 0.9340 - val_loss: 0.4686 - val_acc: 0.8784\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9362\n",
      "Epoch 00018: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2306 - acc: 0.9362 - val_loss: 0.4103 - val_acc: 0.8898\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9383\n",
      "Epoch 00019: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2243 - acc: 0.9384 - val_loss: 0.5088 - val_acc: 0.8584\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9418\n",
      "Epoch 00020: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2109 - acc: 0.9418 - val_loss: 0.4566 - val_acc: 0.8710\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9449\n",
      "Epoch 00021: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2041 - acc: 0.9448 - val_loss: 0.6895 - val_acc: 0.8230\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9462\n",
      "Epoch 00022: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1999 - acc: 0.9462 - val_loss: 0.5307 - val_acc: 0.8425\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9490\n",
      "Epoch 00023: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1856 - acc: 0.9491 - val_loss: 0.4805 - val_acc: 0.8563\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9507\n",
      "Epoch 00024: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1787 - acc: 0.9506 - val_loss: 0.4911 - val_acc: 0.8591\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9526\n",
      "Epoch 00025: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1735 - acc: 0.9526 - val_loss: 0.4394 - val_acc: 0.8842\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9564\n",
      "Epoch 00026: val_loss did not improve from 0.39046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1669 - acc: 0.9564 - val_loss: 0.4044 - val_acc: 0.8912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9547\n",
      "Epoch 00027: val_loss improved from 0.39046 to 0.37174, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/027-0.3717.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1645 - acc: 0.9546 - val_loss: 0.3717 - val_acc: 0.8970\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9558\n",
      "Epoch 00028: val_loss did not improve from 0.37174\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1636 - acc: 0.9558 - val_loss: 0.3897 - val_acc: 0.8915\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9610\n",
      "Epoch 00029: val_loss did not improve from 0.37174\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1437 - acc: 0.9610 - val_loss: 0.5430 - val_acc: 0.8640\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9609\n",
      "Epoch 00030: val_loss did not improve from 0.37174\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1455 - acc: 0.9609 - val_loss: 0.7867 - val_acc: 0.7876\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9629\n",
      "Epoch 00031: val_loss did not improve from 0.37174\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1371 - acc: 0.9629 - val_loss: 0.5761 - val_acc: 0.8330\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9654\n",
      "Epoch 00032: val_loss improved from 0.37174 to 0.37101, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/032-0.3710.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1323 - acc: 0.9654 - val_loss: 0.3710 - val_acc: 0.8949\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9673\n",
      "Epoch 00033: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1227 - acc: 0.9673 - val_loss: 0.4102 - val_acc: 0.8870\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9680\n",
      "Epoch 00034: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1199 - acc: 0.9680 - val_loss: 0.4863 - val_acc: 0.8726\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9676\n",
      "Epoch 00035: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1212 - acc: 0.9676 - val_loss: 0.4824 - val_acc: 0.8712\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9686\n",
      "Epoch 00036: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1180 - acc: 0.9686 - val_loss: 0.3959 - val_acc: 0.8880\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9711\n",
      "Epoch 00037: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1114 - acc: 0.9711 - val_loss: 0.4541 - val_acc: 0.8742\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9724\n",
      "Epoch 00038: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1049 - acc: 0.9724 - val_loss: 0.4640 - val_acc: 0.8840\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9728\n",
      "Epoch 00039: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1032 - acc: 0.9728 - val_loss: 0.8532 - val_acc: 0.7852\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9755\n",
      "Epoch 00040: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0962 - acc: 0.9754 - val_loss: 0.4311 - val_acc: 0.8866\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9767\n",
      "Epoch 00041: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0912 - acc: 0.9767 - val_loss: 0.5155 - val_acc: 0.8586\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9757\n",
      "Epoch 00042: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0921 - acc: 0.9757 - val_loss: 0.5595 - val_acc: 0.8591\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9785\n",
      "Epoch 00043: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0854 - acc: 0.9785 - val_loss: 0.5467 - val_acc: 0.8516\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9806\n",
      "Epoch 00044: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0811 - acc: 0.9806 - val_loss: 0.4356 - val_acc: 0.8852\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9807\n",
      "Epoch 00045: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0790 - acc: 0.9807 - val_loss: 0.5449 - val_acc: 0.8691\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9808\n",
      "Epoch 00046: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0766 - acc: 0.9808 - val_loss: 0.4135 - val_acc: 0.8896\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9786\n",
      "Epoch 00047: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0837 - acc: 0.9786 - val_loss: 0.5805 - val_acc: 0.8619\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9841\n",
      "Epoch 00048: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0679 - acc: 0.9841 - val_loss: 0.3899 - val_acc: 0.8977\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9828\n",
      "Epoch 00049: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0685 - acc: 0.9828 - val_loss: 0.4408 - val_acc: 0.8845\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9837\n",
      "Epoch 00050: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0657 - acc: 0.9837 - val_loss: 0.4515 - val_acc: 0.8889\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9787\n",
      "Epoch 00051: val_loss did not improve from 0.37101\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0845 - acc: 0.9787 - val_loss: 0.5113 - val_acc: 0.8609\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9877\n",
      "Epoch 00052: val_loss improved from 0.37101 to 0.34440, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/052-0.3444.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0562 - acc: 0.9877 - val_loss: 0.3444 - val_acc: 0.9071\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9846\n",
      "Epoch 00053: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0642 - acc: 0.9846 - val_loss: 0.3962 - val_acc: 0.9040\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9878\n",
      "Epoch 00054: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0542 - acc: 0.9877 - val_loss: 0.4420 - val_acc: 0.8863\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9879\n",
      "Epoch 00055: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0538 - acc: 0.9879 - val_loss: 0.6822 - val_acc: 0.8439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9889\n",
      "Epoch 00056: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0509 - acc: 0.9889 - val_loss: 0.5691 - val_acc: 0.8623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9867\n",
      "Epoch 00057: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0566 - acc: 0.9867 - val_loss: 0.6010 - val_acc: 0.8546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9863\n",
      "Epoch 00058: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0580 - acc: 0.9863 - val_loss: 0.4494 - val_acc: 0.8873\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9849\n",
      "Epoch 00059: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0631 - acc: 0.9849 - val_loss: 0.4081 - val_acc: 0.9047\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9901\n",
      "Epoch 00060: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0455 - acc: 0.9900 - val_loss: 0.5936 - val_acc: 0.8600\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9879\n",
      "Epoch 00061: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0506 - acc: 0.9879 - val_loss: 0.4712 - val_acc: 0.8826\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9902\n",
      "Epoch 00062: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0444 - acc: 0.9902 - val_loss: 0.4595 - val_acc: 0.8896\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9919\n",
      "Epoch 00063: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0387 - acc: 0.9919 - val_loss: 0.5671 - val_acc: 0.8707\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9914\n",
      "Epoch 00064: val_loss did not improve from 0.34440\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0407 - acc: 0.9914 - val_loss: 0.5088 - val_acc: 0.8800\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9888\n",
      "Epoch 00065: val_loss improved from 0.34440 to 0.32917, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/065-0.3292.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0489 - acc: 0.9888 - val_loss: 0.3292 - val_acc: 0.9182\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9906\n",
      "Epoch 00066: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0406 - acc: 0.9906 - val_loss: 0.7517 - val_acc: 0.8425\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9927\n",
      "Epoch 00067: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0363 - acc: 0.9927 - val_loss: 0.6423 - val_acc: 0.8432\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9925\n",
      "Epoch 00068: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0344 - acc: 0.9925 - val_loss: 0.3776 - val_acc: 0.9113\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9905\n",
      "Epoch 00069: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0417 - acc: 0.9905 - val_loss: 0.4927 - val_acc: 0.8845\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9913\n",
      "Epoch 00070: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0379 - acc: 0.9913 - val_loss: 0.4119 - val_acc: 0.9101\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9935\n",
      "Epoch 00071: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0318 - acc: 0.9935 - val_loss: 1.0664 - val_acc: 0.7950\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9914\n",
      "Epoch 00072: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0409 - acc: 0.9913 - val_loss: 0.4680 - val_acc: 0.8875\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9902\n",
      "Epoch 00073: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0407 - acc: 0.9901 - val_loss: 0.6986 - val_acc: 0.8369\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9929\n",
      "Epoch 00074: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0339 - acc: 0.9929 - val_loss: 0.4208 - val_acc: 0.9082\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9942\n",
      "Epoch 00075: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0285 - acc: 0.9942 - val_loss: 0.3551 - val_acc: 0.9185\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9945\n",
      "Epoch 00076: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0272 - acc: 0.9945 - val_loss: 0.6841 - val_acc: 0.8556\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9913\n",
      "Epoch 00077: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0380 - acc: 0.9913 - val_loss: 0.5719 - val_acc: 0.8619\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9929\n",
      "Epoch 00078: val_loss did not improve from 0.32917\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0326 - acc: 0.9929 - val_loss: 0.4979 - val_acc: 0.8847\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9956\n",
      "Epoch 00079: val_loss improved from 0.32917 to 0.31801, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv_checkpoint/079-0.3180.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0239 - acc: 0.9955 - val_loss: 0.3180 - val_acc: 0.9252\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9942\n",
      "Epoch 00080: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0296 - acc: 0.9942 - val_loss: 0.8636 - val_acc: 0.8304\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9889\n",
      "Epoch 00081: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0451 - acc: 0.9888 - val_loss: 0.4557 - val_acc: 0.9003\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9937\n",
      "Epoch 00082: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0302 - acc: 0.9937 - val_loss: 0.4487 - val_acc: 0.8940\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9964\n",
      "Epoch 00083: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0221 - acc: 0.9964 - val_loss: 0.6005 - val_acc: 0.8775\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9932\n",
      "Epoch 00084: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0310 - acc: 0.9931 - val_loss: 0.3778 - val_acc: 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9947\n",
      "Epoch 00085: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0261 - acc: 0.9947 - val_loss: 0.4449 - val_acc: 0.9003\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9953\n",
      "Epoch 00086: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0236 - acc: 0.9953 - val_loss: 0.5510 - val_acc: 0.8803\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9901\n",
      "Epoch 00087: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0430 - acc: 0.9901 - val_loss: 0.4980 - val_acc: 0.8928\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9964\n",
      "Epoch 00088: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0199 - acc: 0.9964 - val_loss: 0.4087 - val_acc: 0.9054\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9977\n",
      "Epoch 00089: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0168 - acc: 0.9977 - val_loss: 0.6766 - val_acc: 0.8602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9918\n",
      "Epoch 00090: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0352 - acc: 0.9918 - val_loss: 0.4731 - val_acc: 0.9026\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9933\n",
      "Epoch 00091: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0288 - acc: 0.9932 - val_loss: 0.4814 - val_acc: 0.9003\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9956\n",
      "Epoch 00092: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0215 - acc: 0.9956 - val_loss: 0.4937 - val_acc: 0.8938\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9908\n",
      "Epoch 00093: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0408 - acc: 0.9908 - val_loss: 0.3514 - val_acc: 0.9194\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9970\n",
      "Epoch 00094: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0176 - acc: 0.9970 - val_loss: 0.5202 - val_acc: 0.8908\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9973\n",
      "Epoch 00095: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0171 - acc: 0.9973 - val_loss: 0.3586 - val_acc: 0.9217\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9940\n",
      "Epoch 00096: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0277 - acc: 0.9940 - val_loss: 0.6691 - val_acc: 0.8453\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9957\n",
      "Epoch 00097: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0216 - acc: 0.9957 - val_loss: 0.5031 - val_acc: 0.8910\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9975\n",
      "Epoch 00098: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0158 - acc: 0.9975 - val_loss: 0.5768 - val_acc: 0.8726\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9923\n",
      "Epoch 00099: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0309 - acc: 0.9923 - val_loss: 0.5159 - val_acc: 0.8917\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9945\n",
      "Epoch 00100: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0238 - acc: 0.9945 - val_loss: 0.3874 - val_acc: 0.9227\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9968\n",
      "Epoch 00101: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0189 - acc: 0.9968 - val_loss: 0.3979 - val_acc: 0.9054\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9960\n",
      "Epoch 00102: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0191 - acc: 0.9960 - val_loss: 0.6742 - val_acc: 0.8453\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9968\n",
      "Epoch 00103: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0168 - acc: 0.9968 - val_loss: 0.3369 - val_acc: 0.9238\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9958\n",
      "Epoch 00104: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0196 - acc: 0.9958 - val_loss: 0.6603 - val_acc: 0.8602\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9933\n",
      "Epoch 00105: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0287 - acc: 0.9933 - val_loss: 0.4349 - val_acc: 0.9075\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9982\n",
      "Epoch 00106: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0128 - acc: 0.9981 - val_loss: 0.5202 - val_acc: 0.8938\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9909\n",
      "Epoch 00107: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0371 - acc: 0.9909 - val_loss: 0.4033 - val_acc: 0.9103\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9978\n",
      "Epoch 00108: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0134 - acc: 0.9978 - val_loss: 0.4450 - val_acc: 0.9008\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9975\n",
      "Epoch 00109: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0142 - acc: 0.9975 - val_loss: 0.4057 - val_acc: 0.9057\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9977\n",
      "Epoch 00110: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0157 - acc: 0.9977 - val_loss: 0.5479 - val_acc: 0.8889\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9949\n",
      "Epoch 00111: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0234 - acc: 0.9949 - val_loss: 1.7183 - val_acc: 0.6979\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9968\n",
      "Epoch 00112: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0180 - acc: 0.9968 - val_loss: 0.4342 - val_acc: 0.9052\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9956\n",
      "Epoch 00113: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0214 - acc: 0.9956 - val_loss: 0.5255 - val_acc: 0.8800\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9966\n",
      "Epoch 00114: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0198 - acc: 0.9966 - val_loss: 0.4778 - val_acc: 0.8917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9970\n",
      "Epoch 00115: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0179 - acc: 0.9970 - val_loss: 0.5990 - val_acc: 0.8873\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9977\n",
      "Epoch 00116: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0134 - acc: 0.9977 - val_loss: 1.0907 - val_acc: 0.7831\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9959\n",
      "Epoch 00117: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0199 - acc: 0.9959 - val_loss: 0.5686 - val_acc: 0.8831\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9944\n",
      "Epoch 00118: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0250 - acc: 0.9944 - val_loss: 0.5080 - val_acc: 0.8891\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9982\n",
      "Epoch 00119: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0118 - acc: 0.9982 - val_loss: 0.3326 - val_acc: 0.9308\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9975\n",
      "Epoch 00120: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0146 - acc: 0.9974 - val_loss: 0.5564 - val_acc: 0.8854\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9889\n",
      "Epoch 00121: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0465 - acc: 0.9889 - val_loss: 0.3559 - val_acc: 0.9269\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9958\n",
      "Epoch 00122: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0213 - acc: 0.9958 - val_loss: 0.3691 - val_acc: 0.9171\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9975\n",
      "Epoch 00123: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0154 - acc: 0.9975 - val_loss: 0.7588 - val_acc: 0.8418\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9983\n",
      "Epoch 00124: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0112 - acc: 0.9983 - val_loss: 0.4105 - val_acc: 0.9119\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9971\n",
      "Epoch 00125: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0146 - acc: 0.9971 - val_loss: 0.3471 - val_acc: 0.9278\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9990\n",
      "Epoch 00126: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0086 - acc: 0.9990 - val_loss: 0.3973 - val_acc: 0.9199\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9950\n",
      "Epoch 00127: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0222 - acc: 0.9950 - val_loss: 0.5146 - val_acc: 0.8908\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9964\n",
      "Epoch 00128: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0172 - acc: 0.9964 - val_loss: 0.3886 - val_acc: 0.9164\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9970\n",
      "Epoch 00129: val_loss did not improve from 0.31801\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0151 - acc: 0.9970 - val_loss: 0.4793 - val_acc: 0.9033\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3680 - acc: 0.9057\n",
      "Loss: 0.36801885258742956 Accuracy: 0.9057113\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0133 - acc: 0.7071\n",
      "Epoch 00001: val_loss improved from inf to 1.30550, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/001-1.3055.hdf5\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 1.0133 - acc: 0.7071 - val_loss: 1.3055 - val_acc: 0.6059\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.8596\n",
      "Epoch 00002: val_loss improved from 1.30550 to 0.51458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/002-0.5146.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5142 - acc: 0.8596 - val_loss: 0.5146 - val_acc: 0.8507\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8948\n",
      "Epoch 00003: val_loss improved from 0.51458 to 0.42718, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/003-0.4272.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3866 - acc: 0.8948 - val_loss: 0.4272 - val_acc: 0.8831\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9134\n",
      "Epoch 00004: val_loss improved from 0.42718 to 0.36563, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/004-0.3656.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3178 - acc: 0.9134 - val_loss: 0.3656 - val_acc: 0.9012\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9243\n",
      "Epoch 00005: val_loss did not improve from 0.36563\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2783 - acc: 0.9243 - val_loss: 0.3677 - val_acc: 0.9005\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9308\n",
      "Epoch 00006: val_loss improved from 0.36563 to 0.34668, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/006-0.3467.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2461 - acc: 0.9308 - val_loss: 0.3467 - val_acc: 0.8968\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9360\n",
      "Epoch 00007: val_loss improved from 0.34668 to 0.24949, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/007-0.2495.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2259 - acc: 0.9360 - val_loss: 0.2495 - val_acc: 0.9306\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9430\n",
      "Epoch 00008: val_loss did not improve from 0.24949\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2001 - acc: 0.9430 - val_loss: 0.2954 - val_acc: 0.9213\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9490\n",
      "Epoch 00009: val_loss did not improve from 0.24949\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1866 - acc: 0.9490 - val_loss: 0.2598 - val_acc: 0.9262\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9529\n",
      "Epoch 00010: val_loss did not improve from 0.24949\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1697 - acc: 0.9529 - val_loss: 0.3441 - val_acc: 0.9040\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9561\n",
      "Epoch 00011: val_loss did not improve from 0.24949\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1557 - acc: 0.9561 - val_loss: 0.3315 - val_acc: 0.9012\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9590\n",
      "Epoch 00012: val_loss improved from 0.24949 to 0.22416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/012-0.2242.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1447 - acc: 0.9590 - val_loss: 0.2242 - val_acc: 0.9373\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9624\n",
      "Epoch 00013: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1355 - acc: 0.9624 - val_loss: 0.2812 - val_acc: 0.9175\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9655\n",
      "Epoch 00014: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1253 - acc: 0.9655 - val_loss: 0.2643 - val_acc: 0.9287\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9700\n",
      "Epoch 00015: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1119 - acc: 0.9700 - val_loss: 0.2525 - val_acc: 0.9301\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9697\n",
      "Epoch 00016: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1105 - acc: 0.9697 - val_loss: 0.2299 - val_acc: 0.9376\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9718\n",
      "Epoch 00017: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1018 - acc: 0.9718 - val_loss: 0.2445 - val_acc: 0.9327\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9752\n",
      "Epoch 00018: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0913 - acc: 0.9752 - val_loss: 0.2470 - val_acc: 0.9362\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9776\n",
      "Epoch 00019: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0836 - acc: 0.9776 - val_loss: 0.2271 - val_acc: 0.9408\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9776\n",
      "Epoch 00020: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0821 - acc: 0.9776 - val_loss: 0.4041 - val_acc: 0.8845\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9795\n",
      "Epoch 00021: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0773 - acc: 0.9795 - val_loss: 0.2966 - val_acc: 0.9224\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9813\n",
      "Epoch 00022: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0698 - acc: 0.9813 - val_loss: 0.2445 - val_acc: 0.9425\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9832\n",
      "Epoch 00023: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0649 - acc: 0.9832 - val_loss: 0.2293 - val_acc: 0.9390\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9839\n",
      "Epoch 00024: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0640 - acc: 0.9839 - val_loss: 0.2655 - val_acc: 0.9262\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9862\n",
      "Epoch 00025: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0553 - acc: 0.9862 - val_loss: 0.2307 - val_acc: 0.9397\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9877\n",
      "Epoch 00026: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0494 - acc: 0.9877 - val_loss: 0.2304 - val_acc: 0.9394\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9883\n",
      "Epoch 00027: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0476 - acc: 0.9883 - val_loss: 0.2475 - val_acc: 0.9376\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9885\n",
      "Epoch 00028: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0482 - acc: 0.9885 - val_loss: 0.2411 - val_acc: 0.9341\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9878\n",
      "Epoch 00029: val_loss did not improve from 0.22416\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0495 - acc: 0.9878 - val_loss: 0.3447 - val_acc: 0.9129\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9916\n",
      "Epoch 00030: val_loss improved from 0.22416 to 0.20210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/030-0.2021.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0373 - acc: 0.9915 - val_loss: 0.2021 - val_acc: 0.9436\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9859\n",
      "Epoch 00031: val_loss did not improve from 0.20210\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0545 - acc: 0.9859 - val_loss: 0.2115 - val_acc: 0.9441\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9921\n",
      "Epoch 00032: val_loss did not improve from 0.20210\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0332 - acc: 0.9921 - val_loss: 0.2062 - val_acc: 0.9441\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9931\n",
      "Epoch 00033: val_loss did not improve from 0.20210\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0311 - acc: 0.9931 - val_loss: 0.2312 - val_acc: 0.9406\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9928\n",
      "Epoch 00034: val_loss did not improve from 0.20210\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0329 - acc: 0.9928 - val_loss: 0.2305 - val_acc: 0.9425\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9926\n",
      "Epoch 00035: val_loss improved from 0.20210 to 0.18004, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv_checkpoint/035-0.1800.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0328 - acc: 0.9926 - val_loss: 0.1800 - val_acc: 0.9504\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9938\n",
      "Epoch 00036: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0305 - acc: 0.9938 - val_loss: 0.2294 - val_acc: 0.9439\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9938\n",
      "Epoch 00037: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0284 - acc: 0.9938 - val_loss: 0.2987 - val_acc: 0.9227\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9887\n",
      "Epoch 00038: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0451 - acc: 0.9887 - val_loss: 0.2713 - val_acc: 0.9299\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9954\n",
      "Epoch 00039: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0227 - acc: 0.9954 - val_loss: 0.2218 - val_acc: 0.9443\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9956\n",
      "Epoch 00040: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0214 - acc: 0.9956 - val_loss: 0.2221 - val_acc: 0.9474\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9950\n",
      "Epoch 00041: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0250 - acc: 0.9950 - val_loss: 0.2184 - val_acc: 0.9467\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9941\n",
      "Epoch 00042: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0264 - acc: 0.9941 - val_loss: 0.3028 - val_acc: 0.9173\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9960\n",
      "Epoch 00043: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0208 - acc: 0.9960 - val_loss: 0.2924 - val_acc: 0.9264\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9963\n",
      "Epoch 00044: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0196 - acc: 0.9963 - val_loss: 0.2102 - val_acc: 0.9490\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9923\n",
      "Epoch 00045: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0305 - acc: 0.9923 - val_loss: 0.1911 - val_acc: 0.9513\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9956\n",
      "Epoch 00046: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0205 - acc: 0.9956 - val_loss: 0.3115 - val_acc: 0.9280\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9954\n",
      "Epoch 00047: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0201 - acc: 0.9954 - val_loss: 0.2951 - val_acc: 0.9287\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9968\n",
      "Epoch 00048: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0164 - acc: 0.9968 - val_loss: 0.2039 - val_acc: 0.9467\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9971\n",
      "Epoch 00049: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0146 - acc: 0.9971 - val_loss: 0.2245 - val_acc: 0.9488\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9926\n",
      "Epoch 00050: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0297 - acc: 0.9926 - val_loss: 0.2190 - val_acc: 0.9492\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9967\n",
      "Epoch 00051: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0175 - acc: 0.9967 - val_loss: 0.1835 - val_acc: 0.9543\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9976\n",
      "Epoch 00052: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0117 - acc: 0.9976 - val_loss: 0.2438 - val_acc: 0.9429\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9928\n",
      "Epoch 00053: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0300 - acc: 0.9927 - val_loss: 0.1990 - val_acc: 0.9525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9974\n",
      "Epoch 00054: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0135 - acc: 0.9974 - val_loss: 0.2420 - val_acc: 0.9467\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9980\n",
      "Epoch 00055: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9980 - val_loss: 0.4013 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9933\n",
      "Epoch 00056: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0268 - acc: 0.9933 - val_loss: 0.2567 - val_acc: 0.9401\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9960\n",
      "Epoch 00057: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0178 - acc: 0.9960 - val_loss: 0.2073 - val_acc: 0.9511\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9978\n",
      "Epoch 00058: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0113 - acc: 0.9978 - val_loss: 0.2320 - val_acc: 0.9492\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9987\n",
      "Epoch 00059: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0084 - acc: 0.9987 - val_loss: 0.2348 - val_acc: 0.9513\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9938\n",
      "Epoch 00060: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0250 - acc: 0.9938 - val_loss: 0.2076 - val_acc: 0.9485\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9983\n",
      "Epoch 00061: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0097 - acc: 0.9983 - val_loss: 0.3016 - val_acc: 0.9313\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9972\n",
      "Epoch 00062: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0142 - acc: 0.9972 - val_loss: 0.2001 - val_acc: 0.9504\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9968\n",
      "Epoch 00063: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0141 - acc: 0.9968 - val_loss: 0.2750 - val_acc: 0.9357\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9971\n",
      "Epoch 00064: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0134 - acc: 0.9971 - val_loss: 0.2542 - val_acc: 0.9415\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974\n",
      "Epoch 00065: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0129 - acc: 0.9974 - val_loss: 0.1828 - val_acc: 0.9541\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9979\n",
      "Epoch 00066: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0098 - acc: 0.9979 - val_loss: 0.9282 - val_acc: 0.8062\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9968\n",
      "Epoch 00067: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0146 - acc: 0.9968 - val_loss: 0.3452 - val_acc: 0.9206\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9970\n",
      "Epoch 00068: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0142 - acc: 0.9970 - val_loss: 0.8987 - val_acc: 0.8246\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9950\n",
      "Epoch 00069: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0204 - acc: 0.9950 - val_loss: 0.1804 - val_acc: 0.9550\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9995\n",
      "Epoch 00070: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0045 - acc: 0.9995 - val_loss: 0.2213 - val_acc: 0.9515\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9946\n",
      "Epoch 00071: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0226 - acc: 0.9946 - val_loss: 0.2611 - val_acc: 0.9422\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9993\n",
      "Epoch 00072: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0052 - acc: 0.9993 - val_loss: 0.2644 - val_acc: 0.9467\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9974\n",
      "Epoch 00073: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0120 - acc: 0.9973 - val_loss: 0.2925 - val_acc: 0.9348\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9951\n",
      "Epoch 00074: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0189 - acc: 0.9951 - val_loss: 0.1939 - val_acc: 0.9550\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974\n",
      "Epoch 00075: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0129 - acc: 0.9974 - val_loss: 0.1966 - val_acc: 0.9532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 00076: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0051 - acc: 0.9993 - val_loss: 0.2625 - val_acc: 0.9432\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9970\n",
      "Epoch 00077: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0138 - acc: 0.9969 - val_loss: 0.3783 - val_acc: 0.9168\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9942\n",
      "Epoch 00078: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0221 - acc: 0.9942 - val_loss: 0.2035 - val_acc: 0.9495\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 00079: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0047 - acc: 0.9993 - val_loss: 0.2065 - val_acc: 0.9527\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9957\n",
      "Epoch 00080: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0185 - acc: 0.9957 - val_loss: 0.2493 - val_acc: 0.9432\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 00081: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0076 - acc: 0.9988 - val_loss: 0.2430 - val_acc: 0.9462\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9992\n",
      "Epoch 00082: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0062 - acc: 0.9992 - val_loss: 0.1851 - val_acc: 0.9574\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9982\n",
      "Epoch 00083: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0089 - acc: 0.9982 - val_loss: 0.2529 - val_acc: 0.9418\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 00084: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0152 - acc: 0.9962 - val_loss: 0.4504 - val_acc: 0.9092\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9968\n",
      "Epoch 00085: val_loss did not improve from 0.18004\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0141 - acc: 0.9967 - val_loss: 0.1976 - val_acc: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2356 - acc: 0.9350\n",
      "Loss: 0.23555388354066747 Accuracy: 0.9349948\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7990 - acc: 0.7676\n",
      "Epoch 00001: val_loss improved from inf to 0.72652, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/001-0.7265.hdf5\n",
      "36805/36805 [==============================] - 123s 3ms/sample - loss: 0.7989 - acc: 0.7676 - val_loss: 0.7265 - val_acc: 0.7969\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.9043\n",
      "Epoch 00002: val_loss improved from 0.72652 to 0.33230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/002-0.3323.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3391 - acc: 0.9043 - val_loss: 0.3323 - val_acc: 0.9068\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9295\n",
      "Epoch 00003: val_loss improved from 0.33230 to 0.28525, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/003-0.2852.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2493 - acc: 0.9295 - val_loss: 0.2852 - val_acc: 0.9178\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9433\n",
      "Epoch 00004: val_loss improved from 0.28525 to 0.20102, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/004-0.2010.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1969 - acc: 0.9433 - val_loss: 0.2010 - val_acc: 0.9399\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9530\n",
      "Epoch 00005: val_loss did not improve from 0.20102\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1657 - acc: 0.9529 - val_loss: 0.3304 - val_acc: 0.9001\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9545\n",
      "Epoch 00006: val_loss did not improve from 0.20102\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1562 - acc: 0.9545 - val_loss: 0.2438 - val_acc: 0.9276\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9633\n",
      "Epoch 00007: val_loss did not improve from 0.20102\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1257 - acc: 0.9633 - val_loss: 0.2469 - val_acc: 0.9243\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9673\n",
      "Epoch 00008: val_loss did not improve from 0.20102\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1153 - acc: 0.9672 - val_loss: 0.2617 - val_acc: 0.9236\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9695\n",
      "Epoch 00009: val_loss did not improve from 0.20102\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1079 - acc: 0.9695 - val_loss: 0.2618 - val_acc: 0.9192\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9752\n",
      "Epoch 00010: val_loss improved from 0.20102 to 0.18431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/010-0.1843.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0893 - acc: 0.9752 - val_loss: 0.1843 - val_acc: 0.9462\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9795\n",
      "Epoch 00011: val_loss did not improve from 0.18431\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0774 - acc: 0.9795 - val_loss: 0.1956 - val_acc: 0.9383\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9800\n",
      "Epoch 00012: val_loss did not improve from 0.18431\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0721 - acc: 0.9800 - val_loss: 0.2555 - val_acc: 0.9264\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9818\n",
      "Epoch 00013: val_loss did not improve from 0.18431\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0640 - acc: 0.9818 - val_loss: 0.2156 - val_acc: 0.9373\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9852\n",
      "Epoch 00014: val_loss did not improve from 0.18431\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0565 - acc: 0.9852 - val_loss: 0.2429 - val_acc: 0.9285\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9871\n",
      "Epoch 00015: val_loss improved from 0.18431 to 0.15477, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/015-0.1548.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0499 - acc: 0.9871 - val_loss: 0.1548 - val_acc: 0.9527\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9882\n",
      "Epoch 00016: val_loss did not improve from 0.15477\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0450 - acc: 0.9882 - val_loss: 0.2017 - val_acc: 0.9455\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9895\n",
      "Epoch 00017: val_loss did not improve from 0.15477\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0424 - acc: 0.9895 - val_loss: 0.2198 - val_acc: 0.9392\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9876\n",
      "Epoch 00018: val_loss did not improve from 0.15477\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0484 - acc: 0.9876 - val_loss: 0.1634 - val_acc: 0.9525\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9895\n",
      "Epoch 00019: val_loss improved from 0.15477 to 0.14309, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/019-0.1431.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0433 - acc: 0.9895 - val_loss: 0.1431 - val_acc: 0.9567\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9937\n",
      "Epoch 00020: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0278 - acc: 0.9937 - val_loss: 0.1725 - val_acc: 0.9534\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9928\n",
      "Epoch 00021: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0303 - acc: 0.9928 - val_loss: 0.1627 - val_acc: 0.9548\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9942\n",
      "Epoch 00022: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0247 - acc: 0.9942 - val_loss: 0.2025 - val_acc: 0.9453\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9943\n",
      "Epoch 00023: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0247 - acc: 0.9943 - val_loss: 0.2420 - val_acc: 0.9336\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9948\n",
      "Epoch 00024: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0235 - acc: 0.9948 - val_loss: 0.3606 - val_acc: 0.9166\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9905\n",
      "Epoch 00025: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0365 - acc: 0.9905 - val_loss: 0.2476 - val_acc: 0.9343\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9965\n",
      "Epoch 00026: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0170 - acc: 0.9965 - val_loss: 0.1739 - val_acc: 0.9543\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9965\n",
      "Epoch 00027: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0167 - acc: 0.9965 - val_loss: 0.1849 - val_acc: 0.9543\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9953\n",
      "Epoch 00028: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0212 - acc: 0.9953 - val_loss: 0.2017 - val_acc: 0.9513\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9920\n",
      "Epoch 00029: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0301 - acc: 0.9920 - val_loss: 0.1547 - val_acc: 0.9560\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9952\n",
      "Epoch 00030: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0199 - acc: 0.9952 - val_loss: 0.2226 - val_acc: 0.9429\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9975\n",
      "Epoch 00031: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0120 - acc: 0.9975 - val_loss: 0.2003 - val_acc: 0.9527\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9962\n",
      "Epoch 00032: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0182 - acc: 0.9962 - val_loss: 0.1698 - val_acc: 0.9539\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9968\n",
      "Epoch 00033: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0146 - acc: 0.9968 - val_loss: 0.2271 - val_acc: 0.9411\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9931\n",
      "Epoch 00034: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0261 - acc: 0.9931 - val_loss: 0.1543 - val_acc: 0.9560\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9984\n",
      "Epoch 00035: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0087 - acc: 0.9984 - val_loss: 0.1588 - val_acc: 0.9567\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9985\n",
      "Epoch 00036: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0088 - acc: 0.9985 - val_loss: 0.2315 - val_acc: 0.9464\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9898\n",
      "Epoch 00037: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0371 - acc: 0.9898 - val_loss: 0.1869 - val_acc: 0.9546\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9984\n",
      "Epoch 00038: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0090 - acc: 0.9984 - val_loss: 0.1958 - val_acc: 0.9539\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9983\n",
      "Epoch 00039: val_loss did not improve from 0.14309\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0094 - acc: 0.9983 - val_loss: 0.2372 - val_acc: 0.9446\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9926\n",
      "Epoch 00040: val_loss improved from 0.14309 to 0.12828, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/040-0.1283.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0267 - acc: 0.9926 - val_loss: 0.1283 - val_acc: 0.9641\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9942\n",
      "Epoch 00041: val_loss did not improve from 0.12828\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0221 - acc: 0.9942 - val_loss: 0.1765 - val_acc: 0.9539\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9966\n",
      "Epoch 00042: val_loss did not improve from 0.12828\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0141 - acc: 0.9966 - val_loss: 0.1549 - val_acc: 0.9599\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9993\n",
      "Epoch 00043: val_loss improved from 0.12828 to 0.12242, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv_checkpoint/043-0.1224.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0057 - acc: 0.9993 - val_loss: 0.1224 - val_acc: 0.9658\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9992\n",
      "Epoch 00044: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0052 - acc: 0.9992 - val_loss: 0.1569 - val_acc: 0.9583\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9973 - val_loss: 0.2589 - val_acc: 0.9359\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9972\n",
      "Epoch 00046: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.1901 - val_acc: 0.9546\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0097 - acc: 0.9982 - val_loss: 0.1748 - val_acc: 0.9536\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9977\n",
      "Epoch 00048: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0102 - acc: 0.9977 - val_loss: 0.1982 - val_acc: 0.9546\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9974\n",
      "Epoch 00049: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0110 - acc: 0.9974 - val_loss: 0.2288 - val_acc: 0.9464\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9981\n",
      "Epoch 00050: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0093 - acc: 0.9981 - val_loss: 0.2985 - val_acc: 0.9329\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9976\n",
      "Epoch 00051: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0104 - acc: 0.9976 - val_loss: 0.1485 - val_acc: 0.9639\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9978\n",
      "Epoch 00052: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0096 - acc: 0.9978 - val_loss: 0.2388 - val_acc: 0.9429\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9977\n",
      "Epoch 00053: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0105 - acc: 0.9977 - val_loss: 0.1575 - val_acc: 0.9583\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9983\n",
      "Epoch 00054: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1839 - val_acc: 0.9578\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 00055: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0089 - acc: 0.9977 - val_loss: 0.2494 - val_acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9986\n",
      "Epoch 00056: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.1917 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 00057: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0104 - acc: 0.9969 - val_loss: 0.1724 - val_acc: 0.9578\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 00058: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0045 - acc: 0.9993 - val_loss: 0.2467 - val_acc: 0.9471\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9988\n",
      "Epoch 00059: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.2299 - val_acc: 0.9469\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9965\n",
      "Epoch 00060: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1603 - val_acc: 0.9599\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9965\n",
      "Epoch 00061: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0141 - acc: 0.9965 - val_loss: 0.1699 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 00062: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0038 - acc: 0.9995 - val_loss: 0.2042 - val_acc: 0.9513\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9990\n",
      "Epoch 00063: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.2307 - val_acc: 0.9439\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9964\n",
      "Epoch 00064: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0139 - acc: 0.9964 - val_loss: 0.1470 - val_acc: 0.9639\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 00065: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1370 - val_acc: 0.9648\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 00066: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.1473 - val_acc: 0.9655\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9973\n",
      "Epoch 00067: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9973 - val_loss: 0.2585 - val_acc: 0.9422\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9961\n",
      "Epoch 00068: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0167 - acc: 0.9961 - val_loss: 0.1350 - val_acc: 0.9655\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00069: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1324 - val_acc: 0.9683\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00070: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1275 - val_acc: 0.9704\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9977\n",
      "Epoch 00071: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1468 - val_acc: 0.9632\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978\n",
      "Epoch 00072: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0082 - acc: 0.9978 - val_loss: 0.1295 - val_acc: 0.9683\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00073: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.1584 - val_acc: 0.9609\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9990\n",
      "Epoch 00074: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1559 - val_acc: 0.9609\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9975\n",
      "Epoch 00075: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0099 - acc: 0.9975 - val_loss: 0.2008 - val_acc: 0.9548\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1599 - val_acc: 0.9625\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00077: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.3195 - val_acc: 0.9241\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 00078: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0083 - acc: 0.9980 - val_loss: 0.2414 - val_acc: 0.9495\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00079: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.2617 - val_acc: 0.9460\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n",
      "Epoch 00080: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0052 - acc: 0.9988 - val_loss: 0.1807 - val_acc: 0.9581\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9961\n",
      "Epoch 00081: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0151 - acc: 0.9961 - val_loss: 0.1478 - val_acc: 0.9660\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00082: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1414 - val_acc: 0.9669\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00083: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1978 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9974\n",
      "Epoch 00084: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9974 - val_loss: 0.1540 - val_acc: 0.9627\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00085: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0039 - acc: 0.9993 - val_loss: 0.1489 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9961\n",
      "Epoch 00086: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0139 - acc: 0.9961 - val_loss: 0.1510 - val_acc: 0.9627\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00087: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1431 - val_acc: 0.9644\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9979\n",
      "Epoch 00088: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0085 - acc: 0.9979 - val_loss: 0.1429 - val_acc: 0.9653\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00089: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1431 - val_acc: 0.9658\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9982\n",
      "Epoch 00090: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.1693 - val_acc: 0.9620\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00091: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.1524 - val_acc: 0.9639\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 00092: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1690 - val_acc: 0.9597\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00093: val_loss did not improve from 0.12242\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1729 - val_acc: 0.9602\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1605 - acc: 0.9549\n",
      "Loss: 0.16049782080248942 Accuracy: 0.9549325\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7484 - acc: 0.7750\n",
      "Epoch 00001: val_loss improved from inf to 0.71903, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/001-0.7190.hdf5\n",
      "36805/36805 [==============================] - 129s 4ms/sample - loss: 0.7483 - acc: 0.7751 - val_loss: 0.7190 - val_acc: 0.7778\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9095\n",
      "Epoch 00002: val_loss improved from 0.71903 to 0.32233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/002-0.3223.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2989 - acc: 0.9095 - val_loss: 0.3223 - val_acc: 0.9001\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9358\n",
      "Epoch 00003: val_loss improved from 0.32233 to 0.23115, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/003-0.2312.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2109 - acc: 0.9358 - val_loss: 0.2312 - val_acc: 0.9324\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9491\n",
      "Epoch 00004: val_loss improved from 0.23115 to 0.22678, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/004-0.2268.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1675 - acc: 0.9491 - val_loss: 0.2268 - val_acc: 0.9276\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9542\n",
      "Epoch 00005: val_loss improved from 0.22678 to 0.17398, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/005-0.1740.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1483 - acc: 0.9542 - val_loss: 0.1740 - val_acc: 0.9474\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9648\n",
      "Epoch 00006: val_loss did not improve from 0.17398\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1156 - acc: 0.9648 - val_loss: 0.2108 - val_acc: 0.9364\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9689\n",
      "Epoch 00007: val_loss did not improve from 0.17398\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1050 - acc: 0.9689 - val_loss: 0.2147 - val_acc: 0.9385\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9766\n",
      "Epoch 00008: val_loss did not improve from 0.17398\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0806 - acc: 0.9766 - val_loss: 0.1887 - val_acc: 0.9450\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9755\n",
      "Epoch 00009: val_loss did not improve from 0.17398\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0799 - acc: 0.9755 - val_loss: 0.2533 - val_acc: 0.9276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9790\n",
      "Epoch 00010: val_loss did not improve from 0.17398\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0694 - acc: 0.9790 - val_loss: 0.2794 - val_acc: 0.9101\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9792\n",
      "Epoch 00011: val_loss improved from 0.17398 to 0.15661, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/011-0.1566.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0709 - acc: 0.9792 - val_loss: 0.1566 - val_acc: 0.9513\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9883\n",
      "Epoch 00012: val_loss improved from 0.15661 to 0.14425, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/012-0.1443.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0447 - acc: 0.9883 - val_loss: 0.1443 - val_acc: 0.9618\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9878\n",
      "Epoch 00013: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0429 - acc: 0.9878 - val_loss: 0.2707 - val_acc: 0.9238\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9877\n",
      "Epoch 00014: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0431 - acc: 0.9877 - val_loss: 0.1978 - val_acc: 0.9460\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9908\n",
      "Epoch 00015: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0341 - acc: 0.9908 - val_loss: 0.1797 - val_acc: 0.9478\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9880\n",
      "Epoch 00016: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0408 - acc: 0.9880 - val_loss: 0.1740 - val_acc: 0.9481\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9932\n",
      "Epoch 00017: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0248 - acc: 0.9932 - val_loss: 0.1748 - val_acc: 0.9499\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9937\n",
      "Epoch 00018: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0253 - acc: 0.9937 - val_loss: 0.1914 - val_acc: 0.9455\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9913\n",
      "Epoch 00019: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0311 - acc: 0.9913 - val_loss: 0.1719 - val_acc: 0.9546\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9961\n",
      "Epoch 00020: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0182 - acc: 0.9961 - val_loss: 0.2694 - val_acc: 0.9313\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9940\n",
      "Epoch 00021: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0233 - acc: 0.9940 - val_loss: 0.2289 - val_acc: 0.9404\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9940\n",
      "Epoch 00022: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0221 - acc: 0.9940 - val_loss: 0.1890 - val_acc: 0.9502\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9948\n",
      "Epoch 00023: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0200 - acc: 0.9948 - val_loss: 0.2060 - val_acc: 0.9485\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9927\n",
      "Epoch 00024: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0258 - acc: 0.9927 - val_loss: 0.1566 - val_acc: 0.9592\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9930\n",
      "Epoch 00025: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0251 - acc: 0.9930 - val_loss: 0.1517 - val_acc: 0.9590\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 00026: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9977 - val_loss: 0.1660 - val_acc: 0.9613\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9961\n",
      "Epoch 00027: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 0.1721 - val_acc: 0.9546\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9964\n",
      "Epoch 00028: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0141 - acc: 0.9964 - val_loss: 0.1786 - val_acc: 0.9546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9969\n",
      "Epoch 00029: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0131 - acc: 0.9969 - val_loss: 0.2220 - val_acc: 0.9453\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9967\n",
      "Epoch 00030: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0139 - acc: 0.9967 - val_loss: 0.2311 - val_acc: 0.9439\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9967\n",
      "Epoch 00031: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0129 - acc: 0.9967 - val_loss: 0.1613 - val_acc: 0.9571\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9967\n",
      "Epoch 00032: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.2389 - val_acc: 0.9471\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9930\n",
      "Epoch 00033: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0251 - acc: 0.9930 - val_loss: 0.2137 - val_acc: 0.9509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9951\n",
      "Epoch 00034: val_loss did not improve from 0.14425\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0176 - acc: 0.9951 - val_loss: 0.1795 - val_acc: 0.9576\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9989\n",
      "Epoch 00035: val_loss improved from 0.14425 to 0.13795, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/035-0.1379.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0064 - acc: 0.9989 - val_loss: 0.1379 - val_acc: 0.9651\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9991\n",
      "Epoch 00036: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.2934 - val_acc: 0.9304\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9971\n",
      "Epoch 00037: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0111 - acc: 0.9971 - val_loss: 0.1899 - val_acc: 0.9518\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9959\n",
      "Epoch 00038: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0138 - acc: 0.9959 - val_loss: 0.1833 - val_acc: 0.9539\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00039: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.1955 - val_acc: 0.9569\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 00040: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0072 - acc: 0.9983 - val_loss: 0.2030 - val_acc: 0.9532\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9936\n",
      "Epoch 00041: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0211 - acc: 0.9936 - val_loss: 0.1526 - val_acc: 0.9599\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9989\n",
      "Epoch 00042: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0059 - acc: 0.9989 - val_loss: 0.1461 - val_acc: 0.9646\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00043: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.1994 - val_acc: 0.9550\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9923\n",
      "Epoch 00044: val_loss did not improve from 0.13795\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 0.1957 - val_acc: 0.9553\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9978\n",
      "Epoch 00045: val_loss improved from 0.13795 to 0.13532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/045-0.1353.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0097 - acc: 0.9978 - val_loss: 0.1353 - val_acc: 0.9658\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 00046: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1605 - val_acc: 0.9625\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00047: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.1638 - val_acc: 0.9644\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9984\n",
      "Epoch 00048: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0075 - acc: 0.9984 - val_loss: 0.2553 - val_acc: 0.9408\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9964\n",
      "Epoch 00049: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0126 - acc: 0.9964 - val_loss: 0.2706 - val_acc: 0.9383\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00050: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0071 - acc: 0.9980 - val_loss: 0.1910 - val_acc: 0.9583\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9951\n",
      "Epoch 00051: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.1476 - val_acc: 0.9646\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00052: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1656 - val_acc: 0.9597\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
      "Epoch 00053: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0137 - acc: 0.9960 - val_loss: 0.1779 - val_acc: 0.9578\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00054: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1805 - val_acc: 0.9602\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00055: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0041 - acc: 0.9991 - val_loss: 0.1559 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00056: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1962 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0118 - acc: 0.9964 - val_loss: 0.1955 - val_acc: 0.9616\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 00058: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1496 - val_acc: 0.9632\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00059: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1515 - val_acc: 0.9644\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 00060: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.2135 - val_acc: 0.9532\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00061: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.2003 - val_acc: 0.9550\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00062: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.2053 - val_acc: 0.9571\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9990\n",
      "Epoch 00063: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0047 - acc: 0.9990 - val_loss: 0.1585 - val_acc: 0.9625\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9972\n",
      "Epoch 00064: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.2538 - val_acc: 0.9453\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 00065: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.1645 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00066: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1702 - val_acc: 0.9625\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9955\n",
      "Epoch 00067: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0161 - acc: 0.9955 - val_loss: 0.1660 - val_acc: 0.9630\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00068: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1687 - val_acc: 0.9613\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00069: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1498 - val_acc: 0.9665\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0122 - acc: 0.9959 - val_loss: 0.1798 - val_acc: 0.9620\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00071: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1603 - val_acc: 0.9672\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00072: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.2583 - val_acc: 0.9478\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00073: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1719 - val_acc: 0.9606\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00074: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1631 - val_acc: 0.9658\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00075: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1911 - val_acc: 0.9557\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 00076: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0115 - acc: 0.9965 - val_loss: 0.1910 - val_acc: 0.9553\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991\n",
      "Epoch 00077: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1504 - val_acc: 0.9669\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00078: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.2165 - val_acc: 0.9541\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9982\n",
      "Epoch 00079: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1558 - val_acc: 0.9653\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2224 - val_acc: 0.9467\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 00081: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1683 - val_acc: 0.9644\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00082: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.2071 - val_acc: 0.9588\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 00083: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2615 - val_acc: 0.9434\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00084: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.2288 - val_acc: 0.9499\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9982\n",
      "Epoch 00085: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1936 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9970\n",
      "Epoch 00086: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0095 - acc: 0.9970 - val_loss: 0.1387 - val_acc: 0.9683\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9983\n",
      "Epoch 00087: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0065 - acc: 0.9983 - val_loss: 0.1385 - val_acc: 0.9660\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00088: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1436 - val_acc: 0.9637\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00089: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1491 - val_acc: 0.9660\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 00090: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1661 - val_acc: 0.9632\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00091: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1501 - val_acc: 0.9665\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00092: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.1781 - val_acc: 0.9616\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00093: val_loss did not improve from 0.13532\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1651 - val_acc: 0.9651\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00094: val_loss improved from 0.13532 to 0.12646, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/094-0.1265.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1265 - val_acc: 0.9702\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1672 - val_acc: 0.9623\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00096: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.1392 - val_acc: 0.9683\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00097: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1526 - val_acc: 0.9679\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.4054e-04 - acc: 0.9999\n",
      "Epoch 00098: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 9.4041e-04 - acc: 0.9999 - val_loss: 0.1336 - val_acc: 0.9706\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.4228e-04 - acc: 0.9998\n",
      "Epoch 00099: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 9.4215e-04 - acc: 0.9998 - val_loss: 0.1511 - val_acc: 0.9688\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 00100: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0046 - acc: 0.9984 - val_loss: 0.2382 - val_acc: 0.9513\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00101: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.1580 - val_acc: 0.9658\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00102: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1462 - val_acc: 0.9674\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.6188e-04 - acc: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 6.6180e-04 - acc: 1.0000 - val_loss: 0.1478 - val_acc: 0.9688\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 00104: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 0.1888 - val_acc: 0.9639\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9982\n",
      "Epoch 00105: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0055 - acc: 0.9982 - val_loss: 0.1620 - val_acc: 0.9646\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00106: val_loss did not improve from 0.12646\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1834 - val_acc: 0.9637\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00107: val_loss improved from 0.12646 to 0.12233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv_checkpoint/107-0.1223.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.1223 - val_acc: 0.9727\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.6046e-04 - acc: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 5.6793e-04 - acc: 1.0000 - val_loss: 0.1324 - val_acc: 0.9697\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00109: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0037 - acc: 0.9989 - val_loss: 0.1875 - val_acc: 0.9581\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00110: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.1572 - val_acc: 0.9690\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 00111: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1537 - val_acc: 0.9688\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9982\n",
      "Epoch 00112: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1902 - val_acc: 0.9599\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 00113: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1573 - val_acc: 0.9653\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00114: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1715 - val_acc: 0.9655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.1669e-04 - acc: 0.9999\n",
      "Epoch 00115: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1425 - val_acc: 0.9690\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 00116: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 0.1486 - val_acc: 0.9688\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00117: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.1783 - val_acc: 0.9620\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9989\n",
      "Epoch 00118: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0042 - acc: 0.9989 - val_loss: 0.1369 - val_acc: 0.9713\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.5843e-04 - acc: 0.9999\n",
      "Epoch 00119: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 6.6147e-04 - acc: 0.9999 - val_loss: 0.1413 - val_acc: 0.9695\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.5357e-04 - acc: 0.9999\n",
      "Epoch 00120: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 9.5346e-04 - acc: 0.9999 - val_loss: 0.1439 - val_acc: 0.9693\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00121: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.2501 - val_acc: 0.9553\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.6508e-04 - acc: 0.9998\n",
      "Epoch 00122: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 9.6501e-04 - acc: 0.9998 - val_loss: 0.1594 - val_acc: 0.9681\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4101e-04 - acc: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 3.4098e-04 - acc: 1.0000 - val_loss: 0.1484 - val_acc: 0.9704\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
      "Epoch 00124: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0086 - acc: 0.9973 - val_loss: 0.2621 - val_acc: 0.9529\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 00125: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.1523 - val_acc: 0.9681\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00126: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1678 - val_acc: 0.9662\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 00127: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0058 - acc: 0.9983 - val_loss: 0.2380 - val_acc: 0.9511\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 00128: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1670 - val_acc: 0.9676\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00129: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.2450 - val_acc: 0.9520\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00130: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0067 - acc: 0.9979 - val_loss: 0.2011 - val_acc: 0.9602\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 00131: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1535 - val_acc: 0.9681\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00132: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1478 - val_acc: 0.9702\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00133: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1601 - val_acc: 0.9697\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9979\n",
      "Epoch 00134: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0087 - acc: 0.9979 - val_loss: 0.1614 - val_acc: 0.9674\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00135: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1501 - val_acc: 0.9683\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5476e-04 - acc: 0.9999\n",
      "Epoch 00136: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 4.7404e-04 - acc: 0.9999 - val_loss: 0.1815 - val_acc: 0.9639\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 00137: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0091 - acc: 0.9975 - val_loss: 0.1643 - val_acc: 0.9655\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
      "Epoch 00138: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.1386 - val_acc: 0.9704\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.3328e-04 - acc: 0.9998\n",
      "Epoch 00139: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 7.4479e-04 - acc: 0.9998 - val_loss: 0.1568 - val_acc: 0.9665\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00140: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1666 - val_acc: 0.9674\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
      "Epoch 00141: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1735 - val_acc: 0.9653\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5120e-04 - acc: 0.9999\n",
      "Epoch 00142: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 4.5115e-04 - acc: 0.9999 - val_loss: 0.1414 - val_acc: 0.9700\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7975e-04 - acc: 0.9999\n",
      "Epoch 00143: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 3.7970e-04 - acc: 0.9999 - val_loss: 0.1589 - val_acc: 0.9683\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 00144: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0062 - acc: 0.9983 - val_loss: 0.1942 - val_acc: 0.9616\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9992\n",
      "Epoch 00145: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9992 - val_loss: 0.1618 - val_acc: 0.9676\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00146: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1614 - val_acc: 0.9665\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00147: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.3008 - val_acc: 0.9418\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991\n",
      "Epoch 00148: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1471 - val_acc: 0.9679\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
      "Epoch 00149: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1418 - val_acc: 0.9711\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998    \n",
      "Epoch 00150: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1894 - val_acc: 0.9625\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0663e-04 - acc: 0.9999\n",
      "Epoch 00151: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 5.0844e-04 - acc: 0.9999 - val_loss: 0.1528 - val_acc: 0.9690\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.8440e-04 - acc: 0.9998\n",
      "Epoch 00152: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 8.8428e-04 - acc: 0.9998 - val_loss: 0.1653 - val_acc: 0.9690\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00153: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.3318 - val_acc: 0.9390\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 00154: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0097 - acc: 0.9971 - val_loss: 0.1610 - val_acc: 0.9679\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.0215e-04 - acc: 0.9999\n",
      "Epoch 00155: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 8.0204e-04 - acc: 0.9999 - val_loss: 0.1586 - val_acc: 0.9704\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.5906e-04 - acc: 0.9999\n",
      "Epoch 00156: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 5.5986e-04 - acc: 0.9999 - val_loss: 0.1592 - val_acc: 0.9679\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.6404e-04 - acc: 0.9998\n",
      "Epoch 00157: val_loss did not improve from 0.12233\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 6.6395e-04 - acc: 0.9998 - val_loss: 0.1708 - val_acc: 0.9669\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1889 - acc: 0.9605\n",
      "Loss: 0.18888087004397996 Accuracy: 0.96054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 256)          1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8053 - acc: 0.7711\n",
      "Loss: 0.8053225175118768 Accuracy: 0.7711319\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 256)          1024        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6988 - acc: 0.8019\n",
      "Loss: 0.6988116537299112 Accuracy: 0.80186915\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 384)          1536        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3680 - acc: 0.9057\n",
      "Loss: 0.36801885258742956 Accuracy: 0.9057113\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 512)          2048        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2356 - acc: 0.9350\n",
      "Loss: 0.23555388354066747 Accuracy: 0.9349948\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 512)          2048        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1605 - acc: 0.9549\n",
      "Loss: 0.16049782080248942 Accuracy: 0.9549325\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 512)          2048        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1889 - acc: 0.9605\n",
      "Loss: 0.18888087004397996 Accuracy: 0.96054\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 256)          1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.4823 - acc: 0.5923\n",
      "Loss: 1.4823232943642795 Accuracy: 0.5923157\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 256)          1024        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.0139 - acc: 0.7512\n",
      "Loss: 1.013915139152006 Accuracy: 0.7511942\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 384)          1536        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5153 - acc: 0.8868\n",
      "Loss: 0.5153254756427381 Accuracy: 0.88681203\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 512)          2048        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2584 - acc: 0.9425\n",
      "Loss: 0.2583670324238785 Accuracy: 0.94247144\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 512)          2048        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1878 - acc: 0.9562\n",
      "Loss: 0.18779227632219211 Accuracy: 0.9561786\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 512)          2048        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1990 - acc: 0.9572\n",
      "Loss: 0.19897295174348242 Accuracy: 0.95721704\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
