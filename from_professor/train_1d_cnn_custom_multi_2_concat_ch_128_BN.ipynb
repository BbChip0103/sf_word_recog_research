{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 227456)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 75776)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303232)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 303232)       1212928     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4851728     batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 6,231,056\n",
      "Trainable params: 5,623,824\n",
      "Non-trainable params: 607,232\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 75776)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25216)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100992)       0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 100992)       403968      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1615888     batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 2,268,816\n",
      "Trainable params: 2,065,808\n",
      "Non-trainable params: 203,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 25216)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 16640)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 41856)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 41856)        167424      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           669712      batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,251,216\n",
      "Trainable params: 1,165,968\n",
      "Non-trainable params: 85,248\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 16640)        0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 5376)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22016)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 22016)        88064       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           352272      batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,183,376\n",
      "Trainable params: 1,137,296\n",
      "Non-trainable params: 46,080\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 5376)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1792)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7168)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 7168)         28672       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           114704      batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,215,376\n",
      "Trainable params: 1,198,480\n",
      "Non-trainable params: 16,896\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1792)         0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 512)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2304)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 2304)         9216        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           36880       batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,447,056\n",
      "Trainable params: 1,439,376\n",
      "Non-trainable params: 7,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9917 - acc: 0.5108\n",
      "Epoch 00001: val_loss improved from inf to 1.63963, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv_checkpoint/001-1.6396.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.9918 - acc: 0.5108 - val_loss: 1.6396 - val_acc: 0.5656\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0203 - acc: 0.7310\n",
      "Epoch 00002: val_loss improved from 1.63963 to 1.60884, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv_checkpoint/002-1.6088.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.0202 - acc: 0.7310 - val_loss: 1.6088 - val_acc: 0.6252\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8375\n",
      "Epoch 00003: val_loss improved from 1.60884 to 1.58014, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv_checkpoint/003-1.5801.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5891 - acc: 0.8375 - val_loss: 1.5801 - val_acc: 0.6436\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.9053\n",
      "Epoch 00004: val_loss improved from 1.58014 to 1.45764, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv_checkpoint/004-1.4576.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3468 - acc: 0.9053 - val_loss: 1.4576 - val_acc: 0.6685\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9380\n",
      "Epoch 00005: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2371 - acc: 0.9380 - val_loss: 1.6821 - val_acc: 0.6515\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9540\n",
      "Epoch 00006: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1793 - acc: 0.9540 - val_loss: 1.9030 - val_acc: 0.6243\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9662\n",
      "Epoch 00007: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1439 - acc: 0.9662 - val_loss: 2.1057 - val_acc: 0.6143\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9608\n",
      "Epoch 00008: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1583 - acc: 0.9608 - val_loss: 1.7927 - val_acc: 0.6527\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9654\n",
      "Epoch 00009: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1403 - acc: 0.9653 - val_loss: 2.1820 - val_acc: 0.6236\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9590\n",
      "Epoch 00010: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1574 - acc: 0.9591 - val_loss: 1.9569 - val_acc: 0.6443\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9688\n",
      "Epoch 00011: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1242 - acc: 0.9687 - val_loss: 2.0386 - val_acc: 0.6543\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9547\n",
      "Epoch 00012: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1759 - acc: 0.9547 - val_loss: 2.3087 - val_acc: 0.6273\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9824\n",
      "Epoch 00013: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0829 - acc: 0.9824 - val_loss: 2.0677 - val_acc: 0.6611\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9734\n",
      "Epoch 00014: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1105 - acc: 0.9734 - val_loss: 2.2562 - val_acc: 0.6499\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9713\n",
      "Epoch 00015: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1175 - acc: 0.9713 - val_loss: 2.5129 - val_acc: 0.6271\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9777\n",
      "Epoch 00016: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0939 - acc: 0.9777 - val_loss: 2.2624 - val_acc: 0.6667\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9758\n",
      "Epoch 00017: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0971 - acc: 0.9758 - val_loss: 2.4606 - val_acc: 0.6431\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9754\n",
      "Epoch 00018: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1071 - acc: 0.9754 - val_loss: 2.3948 - val_acc: 0.6529\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9789\n",
      "Epoch 00019: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0907 - acc: 0.9789 - val_loss: 2.7453 - val_acc: 0.6173\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9817\n",
      "Epoch 00020: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0818 - acc: 0.9817 - val_loss: 2.5043 - val_acc: 0.6669\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9840\n",
      "Epoch 00021: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0768 - acc: 0.9840 - val_loss: 2.8577 - val_acc: 0.6301\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9801\n",
      "Epoch 00022: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0903 - acc: 0.9800 - val_loss: 2.4657 - val_acc: 0.6653\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9804\n",
      "Epoch 00023: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0871 - acc: 0.9803 - val_loss: 2.2617 - val_acc: 0.6813\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9841\n",
      "Epoch 00024: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0742 - acc: 0.9841 - val_loss: 2.5972 - val_acc: 0.6494\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9835\n",
      "Epoch 00025: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0775 - acc: 0.9834 - val_loss: 2.4173 - val_acc: 0.6816\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9763\n",
      "Epoch 00026: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1034 - acc: 0.9762 - val_loss: 2.4589 - val_acc: 0.6730\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9827\n",
      "Epoch 00027: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0780 - acc: 0.9827 - val_loss: 2.4566 - val_acc: 0.6844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9864\n",
      "Epoch 00028: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0614 - acc: 0.9864 - val_loss: 2.6424 - val_acc: 0.6655\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9900\n",
      "Epoch 00029: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0511 - acc: 0.9899 - val_loss: 2.6894 - val_acc: 0.6564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9789\n",
      "Epoch 00030: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0975 - acc: 0.9788 - val_loss: 2.5496 - val_acc: 0.6760\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9842\n",
      "Epoch 00031: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0733 - acc: 0.9842 - val_loss: 2.6683 - val_acc: 0.6748\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9795\n",
      "Epoch 00032: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0966 - acc: 0.9795 - val_loss: 2.7338 - val_acc: 0.6741\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9886\n",
      "Epoch 00033: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0599 - acc: 0.9886 - val_loss: 2.7237 - val_acc: 0.6669\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9888\n",
      "Epoch 00034: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0569 - acc: 0.9888 - val_loss: 2.9428 - val_acc: 0.6385\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9908\n",
      "Epoch 00035: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0458 - acc: 0.9907 - val_loss: 2.7952 - val_acc: 0.6681\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9839\n",
      "Epoch 00036: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0764 - acc: 0.9839 - val_loss: 2.5975 - val_acc: 0.6841\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9873\n",
      "Epoch 00037: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0622 - acc: 0.9873 - val_loss: 2.8111 - val_acc: 0.6578\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9898\n",
      "Epoch 00038: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0541 - acc: 0.9898 - val_loss: 2.9525 - val_acc: 0.6569\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9862\n",
      "Epoch 00039: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0674 - acc: 0.9862 - val_loss: 2.6166 - val_acc: 0.6923\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9880\n",
      "Epoch 00040: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0635 - acc: 0.9880 - val_loss: 2.8442 - val_acc: 0.6653\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9868\n",
      "Epoch 00041: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0707 - acc: 0.9868 - val_loss: 2.8619 - val_acc: 0.6671\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9886\n",
      "Epoch 00042: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0570 - acc: 0.9886 - val_loss: 2.9651 - val_acc: 0.6653\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9873\n",
      "Epoch 00043: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0636 - acc: 0.9873 - val_loss: 2.7321 - val_acc: 0.6811\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9895\n",
      "Epoch 00044: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0554 - acc: 0.9895 - val_loss: 2.8618 - val_acc: 0.6704\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9866\n",
      "Epoch 00045: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0690 - acc: 0.9866 - val_loss: 2.6705 - val_acc: 0.6986\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9892\n",
      "Epoch 00046: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0590 - acc: 0.9892 - val_loss: 2.7350 - val_acc: 0.6976\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9901\n",
      "Epoch 00047: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0534 - acc: 0.9901 - val_loss: 2.8464 - val_acc: 0.6853\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9888\n",
      "Epoch 00048: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0564 - acc: 0.9888 - val_loss: 2.7627 - val_acc: 0.6809\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9903\n",
      "Epoch 00049: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0522 - acc: 0.9902 - val_loss: 2.6888 - val_acc: 0.6893\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9871\n",
      "Epoch 00050: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0649 - acc: 0.9871 - val_loss: 2.7588 - val_acc: 0.6890\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9893\n",
      "Epoch 00051: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0590 - acc: 0.9893 - val_loss: 3.2853 - val_acc: 0.6387\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0408 - acc: 0.9927 - val_loss: 2.7479 - val_acc: 0.6886\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9860\n",
      "Epoch 00053: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0662 - acc: 0.9859 - val_loss: 2.8099 - val_acc: 0.6832\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9921\n",
      "Epoch 00054: val_loss did not improve from 1.45764\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0435 - acc: 0.9921 - val_loss: 2.8236 - val_acc: 0.6869\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6617 - acc: 0.6268\n",
      "Loss: 1.6617056480449308 Accuracy: 0.6267913\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6073 - acc: 0.5407\n",
      "Epoch 00001: val_loss improved from inf to 1.45173, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv_checkpoint/001-1.4517.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 1.6074 - acc: 0.5407 - val_loss: 1.4517 - val_acc: 0.5637\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9895 - acc: 0.7186\n",
      "Epoch 00002: val_loss improved from 1.45173 to 1.06259, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv_checkpoint/002-1.0626.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9897 - acc: 0.7185 - val_loss: 1.0626 - val_acc: 0.7063\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7002 - acc: 0.7962\n",
      "Epoch 00003: val_loss improved from 1.06259 to 0.93367, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv_checkpoint/003-0.9337.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7003 - acc: 0.7962 - val_loss: 0.9337 - val_acc: 0.7545\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8457\n",
      "Epoch 00004: val_loss did not improve from 0.93367\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5232 - acc: 0.8456 - val_loss: 1.0127 - val_acc: 0.7279\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8900\n",
      "Epoch 00005: val_loss improved from 0.93367 to 0.83588, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv_checkpoint/005-0.8359.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3756 - acc: 0.8899 - val_loss: 0.8359 - val_acc: 0.7771\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9176\n",
      "Epoch 00006: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2864 - acc: 0.9176 - val_loss: 1.0337 - val_acc: 0.7449\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9415\n",
      "Epoch 00007: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2122 - acc: 0.9415 - val_loss: 0.9470 - val_acc: 0.7610\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9456\n",
      "Epoch 00008: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1976 - acc: 0.9456 - val_loss: 0.9052 - val_acc: 0.7775\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9621\n",
      "Epoch 00009: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1416 - acc: 0.9621 - val_loss: 0.9930 - val_acc: 0.7517\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9685\n",
      "Epoch 00010: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1214 - acc: 0.9685 - val_loss: 1.0187 - val_acc: 0.7638\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9668\n",
      "Epoch 00011: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1288 - acc: 0.9668 - val_loss: 1.0371 - val_acc: 0.7594\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9629\n",
      "Epoch 00012: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1349 - acc: 0.9629 - val_loss: 0.9502 - val_acc: 0.7929\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9832\n",
      "Epoch 00013: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0736 - acc: 0.9831 - val_loss: 1.0541 - val_acc: 0.7675\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9713\n",
      "Epoch 00014: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1094 - acc: 0.9713 - val_loss: 0.9656 - val_acc: 0.7897\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9776\n",
      "Epoch 00015: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0878 - acc: 0.9775 - val_loss: 1.0656 - val_acc: 0.7661\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9770\n",
      "Epoch 00016: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0887 - acc: 0.9770 - val_loss: 0.8739 - val_acc: 0.8143\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9860\n",
      "Epoch 00017: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0585 - acc: 0.9860 - val_loss: 1.1422 - val_acc: 0.7701\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9812\n",
      "Epoch 00018: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0735 - acc: 0.9812 - val_loss: 1.1935 - val_acc: 0.7652\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9839\n",
      "Epoch 00019: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0659 - acc: 0.9839 - val_loss: 0.9544 - val_acc: 0.8092\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9772\n",
      "Epoch 00020: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0851 - acc: 0.9772 - val_loss: 0.9451 - val_acc: 0.8076\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9889\n",
      "Epoch 00021: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0482 - acc: 0.9888 - val_loss: 1.0089 - val_acc: 0.8018\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9827\n",
      "Epoch 00022: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0658 - acc: 0.9827 - val_loss: 1.1563 - val_acc: 0.7694\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9822\n",
      "Epoch 00023: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0706 - acc: 0.9822 - val_loss: 0.9969 - val_acc: 0.8048\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9833\n",
      "Epoch 00024: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0652 - acc: 0.9833 - val_loss: 1.1685 - val_acc: 0.7822\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9910\n",
      "Epoch 00025: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0403 - acc: 0.9910 - val_loss: 1.0528 - val_acc: 0.7883\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9855\n",
      "Epoch 00026: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0559 - acc: 0.9855 - val_loss: 1.1092 - val_acc: 0.7943\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9895\n",
      "Epoch 00027: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0438 - acc: 0.9895 - val_loss: 1.2862 - val_acc: 0.7638\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9821\n",
      "Epoch 00028: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0668 - acc: 0.9821 - val_loss: 1.4073 - val_acc: 0.7554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9860\n",
      "Epoch 00029: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0557 - acc: 0.9860 - val_loss: 1.2493 - val_acc: 0.7841\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9905\n",
      "Epoch 00030: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0379 - acc: 0.9904 - val_loss: 1.2360 - val_acc: 0.7838\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9840\n",
      "Epoch 00031: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0622 - acc: 0.9840 - val_loss: 0.9948 - val_acc: 0.8176\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9940\n",
      "Epoch 00032: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0310 - acc: 0.9940 - val_loss: 1.0860 - val_acc: 0.8160\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9804\n",
      "Epoch 00033: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0741 - acc: 0.9804 - val_loss: 1.2807 - val_acc: 0.7845\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9911\n",
      "Epoch 00034: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0382 - acc: 0.9911 - val_loss: 1.4799 - val_acc: 0.7531\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9900\n",
      "Epoch 00035: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0399 - acc: 0.9900 - val_loss: 1.2823 - val_acc: 0.7796\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9914\n",
      "Epoch 00036: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0372 - acc: 0.9914 - val_loss: 1.1221 - val_acc: 0.8081\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9907\n",
      "Epoch 00037: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0375 - acc: 0.9907 - val_loss: 1.1732 - val_acc: 0.8099\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9902\n",
      "Epoch 00038: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0391 - acc: 0.9902 - val_loss: 1.1230 - val_acc: 0.8111\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9882\n",
      "Epoch 00039: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0477 - acc: 0.9882 - val_loss: 1.5331 - val_acc: 0.7556\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9882\n",
      "Epoch 00040: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0484 - acc: 0.9882 - val_loss: 1.3449 - val_acc: 0.7869\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9877\n",
      "Epoch 00041: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0477 - acc: 0.9877 - val_loss: 1.1499 - val_acc: 0.8123\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9956\n",
      "Epoch 00042: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0214 - acc: 0.9956 - val_loss: 1.2336 - val_acc: 0.7969\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9936\n",
      "Epoch 00043: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0299 - acc: 0.9935 - val_loss: 1.2357 - val_acc: 0.7943\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9847\n",
      "Epoch 00044: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0600 - acc: 0.9847 - val_loss: 1.4271 - val_acc: 0.7741\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9933\n",
      "Epoch 00045: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0321 - acc: 0.9933 - val_loss: 1.2275 - val_acc: 0.7985\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9873\n",
      "Epoch 00046: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0499 - acc: 0.9872 - val_loss: 1.1844 - val_acc: 0.8109\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9916\n",
      "Epoch 00047: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0339 - acc: 0.9916 - val_loss: 1.1863 - val_acc: 0.8060\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9955\n",
      "Epoch 00048: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0228 - acc: 0.9955 - val_loss: 1.1975 - val_acc: 0.8106\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9929\n",
      "Epoch 00049: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0296 - acc: 0.9929 - val_loss: 2.2595 - val_acc: 0.7025\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0444 - acc: 0.9887 - val_loss: 1.2647 - val_acc: 0.7985\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9884\n",
      "Epoch 00051: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0451 - acc: 0.9884 - val_loss: 1.1720 - val_acc: 0.8162\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9920\n",
      "Epoch 00052: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0353 - acc: 0.9920 - val_loss: 1.2221 - val_acc: 0.8104\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9912\n",
      "Epoch 00053: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0354 - acc: 0.9912 - val_loss: 1.1235 - val_acc: 0.8234\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9915\n",
      "Epoch 00054: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0343 - acc: 0.9914 - val_loss: 1.7884 - val_acc: 0.7310\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9923\n",
      "Epoch 00055: val_loss did not improve from 0.83588\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0331 - acc: 0.9922 - val_loss: 1.3080 - val_acc: 0.7973\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9154 - acc: 0.7460\n",
      "Loss: 0.915381581637223 Accuracy: 0.7460021\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4535 - acc: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 1.22151, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/001-1.2215.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 1.4535 - acc: 0.5694 - val_loss: 1.2215 - val_acc: 0.6245\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7849 - acc: 0.7744\n",
      "Epoch 00002: val_loss improved from 1.22151 to 0.82029, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/002-0.8203.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7850 - acc: 0.7744 - val_loss: 0.8203 - val_acc: 0.7650\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.8406\n",
      "Epoch 00003: val_loss improved from 0.82029 to 0.74325, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/003-0.7433.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5578 - acc: 0.8406 - val_loss: 0.7433 - val_acc: 0.7925\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8722\n",
      "Epoch 00004: val_loss improved from 0.74325 to 0.54302, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/004-0.5430.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4344 - acc: 0.8722 - val_loss: 0.5430 - val_acc: 0.8514\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.9051\n",
      "Epoch 00005: val_loss did not improve from 0.54302\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3336 - acc: 0.9050 - val_loss: 0.5505 - val_acc: 0.8505\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9189\n",
      "Epoch 00006: val_loss improved from 0.54302 to 0.48675, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/006-0.4867.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2718 - acc: 0.9188 - val_loss: 0.4867 - val_acc: 0.8621\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9337\n",
      "Epoch 00007: val_loss did not improve from 0.48675\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2211 - acc: 0.9337 - val_loss: 0.5065 - val_acc: 0.8605\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9470\n",
      "Epoch 00008: val_loss did not improve from 0.48675\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1843 - acc: 0.9469 - val_loss: 0.4958 - val_acc: 0.8651\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9537\n",
      "Epoch 00009: val_loss did not improve from 0.48675\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1528 - acc: 0.9536 - val_loss: 0.5203 - val_acc: 0.8651\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9578\n",
      "Epoch 00010: val_loss did not improve from 0.48675\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1417 - acc: 0.9578 - val_loss: 0.5194 - val_acc: 0.8651\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9658\n",
      "Epoch 00011: val_loss improved from 0.48675 to 0.44751, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/011-0.4475.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1214 - acc: 0.9658 - val_loss: 0.4475 - val_acc: 0.8814\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9777\n",
      "Epoch 00012: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0839 - acc: 0.9777 - val_loss: 0.4527 - val_acc: 0.8870\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9746\n",
      "Epoch 00013: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0913 - acc: 0.9746 - val_loss: 0.5063 - val_acc: 0.8796\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9787\n",
      "Epoch 00014: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0775 - acc: 0.9786 - val_loss: 0.5180 - val_acc: 0.8756\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9790\n",
      "Epoch 00015: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0776 - acc: 0.9789 - val_loss: 0.4585 - val_acc: 0.8889\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9812\n",
      "Epoch 00016: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0689 - acc: 0.9813 - val_loss: 0.5920 - val_acc: 0.8609\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9828\n",
      "Epoch 00017: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0638 - acc: 0.9828 - val_loss: 0.4823 - val_acc: 0.8859\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9865\n",
      "Epoch 00018: val_loss did not improve from 0.44751\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0541 - acc: 0.9865 - val_loss: 0.5384 - val_acc: 0.8686\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9849\n",
      "Epoch 00019: val_loss improved from 0.44751 to 0.40617, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv_checkpoint/019-0.4062.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0568 - acc: 0.9849 - val_loss: 0.4062 - val_acc: 0.9085\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9922\n",
      "Epoch 00020: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0356 - acc: 0.9922 - val_loss: 0.4558 - val_acc: 0.8973\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9738\n",
      "Epoch 00021: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0897 - acc: 0.9738 - val_loss: 0.5386 - val_acc: 0.8784\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9890\n",
      "Epoch 00022: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0426 - acc: 0.9890 - val_loss: 0.5052 - val_acc: 0.8954\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9866\n",
      "Epoch 00023: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0501 - acc: 0.9866 - val_loss: 0.4752 - val_acc: 0.8975\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9939\n",
      "Epoch 00024: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0273 - acc: 0.9939 - val_loss: 0.4258 - val_acc: 0.9080\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9904\n",
      "Epoch 00025: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0395 - acc: 0.9903 - val_loss: 0.5407 - val_acc: 0.8833\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9854\n",
      "Epoch 00026: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0533 - acc: 0.9854 - val_loss: 0.4995 - val_acc: 0.8921\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9908\n",
      "Epoch 00027: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0356 - acc: 0.9907 - val_loss: 0.5165 - val_acc: 0.8915\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9866\n",
      "Epoch 00028: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0501 - acc: 0.9865 - val_loss: 0.5379 - val_acc: 0.8873\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9867\n",
      "Epoch 00029: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0488 - acc: 0.9866 - val_loss: 0.4415 - val_acc: 0.9050\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9899\n",
      "Epoch 00030: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0376 - acc: 0.9899 - val_loss: 0.4129 - val_acc: 0.9171\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9953\n",
      "Epoch 00031: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0212 - acc: 0.9952 - val_loss: 0.5250 - val_acc: 0.8880\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9881\n",
      "Epoch 00032: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0455 - acc: 0.9881 - val_loss: 0.4712 - val_acc: 0.9038\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9852\n",
      "Epoch 00033: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0540 - acc: 0.9852 - val_loss: 0.4956 - val_acc: 0.8970\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9900\n",
      "Epoch 00034: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0356 - acc: 0.9900 - val_loss: 0.4342 - val_acc: 0.9050\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9979\n",
      "Epoch 00035: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0129 - acc: 0.9979 - val_loss: 0.4904 - val_acc: 0.8987\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9904\n",
      "Epoch 00036: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0350 - acc: 0.9904 - val_loss: 0.4682 - val_acc: 0.9047\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9902\n",
      "Epoch 00037: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0354 - acc: 0.9902 - val_loss: 0.6326 - val_acc: 0.8856\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9952\n",
      "Epoch 00038: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0206 - acc: 0.9952 - val_loss: 0.5577 - val_acc: 0.8884\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9918\n",
      "Epoch 00039: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0322 - acc: 0.9918 - val_loss: 0.4366 - val_acc: 0.9099\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9954\n",
      "Epoch 00040: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0194 - acc: 0.9953 - val_loss: 0.4400 - val_acc: 0.9117\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9888\n",
      "Epoch 00041: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0389 - acc: 0.9888 - val_loss: 0.4577 - val_acc: 0.9108\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9933\n",
      "Epoch 00042: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0272 - acc: 0.9933 - val_loss: 0.7078 - val_acc: 0.8607\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9945\n",
      "Epoch 00043: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0219 - acc: 0.9944 - val_loss: 0.4614 - val_acc: 0.9150\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9887\n",
      "Epoch 00044: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0411 - acc: 0.9887 - val_loss: 0.4625 - val_acc: 0.9126\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9975\n",
      "Epoch 00045: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0124 - acc: 0.9974 - val_loss: 0.6075 - val_acc: 0.8915\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9882\n",
      "Epoch 00046: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0424 - acc: 0.9882 - val_loss: 0.5292 - val_acc: 0.8949\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9931\n",
      "Epoch 00047: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0281 - acc: 0.9931 - val_loss: 0.4923 - val_acc: 0.9026\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9965\n",
      "Epoch 00048: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0153 - acc: 0.9965 - val_loss: 0.4298 - val_acc: 0.9187\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9962\n",
      "Epoch 00049: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0155 - acc: 0.9963 - val_loss: 0.4801 - val_acc: 0.9059\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9946\n",
      "Epoch 00050: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0217 - acc: 0.9946 - val_loss: 0.5167 - val_acc: 0.9026\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9937\n",
      "Epoch 00051: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0230 - acc: 0.9937 - val_loss: 0.5728 - val_acc: 0.8940\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9922\n",
      "Epoch 00052: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0299 - acc: 0.9922 - val_loss: 0.5183 - val_acc: 0.9085\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9918\n",
      "Epoch 00053: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0292 - acc: 0.9918 - val_loss: 0.6482 - val_acc: 0.8842\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9895\n",
      "Epoch 00054: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0384 - acc: 0.9895 - val_loss: 0.4505 - val_acc: 0.9126\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0187 - acc: 0.9954 - val_loss: 0.4867 - val_acc: 0.9136\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9922\n",
      "Epoch 00056: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0290 - acc: 0.9922 - val_loss: 0.5243 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9955\n",
      "Epoch 00057: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0192 - acc: 0.9955 - val_loss: 0.4585 - val_acc: 0.9101\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9982\n",
      "Epoch 00058: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0087 - acc: 0.9982 - val_loss: 0.4735 - val_acc: 0.9182\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9913\n",
      "Epoch 00059: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0324 - acc: 0.9913 - val_loss: 0.4885 - val_acc: 0.9152\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9941\n",
      "Epoch 00060: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0217 - acc: 0.9941 - val_loss: 0.6843 - val_acc: 0.8751\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9963\n",
      "Epoch 00061: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0158 - acc: 0.9963 - val_loss: 0.4354 - val_acc: 0.9196\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9941\n",
      "Epoch 00062: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0220 - acc: 0.9941 - val_loss: 0.4862 - val_acc: 0.9143\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9983\n",
      "Epoch 00063: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0100 - acc: 0.9983 - val_loss: 0.5550 - val_acc: 0.9059\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9903\n",
      "Epoch 00064: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0356 - acc: 0.9903 - val_loss: 0.4617 - val_acc: 0.9180\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9970\n",
      "Epoch 00065: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0125 - acc: 0.9970 - val_loss: 0.4696 - val_acc: 0.9166\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9952\n",
      "Epoch 00066: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0167 - acc: 0.9951 - val_loss: 0.4673 - val_acc: 0.9215\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9920\n",
      "Epoch 00067: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0288 - acc: 0.9920 - val_loss: 0.5173 - val_acc: 0.9054\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9934\n",
      "Epoch 00068: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0272 - acc: 0.9933 - val_loss: 0.4972 - val_acc: 0.9078\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9941\n",
      "Epoch 00069: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0226 - acc: 0.9941 - val_loss: 0.4894 - val_acc: 0.9143\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4748 - acc: 0.8839\n",
      "Loss: 0.4747863867696324 Accuracy: 0.88390446\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2699 - acc: 0.6202\n",
      "Epoch 00001: val_loss improved from inf to 1.02218, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/001-1.0222.hdf5\n",
      "36805/36805 [==============================] - 121s 3ms/sample - loss: 1.2698 - acc: 0.6202 - val_loss: 1.0222 - val_acc: 0.6853\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.8389\n",
      "Epoch 00002: val_loss improved from 1.02218 to 0.42716, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/002-0.4272.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5486 - acc: 0.8389 - val_loss: 0.4272 - val_acc: 0.8721\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8935\n",
      "Epoch 00003: val_loss improved from 0.42716 to 0.34002, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/003-0.3400.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3658 - acc: 0.8934 - val_loss: 0.3400 - val_acc: 0.9022\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9205\n",
      "Epoch 00004: val_loss improved from 0.34002 to 0.32912, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/004-0.3291.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2745 - acc: 0.9205 - val_loss: 0.3291 - val_acc: 0.9005\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9350\n",
      "Epoch 00005: val_loss improved from 0.32912 to 0.30426, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/005-0.3043.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2199 - acc: 0.9350 - val_loss: 0.3043 - val_acc: 0.9175\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9451\n",
      "Epoch 00006: val_loss did not improve from 0.30426\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1862 - acc: 0.9451 - val_loss: 0.3136 - val_acc: 0.9071\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9556\n",
      "Epoch 00007: val_loss improved from 0.30426 to 0.21508, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/007-0.2151.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1514 - acc: 0.9556 - val_loss: 0.2151 - val_acc: 0.9385\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9607\n",
      "Epoch 00008: val_loss did not improve from 0.21508\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1349 - acc: 0.9607 - val_loss: 0.2844 - val_acc: 0.9164\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9679\n",
      "Epoch 00009: val_loss did not improve from 0.21508\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1103 - acc: 0.9679 - val_loss: 0.2718 - val_acc: 0.9245\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9727\n",
      "Epoch 00010: val_loss did not improve from 0.21508\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0935 - acc: 0.9727 - val_loss: 0.2326 - val_acc: 0.9348\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9786\n",
      "Epoch 00011: val_loss improved from 0.21508 to 0.19545, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/011-0.1954.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0764 - acc: 0.9786 - val_loss: 0.1954 - val_acc: 0.9436\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9812\n",
      "Epoch 00012: val_loss did not improve from 0.19545\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0653 - acc: 0.9811 - val_loss: 0.2409 - val_acc: 0.9369\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9781\n",
      "Epoch 00013: val_loss did not improve from 0.19545\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0753 - acc: 0.9780 - val_loss: 0.2436 - val_acc: 0.9304\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9851\n",
      "Epoch 00014: val_loss did not improve from 0.19545\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0537 - acc: 0.9851 - val_loss: 0.2524 - val_acc: 0.9278\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9859\n",
      "Epoch 00015: val_loss improved from 0.19545 to 0.18358, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/015-0.1836.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0511 - acc: 0.9858 - val_loss: 0.1836 - val_acc: 0.9460\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9836\n",
      "Epoch 00016: val_loss did not improve from 0.18358\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0571 - acc: 0.9835 - val_loss: 0.2195 - val_acc: 0.9394\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9887\n",
      "Epoch 00017: val_loss did not improve from 0.18358\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0443 - acc: 0.9887 - val_loss: 0.2032 - val_acc: 0.9401\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9864\n",
      "Epoch 00018: val_loss did not improve from 0.18358\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0493 - acc: 0.9864 - val_loss: 0.2176 - val_acc: 0.9413\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9952\n",
      "Epoch 00019: val_loss improved from 0.18358 to 0.17371, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/019-0.1737.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0230 - acc: 0.9952 - val_loss: 0.1737 - val_acc: 0.9541\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9932\n",
      "Epoch 00020: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0255 - acc: 0.9932 - val_loss: 0.2810 - val_acc: 0.9255\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9915\n",
      "Epoch 00021: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0307 - acc: 0.9915 - val_loss: 0.3019 - val_acc: 0.9245\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9915\n",
      "Epoch 00022: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0297 - acc: 0.9916 - val_loss: 0.2133 - val_acc: 0.9460\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9950\n",
      "Epoch 00023: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0214 - acc: 0.9950 - val_loss: 0.2054 - val_acc: 0.9448\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9937\n",
      "Epoch 00024: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0251 - acc: 0.9937 - val_loss: 0.2276 - val_acc: 0.9443\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9925\n",
      "Epoch 00025: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0267 - acc: 0.9925 - val_loss: 0.2140 - val_acc: 0.9422\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9924\n",
      "Epoch 00026: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0266 - acc: 0.9924 - val_loss: 0.2270 - val_acc: 0.9439\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9974\n",
      "Epoch 00027: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0134 - acc: 0.9974 - val_loss: 0.2183 - val_acc: 0.9488\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9944\n",
      "Epoch 00028: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0216 - acc: 0.9943 - val_loss: 0.2041 - val_acc: 0.9490\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9930\n",
      "Epoch 00029: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0248 - acc: 0.9930 - val_loss: 0.1838 - val_acc: 0.9511\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9976\n",
      "Epoch 00030: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0113 - acc: 0.9976 - val_loss: 0.1798 - val_acc: 0.9564\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 00031: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0194 - acc: 0.9949 - val_loss: 0.2051 - val_acc: 0.9483\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9921\n",
      "Epoch 00032: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0274 - acc: 0.9920 - val_loss: 0.1896 - val_acc: 0.9506\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9940\n",
      "Epoch 00033: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0207 - acc: 0.9940 - val_loss: 0.1995 - val_acc: 0.9511\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9990\n",
      "Epoch 00034: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0062 - acc: 0.9990 - val_loss: 0.1925 - val_acc: 0.9557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9985\n",
      "Epoch 00035: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0077 - acc: 0.9984 - val_loss: 0.3393 - val_acc: 0.9269\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9898\n",
      "Epoch 00036: val_loss improved from 0.17371 to 0.17297, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv_checkpoint/036-0.1730.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0345 - acc: 0.9898 - val_loss: 0.1730 - val_acc: 0.9529\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9992\n",
      "Epoch 00037: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0061 - acc: 0.9992 - val_loss: 0.1892 - val_acc: 0.9553\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9986\n",
      "Epoch 00038: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0071 - acc: 0.9986 - val_loss: 0.2310 - val_acc: 0.9441\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9954\n",
      "Epoch 00039: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0161 - acc: 0.9954 - val_loss: 0.2454 - val_acc: 0.9390\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9931\n",
      "Epoch 00040: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0220 - acc: 0.9931 - val_loss: 0.2148 - val_acc: 0.9495\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9987\n",
      "Epoch 00041: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1772 - val_acc: 0.9562\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9917\n",
      "Epoch 00042: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0274 - acc: 0.9917 - val_loss: 0.1956 - val_acc: 0.9541\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9953\n",
      "Epoch 00043: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0168 - acc: 0.9953 - val_loss: 0.1771 - val_acc: 0.9571\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00044: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0038 - acc: 0.9996 - val_loss: 0.1832 - val_acc: 0.9571\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9942\n",
      "Epoch 00045: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0198 - acc: 0.9942 - val_loss: 0.1972 - val_acc: 0.9492\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9970\n",
      "Epoch 00046: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0111 - acc: 0.9970 - val_loss: 0.1904 - val_acc: 0.9520\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9987\n",
      "Epoch 00047: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0064 - acc: 0.9987 - val_loss: 0.2142 - val_acc: 0.9509\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9964\n",
      "Epoch 00048: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0139 - acc: 0.9964 - val_loss: 0.2145 - val_acc: 0.9474\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9986\n",
      "Epoch 00049: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0069 - acc: 0.9985 - val_loss: 0.2149 - val_acc: 0.9553\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9943\n",
      "Epoch 00050: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.1872 - val_acc: 0.9548\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00051: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0066 - acc: 0.9985 - val_loss: 0.1906 - val_acc: 0.9557\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00052: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.2165 - val_acc: 0.9474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9949\n",
      "Epoch 00053: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0172 - acc: 0.9949 - val_loss: 0.1756 - val_acc: 0.9581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0055 - acc: 0.9989 - val_loss: 0.2469 - val_acc: 0.9427\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 00055: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0059 - acc: 0.9985 - val_loss: 0.2106 - val_acc: 0.9520\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0146 - acc: 0.9959 - val_loss: 0.1863 - val_acc: 0.9583\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9986\n",
      "Epoch 00057: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.1998 - val_acc: 0.9553\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9946\n",
      "Epoch 00058: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0195 - acc: 0.9945 - val_loss: 0.1952 - val_acc: 0.9509\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9963\n",
      "Epoch 00059: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0132 - acc: 0.9963 - val_loss: 0.1995 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9981\n",
      "Epoch 00060: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0081 - acc: 0.9981 - val_loss: 0.1976 - val_acc: 0.9562\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00061: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1959 - val_acc: 0.9534\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 00062: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0036 - acc: 0.9993 - val_loss: 0.2094 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9949\n",
      "Epoch 00063: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0193 - acc: 0.9949 - val_loss: 0.1830 - val_acc: 0.9571\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00064: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1927 - val_acc: 0.9569\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00065: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1823 - val_acc: 0.9588\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 00066: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.2265 - val_acc: 0.9474\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 00067: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0064 - acc: 0.9984 - val_loss: 0.2196 - val_acc: 0.9488\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9964\n",
      "Epoch 00068: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0130 - acc: 0.9964 - val_loss: 0.1999 - val_acc: 0.9534\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9988\n",
      "Epoch 00069: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0049 - acc: 0.9988 - val_loss: 0.3188 - val_acc: 0.9355\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 00070: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0064 - acc: 0.9985 - val_loss: 0.2101 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9952\n",
      "Epoch 00071: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0155 - acc: 0.9952 - val_loss: 0.2075 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00072: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1889 - val_acc: 0.9590\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00073: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0045 - acc: 0.9989 - val_loss: 0.2584 - val_acc: 0.9499\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 00074: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0179 - acc: 0.9946 - val_loss: 0.2165 - val_acc: 0.9534\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00075: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0034 - acc: 0.9992 - val_loss: 0.1992 - val_acc: 0.9583\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00076: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1967 - val_acc: 0.9569\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 00077: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.2783 - val_acc: 0.9429\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 00078: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.2381 - val_acc: 0.9483\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 00079: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.2297 - val_acc: 0.9553\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9976\n",
      "Epoch 00080: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.2659 - val_acc: 0.9427\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 00081: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.2218 - val_acc: 0.9560\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00082: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.2487 - val_acc: 0.9536\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00083: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.2261 - val_acc: 0.9502\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 00084: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0073 - acc: 0.9977 - val_loss: 0.2028 - val_acc: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00085: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.2292 - val_acc: 0.9546\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 00086: val_loss did not improve from 0.17297\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0039 - acc: 0.9990 - val_loss: 0.2360 - val_acc: 0.9513\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2477 - acc: 0.9412\n",
      "Loss: 0.24774486179292388 Accuracy: 0.94122535\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9789 - acc: 0.6998\n",
      "Epoch 00001: val_loss improved from inf to 0.67501, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/001-0.6750.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.9789 - acc: 0.6998 - val_loss: 0.6750 - val_acc: 0.8213\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8920\n",
      "Epoch 00002: val_loss improved from 0.67501 to 0.39502, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/002-0.3950.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.3606 - acc: 0.8920 - val_loss: 0.3950 - val_acc: 0.8824\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9264\n",
      "Epoch 00003: val_loss improved from 0.39502 to 0.21928, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/003-0.2193.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2430 - acc: 0.9264 - val_loss: 0.2193 - val_acc: 0.9352\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9448\n",
      "Epoch 00004: val_loss did not improve from 0.21928\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1867 - acc: 0.9448 - val_loss: 0.2231 - val_acc: 0.9304\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9523\n",
      "Epoch 00005: val_loss improved from 0.21928 to 0.20334, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/005-0.2033.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1545 - acc: 0.9523 - val_loss: 0.2033 - val_acc: 0.9376\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9631\n",
      "Epoch 00006: val_loss improved from 0.20334 to 0.19716, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/006-0.1972.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1245 - acc: 0.9631 - val_loss: 0.1972 - val_acc: 0.9432\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9696\n",
      "Epoch 00007: val_loss improved from 0.19716 to 0.17800, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/007-0.1780.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1002 - acc: 0.9696 - val_loss: 0.1780 - val_acc: 0.9448\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9723\n",
      "Epoch 00008: val_loss improved from 0.17800 to 0.16438, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/008-0.1644.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0941 - acc: 0.9723 - val_loss: 0.1644 - val_acc: 0.9504\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9786\n",
      "Epoch 00009: val_loss did not improve from 0.16438\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0705 - acc: 0.9785 - val_loss: 0.1656 - val_acc: 0.9499\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9811\n",
      "Epoch 00010: val_loss improved from 0.16438 to 0.14919, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/010-0.1492.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0648 - acc: 0.9811 - val_loss: 0.1492 - val_acc: 0.9539\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9860\n",
      "Epoch 00011: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0497 - acc: 0.9860 - val_loss: 0.2276 - val_acc: 0.9406\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9848\n",
      "Epoch 00012: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0540 - acc: 0.9848 - val_loss: 0.2123 - val_acc: 0.9399\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9895\n",
      "Epoch 00013: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0383 - acc: 0.9895 - val_loss: 0.1678 - val_acc: 0.9495\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9904\n",
      "Epoch 00014: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0359 - acc: 0.9904 - val_loss: 0.2084 - val_acc: 0.9455\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9923\n",
      "Epoch 00015: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0311 - acc: 0.9923 - val_loss: 0.2084 - val_acc: 0.9413\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9905\n",
      "Epoch 00016: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0338 - acc: 0.9905 - val_loss: 0.1976 - val_acc: 0.9453\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9914\n",
      "Epoch 00017: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0321 - acc: 0.9914 - val_loss: 0.1896 - val_acc: 0.9481\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9961\n",
      "Epoch 00018: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.2729 - val_acc: 0.9271\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9888\n",
      "Epoch 00019: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0380 - acc: 0.9888 - val_loss: 0.1989 - val_acc: 0.9522\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9964\n",
      "Epoch 00020: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0157 - acc: 0.9964 - val_loss: 0.2092 - val_acc: 0.9427\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9905\n",
      "Epoch 00021: val_loss did not improve from 0.14919\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0324 - acc: 0.9905 - val_loss: 0.2035 - val_acc: 0.9511\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 00022: val_loss improved from 0.14919 to 0.14629, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/022-0.1463.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0121 - acc: 0.9974 - val_loss: 0.1463 - val_acc: 0.9576\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9951\n",
      "Epoch 00023: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0179 - acc: 0.9951 - val_loss: 0.1802 - val_acc: 0.9506\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9955\n",
      "Epoch 00024: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0173 - acc: 0.9955 - val_loss: 0.1913 - val_acc: 0.9483\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00025: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0181 - acc: 0.9946 - val_loss: 0.1750 - val_acc: 0.9518\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9982\n",
      "Epoch 00026: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0093 - acc: 0.9982 - val_loss: 0.2051 - val_acc: 0.9515\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9940\n",
      "Epoch 00027: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0205 - acc: 0.9940 - val_loss: 0.1837 - val_acc: 0.9541\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9958\n",
      "Epoch 00028: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0149 - acc: 0.9958 - val_loss: 0.1472 - val_acc: 0.9625\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9970\n",
      "Epoch 00029: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.2172 - val_acc: 0.9467\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 00030: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0132 - acc: 0.9962 - val_loss: 0.1988 - val_acc: 0.9527\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9951\n",
      "Epoch 00031: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0164 - acc: 0.9951 - val_loss: 0.1630 - val_acc: 0.9588\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9977\n",
      "Epoch 00032: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0098 - acc: 0.9977 - val_loss: 0.1699 - val_acc: 0.9511\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9954\n",
      "Epoch 00033: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0158 - acc: 0.9954 - val_loss: 0.1872 - val_acc: 0.9557\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9985\n",
      "Epoch 00034: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0072 - acc: 0.9985 - val_loss: 0.1715 - val_acc: 0.9562\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9972\n",
      "Epoch 00035: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.1719 - val_acc: 0.9541\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9970\n",
      "Epoch 00036: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0111 - acc: 0.9970 - val_loss: 0.1971 - val_acc: 0.9520\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9979\n",
      "Epoch 00037: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0087 - acc: 0.9979 - val_loss: 0.1951 - val_acc: 0.9548\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 00038: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0130 - acc: 0.9961 - val_loss: 0.2569 - val_acc: 0.9434\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9932\n",
      "Epoch 00039: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0227 - acc: 0.9932 - val_loss: 0.1583 - val_acc: 0.9599\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 00040: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1466 - val_acc: 0.9632\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9956\n",
      "Epoch 00041: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0156 - acc: 0.9956 - val_loss: 0.1579 - val_acc: 0.9602\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9989\n",
      "Epoch 00042: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0058 - acc: 0.9989 - val_loss: 0.1818 - val_acc: 0.9562\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00043: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.2034 - val_acc: 0.9546\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00044: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1467 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00045: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.1550 - val_acc: 0.9618\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 00046: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0042 - acc: 0.9990 - val_loss: 0.1588 - val_acc: 0.9581\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9976\n",
      "Epoch 00047: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0091 - acc: 0.9976 - val_loss: 0.2351 - val_acc: 0.9448\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9971\n",
      "Epoch 00048: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.1903 - val_acc: 0.9564\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00049: val_loss did not improve from 0.14629\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1584 - val_acc: 0.9588\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00050: val_loss improved from 0.14629 to 0.14352, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/050-0.1435.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1435 - val_acc: 0.9630\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00051: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2169 - val_acc: 0.9527\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 00052: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0074 - acc: 0.9981 - val_loss: 0.1748 - val_acc: 0.9553\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9950\n",
      "Epoch 00053: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0167 - acc: 0.9950 - val_loss: 0.1889 - val_acc: 0.9555\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9943\n",
      "Epoch 00054: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.1438 - val_acc: 0.9646\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9995\n",
      "Epoch 00055: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 0.1435 - val_acc: 0.9641\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00056: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1530 - val_acc: 0.9667\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9960\n",
      "Epoch 00057: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0148 - acc: 0.9960 - val_loss: 0.1611 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00058: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1740 - val_acc: 0.9620\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 00059: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.2015 - val_acc: 0.9555\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 00060: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0093 - acc: 0.9971 - val_loss: 0.1924 - val_acc: 0.9581\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9991\n",
      "Epoch 00061: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.1945 - val_acc: 0.9567\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9951\n",
      "Epoch 00062: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0167 - acc: 0.9951 - val_loss: 0.1856 - val_acc: 0.9567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00063: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1716 - val_acc: 0.9620\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1856 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00065: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.1856 - val_acc: 0.9602\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9965\n",
      "Epoch 00066: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0129 - acc: 0.9965 - val_loss: 0.1671 - val_acc: 0.9623\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9970\n",
      "Epoch 00067: val_loss did not improve from 0.14352\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0103 - acc: 0.9970 - val_loss: 0.1497 - val_acc: 0.9646\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00068: val_loss improved from 0.14352 to 0.13865, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv_checkpoint/068-0.1387.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1387 - val_acc: 0.9665\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 00069: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1746 - val_acc: 0.9604\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 00070: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1516 - val_acc: 0.9637\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9964\n",
      "Epoch 00071: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0129 - acc: 0.9964 - val_loss: 0.1457 - val_acc: 0.9665\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00072: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1910 - val_acc: 0.9578\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0106 - acc: 0.9965 - val_loss: 0.1704 - val_acc: 0.9639\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00074: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1425 - val_acc: 0.9669\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00075: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1523 - val_acc: 0.9653\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00076: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1618 - val_acc: 0.9632\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00077: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.3232 - val_acc: 0.9269\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9966\n",
      "Epoch 00078: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0108 - acc: 0.9966 - val_loss: 0.1887 - val_acc: 0.9611\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00079: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1614 - val_acc: 0.9641\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 00080: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.1582 - val_acc: 0.9658\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9989\n",
      "Epoch 00081: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0035 - acc: 0.9989 - val_loss: 0.1705 - val_acc: 0.9613\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00082: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.2355 - val_acc: 0.9548\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 00083: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0132 - acc: 0.9962 - val_loss: 0.1537 - val_acc: 0.9667\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00084: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1443 - val_acc: 0.9679\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00085: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1565 - val_acc: 0.9660\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 00086: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.2764 - val_acc: 0.9420\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989\n",
      "Epoch 00087: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.1746 - val_acc: 0.9641\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00088: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1695 - val_acc: 0.9658\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00089: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1840 - val_acc: 0.9560\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
      "Epoch 00090: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1771 - val_acc: 0.9599\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00091: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1418 - val_acc: 0.9669\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00092: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9994 - val_loss: 0.2031 - val_acc: 0.9602\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00093: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1679 - val_acc: 0.9639\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00094: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.2352 - val_acc: 0.9520\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00095: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1575 - val_acc: 0.9662\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00096: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1707 - val_acc: 0.9609\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 00097: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1847 - val_acc: 0.9630\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 00098: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.1979 - val_acc: 0.9595\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00099: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.2531 - val_acc: 0.9513\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00100: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1521 - val_acc: 0.9672\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5491e-04 - acc: 0.9999\n",
      "Epoch 00101: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 7.6148e-04 - acc: 0.9999 - val_loss: 0.1549 - val_acc: 0.9686\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9982\n",
      "Epoch 00102: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0059 - acc: 0.9982 - val_loss: 0.1438 - val_acc: 0.9681\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9985\n",
      "Epoch 00103: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0054 - acc: 0.9985 - val_loss: 0.1525 - val_acc: 0.9662\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
      "Epoch 00104: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1725 - val_acc: 0.9651\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00105: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1745 - val_acc: 0.9655\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1866 - val_acc: 0.9620\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9956\n",
      "Epoch 00107: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0157 - acc: 0.9956 - val_loss: 0.1737 - val_acc: 0.9613\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00108: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1482 - val_acc: 0.9693\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9980\n",
      "Epoch 00109: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0076 - acc: 0.9980 - val_loss: 0.1640 - val_acc: 0.9637\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00110: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1563 - val_acc: 0.9679\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 00111: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1579 - val_acc: 0.9665\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2609e-04 - acc: 0.9999\n",
      "Epoch 00112: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 5.2868e-04 - acc: 0.9999 - val_loss: 0.1396 - val_acc: 0.9667\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00113: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1822 - val_acc: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00114: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1704 - val_acc: 0.9646\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00115: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1625 - val_acc: 0.9634\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.2299e-04 - acc: 0.9999\n",
      "Epoch 00116: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 8.2321e-04 - acc: 0.9999 - val_loss: 0.1505 - val_acc: 0.9672\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6258e-04 - acc: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 3.6253e-04 - acc: 1.0000 - val_loss: 0.1479 - val_acc: 0.9679\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9905e-04 - acc: 0.9999\n",
      "Epoch 00118: val_loss did not improve from 0.13865\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 4.0086e-04 - acc: 0.9999 - val_loss: 0.1462 - val_acc: 0.9686\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1867 - acc: 0.9580\n",
      "Loss: 0.18672584858800353 Accuracy: 0.95804775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_128_BN'\n",
    "\n",
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 100992)       403968      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     batch_normalization_v1_43[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 2,268,816\n",
      "Trainable params: 2,065,808\n",
      "Non-trainable params: 203,008\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.6617 - acc: 0.6268\n",
      "Loss: 1.6617056480449308 Accuracy: 0.6267913\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 41856)        167424      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      batch_normalization_v1_49[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,251,216\n",
      "Trainable params: 1,165,968\n",
      "Non-trainable params: 85,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9154 - acc: 0.7460\n",
      "Loss: 0.915381581637223 Accuracy: 0.7460021\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 22016)        88064       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      batch_normalization_v1_56[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,183,376\n",
      "Trainable params: 1,137,296\n",
      "Non-trainable params: 46,080\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4748 - acc: 0.8839\n",
      "Loss: 0.4747863867696324 Accuracy: 0.88390446\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 7168)         28672       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      batch_normalization_v1_64[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,215,376\n",
      "Trainable params: 1,198,480\n",
      "Non-trainable params: 16,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2477 - acc: 0.9412\n",
      "Loss: 0.24774486179292388 Accuracy: 0.94122535\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_68[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 2304)         9216        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       batch_normalization_v1_73[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,447,056\n",
      "Trainable params: 1,439,376\n",
      "Non-trainable params: 7,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1867 - acc: 0.9580\n",
      "Loss: 0.18672584858800353 Accuracy: 0.95804775\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_128_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 100992)       403968      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     batch_normalization_v1_43[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 2,268,816\n",
      "Trainable params: 2,065,808\n",
      "Non-trainable params: 203,008\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 3.1919 - acc: 0.6444\n",
      "Loss: 3.1918744075335446 Accuracy: 0.64444447\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 41856)        167424      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      batch_normalization_v1_49[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,251,216\n",
      "Trainable params: 1,165,968\n",
      "Non-trainable params: 85,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.4743 - acc: 0.7786\n",
      "Loss: 1.4743238660281568 Accuracy: 0.7786085\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 22016)        88064       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      batch_normalization_v1_56[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,183,376\n",
      "Trainable params: 1,137,296\n",
      "Non-trainable params: 46,080\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5562 - acc: 0.9028\n",
      "Loss: 0.5561653031405692 Accuracy: 0.9028037\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 7168)         28672       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      batch_normalization_v1_64[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,215,376\n",
      "Trainable params: 1,198,480\n",
      "Non-trainable params: 16,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3139 - acc: 0.9394\n",
      "Loss: 0.31393852422356233 Accuracy: 0.9393562\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_68[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 2304)         9216        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       batch_normalization_v1_73[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,447,056\n",
      "Trainable params: 1,439,376\n",
      "Non-trainable params: 7,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1936 - acc: 0.9593\n",
      "Loss: 0.1935517892410238 Accuracy: 0.9592939\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
