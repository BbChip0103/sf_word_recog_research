{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      kernel_initializer='he_uniform', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same', kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4316 - acc: 0.3349\n",
      "Epoch 00001: val_loss improved from inf to 1.85335, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_3_conv_checkpoint/001-1.8534.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 2.4313 - acc: 0.3349 - val_loss: 1.8534 - val_acc: 0.4267\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6122 - acc: 0.5174\n",
      "Epoch 00002: val_loss improved from 1.85335 to 1.53331, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_3_conv_checkpoint/002-1.5333.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.6123 - acc: 0.5174 - val_loss: 1.5333 - val_acc: 0.5586\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3209 - acc: 0.5933\n",
      "Epoch 00003: val_loss did not improve from 1.53331\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.3209 - acc: 0.5933 - val_loss: 1.6553 - val_acc: 0.5297\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0947 - acc: 0.6611\n",
      "Epoch 00004: val_loss improved from 1.53331 to 1.39844, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_3_conv_checkpoint/004-1.3984.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0947 - acc: 0.6611 - val_loss: 1.3984 - val_acc: 0.5968\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9246 - acc: 0.7104\n",
      "Epoch 00005: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9246 - acc: 0.7104 - val_loss: 1.4320 - val_acc: 0.6005\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8013 - acc: 0.7469\n",
      "Epoch 00006: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8014 - acc: 0.7469 - val_loss: 1.5408 - val_acc: 0.5921\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.7828\n",
      "Epoch 00007: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6801 - acc: 0.7828 - val_loss: 1.4885 - val_acc: 0.5998\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5863 - acc: 0.8115\n",
      "Epoch 00008: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5863 - acc: 0.8115 - val_loss: 1.6351 - val_acc: 0.5751\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.8308\n",
      "Epoch 00009: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5263 - acc: 0.8308 - val_loss: 1.5687 - val_acc: 0.5947\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8486\n",
      "Epoch 00010: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4616 - acc: 0.8486 - val_loss: 1.4982 - val_acc: 0.6212\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8649\n",
      "Epoch 00011: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4105 - acc: 0.8649 - val_loss: 1.5597 - val_acc: 0.6231\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8814\n",
      "Epoch 00012: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3631 - acc: 0.8815 - val_loss: 1.6631 - val_acc: 0.6098\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8869\n",
      "Epoch 00013: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3432 - acc: 0.8869 - val_loss: 1.6345 - val_acc: 0.6150\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.8978\n",
      "Epoch 00014: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3156 - acc: 0.8977 - val_loss: 1.8261 - val_acc: 0.5998\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9054\n",
      "Epoch 00015: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2910 - acc: 0.9054 - val_loss: 1.7940 - val_acc: 0.6038\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9145\n",
      "Epoch 00016: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2667 - acc: 0.9144 - val_loss: 1.6964 - val_acc: 0.6231\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9191\n",
      "Epoch 00017: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2488 - acc: 0.9191 - val_loss: 1.7542 - val_acc: 0.6238\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9239\n",
      "Epoch 00018: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2382 - acc: 0.9239 - val_loss: 1.8111 - val_acc: 0.6191\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9321\n",
      "Epoch 00019: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2170 - acc: 0.9321 - val_loss: 1.8898 - val_acc: 0.6126\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9340\n",
      "Epoch 00020: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2057 - acc: 0.9340 - val_loss: 1.8667 - val_acc: 0.6177\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9351\n",
      "Epoch 00021: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2073 - acc: 0.9351 - val_loss: 1.7947 - val_acc: 0.6417\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9409\n",
      "Epoch 00022: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1877 - acc: 0.9409 - val_loss: 1.7987 - val_acc: 0.6452\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9432\n",
      "Epoch 00023: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1825 - acc: 0.9432 - val_loss: 2.3065 - val_acc: 0.5588\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9448\n",
      "Epoch 00024: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1727 - acc: 0.9448 - val_loss: 1.9201 - val_acc: 0.6231\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9469\n",
      "Epoch 00025: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1703 - acc: 0.9468 - val_loss: 1.8892 - val_acc: 0.6264\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9476\n",
      "Epoch 00026: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1657 - acc: 0.9475 - val_loss: 2.0591 - val_acc: 0.6201\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9508\n",
      "Epoch 00027: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1588 - acc: 0.9508 - val_loss: 1.9892 - val_acc: 0.6271\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9547\n",
      "Epoch 00028: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1430 - acc: 0.9547 - val_loss: 1.9462 - val_acc: 0.6348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9543\n",
      "Epoch 00029: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1436 - acc: 0.9543 - val_loss: 1.8894 - val_acc: 0.6450\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9547\n",
      "Epoch 00030: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1480 - acc: 0.9547 - val_loss: 2.1237 - val_acc: 0.6171\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9572\n",
      "Epoch 00031: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1394 - acc: 0.9572 - val_loss: 1.9258 - val_acc: 0.6341\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9593\n",
      "Epoch 00032: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1334 - acc: 0.9593 - val_loss: 1.9869 - val_acc: 0.6303\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9614\n",
      "Epoch 00033: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1230 - acc: 0.9614 - val_loss: 1.9231 - val_acc: 0.6443\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9622\n",
      "Epoch 00034: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1226 - acc: 0.9622 - val_loss: 1.9690 - val_acc: 0.6389\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9627\n",
      "Epoch 00035: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1254 - acc: 0.9627 - val_loss: 2.0110 - val_acc: 0.6371\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9628\n",
      "Epoch 00036: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1235 - acc: 0.9628 - val_loss: 1.9982 - val_acc: 0.6471\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9646\n",
      "Epoch 00037: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1160 - acc: 0.9647 - val_loss: 1.9761 - val_acc: 0.6450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9658\n",
      "Epoch 00038: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1089 - acc: 0.9658 - val_loss: 2.0212 - val_acc: 0.6459\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9683\n",
      "Epoch 00039: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1055 - acc: 0.9683 - val_loss: 1.9756 - val_acc: 0.6485\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9685\n",
      "Epoch 00040: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1069 - acc: 0.9685 - val_loss: 2.0179 - val_acc: 0.6504\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9662\n",
      "Epoch 00041: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1083 - acc: 0.9662 - val_loss: 2.1486 - val_acc: 0.6364\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9711\n",
      "Epoch 00042: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1006 - acc: 0.9711 - val_loss: 2.2108 - val_acc: 0.6203\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9726\n",
      "Epoch 00043: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0965 - acc: 0.9726 - val_loss: 2.2188 - val_acc: 0.6254\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9712\n",
      "Epoch 00044: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0993 - acc: 0.9713 - val_loss: 2.0384 - val_acc: 0.6585\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9735\n",
      "Epoch 00045: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0892 - acc: 0.9735 - val_loss: 2.1074 - val_acc: 0.6450\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9729\n",
      "Epoch 00046: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0932 - acc: 0.9729 - val_loss: 2.1558 - val_acc: 0.6268\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9753\n",
      "Epoch 00047: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0836 - acc: 0.9753 - val_loss: 2.2781 - val_acc: 0.6194\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9744\n",
      "Epoch 00048: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0879 - acc: 0.9744 - val_loss: 2.2568 - val_acc: 0.6301\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9741\n",
      "Epoch 00049: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0866 - acc: 0.9741 - val_loss: 1.9689 - val_acc: 0.6597\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9736\n",
      "Epoch 00050: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0897 - acc: 0.9736 - val_loss: 2.0106 - val_acc: 0.6597\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9776\n",
      "Epoch 00051: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0811 - acc: 0.9776 - val_loss: 2.1428 - val_acc: 0.6380\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9769\n",
      "Epoch 00052: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0817 - acc: 0.9769 - val_loss: 2.1022 - val_acc: 0.6594\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9736\n",
      "Epoch 00053: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0893 - acc: 0.9736 - val_loss: 2.0654 - val_acc: 0.6506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9786\n",
      "Epoch 00054: val_loss did not improve from 1.39844\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0747 - acc: 0.9786 - val_loss: 2.0957 - val_acc: 0.6506\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 830us/sample - loss: 1.5318 - acc: 0.5458\n",
      "Loss: 1.531757095991513 Accuracy: 0.54579437\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3003 - acc: 0.3340\n",
      "Epoch 00001: val_loss improved from inf to 1.46679, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/001-1.4668.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 2.3001 - acc: 0.3341 - val_loss: 1.4668 - val_acc: 0.5532\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5234 - acc: 0.5323\n",
      "Epoch 00002: val_loss improved from 1.46679 to 1.21550, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/002-1.2155.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.5234 - acc: 0.5323 - val_loss: 1.2155 - val_acc: 0.6224\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2712 - acc: 0.6098\n",
      "Epoch 00003: val_loss did not improve from 1.21550\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.2712 - acc: 0.6098 - val_loss: 1.2198 - val_acc: 0.6229\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1142 - acc: 0.6573\n",
      "Epoch 00004: val_loss improved from 1.21550 to 1.04870, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/004-1.0487.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.1142 - acc: 0.6573 - val_loss: 1.0487 - val_acc: 0.6879\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9987 - acc: 0.6911\n",
      "Epoch 00005: val_loss did not improve from 1.04870\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9986 - acc: 0.6911 - val_loss: 1.0735 - val_acc: 0.6809\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7233\n",
      "Epoch 00006: val_loss improved from 1.04870 to 1.03250, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/006-1.0325.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8963 - acc: 0.7233 - val_loss: 1.0325 - val_acc: 0.6965\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8259 - acc: 0.7426\n",
      "Epoch 00007: val_loss improved from 1.03250 to 0.97381, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/007-0.9738.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8258 - acc: 0.7426 - val_loss: 0.9738 - val_acc: 0.7188\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7665\n",
      "Epoch 00008: val_loss improved from 0.97381 to 0.96514, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/008-0.9651.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7529 - acc: 0.7665 - val_loss: 0.9651 - val_acc: 0.7156\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.7846\n",
      "Epoch 00009: val_loss improved from 0.96514 to 0.94920, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/009-0.9492.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6946 - acc: 0.7846 - val_loss: 0.9492 - val_acc: 0.7284\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6420 - acc: 0.8001\n",
      "Epoch 00010: val_loss did not improve from 0.94920\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6420 - acc: 0.8001 - val_loss: 1.1690 - val_acc: 0.6622\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8093\n",
      "Epoch 00011: val_loss did not improve from 0.94920\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6010 - acc: 0.8093 - val_loss: 0.9572 - val_acc: 0.7398\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8248\n",
      "Epoch 00012: val_loss improved from 0.94920 to 0.93477, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/012-0.9348.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5491 - acc: 0.8248 - val_loss: 0.9348 - val_acc: 0.7361\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8345\n",
      "Epoch 00013: val_loss did not improve from 0.93477\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5190 - acc: 0.8345 - val_loss: 0.9687 - val_acc: 0.7372\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8445\n",
      "Epoch 00014: val_loss did not improve from 0.93477\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4835 - acc: 0.8444 - val_loss: 1.0430 - val_acc: 0.7121\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8509\n",
      "Epoch 00015: val_loss improved from 0.93477 to 0.91028, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_4_conv_checkpoint/015-0.9103.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4620 - acc: 0.8509 - val_loss: 0.9103 - val_acc: 0.7536\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8633\n",
      "Epoch 00016: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4268 - acc: 0.8633 - val_loss: 1.0708 - val_acc: 0.7107\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8700\n",
      "Epoch 00017: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4069 - acc: 0.8700 - val_loss: 0.9332 - val_acc: 0.7435\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8760\n",
      "Epoch 00018: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3835 - acc: 0.8760 - val_loss: 0.9815 - val_acc: 0.7396\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8801\n",
      "Epoch 00019: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3666 - acc: 0.8800 - val_loss: 1.0574 - val_acc: 0.7228\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8889\n",
      "Epoch 00020: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3427 - acc: 0.8889 - val_loss: 0.9440 - val_acc: 0.7496\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8902\n",
      "Epoch 00021: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3310 - acc: 0.8902 - val_loss: 1.1704 - val_acc: 0.7039\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8983\n",
      "Epoch 00022: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3119 - acc: 0.8983 - val_loss: 1.0619 - val_acc: 0.7347\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9021\n",
      "Epoch 00023: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3000 - acc: 0.9021 - val_loss: 1.1577 - val_acc: 0.7067\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9035\n",
      "Epoch 00024: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2920 - acc: 0.9034 - val_loss: 0.9682 - val_acc: 0.7563\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9080\n",
      "Epoch 00025: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2793 - acc: 0.9080 - val_loss: 0.9713 - val_acc: 0.7622\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9142\n",
      "Epoch 00026: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2613 - acc: 0.9141 - val_loss: 0.9861 - val_acc: 0.7559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9173\n",
      "Epoch 00027: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2534 - acc: 0.9173 - val_loss: 1.0605 - val_acc: 0.7424\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9189\n",
      "Epoch 00028: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2479 - acc: 0.9189 - val_loss: 0.9465 - val_acc: 0.7608\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9235\n",
      "Epoch 00029: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2336 - acc: 0.9235 - val_loss: 1.0830 - val_acc: 0.7361\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9252\n",
      "Epoch 00030: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2296 - acc: 0.9253 - val_loss: 1.0130 - val_acc: 0.7554\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9307\n",
      "Epoch 00031: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2136 - acc: 0.9307 - val_loss: 1.0316 - val_acc: 0.7508\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9301\n",
      "Epoch 00032: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2133 - acc: 0.9301 - val_loss: 1.0190 - val_acc: 0.7540\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9326\n",
      "Epoch 00033: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2091 - acc: 0.9326 - val_loss: 1.0281 - val_acc: 0.7505\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9338\n",
      "Epoch 00034: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2025 - acc: 0.9338 - val_loss: 1.0451 - val_acc: 0.7552\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9368\n",
      "Epoch 00035: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1945 - acc: 0.9367 - val_loss: 1.1762 - val_acc: 0.7356\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9373\n",
      "Epoch 00036: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1890 - acc: 0.9373 - val_loss: 1.0117 - val_acc: 0.7608\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9371\n",
      "Epoch 00037: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1953 - acc: 0.9371 - val_loss: 1.0254 - val_acc: 0.7533\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9420\n",
      "Epoch 00038: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1800 - acc: 0.9420 - val_loss: 1.1243 - val_acc: 0.7324\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9407\n",
      "Epoch 00039: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1781 - acc: 0.9407 - val_loss: 1.0875 - val_acc: 0.7468\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9431\n",
      "Epoch 00040: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1742 - acc: 0.9431 - val_loss: 1.1186 - val_acc: 0.7475\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9446\n",
      "Epoch 00041: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1723 - acc: 0.9446 - val_loss: 1.0004 - val_acc: 0.7659\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9440\n",
      "Epoch 00042: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1717 - acc: 0.9440 - val_loss: 1.0533 - val_acc: 0.7636\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9476\n",
      "Epoch 00043: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1621 - acc: 0.9476 - val_loss: 1.0141 - val_acc: 0.7727\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9482\n",
      "Epoch 00044: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1571 - acc: 0.9482 - val_loss: 1.0364 - val_acc: 0.7675\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9495\n",
      "Epoch 00045: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1550 - acc: 0.9495 - val_loss: 1.0030 - val_acc: 0.7710\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9527\n",
      "Epoch 00046: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1507 - acc: 0.9527 - val_loss: 1.0247 - val_acc: 0.7643\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9510\n",
      "Epoch 00047: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1496 - acc: 0.9510 - val_loss: 1.1140 - val_acc: 0.7517\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9522\n",
      "Epoch 00048: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1503 - acc: 0.9522 - val_loss: 1.1379 - val_acc: 0.7494\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9527\n",
      "Epoch 00049: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1439 - acc: 0.9527 - val_loss: 1.0787 - val_acc: 0.7610\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9563\n",
      "Epoch 00050: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1393 - acc: 0.9563 - val_loss: 1.1788 - val_acc: 0.7389\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9537\n",
      "Epoch 00051: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1458 - acc: 0.9536 - val_loss: 1.0872 - val_acc: 0.7596\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9565\n",
      "Epoch 00052: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1329 - acc: 0.9565 - val_loss: 1.1501 - val_acc: 0.7536\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9581\n",
      "Epoch 00053: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1303 - acc: 0.9581 - val_loss: 1.1187 - val_acc: 0.7631\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9574\n",
      "Epoch 00054: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1313 - acc: 0.9574 - val_loss: 1.1071 - val_acc: 0.7645\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9598\n",
      "Epoch 00055: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1246 - acc: 0.9598 - val_loss: 1.0656 - val_acc: 0.7778\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9600\n",
      "Epoch 00056: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1250 - acc: 0.9600 - val_loss: 1.1092 - val_acc: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9627\n",
      "Epoch 00057: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1190 - acc: 0.9626 - val_loss: 1.0677 - val_acc: 0.7701\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9611\n",
      "Epoch 00058: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1232 - acc: 0.9611 - val_loss: 1.1454 - val_acc: 0.7543\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9622\n",
      "Epoch 00059: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1190 - acc: 0.9622 - val_loss: 1.1529 - val_acc: 0.7589\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9630\n",
      "Epoch 00060: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1179 - acc: 0.9630 - val_loss: 1.1044 - val_acc: 0.7694\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9627\n",
      "Epoch 00061: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1170 - acc: 0.9626 - val_loss: 1.0953 - val_acc: 0.7710\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9644\n",
      "Epoch 00062: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1126 - acc: 0.9644 - val_loss: 1.1196 - val_acc: 0.7601\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9644\n",
      "Epoch 00063: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1109 - acc: 0.9644 - val_loss: 1.1222 - val_acc: 0.7661\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9634\n",
      "Epoch 00064: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1140 - acc: 0.9634 - val_loss: 1.0768 - val_acc: 0.7741\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9653\n",
      "Epoch 00065: val_loss did not improve from 0.91028\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1091 - acc: 0.9653 - val_loss: 1.1226 - val_acc: 0.7671\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 886us/sample - loss: 0.9896 - acc: 0.7171\n",
      "Loss: 0.9895950038608856 Accuracy: 0.71713394\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1487 - acc: 0.3613\n",
      "Epoch 00001: val_loss improved from inf to 1.32555, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/001-1.3255.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.1487 - acc: 0.3613 - val_loss: 1.3255 - val_acc: 0.5772\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3783 - acc: 0.5680\n",
      "Epoch 00002: val_loss improved from 1.32555 to 1.07061, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/002-1.0706.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3783 - acc: 0.5680 - val_loss: 1.0706 - val_acc: 0.6851\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1411 - acc: 0.6462\n",
      "Epoch 00003: val_loss improved from 1.07061 to 0.91933, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/003-0.9193.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1410 - acc: 0.6462 - val_loss: 0.9193 - val_acc: 0.7209\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9963 - acc: 0.6941\n",
      "Epoch 00004: val_loss improved from 0.91933 to 0.86698, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/004-0.8670.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9963 - acc: 0.6941 - val_loss: 0.8670 - val_acc: 0.7417\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9034 - acc: 0.7219\n",
      "Epoch 00005: val_loss improved from 0.86698 to 0.82154, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/005-0.8215.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9036 - acc: 0.7218 - val_loss: 0.8215 - val_acc: 0.7538\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.7445\n",
      "Epoch 00006: val_loss improved from 0.82154 to 0.78876, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/006-0.7888.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8314 - acc: 0.7444 - val_loss: 0.7888 - val_acc: 0.7612\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7637\n",
      "Epoch 00007: val_loss improved from 0.78876 to 0.76810, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/007-0.7681.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7730 - acc: 0.7636 - val_loss: 0.7681 - val_acc: 0.7708\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7826\n",
      "Epoch 00008: val_loss did not improve from 0.76810\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7169 - acc: 0.7826 - val_loss: 0.7917 - val_acc: 0.7727\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6719 - acc: 0.7930\n",
      "Epoch 00009: val_loss improved from 0.76810 to 0.73531, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/009-0.7353.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6721 - acc: 0.7930 - val_loss: 0.7353 - val_acc: 0.7904\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8054\n",
      "Epoch 00010: val_loss improved from 0.73531 to 0.70185, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/010-0.7019.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6338 - acc: 0.8053 - val_loss: 0.7019 - val_acc: 0.7911\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8160\n",
      "Epoch 00011: val_loss did not improve from 0.70185\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5937 - acc: 0.8160 - val_loss: 0.7713 - val_acc: 0.7743\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.8286\n",
      "Epoch 00012: val_loss improved from 0.70185 to 0.69401, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/012-0.6940.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5582 - acc: 0.8287 - val_loss: 0.6940 - val_acc: 0.7955\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8374\n",
      "Epoch 00013: val_loss did not improve from 0.69401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5259 - acc: 0.8374 - val_loss: 0.7418 - val_acc: 0.7892\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8442\n",
      "Epoch 00014: val_loss did not improve from 0.69401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5036 - acc: 0.8442 - val_loss: 0.7135 - val_acc: 0.7918\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.8550\n",
      "Epoch 00015: val_loss did not improve from 0.69401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4700 - acc: 0.8549 - val_loss: 0.7789 - val_acc: 0.7768\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8608\n",
      "Epoch 00016: val_loss did not improve from 0.69401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4424 - acc: 0.8608 - val_loss: 0.7637 - val_acc: 0.7866\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8651\n",
      "Epoch 00017: val_loss improved from 0.69401 to 0.65965, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_5_conv_checkpoint/017-0.6596.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4281 - acc: 0.8652 - val_loss: 0.6596 - val_acc: 0.8146\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8729\n",
      "Epoch 00018: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4076 - acc: 0.8729 - val_loss: 0.7144 - val_acc: 0.7976\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8749\n",
      "Epoch 00019: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3949 - acc: 0.8748 - val_loss: 0.7007 - val_acc: 0.7999\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8843\n",
      "Epoch 00020: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3676 - acc: 0.8843 - val_loss: 0.7940 - val_acc: 0.7813\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8886\n",
      "Epoch 00021: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3523 - acc: 0.8885 - val_loss: 0.6720 - val_acc: 0.8169\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3419 - acc: 0.8907\n",
      "Epoch 00022: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3419 - acc: 0.8906 - val_loss: 0.7362 - val_acc: 0.7906\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8949\n",
      "Epoch 00023: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3283 - acc: 0.8949 - val_loss: 0.6887 - val_acc: 0.8099\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9024\n",
      "Epoch 00024: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3020 - acc: 0.9024 - val_loss: 0.7249 - val_acc: 0.8006\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9056\n",
      "Epoch 00025: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2950 - acc: 0.9056 - val_loss: 0.8194 - val_acc: 0.7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9066\n",
      "Epoch 00026: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2910 - acc: 0.9066 - val_loss: 0.7893 - val_acc: 0.7859\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9106\n",
      "Epoch 00027: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2794 - acc: 0.9106 - val_loss: 0.7071 - val_acc: 0.8185\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9151\n",
      "Epoch 00028: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2633 - acc: 0.9150 - val_loss: 0.7456 - val_acc: 0.8029\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9144\n",
      "Epoch 00029: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2658 - acc: 0.9144 - val_loss: 0.7418 - val_acc: 0.8032\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9193\n",
      "Epoch 00030: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2461 - acc: 0.9193 - val_loss: 0.7297 - val_acc: 0.8120\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9225\n",
      "Epoch 00031: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2417 - acc: 0.9225 - val_loss: 0.7227 - val_acc: 0.8130\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9244\n",
      "Epoch 00032: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2324 - acc: 0.9244 - val_loss: 0.7510 - val_acc: 0.8071\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9270\n",
      "Epoch 00033: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2283 - acc: 0.9270 - val_loss: 0.6939 - val_acc: 0.8234\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9298\n",
      "Epoch 00034: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2133 - acc: 0.9298 - val_loss: 0.8328 - val_acc: 0.7913\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9337\n",
      "Epoch 00035: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2059 - acc: 0.9337 - val_loss: 0.7365 - val_acc: 0.8139\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9313\n",
      "Epoch 00036: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2133 - acc: 0.9313 - val_loss: 0.7121 - val_acc: 0.8192\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9367\n",
      "Epoch 00037: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2010 - acc: 0.9367 - val_loss: 0.7181 - val_acc: 0.8134\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9372\n",
      "Epoch 00038: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1930 - acc: 0.9372 - val_loss: 0.8032 - val_acc: 0.8025\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9380\n",
      "Epoch 00039: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1907 - acc: 0.9380 - val_loss: 0.7154 - val_acc: 0.8183\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9401\n",
      "Epoch 00040: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1832 - acc: 0.9401 - val_loss: 0.7018 - val_acc: 0.8246\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9409\n",
      "Epoch 00041: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1821 - acc: 0.9409 - val_loss: 0.7157 - val_acc: 0.8241\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9443\n",
      "Epoch 00042: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1741 - acc: 0.9443 - val_loss: 0.7029 - val_acc: 0.8262\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9445\n",
      "Epoch 00043: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1700 - acc: 0.9445 - val_loss: 0.7366 - val_acc: 0.8241\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9440\n",
      "Epoch 00044: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1710 - acc: 0.9440 - val_loss: 0.7033 - val_acc: 0.8283\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9479\n",
      "Epoch 00045: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1641 - acc: 0.9479 - val_loss: 0.7419 - val_acc: 0.8164\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9492\n",
      "Epoch 00046: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1585 - acc: 0.9492 - val_loss: 0.7569 - val_acc: 0.8188\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9496\n",
      "Epoch 00047: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1568 - acc: 0.9496 - val_loss: 0.7105 - val_acc: 0.8276\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9525\n",
      "Epoch 00048: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1479 - acc: 0.9525 - val_loss: 0.7807 - val_acc: 0.8139\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9510\n",
      "Epoch 00049: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1473 - acc: 0.9510 - val_loss: 0.8437 - val_acc: 0.8015\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9530\n",
      "Epoch 00050: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1483 - acc: 0.9530 - val_loss: 0.7587 - val_acc: 0.8237\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9533\n",
      "Epoch 00051: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1406 - acc: 0.9532 - val_loss: 0.7449 - val_acc: 0.8267\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9527\n",
      "Epoch 00052: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1439 - acc: 0.9527 - val_loss: 0.8277 - val_acc: 0.8111\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9560\n",
      "Epoch 00053: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1369 - acc: 0.9560 - val_loss: 0.7307 - val_acc: 0.8297\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9568\n",
      "Epoch 00054: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1362 - acc: 0.9568 - val_loss: 0.7515 - val_acc: 0.8225\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9580\n",
      "Epoch 00055: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1322 - acc: 0.9579 - val_loss: 0.7786 - val_acc: 0.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9573\n",
      "Epoch 00056: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1352 - acc: 0.9573 - val_loss: 0.7495 - val_acc: 0.8253\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9599\n",
      "Epoch 00057: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1268 - acc: 0.9599 - val_loss: 0.7607 - val_acc: 0.8199\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9607\n",
      "Epoch 00058: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1257 - acc: 0.9607 - val_loss: 0.8154 - val_acc: 0.8181\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9621\n",
      "Epoch 00059: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1189 - acc: 0.9621 - val_loss: 0.9238 - val_acc: 0.8020\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9614\n",
      "Epoch 00060: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1231 - acc: 0.9614 - val_loss: 0.7679 - val_acc: 0.8192\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9620\n",
      "Epoch 00061: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1212 - acc: 0.9619 - val_loss: 0.7579 - val_acc: 0.8272\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9643\n",
      "Epoch 00062: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1144 - acc: 0.9644 - val_loss: 0.7831 - val_acc: 0.8276\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9640\n",
      "Epoch 00063: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1130 - acc: 0.9640 - val_loss: 0.7323 - val_acc: 0.8348\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9648\n",
      "Epoch 00064: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1111 - acc: 0.9648 - val_loss: 0.7906 - val_acc: 0.8241\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9637\n",
      "Epoch 00065: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1152 - acc: 0.9637 - val_loss: 0.7909 - val_acc: 0.8223\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9657\n",
      "Epoch 00066: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1089 - acc: 0.9657 - val_loss: 0.7739 - val_acc: 0.8237\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9646\n",
      "Epoch 00067: val_loss did not improve from 0.65965\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1095 - acc: 0.9646 - val_loss: 0.7463 - val_acc: 0.8323\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 920us/sample - loss: 0.7483 - acc: 0.7832\n",
      "Loss: 0.7482678103422202 Accuracy: 0.78317755\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2206 - acc: 0.3245\n",
      "Epoch 00001: val_loss improved from inf to 1.34546, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/001-1.3455.hdf5\n",
      "36805/36805 [==============================] - 98s 3ms/sample - loss: 2.2204 - acc: 0.3246 - val_loss: 1.3455 - val_acc: 0.5674\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4054 - acc: 0.5573\n",
      "Epoch 00002: val_loss improved from 1.34546 to 1.02997, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/002-1.0300.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4054 - acc: 0.5573 - val_loss: 1.0300 - val_acc: 0.6902\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1189 - acc: 0.6514\n",
      "Epoch 00003: val_loss improved from 1.02997 to 0.83746, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/003-0.8375.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.1189 - acc: 0.6514 - val_loss: 0.8375 - val_acc: 0.7561\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9650 - acc: 0.7011\n",
      "Epoch 00004: val_loss improved from 0.83746 to 0.74747, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/004-0.7475.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9652 - acc: 0.7011 - val_loss: 0.7475 - val_acc: 0.7757\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8597 - acc: 0.7377\n",
      "Epoch 00005: val_loss improved from 0.74747 to 0.70338, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/005-0.7034.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8597 - acc: 0.7376 - val_loss: 0.7034 - val_acc: 0.7950\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7858 - acc: 0.7616\n",
      "Epoch 00006: val_loss improved from 0.70338 to 0.68608, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/006-0.6861.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7859 - acc: 0.7616 - val_loss: 0.6861 - val_acc: 0.8004\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7832\n",
      "Epoch 00007: val_loss improved from 0.68608 to 0.62155, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/007-0.6215.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7201 - acc: 0.7832 - val_loss: 0.6215 - val_acc: 0.8232\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8003\n",
      "Epoch 00008: val_loss improved from 0.62155 to 0.61528, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/008-0.6153.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6658 - acc: 0.8003 - val_loss: 0.6153 - val_acc: 0.8232\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6183 - acc: 0.8165\n",
      "Epoch 00009: val_loss did not improve from 0.61528\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6184 - acc: 0.8165 - val_loss: 0.6177 - val_acc: 0.8204\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8288\n",
      "Epoch 00010: val_loss did not improve from 0.61528\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5787 - acc: 0.8287 - val_loss: 0.6236 - val_acc: 0.8220\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.8360\n",
      "Epoch 00011: val_loss improved from 0.61528 to 0.53251, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/011-0.5325.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5543 - acc: 0.8361 - val_loss: 0.5325 - val_acc: 0.8507\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8459\n",
      "Epoch 00012: val_loss improved from 0.53251 to 0.49747, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/012-0.4975.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5153 - acc: 0.8459 - val_loss: 0.4975 - val_acc: 0.8640\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8520\n",
      "Epoch 00013: val_loss did not improve from 0.49747\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4955 - acc: 0.8520 - val_loss: 0.5321 - val_acc: 0.8535\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8605\n",
      "Epoch 00014: val_loss did not improve from 0.49747\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4695 - acc: 0.8605 - val_loss: 0.5367 - val_acc: 0.8446\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8694\n",
      "Epoch 00015: val_loss improved from 0.49747 to 0.48585, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/015-0.4858.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4444 - acc: 0.8693 - val_loss: 0.4858 - val_acc: 0.8640\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8710\n",
      "Epoch 00016: val_loss did not improve from 0.48585\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4280 - acc: 0.8710 - val_loss: 0.4970 - val_acc: 0.8588\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8803\n",
      "Epoch 00017: val_loss improved from 0.48585 to 0.47009, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/017-0.4701.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4053 - acc: 0.8803 - val_loss: 0.4701 - val_acc: 0.8703\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8811\n",
      "Epoch 00018: val_loss improved from 0.47009 to 0.42436, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/018-0.4244.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3902 - acc: 0.8812 - val_loss: 0.4244 - val_acc: 0.8828\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8879\n",
      "Epoch 00019: val_loss did not improve from 0.42436\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3739 - acc: 0.8879 - val_loss: 0.4331 - val_acc: 0.8852\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8895\n",
      "Epoch 00020: val_loss did not improve from 0.42436\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3655 - acc: 0.8895 - val_loss: 0.4383 - val_acc: 0.8784\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8946\n",
      "Epoch 00021: val_loss did not improve from 0.42436\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3506 - acc: 0.8946 - val_loss: 0.4713 - val_acc: 0.8749\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8967\n",
      "Epoch 00022: val_loss did not improve from 0.42436\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3385 - acc: 0.8967 - val_loss: 0.4911 - val_acc: 0.8640\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.9036\n",
      "Epoch 00023: val_loss did not improve from 0.42436\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3205 - acc: 0.9036 - val_loss: 0.4675 - val_acc: 0.8696\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9066\n",
      "Epoch 00024: val_loss improved from 0.42436 to 0.40985, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/024-0.4099.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3076 - acc: 0.9066 - val_loss: 0.4099 - val_acc: 0.8875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9094\n",
      "Epoch 00025: val_loss improved from 0.40985 to 0.40834, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/025-0.4083.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2965 - acc: 0.9094 - val_loss: 0.4083 - val_acc: 0.8856\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9121\n",
      "Epoch 00026: val_loss did not improve from 0.40834\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2889 - acc: 0.9121 - val_loss: 0.4290 - val_acc: 0.8770\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9149\n",
      "Epoch 00027: val_loss improved from 0.40834 to 0.40188, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/027-0.4019.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2788 - acc: 0.9150 - val_loss: 0.4019 - val_acc: 0.8884\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9163\n",
      "Epoch 00028: val_loss improved from 0.40188 to 0.39584, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/028-0.3958.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2741 - acc: 0.9163 - val_loss: 0.3958 - val_acc: 0.8947\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9192\n",
      "Epoch 00029: val_loss did not improve from 0.39584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2642 - acc: 0.9192 - val_loss: 0.4109 - val_acc: 0.8884\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9202\n",
      "Epoch 00030: val_loss did not improve from 0.39584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2548 - acc: 0.9202 - val_loss: 0.4020 - val_acc: 0.8866\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9251\n",
      "Epoch 00031: val_loss did not improve from 0.39584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2454 - acc: 0.9251 - val_loss: 0.4077 - val_acc: 0.8898\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9260\n",
      "Epoch 00032: val_loss did not improve from 0.39584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2389 - acc: 0.9259 - val_loss: 0.4393 - val_acc: 0.8873\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9272\n",
      "Epoch 00033: val_loss did not improve from 0.39584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2364 - acc: 0.9272 - val_loss: 0.4159 - val_acc: 0.8861\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9281\n",
      "Epoch 00034: val_loss improved from 0.39584 to 0.38379, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/034-0.3838.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2280 - acc: 0.9281 - val_loss: 0.3838 - val_acc: 0.8963\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9295\n",
      "Epoch 00035: val_loss did not improve from 0.38379\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2229 - acc: 0.9295 - val_loss: 0.4192 - val_acc: 0.8935\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9318\n",
      "Epoch 00036: val_loss did not improve from 0.38379\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2190 - acc: 0.9317 - val_loss: 0.4123 - val_acc: 0.8882\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9328\n",
      "Epoch 00037: val_loss did not improve from 0.38379\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2156 - acc: 0.9328 - val_loss: 0.4259 - val_acc: 0.8828\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9354\n",
      "Epoch 00038: val_loss did not improve from 0.38379\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2035 - acc: 0.9354 - val_loss: 0.3914 - val_acc: 0.8938\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9383\n",
      "Epoch 00039: val_loss improved from 0.38379 to 0.37461, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_6_conv_checkpoint/039-0.3746.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1948 - acc: 0.9382 - val_loss: 0.3746 - val_acc: 0.8980\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9388\n",
      "Epoch 00040: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1960 - acc: 0.9386 - val_loss: 0.3975 - val_acc: 0.8961\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9398\n",
      "Epoch 00041: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1940 - acc: 0.9397 - val_loss: 0.4055 - val_acc: 0.8915\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9413\n",
      "Epoch 00042: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1868 - acc: 0.9413 - val_loss: 0.3920 - val_acc: 0.8984\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9428\n",
      "Epoch 00043: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1782 - acc: 0.9428 - val_loss: 0.4372 - val_acc: 0.8926\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9446\n",
      "Epoch 00044: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1761 - acc: 0.9446 - val_loss: 0.3989 - val_acc: 0.8966\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9450\n",
      "Epoch 00045: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1747 - acc: 0.9450 - val_loss: 0.4194 - val_acc: 0.8901\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9459\n",
      "Epoch 00046: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1700 - acc: 0.9459 - val_loss: 0.3862 - val_acc: 0.8994\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9474\n",
      "Epoch 00047: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1667 - acc: 0.9474 - val_loss: 0.4024 - val_acc: 0.8919\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9490\n",
      "Epoch 00048: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1612 - acc: 0.9490 - val_loss: 0.4151 - val_acc: 0.8963\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9497\n",
      "Epoch 00049: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1597 - acc: 0.9497 - val_loss: 0.4083 - val_acc: 0.8963\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9520\n",
      "Epoch 00050: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1513 - acc: 0.9520 - val_loss: 0.4244 - val_acc: 0.8977\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9530\n",
      "Epoch 00051: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1496 - acc: 0.9530 - val_loss: 0.4113 - val_acc: 0.9019\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9524\n",
      "Epoch 00052: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1499 - acc: 0.9524 - val_loss: 0.3933 - val_acc: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9557\n",
      "Epoch 00053: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1424 - acc: 0.9557 - val_loss: 0.4275 - val_acc: 0.9005\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9555\n",
      "Epoch 00054: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1409 - acc: 0.9555 - val_loss: 0.4001 - val_acc: 0.8973\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9570\n",
      "Epoch 00055: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1374 - acc: 0.9570 - val_loss: 0.3897 - val_acc: 0.9036\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9557\n",
      "Epoch 00056: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1383 - acc: 0.9557 - val_loss: 0.4281 - val_acc: 0.8952\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9574\n",
      "Epoch 00057: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1336 - acc: 0.9575 - val_loss: 0.3988 - val_acc: 0.8982\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9575\n",
      "Epoch 00058: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1324 - acc: 0.9575 - val_loss: 0.4342 - val_acc: 0.8903\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9598\n",
      "Epoch 00059: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1286 - acc: 0.9598 - val_loss: 0.4132 - val_acc: 0.8991\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9607\n",
      "Epoch 00060: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1239 - acc: 0.9607 - val_loss: 0.4215 - val_acc: 0.9019\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9605\n",
      "Epoch 00061: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1231 - acc: 0.9605 - val_loss: 0.4028 - val_acc: 0.9026\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9603\n",
      "Epoch 00062: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1213 - acc: 0.9603 - val_loss: 0.3924 - val_acc: 0.9029\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9637\n",
      "Epoch 00063: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1151 - acc: 0.9637 - val_loss: 0.4173 - val_acc: 0.8945\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9617\n",
      "Epoch 00064: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1182 - acc: 0.9617 - val_loss: 0.3962 - val_acc: 0.9043\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9628\n",
      "Epoch 00065: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1152 - acc: 0.9628 - val_loss: 0.3858 - val_acc: 0.9071\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9643\n",
      "Epoch 00066: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1098 - acc: 0.9642 - val_loss: 0.3887 - val_acc: 0.9036\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9640\n",
      "Epoch 00067: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1099 - acc: 0.9640 - val_loss: 0.3906 - val_acc: 0.9045\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9646\n",
      "Epoch 00068: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1097 - acc: 0.9646 - val_loss: 0.4200 - val_acc: 0.8973\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9642\n",
      "Epoch 00069: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1101 - acc: 0.9642 - val_loss: 0.4907 - val_acc: 0.8859\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9652\n",
      "Epoch 00070: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1078 - acc: 0.9652 - val_loss: 0.4350 - val_acc: 0.9073\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9666\n",
      "Epoch 00071: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1068 - acc: 0.9666 - val_loss: 0.3860 - val_acc: 0.9078\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9693\n",
      "Epoch 00072: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0955 - acc: 0.9693 - val_loss: 0.4245 - val_acc: 0.8947\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9700\n",
      "Epoch 00073: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0947 - acc: 0.9699 - val_loss: 0.3973 - val_acc: 0.9071\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9669\n",
      "Epoch 00074: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1001 - acc: 0.9668 - val_loss: 0.4425 - val_acc: 0.8917\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9683\n",
      "Epoch 00075: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1007 - acc: 0.9683 - val_loss: 0.4092 - val_acc: 0.9026\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9720\n",
      "Epoch 00076: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0894 - acc: 0.9720 - val_loss: 0.4035 - val_acc: 0.9119\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9701\n",
      "Epoch 00077: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0944 - acc: 0.9701 - val_loss: 0.4292 - val_acc: 0.9045\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9712\n",
      "Epoch 00078: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0901 - acc: 0.9712 - val_loss: 0.3749 - val_acc: 0.9089\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9729\n",
      "Epoch 00079: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0879 - acc: 0.9729 - val_loss: 0.4696 - val_acc: 0.8910\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9728\n",
      "Epoch 00080: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0869 - acc: 0.9728 - val_loss: 0.4330 - val_acc: 0.9001\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9722\n",
      "Epoch 00081: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0881 - acc: 0.9722 - val_loss: 0.4426 - val_acc: 0.9043\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9739\n",
      "Epoch 00082: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0819 - acc: 0.9739 - val_loss: 0.4471 - val_acc: 0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9730\n",
      "Epoch 00083: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0854 - acc: 0.9730 - val_loss: 0.5440 - val_acc: 0.8880\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9704\n",
      "Epoch 00084: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0942 - acc: 0.9704 - val_loss: 0.4098 - val_acc: 0.9045\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9752\n",
      "Epoch 00085: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0768 - acc: 0.9752 - val_loss: 0.4869 - val_acc: 0.8919\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9709\n",
      "Epoch 00086: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0908 - acc: 0.9708 - val_loss: 0.3862 - val_acc: 0.9129\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9751\n",
      "Epoch 00087: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0814 - acc: 0.9751 - val_loss: 0.4041 - val_acc: 0.9094\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9751\n",
      "Epoch 00088: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0757 - acc: 0.9751 - val_loss: 0.4207 - val_acc: 0.9073\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9742\n",
      "Epoch 00089: val_loss did not improve from 0.37461\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0796 - acc: 0.9742 - val_loss: 0.4167 - val_acc: 0.9092\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmX0m+wJhCRD2fV9EQcC64oJbEa3W7VGrVVt/WhVr7aNtbbW1Vm21Vq2tWtcHa0WhoiKLCqiIqCBbgAAJELInk0ySWc7vjzOTBQIkJMME5vt+veaVmbueezNzvvcs91yltUYIIYQAsMQ6AUIIIToPCQpCCCEaSFAQQgjRQIKCEEKIBhIUhBBCNJCgIIQQooEEBSGEEA0kKAghhGggQUEIIUQDW6wT0FaZmZk6Jycn1skQQohjypdfflmste5yuOWOuaCQk5PD6tWrY50MIYQ4piildrRmOak+EkII0UCCghBCiAYSFIQQQjQ45toUWuL3+8nPz6e2tjbWSTlmuVwusrOzsdvtsU6KECKGjougkJ+fT1JSEjk5OSilYp2cY47WmpKSEvLz8+nbt2+skyOEiKHjovqotraWjIwMCQhHSClFRkaGlLSEEMdHUAAkILSTnD8hBBxHQeFwgsEa6uoKCIX8sU6KEEJ0WnETFEKhOurr96B1xweF8vJynnrqqSNa9+yzz6a8vLzVy99///088sgjR7QvIYQ4nLgJCkqZQ9U61OHbPlRQCAQCh1x34cKFpKamdniahBDiSMRNUGg81I4PCnPnzmXr1q2MGTOGO++8k6VLl3LyyScza9Yshg0bBsAFF1zA+PHjGT58OM8880zDujk5ORQXF5OXl8fQoUO5/vrrGT58OGeccQY+n++Q+127di2TJ09m1KhRXHjhhZSVlQHwxBNPMGzYMEaNGsWll14KwLJlyxgzZgxjxoxh7NixVFVVdfh5EEIc+46LLqlNbdlyG17v2hbmhAgGq7FY3CjVtsNOTBzDwIGPHXT+Qw89xLp161i71ux36dKlrFmzhnXr1jV08Xz++edJT0/H5/MxceJELr74YjIyMvZL+xZeffVVnn32WS655BLefPNNrrjiioPu98orr+TPf/4z06dP55e//CUPPPAAjz32GA899BDbt2/H6XQ2VE098sgjPPnkk0yZMgWv14vL5WrTORBCxIc4KilE6KOyl0mTJjXr8//EE08wevRoJk+ezK5du9iyZcsB6/Tt25cxY8YAMH78ePLy8g66/YqKCsrLy5k+fToAV111FcuXLwdg1KhRXH755fzrX//CZjMBcMqUKdx+++088cQTlJeXN0wXQoimjruc4WBX9KGQn+rqr3E6e+NwdI16OhISEhreL126lA8//JCVK1fi8XiYMWNGi/cEOJ3OhvdWq/Ww1UcHs2DBApYvX84777zDgw8+yLfffsvcuXM555xzWLhwIVOmTGHRokUMGTLkiLYvhDh+xU1JIZoNzUlJSYeso6+oqCAtLQ2Px8PGjRtZtWpVu/eZkpJCWloaH3/8MQAvvfQS06dPJxQKsWvXLk455RQefvhhKioq8Hq9bN26lZEjR3L33XczceJENm7c2O40CCGOP8ddSeHgotfQnJGRwZQpUxgxYgQzZ87knHPOaTb/rLPO4umnn2bo0KEMHjyYyZMnd8h+X3jhBW688UZqamro168f//jHPwgGg1xxxRVUVFSgteYnP/kJqamp3HfffSxZsgSLxcLw4cOZOXNmh6RBCHF8UVofnTr2jjJhwgS9/0N2NmzYwNChQw+7blXVGuz2LrhcvaKVvGNaa8+jEOLYo5T6Ums94XDLxU31EUSqkDq+pCCEEMeLuAoKYEXrYKwTIYQQnVZcBQWlLFFpaBZCiONFXAUFc7gSFIQQ4mDiKigoJdVHQghxKHEWFKSkIIQQhxJXQaEzNTQnJia2aboQQhwNcRUUpKQghBCHFrWgoJTqpZRaopT6Tim1Xin10xaWUUqpJ5RSuUqpb5RS46KVHiM6vY/mzp3Lk08+2fA58iAcr9fLqaeeyrhx4xg5ciRvv/12q7eptebOO+9kxIgRjBw5ktdffx2APXv2MG3aNMaMGcOIESP4+OOPCQaDXH311Q3L/ulPf+rwYxRCxIdoDnMRAO7QWq9RSiUBXyqlPtBaf9dkmZnAwPDrBOCv4b9H7rbbYG1LQ2eDI1SPTdehrUm06YnEY8bAYwcfOnvOnDncdttt3HzzzQC88cYbLFq0CJfLxVtvvUVycjLFxcVMnjyZWbNmtep5yP/+979Zu3YtX3/9NcXFxUycOJFp06bxyiuvcOaZZ3LvvfcSDAapqalh7dq1FBQUsG7dOoA2PclNCCGailpQ0FrvAfaE31cppTYAPYGmQeF84EVtxtpYpZRKVUp1D6/b8RThkbN1+EPHGDt2LPv27WP37t0UFRWRlpZGr1698Pv9/PznP2f58uVYLBYKCgooLCykW7duh93mJ598wmWXXYbVaiUrK4vp06fzxRdfMHHiRK699lr8fj8XXHABY8aMoV+/fmzbto1bb72Vc845hzPOOKPDjk0IEV+OyoB4SqkcYCzw2X6zegK7mnzOD0878qBwiCv6QH0RdXU7SEgYhbI4jngXLZk9ezbz5s1j7969zJkzB4CXX36ZoqIivvzyS+x2Ozk5OS0Omd0W06ZNY/ny5SxYsICrr76a22+/nSuvvJKvv/6aRYsW8fTTT/PGG2/w/PPPd8RhCSHiTNQbmpVSicCbwG1a68oj3MYNSqnVSqnVRUVF7UhL9IbPnjNnDq+99hrz5s1j9uzZgBkyu2vXrtjtdpYsWcKOHTtavb2TTz6Z119/nWAwSFFREcuXL2fSpEns2LGDrKwsrr/+eq677jrWrFlDcXExoVCIiy++mN/85jesWbOmw49PCBEfolpSUErZMQHhZa31v1tYpABoOmRpdnhaM1rrZ4BnwIySeuQpit7w2cOHD6eqqoqePXvSvXt3AC6//HLOO+88Ro4cyYQJE9r0UJsLL7yQlStXMnr0aJRS/P73v6dbt2688MIL/OEPf8But5OYmMiLL75IQUEB11xzDaGQOa7f/e53HX58Qoj4ELWhs5VpTX0BKNVa33aQZc4BbgHOxjQwP6G1nnSo7bZn6OxAoBKfbzNu92BstqTWHUgckaGzhTh+tXbo7GiWFKYAPwS+VUpFugP9HOgNoLV+GliICQi5QA1wTRTTQzRLCkIIcTyIZu+jTzhMF59wr6Obo5WG/SllDe+3c9zVLIQQnU0c3tEcnYZmIYQ4HsRVUJDqIyGEOLS4CgpSfSSEEIcWV0GhsYlDSgpCCNGSuAoKppdsxw+fXV5ezlNPPXVE65599tkyVpEQotOIq6AA0Rk++1BBIRAIHHLdhQsXkpqa2qHpEUKIIxV3QSEaw2fPnTuXrVu3MmbMGO68806WLl3KySefzKxZsxg2bBgAF1xwAePHj2f48OE888wzDevm5ORQXFxMXl4eQ4cO5frrr2f48OGcccYZ+Hy+A/b1zjvvcMIJJzB27FhOO+00CgsLAfB6vVxzzTWMHDmSUaNG8eabbwLw3nvvMW7cOEaPHs2pp57aoccthDj+HJUB8Y6mQ4ycDUAw2A+lFJY2hMPDjJzNQw89xLp161gb3vHSpUtZs2YN69ato2/fvgA8//zzpKen4/P5mDhxIhdffDEZGRnNtrNlyxZeffVVnn32WS655BLefPNNrrjiimbLTJ06lVWrVqGU4rnnnuP3v/89f/zjH/n1r39NSkoK3377LQBlZWUUFRVx/fXXs3z5cvr27UtpaWnrD1oIEZeOu6BwOK15lkFHmDRpUkNAAHjiiSd46623ANi1axdbtmw5ICj07duXMWPGADB+/Hjy8vIO2G5+fj5z5sxhz5491NfXN+zjww8/5LXXXmtYLi0tjXfeeYdp06Y1LJOent6hxyiEOP4cd0HhUFf0ADU1BWgdICEhumP8JCQkNLxfunQpH374IStXrsTj8TBjxowWh9B2Op0N761Wa4vVR7feeiu33347s2bNYunSpdx///1RSb8QIj7FXZuCaWju2N5HSUlJVFVVHXR+RUUFaWlpeDweNm7cyKpVq454XxUVFfTs2ROAF154oWH66aef3uyRoGVlZUyePJnly5ezfft2AKk+EkIcVtwFhWg0NGdkZDBlyhRGjBjBnXfeecD8s846i0AgwNChQ5k7dy6TJ08+4n3df//9zJ49m/Hjx5OZmdkw/Re/+AVlZWWMGDGC0aNHs2TJErp06cIzzzzDRRddxOjRoxse/iOEEAcTtaGzo6U9Q2cD1NbuxO8vISlpbDSSd0yTobOFOH61dujsuCspROM+BSGEOF7EXVAAK6BlpFQhhGhB3AUFGT5bCCEOLu6CggyfLYQQBxd3QUGGzxZCiIOLu6AgJQUhhDi4uAsKjSWF2AaFxMTEmO5fCCFaEodBIdLQLNVHQgixv7gLCtGoPpo7d26zISbuv/9+HnnkEbxeL6eeeirjxo1j5MiRvP3224fd1sGG2G5pCOyDDZcthBBH6rgbEO+2925j7d5DjJ2NJhj0YrG4UMreqm2O6TaGx846+Eh7c+bM4bbbbuPmm28G4I033mDRokW4XC7eeustkpOTKS4uZvLkycyaNeuQI7W2NMR2KBRqcQjslobLFkKI9jjugkLrddzwHmPHjmXfvn3s3r2boqIi0tLS6NWrF36/n5///OcsX74ci8VCQUEBhYWFdOvW7aDbammI7aKiohaHwG5puGwhhGiP4y4oHOqKHkwDs9e7BoejJ05n9w7b7+zZs5k3bx579+5tGHju5ZdfpqioiC+//BK73U5OTk6LQ2ZHtHaIbSGEiJa4a1MwDc2Kjh4+e86cObz22mvMmzeP2bNnA2aY665du2K321myZAk7duw45DYONsT2wYbAbmm4bCGEaI+4CwpGxw+fPXz4cKqqqujZsyfdu5sSyOWXX87q1asZOXIkL774IkOGDDnkNg42xPbBhsBuabhsIYRoj7gbOhvA6/0GqzUJt7vv4ReOIzJ0thDHLxk6+xBk+GwhhGhZXAaFaFQfCSHE8eC4CQptqQYzQ13IHc1NHWvViEKI6DgugoLL5aKkpKQNGZuUFJrSWlNSUoLL5Yp1UoQQMXZc3KeQnZ1Nfn4+RUVFrVre7y8iFKrH6Tz4ncXxxuVykZ2dHetkCCFi7LgICna7veFu39bYtOlRSkoWMmZMQRRTJYQQx57jovqorSyWBIJBb6yTIYQQnU5cBgWrNZFg0CuNq0IIsZ+4DQoQIhSScYWEEKKpqAUFpdTzSql9Sql1B5k/QylVoZRaG379Mlpp2Z8JCkgVkhBC7CeaDc3/BP4CvHiIZT7WWp8bxTS0qDEoVANdjvbuhRCi04paSUFrvRwojdb228NqTQCkpCCEEPuLdZvCiUqpr5VS/1VKDT9aO5XqIyGEaFks71NYA/TRWnuVUmcD/wEGtrSgUuoG4AaA3r17t3vHEhSEEKJlMSspaK0rtdbe8PuFgF0plXmQZZ/RWk/QWk/o0qX9bQCRoBAKVbd7W0IIcTyJWVBQSnVT4SfYK6UmhdNScjT2LSUFIYRoWdSqj5RSrwIzgEylVD7wv4AdQGv9NPB94CalVADwAZfqo3Q3mTQ0CyFEy6IWFLTWlx1m/l8wXVaPOikpCCFEy2Ld+ygmpKQghBAti8ugoJQVi8UdvnlNCCFERFwGBTClBSkpCCFEc3EbFGy2VPz+TnnDtRBCxEzcBgWnsw+1tXmxToYQQnQqcRsU3O5+1NZui3UyhBCiU4nboOBy9cXvLyIQkHYFIYSIiNug4Hb3A6C2dnuMUyKEEJ1H/ASFd9+F3r0hLw8wJQWQoCCEEE3FT1BwuWDXLtixI/zRlBR8PmlXEEKIiPgJCn36mL87dwJgt2dgtSZJSUEIIZqIn6CQnW3+hksKSilcrr7SA0kIIZqIn6DgdkPXrg0lBTOpHz6flBSEECIifoICmCqkcEkBCJcUtnOURuwWQohOL76CQu/ezUoKLlc/QqEa/P59MUyUEEJ0HvEVFCIlhXDJwO023VKlB5IQQhjxFRR69wafD0rMUz8j3VKlB5IQQhjxFRT265bqcuUAUlIQQoiI+AoKvXubv+HGZqvVjcPRXbqlCiFEWHwFhf1KCtDYA0kIIUQrg4JS6qdKqWRl/F0ptUYpdUa0E9fh0tPB42nWLdXcqyAlBSGEgNaXFK7VWlcCZwBpwA+Bh6KWqmhRqsVuqXV1+YRC9TFMmBBCdA6tDQoq/Pds4CWt9fom044tLdzABiFqa3cefB0hhIgTrQ0KXyql3scEhUVKqSQgFL1kRdF+JQV5roIQQjSytXK5/wHGANu01jVKqXTgmuglK4r69IF9+8z9Cm53k+cqSLuCEEK0tqRwIrBJa12ulLoC+AVQEb1kRVGkW+quXQA4nT1QyiED4wkhBK0PCn8FapRSo4E7gK3Ai1FLVTTt1y1VKSsuVx8pKQghBK0PCgFthhI9H/iL1vpJICl6yYqi/W5gA9MDSdoUhBCi9UGhSil1D6Yr6gKllAWwRy9ZUdSzJ1gs+zU295V7FYQQgtYHhTlAHeZ+hb1ANvCHqKUqmux26NHjgJJCIFCK318Ww4QJIUTstSoohAPBy0CKUupcoFZrfWy2KYBpV2hSUkhKGgdAZeWqWKVICCE6hdYOc3EJ8DkwG7gE+Ewp9f1oJiyqevduVlJITp6MUjbKy5fFMFFCCBF7rb1P4V5gotZ6H4BSqgvwITAvWgmLqt69Yd48CIXAYsFqTSApaSIVFctjnTIhhIip1rYpWCIBIaykDet2Pn36gN8PhYUNk1JTp1NV9QXBYHUMEyaEELHV2oz9PaXUIqXU1Uqpq4EFwMLoJSvKWuiWmpIyDa0DVFSsjFGihBAi9lrb0Hwn8AwwKvx6Rmt9dzQTFlUtPFchJWUKYJEqJCFEXGttmwJa6zeBN6OYlqOnhZKCzZZMYuJYaWwWQsS1Q5YUlFJVSqnKFl5VSqnKw6z7vFJqn1Jq3UHmK6XUE0qpXKXUN0qpce05kDZJTobU1GZBAUy7QmXlZwSDtUctKUII0ZkcMihorZO01sktvJK01smH2fY/gbMOMX8mMDD8ugEzvtLRM2gQfPdds0mpqdPQuo6qqs+PalKEEKKziFoPIq31cqD0EIucD7yojVVAqlKqe7TSc4Bx42DNGtC6YVJKysmAorxc2hWEEPEplt1KewK7mnzOD087OsaNg4oK2N44EJ7dnk5CwkgqKqRdQQgRn46Jew2UUjcopVYrpVYXFRV1zEbHhZsw1qxpNjk1dRoVFSsIhfwdsx8hhDiGtLr3URQUAL2afM4OTzuA1voZTJdYJkyYoFtaps1GjACbzQSF7zeO2JGSMp2Cgr9QVfUlKSmTO2RXQoiOVV8PJSVQWgrV1RAImPtRAdLToWtXyMgAq9U8ZLGmBiorYe9e2LPH/AVISjIvlwvq6qC21vytrjYvr9d8VqrxZbM1vkIhqKoyy9XUmP05neaVkGDSkpZm3ufnm4qJvDwzUHPv3tCrF2RmmvTs2mWWqalpPE6lwOEwL6cTzj8fLr00uuc2lkFhPnCLUuo14ASgQmu956jt3ek0geGAksLJAJSXL5WgII45WptMMBAAt9sMChyZXldnMq/aJp3rQiGTWZaXm1d1tclw6+tNJhvJCC2W5n+DQfNU2717zcAASpnMNTnZ7Le+3uyvrq4xU66pMZ+bbisYNPsJBExarFYz3WIx0+vqzLZqa5tvp7oVAw8o1Xjs7eFwNJ4rrU2a95+fmGiOOxRqPPbq6gP3nZAAffua6YsXm4ASkZYG2dnmPEaEQuY8RP4n48e371haI2pBQSn1KjADyFRK5QP/S/gZDFrrpzF3RJ8N5AI1xOKZz+PGwTvvmP9Q+BvkcGSRmDiOoqJ59Okz96gnSRz7mmYMkQws8jfyqq01P/ZIxldR0fiKZJ51dSaztFrNVanVajLwfftMRlxR0fwKNrK/pqxWcxVcW3tgZtYRkpMhK8vsv7LSvHy+xqtlp9Nklh6PeTkc5ucWyWAjx2azmQAWCjUGCrvdZJQOhzkGj6dxW+np5pWRYTJau91sA0wJoqjInKdQyMz3eEzG3a0bdO9u/irVeJVfW2vS6nI1XuVH1rPsV8keSX8g0Hglf7DvQWWlKc14vWbE/oyMxmAF5n9YXGzOYWJix/9/jkTUgoLW+rLDzNfAzdHaf6uMHQvPPw8FBSZEh2VlXc7WrXdQU7MJj2dwDBMoOorfb36cJSXmR1haajKvSCYUCDRmxLW1jVek+2fqNTXmBx7JTHw+s37k5fcfeeZrtUJKismMIhmq3d6YPr/fXEVmZcGwYeZWGzCZlNaNmZnHYzJIn6/x5XabTCcpySwXyZiUarxtJzXVrOt0mowuUsoIhRoz8cjLYoEuXcx2j2VZWW1fRynzv7JaD72cxdJ4Xg8mJcW8OpNYVh/FXtPG5iZBoWvXS9m69WcUFr5M376/ilHi4kt1tbmyixSTm1YZ+HyN1QiRK/CmGV5VlbniKi83fyP1wnV1Zl5pafNiemtFrm49nuZXjllZMGBAY5VBJIOwWk1G2rQOOLJOJFOObMflMstGlk9JMcs1vYoUIhbiOyiMHm1+hWvWwKxZDZOdzh6kpn6PwsKXycl5ACW/1MPS2lw5FxdDWZm5oo5k2uXlZnpJiXkfybR9vsbGtyPtVKaUyWxTUswVWaROOzXVZMrJyY1VDWlppviemWn+ut3mai5ShdG0+sDlkgxaxKf4DgoJCTBkyAGNzWCqkDZtupbKys/issE5GDS9JPbuNRl2cXFjxh7p9RF5Rab5D9OL12YzmXfTjLdHD7jgAtP41r1786oLl8tk3G53Y3VK5Co8UrfscEjmvb/q+mp8AR+Znsx2bysYChIIBfCH/ARDQTSakA7hsDpIdBxYCV5dX823+76luKaYkpoSSn2lJDgS6JbYjayELFJdqfhDfuqD9fiDfvqm9aVrQtdDpuGj7R/x6c5PCWqTFpvFxmn9TuOkXidhUabCX2vN2r1rWb17NTMHziQ7OfuA7eyq2MXSvKUs27GMT3Z+QqIjkfHdxzO+x3hyUnMoqCxgZ8VOdlftZkbODGYPn43N0nIW6fP7yK/Mx6Is9Evrd8gLx5AOsblkMwPSBzTb3l7vXu776D5eWfcKKc4Uc44Ss8hwZ5DsTCbZmYzL5sJb76Wqrgqv38u5A8/lspGHrJlvN6Xb2zR/lE2YMEGvXr264zZ4xRWwdKm5ZG0iEKjg00+z6NHjegYO/HPH7a+TqKmBbdtMxl9W1tjImZcHX38N69aZK/n9uVzmKjvSyNe0wS8z07zS0sDt1lRZdrHHv4FuaSmc0G8Y2V2S25WBB0NBVu9ezcr8lTitTjI8GWR6MumR1IO+qX1x2pwHXbe6vpovdn/Bzoqd7KrYRX5lPhpNkiOJREciae40eqf0Jic1h17JvdhdtZuvC79m7d61FNcUMzB9IEMyhzAgfQA1/hoKqgooqCwgpEMM7zqckV1H0i2xG7mluSzbsYxlO5ZR469hbLexjO8+nuFdh1PqK2VH+Q52VuzEW+9tSFtIh/AFfNT4a6jx17Cveh/5lfnsqtxFSU0Jyc5kUl2ppLpSUUpRH6ynLlCHUoouni4NGUlBVQHr9q0jrzwPgL6pfTmx14lM6jEJjaaouoiimiKcVicTe05kUs9JDEwfyOaSzXyy8xM+2fUJuaW5lNSUUFxTTFltGSEdOug5HZQxiBOzT+SEnidQWF3I4u2L+Sz/M/xtvMenR1IPxnYby+TsyZw36DxGZY1CKcXG4o387P2fsWDLgoZlLcqC1hqNpldyLy4dcSkWZWHed/PYWrYVAKuyMmvwLG6acBM2i42FWxayYMsCNhRvACDNlcbU3lOp9lezZs8aymvLm6Un2ZlMZV0l/dP6c/eUuzln0Dmsyl/F8h3LWbFrBdvLt1NcU9ywfLo7nYk9zPmcnD2ZydmTSXen46338s+1/+Txzx4ntzSXLp4uXDT0Ir4/7Pus3LWShz99mPpgPZePuhybsrG3ei97qvZQXltOZV0lFXUV1Afr8dg9JDoSSXIkcdOEm7jjpDvadH4jlFJfaq0nHHa5uA8Kjz4Kd9xhLon3a3Vav3425eXLOPHEAiwWe8ftM0qCQVPnXlRVwcdbvuLbLeVs3FHOjt1VhMp6YykeTaC4D8VFqqGf9v4yMkyt2ujRMHKkuZK3JhWxoOhJ9tZvJc2TRJIjiXR3OhN6TOCE7BNIdCQS0iE+L/ic+Zvm89H2j1hftL5ZxgfQK7kXAzMGkuxMJtGRSII9gar6KvZV72Nf9T58fh+prlTS3emkudPw2Dy4bC5cNhc7K3eyeNtiymrLWky3RVnondKbAekDGJg+kIHpA+mf3p+88jwWblnI0ryl1AXrGo/TnYHVYqWqrgpfoIXoFxYJPrurdh/2/Ltt7oZtZSVkkexMZkvplsOuF0m/x+7BbXOT6cmkV0ovspOyyfRk4q33Ul5X3pB5OawOnFYnQR2kuKaYQm8hRTVFdE/szvCuwxmWOQyXzcVnBZ+xYtcK9nhNT2+rspLpyaTaX93wv7FZbARCAQC6JnRlRNcRZHoyyXBnkOZKw2lzYrPYsFvsWC1WLMqCRVmoqqvi892fs3LXSopqilAoxnUfx6l9T2VK7yl0S+xGhjuDdHc61f5q9nr3ste7l4raCpN+mxOLsrC5ZDNf7f2KNXvWsKFoQ0NmP77HeN7d/C4eu4f7pt3HzRNvxmVzoZSiqq6Ktze9zavrXuX9re8DcGrfU5k9bDbje4zn1W9f5fm1zzdk3A6rg+l9pjNzwEy+1/d7jMwa2ayEsa1sGwVVBWQnZ9MzqSd2q535m+bz4McPsnp3Y17jtrk5IfsEhmQMITs5m14pvagN1PJFwRd8vvtz1u1b1xBEh2QOYa93L+W15UzOnswPRvyAFfkreGfTO1T7TX/ai4ZexMOnPcyA9AEH/V6EdKghre0lQaG1li6FU06B//4Xzmo+fl9R0X9Yv/5CRo5cSEbGzI7bZzv4/Y09YfLzYdky8/r480ol6LTgAAAgAElEQVQqu8+H4W9A/0Vgq29xfVswidTAYDJc3eiR3I1+mT2Y2mcqpw2cRpc0J84mF9v5lfk8suIRnvnyGWoDtfRO6Y233ou33tuQwVqVlVFZo9hdtZvC6kKsysqJvU5kTNYYhncdztDMoVTUVbB+33rWF61ne/l2quqqqKqvwlvvJdmZTFZCFl0TuuKyuSivLaestoxSXyk+v4/aQC21gVrS3emc3u90zuh/BtNzphPSIUpqSijxlZBfmc+Wki3kluWypWQLW0q3NLv6G5wxmHMGnsPp/U9nQPoAeib1xG1v7DYTDAXNVXzFjoYr+azELMZ0G8OgjEHYLDa89V42l2wmtzSXREciPZN60jO5J1pr1u1bx7p969hSuoVhXYYxI2cGgzMGo5SioraCr/Z+xcbijXTxdKFPah96p/QmxdnY5UQphd1ij0rbldaawupCnFYnKa4ULMpCMBRkY/FGPi/4nO+KvmNol6FM7T2VgekD25wGrTV55XmkuFJId6e3K617vXtZsHkB8zfPZ8WuFVw05CJ+/b1fH7J6qcxXhlKKVFfzLj51gTre2fxOQ1VTS1Vdh6O15sNtH/Ltvm85MftExvcYj8N6kP6ngLfeyxcFX7AyfyUr81eS4kzhlkm3MDm7sfq5xl/DB1s/oFtiN07IPqHNaWoPCQqtVV5u6jt++1u4555ms0KhOlas6EZ6+jkMG/avjttnmNaar/Z+xcc7Pm646m5a51hfb5o7Pv4Yln9az9Li1/DW1UBZPyjtD9oKAxfgGTef2m5LCCk/KfRijGM241LOZNygLowdlkJmcgLby7fzTeE3fFP4DbmluRRWF7LXu5d91fsI6RCJjkRO63cavZJ7saV0C5tLNpNXnodCccWoK5g7dS5DMoc0nrbaclblr2LFrhWszF9JhjuD8wefz1kDziLNndbh56ottNaU+krJLc0l05NJ//T+MU2PEJ2BBIW26N/fdE/9v/87YNamTTdQWPgyJ564E7s9o0N2l1+Zzz+++gcvf/sym0o2NUxPsqcwzH0aycWnUvzlVNYvHU69PwSjXsJ22gMEEne0uL1BGYOYNWgWFw+7mEk9J7WpuFldX82SvCUs2LyAhbkLKfWVMihjEIMyBjE0cyhXjr6SnNSc9h6yECLGJCi0xezZ5pJ869YDZnm961i9ehS9e8+lX7/ftntXb214i2vevoaKugqGuqeTsfsH7P7kDPLqVhPquwgGvAcpptHbqVNJciRT7N/JhB4T+PUpv2Zk15HkluaytWwr1fXVnNH/DAZndtwNdlpr6YIrxHGotUEhvrukRowbB/PmmZGyujd/pENi4gi6dp1Dfv4TZGf/PxyOLm3evN8Pn6zw88Cnc1nmfxRXyUR4+RU2lA4gJQWmTYPLRuUwbNj3GTJE4+6xnc/3fswnOz8hvyqfG8c/wazBsxoy657JPZmeM71DDn1/EhCEiG9SUgDYtMncr/Cb38C99x4wu7p6I198MZxeve6gf//fA+AP+vnz53/mvdz30JhzqFAkO5PJcGeQ6swgf4eTbzZUs2VHNXWZn0GPL0lYfwvT6x7hpElOTjvNDHBlk9AshIgyqT5qq9NPh40bze21LeTSGzb8kKKiN5k8eTur9mzixwt+zPqi9YzOGk2CIwEw3cdKvBXsLiuhWpeAJQgBF06VQLo7nf+d9it+NCXK494KIUQLpPqorW6+GS680IyaeuGFDZODoSDfFX3HuroTWJr3Mg/mTuX9Xbn0SenD25e+zazBZniMr7+GP/4RvnzNVBedfY7mhh+FOOsMa7NunkII0ZlJUIg491zzxIunnoILLySkQ/x7w7/55ZJfNtwJCZDuyOWuE2/lf095CLfNwwcfwB/+AB98YEbNuPFGuPVWGDhQAYcZRlEIIToZCQoRNhv86EfoX/yCBR89zX1b/sbavWsZmjmUv8/6O2O6jaGn28rGryfQrVst69d6uO02WLHCjM3+29+agJAW2y76QgjRLhIUmvho5hDuzVOs+vgm+qf156ULX+KyEZdhtTRe8efa7uG22/rx3nvmkX9PPw1XX41UEQkhjgsSFDCjJ17z9jUs3r6YnllunvlAc/XC1diTG2+dD4Xgr3+Fe+55AJ/Pz9VXv8pjj11KSop04RRCHD86ZqSlY9yPF/6YVfmr+NOZfyL3jHe5/tNa7P94oWH++vUwdSrccgtMnqxYsuQNrrrqB/j982OYaiGE6HhxHxQ+2PoB725+l19O/yW3Tb4N18mnwGmnwV13wSef8PTT5qmdmzfDiy/CokVw0kmX4vEMIzf3DkKhusPvRAghjhFxHRQCoQC3v387/dL68dMTfmomKgWvvw45OfzpzPe46SZzC8OGDfDDH5rZFouNAQMepbZ2KwUFf4ntQQghRAeK66Dw3JrnWLdvHX84/Q/NH9CSns7Dsz7l9prfcHHiIt56vowu+41ukZ5+JunpZ5OX9wA+X95RTbcQQkRL3AaF8tpy7ltyH9P7TOfCIRc2m/eb38DcRzK57LR9vFZ7AY4Lz2nxkZ0DBz4JwMaNV6MP8YQqIYQ4VsRtUHhw+YOU1JTwpzP/1GwQuDffhPvuM0/pfOm9rtj+9U/zbMrx40090gcfmKfUA253DgMGPE5FxTLy8x+L0ZEIIUTHicugUBeo4+kvn+YHI3/A2O5jG6Zv3QrXXguTJsHf/w5WKzBnDuzaBQ8/bLohnXGGeR/WrdvVZGScz7ZtP6e6en0MjkYIITpOXAaFj7Z/hLfey+UjL2+YVlsLl1wCFotpZ3Y0fepeSorpjbR9O5x/PvzqVyZQYIaaHjz4GWy2FDZsuIJQqOXHYAohxLEgLoPC/E3zSbAncErfUxqm3X67aTZ44QXIyTnIik4nPP64qT66++6GyQ5HVwYPfhavdy1bt94V3cQLIUQUxV1Q0Fozf/N8zhxwJi6bC4AFC8zdyj/7GcyadZgN9OljSg2vvgqffNIwOTNzFj17/pSCgsfZu7fjn+cshBBHQ9wFhTV71rC7ajezBjXm/n/6E/TubQa1a5W77zYjqv7kJxAMNkzu3/8PpKRMZ/PmG6iqWtvBKRdCiOiLu6Dw9qa3sSgL5ww6B4DcXFi8GK6/Huz2Vm7E4zHjZX/1FTzzTENvJIvFzvDhr2OzpbN+/YX4/SXg88E335hboZsEECGE6IzibkC8+ZvmM6XXFDI9mQA895zpZXTNNW3c0CWXmGcv/PjHcM89MHw4DBmCIxhkYlE/qgs+IVScjd5dh4o83e655+B//qdjD0gIITpQXJUUdpTv4OvCrxuellZfD//4h3m+Ts+ebdyYUvDWW/CXv8APfmCex7BwISxdir2gEo9nEBWDatl302BCr7xsgsaTTzaUKoQQojOKq5LC/E1mVNNIUJg/H/btgxtuOMINpqebx3i2wAHU5z9Obu5tlGb9lyE/vgl18y3w+edwwglHuEMhhIiu+AoKm+czOGMwgzIGAaY5oFcvOPPM6OwvO/unBAJV5OXdh22ygwGJiai//lWCghCi04qb6qOK2gqW5i3l/MHnA7Btmxmx4rrrwncuR0mfPvfSq9edFFQ+T8nMVPRrr0JJSfR2KIQQ7RA3QeG93PcIhAINVUfPPWfuXr722ujuVylFv34PM3Dgk+w4uwRVV0/5Y9fJAHpCiE4pboLCjJwZPHves0zOngzAa6/BWWdBdnb0962UomfPHzP80g14x6bifP4/fPv12QSDtdHfuRBCtEHcBIWsxCyuG3cdVosVr9cMY3TSSUc3DS5XHxJ+9iTu3aA/WMT69d9veaykUAgKCuDTT03DtBDi2HLjjXDllbFOxRGJalBQSp2llNqklMpVSs1tYf7VSqkipdTa8Ou6aKYnYuNG83fo0KOxt+bUxRdDly4Meb0vFQUL2LDhckKhgJlZUWH6x7pcpggzdapplH7hhUNvVAjReRQWmmGWX38dvN5Yp6bNohYUlFJW4ElgJjAMuEwpNayFRV/XWo8Jv56LVnqa+u4783dYS6mJtvCges6vdjDpF30o2zaPTZuuIbh7B0yfDu+/D7fcYgZjWrgQZswwN8ht2hSDxAoh2uyllyAQMDdCLV0a69S0WTRLCpOAXK31Nq11PfAacH4U99dqGzaYe836949RAi67DP7v/3Cu28Oku7LwLfkX9ScMILR5A6G3/wOPPmqKnzNnwssvm2E1LrnEjO8thOi8tDalhAkTzO/2vfdinaI2i2ZQ6AnsavI5Pzxtfxcrpb5RSs1TSvWKYnoabNgAAwe2YayjaLjoIliwAMcuL+NuBnulZu0f6lmdcQclJQsbl+vRw1QfffMN3HGH+dKtWwcPPWQ+FxV1XJqCQRmfSXQOgUCsU3BkVq409dM33QSnnGLGPDvGxLqh+R0gR2s9CvgAaLHyXCl1g1JqtVJqdVEHZIIbNsSo6mh/p51mRuObNQvrJ1/R65J/A/Dtt+ewadMNBALh+sizzzYB4KmnzNDdI0ea8ZYefxxGjTJVTu3h98PTT5s7+Xr0gFdekeE4ROysXw9ZWWYMmmPNc89BYqIp2Z95phlxc+vW5sv88Y/w4ouxSV9raK2j8gJOBBY1+XwPcM8hlrcCFYfb7vjx43V71NZqbbFo/YtftGszURMM1urc3Lv0kiVKr1zZX5eXrzAz6uq0vvBCrc89V+u//U3r/Hytv/5a62HDtAat77jDHFxbhEJav/aa1v37m21MmaL1xInm/Zlnar11a8cfoBCHc/755juYmKj19u2xTUswqLXP17plKyu1TkjQ+rrrzOfNm81xPPlk4zJbtpgMqGtXrevrOz69hwCs1q3Ju1uz0JG8MENobAP6YoYC+hoYvt8y3Zu8vxBYdbjttjcofPutOeqXX27XZqKurGyZXrGij16yxKJzc+/SgcBBvpjV1VrfdJM5qO9/33yJW8Pvb1xv1Cit333XBIlAQOsnnjA/SLdb688+67iDEuJwVqww38kf/UjrpCStTzml9d/paLj7bq1TU7VeufLwyz77rEl7ZNlQSOu+fbU+77zGZa691iwDWs+fH500H0TMg4JJA2cDm4GtwL3hab8CZoXf/w5YHw4YS4Ahh9tme4PCG2+Yo/7qq3Zt5qjw+yv0xo3X6SVL0J99NlRXVBwig/7DH8yBPfDA4Tfs9ZovKpgvfUs/up07te7Vy5RE2loCEeJIhEJaT5+udVaW1lVVjZnsn/8cm/RUV5uAECm1LF9+8GVDIa0nTza/l1CocfpNN5nSQ12d1nl5WttsZlqXLlpffHH0j6GJThEUovFqb1C4/36tldK6pqZdmzmqSkre0ytWZOslSyx648brtde7/sCFQiGtr7zS/EvnzWt5Q16viYYTJ5oibNNibUsWLjTbu+++9h9EW4RCWn/0kdYVFUd3v61RXt78Ry86znvvme/bX/5iPodCWp91ltYej6l2aas///ngv4XWeOGFxmqFwYNNOj76yASL99/Xeu5crWfONIEgMdEs+8c/Nt/G22+b6R99ZIKB3a71rl1a33abeV9cfOTpayMJCgcxZ44p0R1r/P5yvWnTzXrZMpdesgS9du3purj4vzrUNIPy+czVisej9bJlpkpo7lxz9dWtm24otrrdWv/nP63b8Q9/aK5u1q5teX4opPXixVpv2tTuY9Ram1LJVVeZdPboofWbbx55JlxT07H1tsuWae10muAbCHTcdo+Ez9f6c/755y1fBa1apfX48Vr/7netrzePlmBQ67FjzY+zrq5x+q5dWqekaH3SSW37X/71r43f9wcfPPR3KD9f60WLDpw+darWAweadffu1Xr4cK0dDpOZg/ldjBmj9QUXaP3Tn2r9+OMmYDRVWWmWu+IKs+4NN5jpa9c2D4BHgQSFgxg1Suuzz27XJmKqrq5I5+U9qD/9tIdesgT99dczdU1NbuMCu3dr3bNn4w/CZtN60iStr7lG69/8RutXX21b411xsWkUGz/etEM09cUXWs+YYfbjcmn91FPtu4ouKtL65JPN9n7yE61Hjzbvzz3XFL1b8umnpsSzfLm5ivf7TQnnBz8wwW/cOK0LC488TRHbtmmdmal1WppJ0zXXNK9227dP68ce03rPnsNvy+czpbamSku1fuQRrQcNMhnNww9rvWNHy+sXF2t9wgkmHXfe2TwT3d9vf6sb2o1ym3xPPvrIVGskJ5v5ffpo/cor5n/w7rta33uv1pddpvWCBYf/n65dazLW9nj+eZOOl146cN6rr+qGzhRNVVVpffrp5jvY9Du9eLHWVqv5oV9xhVn3xhsPDOShkNYvvmiCDjS/UFq/3kz7/e8bpxUVmUbku+7S+r//NftvjchvxGo136OI0aNNqb0lfr/pBHLDDaZXzFNPmVJHOzp/SFBoQSBgLvR+9rMj3kSnEQzW6Z07H9XLlyfqpUudevv2+3UgEL5KWbfOZCpLlx545XIkIg0xM2dqfcst5kcxe7aZlpmp9aOPmt5KYK6aWioS5+dr/atfmcbwhx82DYp1deaHtWaNyZD69TP/oFdfNev4/aY47vGYzHjJkubbfOkl80OLBEAwGR2Y5a+80gSGgQNbHwjr601vhKZVV5WVWo8YYeqXN23S+pe/NPu44QaTuT/4oGkUjWSu33134Harq01VxiWXmONRyqTr4otNycjjMetPnWpKe5HjmTbNNEhGMuYdO7QeMsQE4YsuMstMmNBy9cojj+iGnmRpaSbzmz9f63feMed5+HBzEbF4sQlETc+j1ap1erp5P3Fiy8Ghqkrrm29uvCj4xS9an1FGBINa//rX5nxMnnzwElhkP2++2Xg+Z8ww1aBJSebYXnvNnIe0NFOlU1Fh0jx3rln39NNN5rp4sfkfRc7flCkmaHbp0ngB8f/+nykRdMQFxUMPmf1cdVXz6Y8+aqavb1IdXFZmAlGvXmZecrI5xsj/5a67jjgZEhRasGWLOeK///2IN9Hp1Nbm63Xr5uglS9Aff5yqN2++VXu96zp2J6GQKR5nZ5uMwuk0me/Pf96YeQaD5ktut5sf5emnm6v9J54wXQwjmXfv3o1f8EgxPPLq1q3lXh5bt5ofud2u9T//aaY99ZTJSL73PdP1b8ECc1V8441av/VW49Xzp5+azLxHDxMsW/LddyYTOOkkk7lFMrk5c0wGet55Jv0ffNB4Pu65Rzc0QILWs2Zp/X//Z0pVaWlaf/yxWXbDBq1//OPG5bp2NZ8feMAEyIEDzbxrr23e+2HrVhNscnJ0w5X+k0+aUmBKSmOj55tvmuNLTDTHsGiRKYk88YRZb/ZsE1y3bTOlpkiGP2FC8+AdCJi684ceMhcTXq85h88+25iG/v1N2t9+25TGcnLM/+DWW02pIvI/fOqp1mWmZWWmFAhaX375oS9gamtNcEpONkH79NPNvv/1L3NskUCammq+o01LRVqbcxcJ3JGXw2EuUAIBs02Hw3xXfT6zjdmzD38MrbF9uwk8+1/lFxaakvxdd5kS5l13NX5PTjnFBPBg0Pz/CgpMNaCUFDo2KMyfb454xYoj3kSnVV7+qV6//gd66VKHXrIEvWbNVF1Y+IYOBv2HX/lIHKxKYc0ac4U+fnzjVXuXLuYLH7maLSzU+t//NtMefNBkpt98c+heTmVlWp96qtneWWeZv+ed17q68G++0bp7d5OZ3nefqR/W2mR8c+eaH6bLZX64t99uqhRuvlnrjIzGDGT/ut9QyJQYTj21ea+UrVtNFZDT2Vht4HCYq8TFi9veFuH3m/QMHWq21b27uT+lqR07TGbmdDYGtEiprWk9vM9nMvULLmhbI359vaneOffcxv8pmID2ySeNy61aZQJrZP64ceb8PvigCVhXXGGqdE4+2bQfZGSYc//nP7eu2jEvzwTcyMXEP/7RPI333mu2uX+JMiIUMm0UH35o7vVZv1+HjUjJatYs8/f991t/jo7UeeeZQOBymRLBZZdFrWukBIUWPPywOeKysiPeRKdXV1ekd+58RK9c2U8vWYJesaKX3rHjIe31btCh0FHu7x0MmmqjQ9V5t0V9vanTBfPjaUvD47Ztjd1wHQ6TQfXpYz5ffXXLV7b19eZK4sUX25bO4mKT8XXvbqpGOqIKIhg0vXN27Tr4MtXV5gr+Jz8xXY076rw3VVtrMt3nn2/5yj4UMm1Nv/mNOQeREmJiomlEHjfOdHw491xTOvj007btf8ECc8X/t7+1PL89bVrBYGMg79v36NwfsWiRqeK87jpT4o2i1gYFZZY9dkyYMEGvXr36iNa9+mozIsTu3R2bps5I6yAlJQvIz3+c8vKPALDZ0khOnkxq6vfo1u2HOBxZMU7lEdDaDIMwbJh5dF5bbd4Mjz0G//ynGRHxqafg5JM7PJlEfldKdfy2jyU1Neb/5HJ13DaDweg9Q3fHDjjxRLjvPjN+0XFEKfWl1nrCYZeLp6BwwglmWJLFizs4UZ1cTU0uFRXLqaxcSUXFCmpqvkMpGxkZ59Ojxw2kpZ2KGek8jvj9ZqjceM+0xYGiGXRiqLVBwXY0EtMZaG0Gwrvqqlin5OjzeAbg8Qyge3fzQOrq6g3s2fMce/e+QHHxm9jtXcjMvIAuXS4mNfV7WCyxHD72KInpELmiUzsOA0JbxHqU1KNm926oqorN09Y6m4SEoQwY8EdOOqmAYcPeIDX1FAoLX+Gbb85i1ao+7Nr1KMFgdayTKYSIgbgJCpGnrUlQaGSxOOnadTbDh7/OlClFjBjxHzyeIWzdegcrV/YhL+/X1NbmxzqZQoijKG6Cgt1unmzZKZ6j0AlZrW4yM89nzJiPGDt2BSkpJ5KX90tWrerFV19No6DgKerq9sY6mUKIKIurhmbRNjU1uezb9xr79r1KTY0paiUlTSAj41zS02eSmDg2PtofhDgOSO8j0WG01lRXr6OkZD4lJe9SWfkZoFHKSWLiaJKSJpCSMpW0tNNwOLrEOrlCiBZIUBBRU19fRHn5R1RVrQ6/viQYrAIgMXEMqakzcLsH4nLl4HLl4HYPlBKFEDEmXVJF1DgcXejadQ5du84BzI1yVVVrKCv7gLKyD9i9+2lCodqG5a3WJFJTp5OaeippaaeSkDAcpeKmOUuIY4oEBdFuSllJTp5IcvJE+vT5OVqHqK8vpLY2D59vK5WVn1JW9iElJe8CYLNlhIPEDFJTZ0iQEKITkaAgOpxSFpzO7jid3UlJOZFu3a4AoLZ2B2VlH1FevoyKimUUF/8baAwSKSlTSUgYSWLiSOz2rii521iIo06CgjhqXK4+dO9+Dd27XwOAz5dHRcUyysuXUl6+tCFIgAkUDkc37PY0bLY0XK4+JCefSHLyibhcORIwhIgSaWgWnUZ9/T6qq9dRXf0t1dXf4fcXEQiU4feX4vNtJRQyd1nb7Vl4PINxu/vjcvXD5crB6eyJ09kDh6MnNltijI9EiM5HGprFMcfh6IrD8T3S0r53wLxQKEB19ToqK1dQWfk5Pl8upaX/pb7+wBvqLBYPDkcWDkc3EhPHkJl5Eamp06UHlBCtICUFcUwLBqupq8unrm43dXUF1NcXUF9fGH7tobLyM0KhGmy2NDIyziU1dQYpKVNxuwdKFZSIK1JSEHHBak3A4xmMxzO4xfnBYA2lpe9TXPxvSkoWUFj4EgB2exfs9kxCoVpCIR9aB7Fak7Bak7DZknG7B5CYOI6kpLEkJIyWKikRN6SkIOKG1iFqajZRUfEJlZUrCAarsVhcWCwuwEIw6CUYrCIQKKemZgN+f1F4TQuJiaNJTj6JlJSTcDqzsVqTsdlSwoHEg8Xikm61olOTO5qFaAetNfX1u6mqWkNV1RdUVKygsnJVQ2N3SywWN05nz3DDdx+czuyGtg2Hoxtudz/paitiRqqPhGgHpVS4R1NPMjPPA0xjd03Nd9TX7yMYrCQQqCAYrCIU8hEM+ggGvdTV5VNbm4fX+y5+f+EB27Vak3G7B+J0dsdiScBq9WCzpZKYOJbk5BNxu/tL0BAxJUFBiFayWGwkJo5q9fKhkB+/v6ih0dvny8Xn20JNzRbq6goIBmsIharx+0sIhXyAaetwufpitSZitSZisbgBDWi0DuFy9SEpaTxJSePDjeVSZSU6lgQFIaLEYrHjdPbA6ewBjD3ocloHqa5eT2XlSiorV1FXt4dg0NskWKhw5q8pLV3QMK6UUnas1gQsFg9Wqwe7PTN8w18WNlsKWvsJherROoDDkdUwQKHD0a2hLcVicWOzpUjpRDSQoCBEjCllJTFxFImJo+jR40eHXDYU8lNT8x1VVV/i820JlzZqCAar8fuL8fm2UlHxKYFAJRaLA6UcKGXB7y/GlDha2r8jfONfD5zO7Ibg4XT2wmp1o5QdpRxYre5ww3oiNlsKFoszCmdDxJoEBSGOIRaLncTE0SQmjm7TeqFQfUN7R339PrSuIxSqJRisCVdvmfs8vN6vKC7+D1rXH2aLCpcrB49nKB7PUJSy4fcX4/cXEQxWY7enh7v9dsHl6oPbPQC3ewAOR7fDlkpM5xctVWMxIkFBiDhgsThwu/vhdvc77LJmlNs91NXlEwrVhaug/OEG9SoCgSoCgRJqajZSXf0dZWUfAhq7PRO7vQtWawJebz5+fzGBQCnNSygWoHlQMEFCEWk3gVA4zQnYbKnYbKnh9hUHFosTqzWRlJRpZGSch8cz4KDHEQzWEAhUoHUArYNYLC6czm5tPHPxR4KCEKIZM8qt6XnVGiYjVy2WAEKhAHV1O/H5tuDz5VJXt2f/tWkaNJSyEgkcwaCXQKCcQKCcYNCL1nUEg9XU1u6kuPg/bN16Ox7PEBITx4SruOxoHaS2djs+3xbq6/ffF3g8w8jMnEVGxrlYLG58vm3U1m4jECgLV5/1wuXqhc2Wgc2Wgs2WHE7Tkamr20NFxSf4fJvJzLyIhISh+52fempqNuPxDMJicRzxfjqS3KcghDjm+HzbKSl5l5KSd6mt3SBFnKsAAAe2SURBVB4uyfgBwtVaA3G7B2C3Z6KUDbASCJRQUrKQiorlaB1otj2lbAdMi7BY3FgsTpRyYrE40DoQvhO+FovFQ0LCMDyeYXg8A8PdkvdQX7+X6upvqa3d1mxbGRnn07v3XVgsLvbu/SeFha8QCJRgsXhISZlCauqMcM+ywbhcvVHKgtaaQKCcuroCbLZkXK7eR3TO5OY1IYRogd9fTnn5Yky7iKlSs1qT8PuLqK3dSV1dPoFAafg+lEqCQW+4Gq0OretRyo7F4sRicREIVFBTs4Hq6vUEAmWAaugF5nYPICVlKikpU3E6s9m9+28UFPwlXKUGSjnJzDyf9PQz8Hq/prx8KdXV3zak02JxYbdn4ffva+iy3Lv3XPr1+90RHbcEBSGEOEoiV/Om7ePgo/EGAl727XsZUHTpMhu7Pa3ZfL+/hOrqddTUbKKmZjP19XtxOLIaqvMSEkaTkDDkiNIodzQLIcRRopQ6IINvic2WeMhux3Z75FG10zsyeW0ifb6EEEI0iGpQUEqdpZTapJTKVUrNbWG+Uyn1enj+Z0qpnGimRwghxKFFLSgo04/rSWAmMAy4TCk1bL/F/gco01oPAP4EPByt9AghhDi8aJYUJgG5Wutt2twe+Rpw/n7LnA+8EH4/DzhVySAsQggRM9EMCj2BXU0+54entbiMNp2EK4CMKKZJCCHEIRwTDc1KqRuUUquVUquLiooOv4IQQogjEs2gUAD0avI5OzytxWWUue0wBSjZf0Na62e01hO01hO6dOkSpeQKIYSIZlD4AhiolOqrlHIAlwLz91tmPnBV+P33gY/0sXY3nRBCHEeiekezUups4DHACjyvtX5QKfUrYLXWer5SygW8hHkCSSlwqdZ628G3CEqpImDHESYpEyg+wnWPZ3JeDiTn5EByTg50LJ2TPlrrw1a1HHPDXLSHUmp1a27zjjdyXg4k5+RAck4OdDyek2OioVkIIcTRIUFBCCFEg3gLCs/EOgGdlJyXA8k5OZCckwMdd+ckrtoUhBBCHFq8lRSEEEIcQtwEhcON2BoPlFK9lFJLlFLfKaXWK6V+Gp6erpT6QCm1Jfz38APDH2eUUlal1FdKqXfDn/uGR+7NDY/k2zkeoHuUKKVSlVLzlFIblVIblFInxvv3RCn1/8K/m3VKqVeVUq7j8XsSF0GhlSO2xoMAcIfWehgwGbg5fB7mAou11gOBxeHP8eanwIYmnx8G/hQewbcMM6JvPHkceE9rPQQYjTk3cfs9UUr1BH4CTNBaj8Dce3Upx+H3JC6CAq0bsfW4p7Xeo7VeE35fhfmh96T5aLUvABfEJoWxoZTKBs4Bngt/VsD3MCP3QpydE6VUCjAN+DuA1rpea11OnH9PME+qdIeH5PEAezgOvyfxEhRaM2JrXAk/0Ggs8BmQpbXeE561F8iKUbJi5THgLiAU/pwB/7+9+wmxqgzjOP79hRXpBBYkWFFqgURQo0FEGki2aCHhoj+QSgTt2rgIwigi12KrUKEIw1n0b6RtZDHkoizTCmxXURPUCJZhUIj9XLzvPd1GYYaBuedyz++zu+ecObzn8px5znnOPc/LH7VzL3QvXlYDp4E3a0ntdUnL6HCc2P4F2AP8REkGZ4HjjGCcdCUpRB9JY8D7wE7bf/avq72nOvOTNElbgBnbx9seyxBZAqwH9tleB/zFrFJRB+PkOsqd0mrgRmAZ8HCrg1okXUkK8+nY2gmSrqQkhAnbk3Xxb5JW1vUrgZm2xteCDcAjkn6klBUfpNTTl9cyAXQvXqaBaduf18/vUZJEl+PkIeAH26dtnwcmKbEzcnHSlaQwn46tI6/Wyt8AvrO9t29Vf7fap4APBj22ttjeZftm26socfGx7W3AJ5TOvdC97+RX4GdJa+uizcApOhwnlLLRfZKW1vOo952MXJx05uW1y3VsbXlIAydpI/Ap8C3/1c9foDxXeAe4hdKB9nHbZ1oZZIskbQKes71F0hrKncP1wAlgu+1/2hzfIEkapzx4vwr4HniachHZ2TiR9ArwBOVXfCeAZyjPEEYqTjqTFCIiYm5dKR9FRMQ8JClEREQjSSEiIhpJChER0UhSiIiIRpJCxABJ2tTrxBoxjJIUIiKikaQQcRmStks6JumkpAN1voVzkl6tPfWPSLqhbjsu6TNJ30g63JtnQNLtkj6S9LWkryTdVnc/1jdXwUR9QzZiKCQpRMwi6Q7Km6sbbI8DF4BtlCZoX9q+E5gCXq5/8hbwvO27KG+L95ZPAK/Zvhu4n9JdE0p32p2UuT3WUHroRAyFJXNvEtE5m4F7gC/qRfw1lOZv/wJv120OAZN17oHltqfq8oPAu5KuBW6yfRjA9t8AdX/HbE/XzyeBVcDRxT+siLklKURcSsBB27v+t1B6adZ2C+0R098b5wI5D2OIpHwUcakjwKOSVkAzh/WtlPOl1xHzSeCo7bPA75IeqMt3AFN1ZrtpSVvrPq6WtHSgRxGxALlCiZjF9ilJLwIfSroCOA88S5ls5t66boby3AFKy+T99Z9+r6MolARxQNLuuo/HBngYEQuSLqkR8yTpnO2xtscRsZhSPoqIiEbuFCIiopE7hYiIaCQpREREI0khIiIaSQoREdFIUoiIiEaSQkRENC4CYp5S0sYID9IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 982us/sample - loss: 0.4655 - acc: 0.8719\n",
      "Loss: 0.46552579213525647 Accuracy: 0.8718588\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3912 - acc: 0.2755\n",
      "Epoch 00001: val_loss improved from inf to 1.46203, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/001-1.4620.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.3912 - acc: 0.2755 - val_loss: 1.4620 - val_acc: 0.5611\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4897 - acc: 0.5234\n",
      "Epoch 00002: val_loss improved from 1.46203 to 1.02697, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/002-1.0270.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4897 - acc: 0.5234 - val_loss: 1.0270 - val_acc: 0.6923\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1431 - acc: 0.6394\n",
      "Epoch 00003: val_loss improved from 1.02697 to 0.76438, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/003-0.7644.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1430 - acc: 0.6395 - val_loss: 0.7644 - val_acc: 0.7841\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7105\n",
      "Epoch 00004: val_loss improved from 0.76438 to 0.66197, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/004-0.6620.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9362 - acc: 0.7105 - val_loss: 0.6620 - val_acc: 0.8092\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7920 - acc: 0.7593\n",
      "Epoch 00005: val_loss improved from 0.66197 to 0.60557, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/005-0.6056.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7920 - acc: 0.7593 - val_loss: 0.6056 - val_acc: 0.8304\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7880\n",
      "Epoch 00006: val_loss did not improve from 0.60557\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6955 - acc: 0.7880 - val_loss: 0.6244 - val_acc: 0.8157\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.8156\n",
      "Epoch 00007: val_loss did not improve from 0.60557\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6148 - acc: 0.8156 - val_loss: 0.7010 - val_acc: 0.8006\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.8336\n",
      "Epoch 00008: val_loss improved from 0.60557 to 0.47601, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/008-0.4760.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5574 - acc: 0.8336 - val_loss: 0.4760 - val_acc: 0.8677\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8481\n",
      "Epoch 00009: val_loss improved from 0.47601 to 0.43149, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/009-0.4315.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.5029 - acc: 0.8481 - val_loss: 0.4315 - val_acc: 0.8805\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8599\n",
      "Epoch 00010: val_loss improved from 0.43149 to 0.37914, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/010-0.3791.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4633 - acc: 0.8599 - val_loss: 0.3791 - val_acc: 0.8994\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8727\n",
      "Epoch 00011: val_loss improved from 0.37914 to 0.36604, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/011-0.3660.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4245 - acc: 0.8727 - val_loss: 0.3660 - val_acc: 0.8968\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8810\n",
      "Epoch 00012: val_loss improved from 0.36604 to 0.34404, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/012-0.3440.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3972 - acc: 0.8810 - val_loss: 0.3440 - val_acc: 0.9061\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8877\n",
      "Epoch 00013: val_loss did not improve from 0.34404\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3726 - acc: 0.8876 - val_loss: 0.3804 - val_acc: 0.8926\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8945\n",
      "Epoch 00014: val_loss improved from 0.34404 to 0.32955, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/014-0.3295.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3487 - acc: 0.8945 - val_loss: 0.3295 - val_acc: 0.9094\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8986\n",
      "Epoch 00015: val_loss improved from 0.32955 to 0.28696, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/015-0.2870.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3284 - acc: 0.8986 - val_loss: 0.2870 - val_acc: 0.9227\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9071\n",
      "Epoch 00016: val_loss did not improve from 0.28696\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3091 - acc: 0.9071 - val_loss: 0.3221 - val_acc: 0.9099\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9085\n",
      "Epoch 00017: val_loss improved from 0.28696 to 0.28604, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/017-0.2860.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2962 - acc: 0.9085 - val_loss: 0.2860 - val_acc: 0.9227\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9165\n",
      "Epoch 00018: val_loss improved from 0.28604 to 0.28373, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/018-0.2837.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2773 - acc: 0.9166 - val_loss: 0.2837 - val_acc: 0.9224\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9176\n",
      "Epoch 00019: val_loss improved from 0.28373 to 0.27598, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/019-0.2760.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2702 - acc: 0.9176 - val_loss: 0.2760 - val_acc: 0.9196\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9219\n",
      "Epoch 00020: val_loss did not improve from 0.27598\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2526 - acc: 0.9219 - val_loss: 0.2874 - val_acc: 0.9210\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9268\n",
      "Epoch 00021: val_loss improved from 0.27598 to 0.25917, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/021-0.2592.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2415 - acc: 0.9268 - val_loss: 0.2592 - val_acc: 0.9276\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9270\n",
      "Epoch 00022: val_loss improved from 0.25917 to 0.25789, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/022-0.2579.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2332 - acc: 0.9270 - val_loss: 0.2579 - val_acc: 0.9257\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9314\n",
      "Epoch 00023: val_loss did not improve from 0.25789\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2219 - acc: 0.9314 - val_loss: 0.2805 - val_acc: 0.9299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9330\n",
      "Epoch 00024: val_loss did not improve from 0.25789\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2134 - acc: 0.9330 - val_loss: 0.2928 - val_acc: 0.9206\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9351\n",
      "Epoch 00025: val_loss improved from 0.25789 to 0.24819, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/025-0.2482.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2063 - acc: 0.9351 - val_loss: 0.2482 - val_acc: 0.9324\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9366\n",
      "Epoch 00026: val_loss did not improve from 0.24819\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2027 - acc: 0.9366 - val_loss: 0.2735 - val_acc: 0.9313\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9394\n",
      "Epoch 00027: val_loss did not improve from 0.24819\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1948 - acc: 0.9394 - val_loss: 0.2553 - val_acc: 0.9304\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9413\n",
      "Epoch 00028: val_loss did not improve from 0.24819\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1877 - acc: 0.9413 - val_loss: 0.2617 - val_acc: 0.9317\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9436\n",
      "Epoch 00029: val_loss improved from 0.24819 to 0.24402, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/029-0.2440.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1813 - acc: 0.9435 - val_loss: 0.2440 - val_acc: 0.9355\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9446\n",
      "Epoch 00030: val_loss did not improve from 0.24402\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1759 - acc: 0.9446 - val_loss: 0.2677 - val_acc: 0.9262\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9492\n",
      "Epoch 00031: val_loss did not improve from 0.24402\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1616 - acc: 0.9492 - val_loss: 0.2511 - val_acc: 0.9315\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9484\n",
      "Epoch 00032: val_loss improved from 0.24402 to 0.23118, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/032-0.2312.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1605 - acc: 0.9484 - val_loss: 0.2312 - val_acc: 0.9366\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9513\n",
      "Epoch 00033: val_loss did not improve from 0.23118\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1567 - acc: 0.9512 - val_loss: 0.2694 - val_acc: 0.9304\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9527\n",
      "Epoch 00034: val_loss improved from 0.23118 to 0.22699, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/034-0.2270.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1530 - acc: 0.9526 - val_loss: 0.2270 - val_acc: 0.9378\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9517\n",
      "Epoch 00035: val_loss did not improve from 0.22699\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1523 - acc: 0.9517 - val_loss: 0.2299 - val_acc: 0.9383\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9549\n",
      "Epoch 00036: val_loss did not improve from 0.22699\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1449 - acc: 0.9549 - val_loss: 0.2318 - val_acc: 0.9355\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9563\n",
      "Epoch 00037: val_loss did not improve from 0.22699\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1378 - acc: 0.9563 - val_loss: 0.2288 - val_acc: 0.9408\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9586\n",
      "Epoch 00038: val_loss did not improve from 0.22699\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1304 - acc: 0.9586 - val_loss: 0.2622 - val_acc: 0.9362\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9580\n",
      "Epoch 00039: val_loss improved from 0.22699 to 0.21627, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/039-0.2163.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1329 - acc: 0.9580 - val_loss: 0.2163 - val_acc: 0.9450\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9608\n",
      "Epoch 00040: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1256 - acc: 0.9608 - val_loss: 0.2219 - val_acc: 0.9397\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9626\n",
      "Epoch 00041: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1200 - acc: 0.9626 - val_loss: 0.2527 - val_acc: 0.9364\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9618\n",
      "Epoch 00042: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1195 - acc: 0.9619 - val_loss: 0.2177 - val_acc: 0.9467\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9625\n",
      "Epoch 00043: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1182 - acc: 0.9625 - val_loss: 0.2194 - val_acc: 0.9448\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9644\n",
      "Epoch 00044: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1114 - acc: 0.9644 - val_loss: 0.2692 - val_acc: 0.9324\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9672\n",
      "Epoch 00045: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1071 - acc: 0.9672 - val_loss: 0.2219 - val_acc: 0.9425\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9669\n",
      "Epoch 00046: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1035 - acc: 0.9669 - val_loss: 0.2328 - val_acc: 0.9383\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9661\n",
      "Epoch 00047: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1064 - acc: 0.9660 - val_loss: 0.2632 - val_acc: 0.9371\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9681\n",
      "Epoch 00048: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1001 - acc: 0.9681 - val_loss: 0.2243 - val_acc: 0.9383\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9670\n",
      "Epoch 00049: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1050 - acc: 0.9670 - val_loss: 0.2170 - val_acc: 0.9455\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9711\n",
      "Epoch 00050: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0938 - acc: 0.9711 - val_loss: 0.2552 - val_acc: 0.9422\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9686\n",
      "Epoch 00051: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0982 - acc: 0.9685 - val_loss: 0.2236 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9686\n",
      "Epoch 00052: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0981 - acc: 0.9686 - val_loss: 0.2200 - val_acc: 0.9436\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9720\n",
      "Epoch 00053: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0888 - acc: 0.9720 - val_loss: 0.2202 - val_acc: 0.9443\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9730\n",
      "Epoch 00054: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0841 - acc: 0.9730 - val_loss: 0.2450 - val_acc: 0.9418\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9734\n",
      "Epoch 00055: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0839 - acc: 0.9734 - val_loss: 0.2374 - val_acc: 0.9474\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9751\n",
      "Epoch 00056: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0803 - acc: 0.9751 - val_loss: 0.2208 - val_acc: 0.9432\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9739\n",
      "Epoch 00057: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0826 - acc: 0.9739 - val_loss: 0.2617 - val_acc: 0.9352\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9720\n",
      "Epoch 00058: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0866 - acc: 0.9720 - val_loss: 0.2168 - val_acc: 0.9467\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9756\n",
      "Epoch 00059: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0769 - acc: 0.9755 - val_loss: 0.2565 - val_acc: 0.9432\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9762\n",
      "Epoch 00060: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0761 - acc: 0.9763 - val_loss: 0.2385 - val_acc: 0.9422\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9770\n",
      "Epoch 00061: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0726 - acc: 0.9770 - val_loss: 0.2931 - val_acc: 0.9294\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9768\n",
      "Epoch 00062: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0719 - acc: 0.9768 - val_loss: 0.2589 - val_acc: 0.9352\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9780\n",
      "Epoch 00063: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0693 - acc: 0.9780 - val_loss: 0.2274 - val_acc: 0.9448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9788\n",
      "Epoch 00064: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0662 - acc: 0.9788 - val_loss: 0.2476 - val_acc: 0.9397\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9787\n",
      "Epoch 00065: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0685 - acc: 0.9787 - val_loss: 0.2465 - val_acc: 0.9439\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9779\n",
      "Epoch 00066: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0703 - acc: 0.9779 - val_loss: 0.2468 - val_acc: 0.9453\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9789\n",
      "Epoch 00067: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0684 - acc: 0.9789 - val_loss: 0.2322 - val_acc: 0.9441\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9805\n",
      "Epoch 00068: val_loss improved from 0.21627 to 0.21312, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_7_conv_checkpoint/068-0.2131.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0612 - acc: 0.9805 - val_loss: 0.2131 - val_acc: 0.9518\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9792\n",
      "Epoch 00069: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0639 - acc: 0.9791 - val_loss: 0.2641 - val_acc: 0.9439\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9799\n",
      "Epoch 00070: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0627 - acc: 0.9799 - val_loss: 0.2394 - val_acc: 0.9441\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9785\n",
      "Epoch 00071: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0676 - acc: 0.9784 - val_loss: 0.2503 - val_acc: 0.9413\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9805\n",
      "Epoch 00072: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0596 - acc: 0.9805 - val_loss: 0.2414 - val_acc: 0.9513\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9833\n",
      "Epoch 00073: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0540 - acc: 0.9833 - val_loss: 0.2952 - val_acc: 0.9376\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9808\n",
      "Epoch 00074: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0595 - acc: 0.9808 - val_loss: 0.2636 - val_acc: 0.9357\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9828\n",
      "Epoch 00075: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0555 - acc: 0.9828 - val_loss: 0.2657 - val_acc: 0.9359\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9828\n",
      "Epoch 00076: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0541 - acc: 0.9828 - val_loss: 0.2436 - val_acc: 0.9460\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9821\n",
      "Epoch 00077: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0573 - acc: 0.9820 - val_loss: 0.2749 - val_acc: 0.9399\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9777\n",
      "Epoch 00078: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0683 - acc: 0.9777 - val_loss: 0.2179 - val_acc: 0.9502\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9852\n",
      "Epoch 00079: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0484 - acc: 0.9852 - val_loss: 0.2305 - val_acc: 0.9448\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9854\n",
      "Epoch 00080: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0474 - acc: 0.9854 - val_loss: 0.2522 - val_acc: 0.9439\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9836\n",
      "Epoch 00081: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0514 - acc: 0.9836 - val_loss: 0.2788 - val_acc: 0.9385\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9849\n",
      "Epoch 00082: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0484 - acc: 0.9849 - val_loss: 0.2411 - val_acc: 0.9462\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9840\n",
      "Epoch 00083: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0516 - acc: 0.9840 - val_loss: 0.2361 - val_acc: 0.9453\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9862\n",
      "Epoch 00084: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0451 - acc: 0.9861 - val_loss: 0.2367 - val_acc: 0.9485\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9833\n",
      "Epoch 00085: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0516 - acc: 0.9833 - val_loss: 0.2580 - val_acc: 0.9464\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9843\n",
      "Epoch 00086: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0485 - acc: 0.9843 - val_loss: 0.2238 - val_acc: 0.9481\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9880\n",
      "Epoch 00087: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0407 - acc: 0.9880 - val_loss: 0.2325 - val_acc: 0.9474\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9855\n",
      "Epoch 00088: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0456 - acc: 0.9854 - val_loss: 0.3029 - val_acc: 0.9376\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9844\n",
      "Epoch 00089: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0505 - acc: 0.9844 - val_loss: 0.2609 - val_acc: 0.9427\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9861\n",
      "Epoch 00090: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0431 - acc: 0.9861 - val_loss: 0.2595 - val_acc: 0.9462\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9878\n",
      "Epoch 00091: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0394 - acc: 0.9878 - val_loss: 0.3229 - val_acc: 0.9338\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9871\n",
      "Epoch 00092: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0419 - acc: 0.9871 - val_loss: 0.2429 - val_acc: 0.9490\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9877\n",
      "Epoch 00093: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0401 - acc: 0.9877 - val_loss: 0.2750 - val_acc: 0.9399\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9866\n",
      "Epoch 00094: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0416 - acc: 0.9866 - val_loss: 0.2153 - val_acc: 0.9509\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9869\n",
      "Epoch 00095: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0420 - acc: 0.9869 - val_loss: 0.2516 - val_acc: 0.9408\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9867\n",
      "Epoch 00096: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0395 - acc: 0.9867 - val_loss: 0.2797 - val_acc: 0.9408\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9870\n",
      "Epoch 00097: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0414 - acc: 0.9869 - val_loss: 0.3122 - val_acc: 0.9469\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9848\n",
      "Epoch 00098: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0491 - acc: 0.9848 - val_loss: 0.2245 - val_acc: 0.9474\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9894\n",
      "Epoch 00099: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0336 - acc: 0.9893 - val_loss: 0.2616 - val_acc: 0.9481\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9873\n",
      "Epoch 00100: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0403 - acc: 0.9873 - val_loss: 0.2628 - val_acc: 0.9425\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9894\n",
      "Epoch 00101: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0336 - acc: 0.9894 - val_loss: 0.2330 - val_acc: 0.9509\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9882\n",
      "Epoch 00102: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0360 - acc: 0.9882 - val_loss: 0.2987 - val_acc: 0.9406\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9882\n",
      "Epoch 00103: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0365 - acc: 0.9881 - val_loss: 0.2512 - val_acc: 0.9511\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9866\n",
      "Epoch 00104: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0423 - acc: 0.9866 - val_loss: 0.2474 - val_acc: 0.9478\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9902\n",
      "Epoch 00105: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0327 - acc: 0.9902 - val_loss: 0.2656 - val_acc: 0.9467\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9880\n",
      "Epoch 00106: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0373 - acc: 0.9880 - val_loss: 0.3054 - val_acc: 0.9322\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9908\n",
      "Epoch 00107: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0321 - acc: 0.9908 - val_loss: 0.2645 - val_acc: 0.9471\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9900\n",
      "Epoch 00108: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0325 - acc: 0.9900 - val_loss: 0.2775 - val_acc: 0.9413\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9864\n",
      "Epoch 00109: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0428 - acc: 0.9864 - val_loss: 0.2566 - val_acc: 0.9492\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9899\n",
      "Epoch 00110: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0321 - acc: 0.9899 - val_loss: 0.2675 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9910\n",
      "Epoch 00111: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0293 - acc: 0.9910 - val_loss: 0.2292 - val_acc: 0.9529\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9913\n",
      "Epoch 00112: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0287 - acc: 0.9913 - val_loss: 0.2506 - val_acc: 0.9481\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9874\n",
      "Epoch 00113: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0403 - acc: 0.9874 - val_loss: 0.2549 - val_acc: 0.9499\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9898\n",
      "Epoch 00114: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0315 - acc: 0.9898 - val_loss: 0.2671 - val_acc: 0.9474\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9913\n",
      "Epoch 00115: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0274 - acc: 0.9913 - val_loss: 0.2673 - val_acc: 0.9450\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9894\n",
      "Epoch 00116: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0342 - acc: 0.9894 - val_loss: 0.2301 - val_acc: 0.9513\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9907\n",
      "Epoch 00117: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.2826 - val_acc: 0.9425\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9908\n",
      "Epoch 00118: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0292 - acc: 0.9908 - val_loss: 0.2725 - val_acc: 0.9450\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 998us/sample - loss: 0.2775 - acc: 0.9259\n",
      "Loss: 0.277451082336816 Accuracy: 0.9258567\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4235 - acc: 0.2815\n",
      "Epoch 00001: val_loss improved from inf to 1.36629, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/001-1.3663.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 2.4235 - acc: 0.2815 - val_loss: 1.3663 - val_acc: 0.5986\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4048 - acc: 0.5509\n",
      "Epoch 00002: val_loss improved from 1.36629 to 0.87731, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/002-0.8773.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.4048 - acc: 0.5509 - val_loss: 0.8773 - val_acc: 0.7417\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9892 - acc: 0.6914\n",
      "Epoch 00003: val_loss improved from 0.87731 to 0.59079, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/003-0.5908.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.9892 - acc: 0.6914 - val_loss: 0.5908 - val_acc: 0.8360\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.7677\n",
      "Epoch 00004: val_loss improved from 0.59079 to 0.46959, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/004-0.4696.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7626 - acc: 0.7677 - val_loss: 0.4696 - val_acc: 0.8691\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.8123\n",
      "Epoch 00005: val_loss improved from 0.46959 to 0.41563, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/005-0.4156.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.6148 - acc: 0.8123 - val_loss: 0.4156 - val_acc: 0.8807\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8420\n",
      "Epoch 00006: val_loss improved from 0.41563 to 0.36878, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/006-0.3688.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.5204 - acc: 0.8420 - val_loss: 0.3688 - val_acc: 0.8915\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8641\n",
      "Epoch 00007: val_loss improved from 0.36878 to 0.30878, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/007-0.3088.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.4478 - acc: 0.8641 - val_loss: 0.3088 - val_acc: 0.9113\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8792\n",
      "Epoch 00008: val_loss improved from 0.30878 to 0.26888, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/008-0.2689.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3974 - acc: 0.8791 - val_loss: 0.2689 - val_acc: 0.9231\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8905\n",
      "Epoch 00009: val_loss improved from 0.26888 to 0.26226, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/009-0.2623.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3559 - acc: 0.8905 - val_loss: 0.2623 - val_acc: 0.9250\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.9033\n",
      "Epoch 00010: val_loss improved from 0.26226 to 0.23305, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/010-0.2331.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3198 - acc: 0.9033 - val_loss: 0.2331 - val_acc: 0.9299\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9114\n",
      "Epoch 00011: val_loss improved from 0.23305 to 0.22723, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/011-0.2272.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2890 - acc: 0.9113 - val_loss: 0.2272 - val_acc: 0.9313\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9152\n",
      "Epoch 00012: val_loss improved from 0.22723 to 0.22194, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/012-0.2219.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2703 - acc: 0.9152 - val_loss: 0.2219 - val_acc: 0.9317\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9245\n",
      "Epoch 00013: val_loss improved from 0.22194 to 0.20187, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/013-0.2019.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2478 - acc: 0.9245 - val_loss: 0.2019 - val_acc: 0.9415\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9284\n",
      "Epoch 00014: val_loss improved from 0.20187 to 0.20096, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/014-0.2010.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2307 - acc: 0.9283 - val_loss: 0.2010 - val_acc: 0.9425\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9293\n",
      "Epoch 00015: val_loss improved from 0.20096 to 0.18724, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/015-0.1872.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2255 - acc: 0.9293 - val_loss: 0.1872 - val_acc: 0.9462\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9377\n",
      "Epoch 00016: val_loss did not improve from 0.18724\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1979 - acc: 0.9378 - val_loss: 0.2196 - val_acc: 0.9352\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9405\n",
      "Epoch 00017: val_loss did not improve from 0.18724\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1908 - acc: 0.9406 - val_loss: 0.2041 - val_acc: 0.9460\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9449\n",
      "Epoch 00018: val_loss improved from 0.18724 to 0.16357, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/018-0.1636.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1761 - acc: 0.9449 - val_loss: 0.1636 - val_acc: 0.9511\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9466\n",
      "Epoch 00019: val_loss did not improve from 0.16357\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1702 - acc: 0.9466 - val_loss: 0.1708 - val_acc: 0.9513\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9490\n",
      "Epoch 00020: val_loss improved from 0.16357 to 0.16354, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/020-0.1635.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1630 - acc: 0.9490 - val_loss: 0.1635 - val_acc: 0.9509\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9511\n",
      "Epoch 00021: val_loss improved from 0.16354 to 0.16027, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/021-0.1603.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1535 - acc: 0.9511 - val_loss: 0.1603 - val_acc: 0.9557\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9538\n",
      "Epoch 00022: val_loss did not improve from 0.16027\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1460 - acc: 0.9538 - val_loss: 0.2041 - val_acc: 0.9392\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9564\n",
      "Epoch 00023: val_loss did not improve from 0.16027\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1365 - acc: 0.9564 - val_loss: 0.1666 - val_acc: 0.9513\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9586\n",
      "Epoch 00024: val_loss improved from 0.16027 to 0.15097, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/024-0.1510.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1317 - acc: 0.9586 - val_loss: 0.1510 - val_acc: 0.9592\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9634\n",
      "Epoch 00025: val_loss improved from 0.15097 to 0.14295, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/025-0.1429.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1198 - acc: 0.9634 - val_loss: 0.1429 - val_acc: 0.9595\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9624\n",
      "Epoch 00026: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1204 - acc: 0.9624 - val_loss: 0.1530 - val_acc: 0.9555\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9647\n",
      "Epoch 00027: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1120 - acc: 0.9647 - val_loss: 0.1631 - val_acc: 0.9557\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9646\n",
      "Epoch 00028: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1137 - acc: 0.9646 - val_loss: 0.1603 - val_acc: 0.9529\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9680\n",
      "Epoch 00029: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1022 - acc: 0.9680 - val_loss: 0.1786 - val_acc: 0.9506\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9672\n",
      "Epoch 00030: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1048 - acc: 0.9672 - val_loss: 0.1710 - val_acc: 0.9541\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9684\n",
      "Epoch 00031: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0968 - acc: 0.9684 - val_loss: 0.1464 - val_acc: 0.9569\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9706\n",
      "Epoch 00032: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0908 - acc: 0.9706 - val_loss: 0.1548 - val_acc: 0.9548\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9737\n",
      "Epoch 00033: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0836 - acc: 0.9737 - val_loss: 0.1775 - val_acc: 0.9504\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9711\n",
      "Epoch 00034: val_loss did not improve from 0.14295\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0891 - acc: 0.9711 - val_loss: 0.1661 - val_acc: 0.9548\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9746\n",
      "Epoch 00035: val_loss improved from 0.14295 to 0.13950, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_8_conv_checkpoint/035-0.1395.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0800 - acc: 0.9746 - val_loss: 0.1395 - val_acc: 0.9599\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9746\n",
      "Epoch 00036: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0779 - acc: 0.9747 - val_loss: 0.1661 - val_acc: 0.9576\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9773\n",
      "Epoch 00037: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0732 - acc: 0.9773 - val_loss: 0.1585 - val_acc: 0.9599\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9774\n",
      "Epoch 00038: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0708 - acc: 0.9774 - val_loss: 0.1591 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9757\n",
      "Epoch 00039: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0764 - acc: 0.9756 - val_loss: 0.1618 - val_acc: 0.9522\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9788\n",
      "Epoch 00040: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0668 - acc: 0.9788 - val_loss: 0.1431 - val_acc: 0.9613\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9801\n",
      "Epoch 00041: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0622 - acc: 0.9801 - val_loss: 0.1478 - val_acc: 0.9611\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9817\n",
      "Epoch 00042: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0592 - acc: 0.9817 - val_loss: 0.1648 - val_acc: 0.9588\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9812\n",
      "Epoch 00043: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0596 - acc: 0.9812 - val_loss: 0.1695 - val_acc: 0.9560\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9836\n",
      "Epoch 00044: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0538 - acc: 0.9836 - val_loss: 0.1787 - val_acc: 0.9543\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9830\n",
      "Epoch 00045: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0547 - acc: 0.9829 - val_loss: 0.1745 - val_acc: 0.9555\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9797\n",
      "Epoch 00046: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0644 - acc: 0.9797 - val_loss: 0.1509 - val_acc: 0.9618\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9840\n",
      "Epoch 00047: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0487 - acc: 0.9840 - val_loss: 0.1547 - val_acc: 0.9597\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9838\n",
      "Epoch 00048: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0502 - acc: 0.9838 - val_loss: 0.1767 - val_acc: 0.9543\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9872\n",
      "Epoch 00049: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0433 - acc: 0.9872 - val_loss: 0.1583 - val_acc: 0.9569\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9850\n",
      "Epoch 00050: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0489 - acc: 0.9849 - val_loss: 0.1573 - val_acc: 0.9574\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9854\n",
      "Epoch 00051: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0474 - acc: 0.9853 - val_loss: 0.1641 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9846\n",
      "Epoch 00052: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0493 - acc: 0.9845 - val_loss: 0.1731 - val_acc: 0.9548\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9864\n",
      "Epoch 00053: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0445 - acc: 0.9864 - val_loss: 0.1714 - val_acc: 0.9557\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9882\n",
      "Epoch 00054: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0393 - acc: 0.9882 - val_loss: 0.1973 - val_acc: 0.9555\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9855\n",
      "Epoch 00055: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0479 - acc: 0.9855 - val_loss: 0.1868 - val_acc: 0.9571\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9886\n",
      "Epoch 00056: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0382 - acc: 0.9886 - val_loss: 0.1534 - val_acc: 0.9609\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9889\n",
      "Epoch 00057: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0347 - acc: 0.9889 - val_loss: 0.2017 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9898\n",
      "Epoch 00058: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0346 - acc: 0.9898 - val_loss: 0.2048 - val_acc: 0.9495\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9878\n",
      "Epoch 00059: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0386 - acc: 0.9878 - val_loss: 0.1909 - val_acc: 0.9543\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9892\n",
      "Epoch 00060: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0360 - acc: 0.9892 - val_loss: 0.1569 - val_acc: 0.9616\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9902\n",
      "Epoch 00061: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0317 - acc: 0.9902 - val_loss: 0.1861 - val_acc: 0.9557\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9901\n",
      "Epoch 00062: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0334 - acc: 0.9901 - val_loss: 0.1919 - val_acc: 0.9571\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9896\n",
      "Epoch 00063: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.1587 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9915\n",
      "Epoch 00064: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.1916 - val_acc: 0.9541\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9873\n",
      "Epoch 00065: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0405 - acc: 0.9873 - val_loss: 0.1900 - val_acc: 0.9553\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9907\n",
      "Epoch 00066: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0302 - acc: 0.9907 - val_loss: 0.1660 - val_acc: 0.9592\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9912\n",
      "Epoch 00067: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0302 - acc: 0.9912 - val_loss: 0.1520 - val_acc: 0.9634\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9916\n",
      "Epoch 00068: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0270 - acc: 0.9916 - val_loss: 0.1630 - val_acc: 0.9583\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9921\n",
      "Epoch 00069: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0267 - acc: 0.9921 - val_loss: 0.1623 - val_acc: 0.9616\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9919\n",
      "Epoch 00070: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0274 - acc: 0.9919 - val_loss: 0.2212 - val_acc: 0.9436\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9885\n",
      "Epoch 00071: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0364 - acc: 0.9885 - val_loss: 0.1698 - val_acc: 0.9578\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9922\n",
      "Epoch 00072: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0269 - acc: 0.9922 - val_loss: 0.2036 - val_acc: 0.9513\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9927\n",
      "Epoch 00073: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0249 - acc: 0.9927 - val_loss: 0.1748 - val_acc: 0.9595\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9892\n",
      "Epoch 00074: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0340 - acc: 0.9892 - val_loss: 0.1754 - val_acc: 0.9590\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9939\n",
      "Epoch 00075: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0213 - acc: 0.9939 - val_loss: 0.1915 - val_acc: 0.9588\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 00076: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0217 - acc: 0.9936 - val_loss: 0.2202 - val_acc: 0.9527\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9933\n",
      "Epoch 00077: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0224 - acc: 0.9933 - val_loss: 0.1906 - val_acc: 0.9585\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9921\n",
      "Epoch 00078: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0248 - acc: 0.9921 - val_loss: 0.1748 - val_acc: 0.9585\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9940\n",
      "Epoch 00079: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0214 - acc: 0.9939 - val_loss: 0.1948 - val_acc: 0.9518\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9913\n",
      "Epoch 00080: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0289 - acc: 0.9913 - val_loss: 0.1779 - val_acc: 0.9567\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9949\n",
      "Epoch 00081: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0173 - acc: 0.9949 - val_loss: 0.1776 - val_acc: 0.9602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 00082: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.2621 - val_acc: 0.9455\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9933\n",
      "Epoch 00083: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0214 - acc: 0.9933 - val_loss: 0.1937 - val_acc: 0.9604\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 00084: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.2131 - val_acc: 0.9569\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9918\n",
      "Epoch 00085: val_loss did not improve from 0.13950\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0267 - acc: 0.9918 - val_loss: 0.1649 - val_acc: 0.9590\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmT2TmewhgUDYZN/CWiwKWq3ihlpFtNqq7aNPq13s4lNqW6u1fbTWx1qr1getPmqtyw+3qlREC6IVF0BUVtlJwpKF7Jl9zu+PM9lgEgLJJIH5vl+v+8rMXc+9uXO+995z7jlKa40QQggBYOntBAghhOg7JCgIIYRoJkFBCCFEMwkKQgghmklQEEII0UyCghBCiGYSFIQQQjSToCCEEKKZBAUhhBDNbIlasVJqEPAkkAdoYJHW+k+HzHMa8AqwMzbqRa31bzpab05Ojh4yZEi3p1cIIU5ka9asqdBa5x5pvoQFBSAM/ERrvVYp5QXWKKWWaa03HjLfu1rr8zu70iFDhrB69epuTagQQpzolFK7OzNfwh4faa33aa3Xxj7XAZuAgkRtTwghRNf1SJmCUmoIMBn4MM7kk5VSnyql/qmUGtcT6RFCCBFfIh8fAaCU8gAvADdprWsPmbwWGKy1rldKnQu8DIyIs47rgesBCgsLE5xiIYRIXiqRTWcrpezAa8BSrfW9nZh/FzBNa13R3jzTpk3Th5YphEIhSkpK8Pv9XUxx8nK5XAwcOBC73d7bSRFCJIBSao3WetqR5ktk7SMF/BXY1F5AUErlAwe01lopNQPzOKvyaLdVUlKC1+tlyJAhmM2Ko6G1prKykpKSEoYOHdrbyRFC9KJEPj6aBXwD+FwptS427hagEEBr/TBwKfBdpVQY8AGX62O4dfH7/RIQukApRXZ2NuXl5b2dFCFEL0tYUNBavwd0mEtrrR8AHuiO7UlA6Bo5fkIISKI3miMRH4FAKdFoqLeTIoQQfVbSBIVo1E8wuA+tuz8oVFdX89BDDx3Tsueeey7V1dWdnv+2227jnnvuOaZtCSHEkSRNUFDK7KrW0W5fd0dBIRwOd7jskiVLyMjI6PY0CSHEsUiaoNCyq90fFBYuXMj27dspKiri5ptvZsWKFZx66qnMmzePsWPHAnDRRRcxdepUxo0bx6JFi5qXHTJkCBUVFezatYsxY8Zw3XXXMW7cOM466yx8Pl+H2123bh0zZ85k4sSJXHzxxVRVVQFw//33M3bsWCZOnMjll18OwDvvvENRURFFRUVMnjyZurq6bj8OQojjX8JfXutpW7feRH39ujhTIkQijVgsKSh1dLvt8RQxYsR97U6/6667WL9+PevWme2uWLGCtWvXsn79+uYqno899hhZWVn4fD6mT5/OJZdcQnZ29iFp38ozzzzDI488wmWXXcYLL7zAVVdd1e52v/nNb/LnP/+ZOXPmcOutt3L77bdz3333cdddd7Fz506cTmfzo6l77rmHBx98kFmzZlFfX4/L5TqqYyCESA5JdKfQVLsmcS/rtTZjxow2df7vv/9+Jk2axMyZMykuLmbr1q2HLTN06FCKiooAmDp1Krt27Wp3/TU1NVRXVzNnzhwArr76alauXAnAxIkTufLKK/nb3/6GzWYC4KxZs/jxj3/M/fffT3V1dfN4IYRo7YTLGdq7oo9GgzQ0fIbTORiH44itx3ZZampq8+cVK1bw1ltvsWrVKtxuN6eddlrct6+dTmfzZ6vVesTHR+15/fXXWblyJa+++iq/+93v+Pzzz1m4cCHnnXceS5YsYdasWSxdupTRo0cf0/qFECeuJLpTSFyZgtfr7fAZfU1NDZmZmbjdbjZv3swHH3zQ5W2mp6eTmZnJu+++C8BTTz3FnDlziEajFBcXc/rpp/P73/+empoa6uvr2b59OxMmTOBnP/sZ06dPZ/PmzV1OgxDixHPC3Sm0p6X2UaTb152dnc2sWbMYP34855xzDuedd16b6XPnzuXhhx9mzJgxjBo1ipkzZ3bLdp944gm+853v0NjYyLBhw3j88ceJRCJcddVV1NTUoLXmBz/4ARkZGfzqV79i+fLlWCwWxo0bxznnnNMtaRBCnFgS2iBeIsRrEG/Tpk2MGTPmiMvW1a3B4cjD6RyYqOQd1zp7HIUQx5/ONoiXRI+PACwJeU9BCCFOFEkVFJSSoCCEEB1JqqAAVqD7yxSEEOJEkVRBQe4UhBCiY0kXFBJRJVUIIU4USRUUpKBZCCE6llRBwdwp9I0yBY/Hc1TjhRCiJyRVUACr3CkIIUQHkiooJKpMYeHChTz44IPN35s6wqmvr+eMM85gypQpTJgwgVdeeaXT69Rac/PNNzN+/HgmTJjAc889B8C+ffuYPXs2RUVFjB8/nnfffZdIJMI111zTPO8f//jHbt9HIURyOPGaubjpJlgXr+lscEQD2HQIrEf5iKaoCO5rv+nsBQsWcNNNN3HjjTcC8Pzzz7N06VJcLhcvvfQSaWlpVFRUMHPmTObNm9ep/pBffPFF1q1bx6effkpFRQXTp09n9uzZ/P3vf+fss8/mF7/4BZFIhMbGRtatW0dpaSnr168HOKqe3IQQorUTLygckUbT0pB2d5g8eTJlZWXs3buX8vJyMjMzGTRoEKFQiFtuuYWVK1disVgoLS3lwIED5OfnH3Gd7733HldccQVWq5W8vDzmzJnDxx9/zPTp0/nWt75FKBTioosuoqioiGHDhrFjxw6+//3vc95553HWWWd1494JIZLJiRcUOriiDwX2EwyW4PFMBmXt1s3Onz+fxYsXs3//fhYsWADA008/TXl5OWvWrMFutzNkyJC4TWYfjdmzZ7Ny5Upef/11rrnmGn784x/zzW9+k08//ZSlS5fy8MMP8/zzz/PYY491x24JIZJMEpYpJKaf5gULFvDss8+yePFi5s+fD5gms/v164fdbmf58uXs3r270+s79dRTee6554hEIpSXl7Ny5UpmzJjB7t27ycvL47rrruM//uM/WLt2LRUVFUSjUS655BJ++9vfsnbt2m7fPyFEcjjx7hQ60BQUElHYPG7cOOrq6igoKKB///4AXHnllVxwwQVMmDCBadOmHVWnNhdffDGrVq1i0qRJKKW4++67yc/P54knnuAPf/gDdrsdj8fDk08+SWlpKddeey3RqNmvO++8s9v3TwiRHJKq6exQ6CB+/w7c7rFYre5EJfG4JU1nC3Hikqaz41CxcgR5V0EIIeJLqqCQyC45hRDiRJBUQSGRBc1CCHEiSKqg0LK7faP9IyGE6GuSKihImYIQQnQsqYKClCkIIUTHkiooJKpMobq6moceeuiYlj333HOlrSIhRJ+RsKCglBqklFqulNqolNqglPphnHmUUup+pdQ2pdRnSqkpiUpPbIuxv91bptBRUAiHwx0uu2TJEjIyMro1PUIIcawSeacQBn6itR4LzARuVEqNPWSec4ARseF64C8JTE+sddLu71Nh4cKFbN++naKiIm6++WZWrFjBqaeeyrx58xg71uzyRRddxNSpUxk3bhyLFi1qXnbIkCFUVFSwa9cuxowZw3XXXce4ceM466yz8Pl8h23r1Vdf5Utf+hKTJ0/mzDPP5MCBAwDU19dz7bXXMmHCBCZOnMgLL7wAwBtvvMGUKVOYNGkSZ5xxRrfutxDixJOwZi601vuAfbHPdUqpTUABsLHVbBcCT2rzWvUHSqkMpVT/2LLHpIOWswGIREaglA3LUYTDI7SczV133cX69etZF9vwihUrWLt2LevXr2fo0KEAPPbYY2RlZeHz+Zg+fTqXXHIJ2dnZbdazdetWnnnmGR555BEuu+wyXnjhBa666qo285xyyil88MEHKKV49NFHufvuu/mf//kf7rjjDtLT0/n8888BqKqqory8nOuuu46VK1cydOhQDh482PmdFkIkpR5p+0gpNQSYDHx4yKQCoLjV95LYuGMOCp2T+KY9ZsyY0RwQAO6//35eeuklAIqLi9m6dethQWHo0KEUFRUBMHXqVHbt2nXYektKSliwYAH79u0jGAw2b+Ott97i2WefbZ4vMzOTV199ldmzZzfPk5WV1a37KIQ48SQ8KCilPMALwE1a69pjXMf1mMdLFBYWdjhvR1f0AA0Ne1DKjts94liS0mmpqanNn1esWMFbb73FqlWrcLvdnHbaaXGb0HY6nc2frVZr3MdH3//+9/nxj3/MvHnzWLFiBbfddltC0i+ESE4JrX2klLJjAsLTWusX48xSCgxq9X1gbFwbWutFWutpWutpubm5XUxV93fJ6fV6qaura3d6TU0NmZmZuN1uNm/ezAcffHDM26qpqaGgoACAJ554onn8V7/61TZdglZVVTFz5kxWrlzJzp07AeTxkRDiiBJZ+0gBfwU2aa3vbWe2fwDfjNVCmgnUdKU8oXPpsnR7QXN2djazZs1i/Pjx3HzzzYdNnzt3LuFwmDFjxrBw4UJmzpx5zNu67bbbmD9/PlOnTiUnJ6d5/C9/+UuqqqoYP348kyZNYvny5eTm5rJo0SK+9rWvMWnSpObOf4QQoj0JazpbKXUK8C7wOS2X5rcAhQBa64djgeMBYC7QCFyrtV4dZ3XNutJ0NoDPt41oNEBq6rij2JvkIE1nC3Hi6mzT2YmsffQeR+gKOVbr6MZEpSG+7r9TEEKIE0VSvdEMTe0fSYN4QggRT9IFBblTEEKI9iVdUDDtH0U53rohFUKInpB0QUFaShVCiPYlXVCQPhWEEKJ9SRcU+sqdgsfj6dXtCyFEPEkXFKSfZiGEaF8SBgVr7FP3VUtduHBhmyYmbrvtNu655x7q6+s544wzmDJlChMmTOCVV1454rraa2I7XhPY7TWXLYQQx6pHWkntSTe9cRPr9rffdrbWEaLRRiwWd6sA0bGi/CLum9t+S3sLFizgpptu4sYbzXt4zz//PEuXLsXlcvHSSy+RlpZGRUUFM2fOZN68ebF+HeKL18R2NBqN2wR2vOayhRCiK064oNB53VcldfLkyZSVlbF3717Ky8vJzMxk0KBBhEIhbrnlFlauXInFYqG0tJQDBw6Qn5/f7rriNbFdXl4etwnseM1lCyFEV5xwQaGjK3qASMRHY+MGXK5h2O3d17/A/PnzWbx4Mfv3729ueO7pp5+mvLycNWvWYLfbGTJkSNwms5t0toltIYRIlCQsU2gqaO7epi4WLFjAs88+y+LFi5k/fz5gmrnu168fdrud5cuXs3v37g7X0V4T2+01gR2vuWwhhOiKpAsK0FSO0L21j8aNG0ddXR0FBQX0798fgCuvvJLVq1czYcIEnnzySUaPHt3hOtprYru9JrDjNZcthBBdkbCmsxOlq01nax2lvn4tDkcBTmf/RCTxuCVNZwtx4ups09lJeKfQVPNH3lMQQohDJV1QMNVBLd1epiCEECeCEyYoHM1jMPN+gtwptHa8PUYUQiTGCREUXC4XlZWVR5GxSZ8KrWmtqaysxOVy9XZShBC97IR4T2HgwIGUlJRQXl7eqfkDgXKUOojDEUhwyo4fLpeLgQMH9nYyhBC97IQICna7vflt385Yu/Y/sFhSGDPmrQSmSgghjj8nxOOjo2W1phKNNvR2MoQQos9J2qAQiUhQEEKIQyVlULBYJCgIIUQ8SRkU5E5BCCHiS9qgIGUKQghxuKQNCpFIg7ywJYQQh0jKoGCxpAKaaNTX20kRQog+JSmDgtWaCiDlCkIIcQgJCkIIIZolaVDwAEhhsxBCHCJJg4LcKQghRDxJGRRMQbMEBSGEOFTCgoJS6jGlVJlSan07009TStUopdbFhlsTlZZDyZ2CEELEl8hWUv8PeAB4soN53tVan5/ANMTVFBSkTEEIIdpK2J2C1nolcDBR6+8KuVMQQoj4ertM4WSl1KdKqX8qpcb11EalTEEIIeLrzU521gKDtdb1SqlzgZeBEfFmVEpdD1wPUFhY2OUNy52CEELE12t3ClrrWq11fezzEsCulMppZ95FWutpWutpubm5Xd62xeIClJQpCCHEIXotKCil8pVSKvZ5RiwtlT20bWk+Wwgh4kjY4yOl1DPAaUCOUqoE+DVgB9BaPwxcCnxXKRUGfMDlugebLTUd7dT31OaEEOK4kLCgoLW+4gjTH8BUWe0VcqcghBCH6+3aR73GavVIUBBCiEMkcVCQ3teEEOJQSR0U5E5BCCHaStqgYLNlEwqV93YyhBCiT0naoOB0FhAIlEo/zUII0UryBIVVq2D+fNi/HzBBIRr1EQ5X93LChBCi70ieoFBRAYsXQ3ExAE7nQAACgdLeTJUQQvQpyRMU8vLM3wMHAHOnABAIlPRWioQQos9J2qDgcJigEAzKnYIQQjRJ2qDgdA4A5PGREEK0ljxBweWCtLTmoGCxOLDbcyUoCCFEK50KCkqpHyql0pTxV6XUWqXUWYlOXLfLy2uufQQt1VKFEEIYnb1T+JbWuhY4C8gEvgHclbBUJUp+fvOdApgaSFKmIIQQLTobFFTs77nAU1rrDa3GHT/y8toEBYejQGofCSFEK50NCmuUUm9igsJSpZQXiCYuWQlySFBwOgsIhSqIRgO9mCghhOg7OtufwreBImCH1rpRKZUFXJu4ZCVIXh5UVUEwCA5Hq3cV9pKSMrSXEyeEEL2vs3cKJwNbtNbVSqmrgF8CNYlLVoI0VUstKwNav8Am5QpCCAGdDwp/ARqVUpOAnwDbgScTlqpEkRfYhBCiQ50NCuFY/8kXAg9orR8EvIlLVoIc9gKbtH8khBCtdbZMoU4p9XNMVdRTlVIWwJ64ZCXIIUHBZkvHYnFLDSQhhIjp7J3CAiCAeV9hPzAQ+EPCUpUohwQFpZS8wCaEEK10KijEAsHTQLpS6nzAr7U+/soU3G7weA6rlipBQQghjM42c3EZ8BEwH7gM+FApdWkiE5YwcV5gk4JmIYQwOlum8Atguta6DEAplQu8BSxOVMISJm77R3vRWqPU8feSthBCdKfOlilYmgJCTOVRLNu3xHmrWesgoVBFLyZKCCH6hs5m7G8opZYqpa5RSl0DvA4sSVyyEihOo3ggPbAJIQR08vGR1vpmpdQlwKzYqEVa65cSl6wEysuDykoIhcBub/NWs9c7uZcTJ4QQvauzZQporV8AXkhgWnpGU7XU8nIYMEDeahZCiFY6DApKqTpAx5sEaK11WkJSlUit31UYMACHIx+wSLVUIYTgCEFBa338NWVxJIe8wGax2HA48iQoCCEEx2sNoq44JCiAvMAmhBBNJCgg3XIKIUST5AsKHo9p7kK65RRCiMMkLCgopR5TSpUppda3M10ppe5XSm1TSn2mlJqSqLQcJs4LbOFwNZFIY48lQQgh+qJE3in8HzC3g+nnACNiw/WYjnx6RpygANKvghBCdPo9haOltV6plBrSwSwXAk/GOu/5QCmVoZTqr7Xel6g0NcvLg+3bm7+2Dgpu94iEb14I0bdpDQ0NEI2CxQJKtfxt+myzmc+tRaPg85nPbnfb6eGweW+2rg5cLjM9JQXsHfRMY7GYoSclLCh0QgFQ3Op7SWzcYUFBKXU95m6CwsLCrm85Lw/ef7/5q7zAJo5WJGJ+/JEIWK1msFhMRlJTA7W14PebjKNp8HohKwtSU01mEQ6b7sL37TMZRVOGA2YdlZVmqK83y3i9ZrDbTebTNFitZpwt9mtubDTpaGho+zkQMMtnZJjBYoGDB1uGUMisT2uzX6FQy2C1muK41FSTmYVCZv/9frPeYNAMoZBZvolSLcfHajXrbUqPzwfp6ablmf79Tdqqqsw+V1S0HN+mIRpt+7f10LQtpcxxyMqCnBwzKGXeVS0rM38DgZblrFbIzobcXDOv3w+lpWZo7MTTZIfDZPA2m0lvU0AAMy493exXba3ZNx3vra8jcDrNNlJS4Ac/gJ///OjXcTR6Myh0mtZ6EbAIYNq0acdwWA+Rn2/OunAYbDZcrsGAlcbGLV1eteheWpsfWl2dGaqqWjKxhgaTUaWnmyESMf/Wpsw0Emn5EYZCJqOtrjZ/mzIvrU2mkZJiMju324xrWsfBgyZzCATM4Peb9ASDx75PDkdLBhiNds9x6ojFYjJzh8Mcw0PTnpICmZkm82m6CrZYTKBxOMzfcNgc7/p6czyaMkOXyyzncLTMa7W2rDsaNcv6/S2ZcGoq9OtntltdDSUl8PHHJuNsysyzs81gsbQNKk3fDx0PLf/PUMj83yoq4NNPzbh+/WDkSPjyl1sycau15eq9osIEDKcTJk+G88832YTN1jYAN20jGjXbaTovQqGW8yc11UyvqWm5QPB6TeDJzYW0tJbzyOczaWhPONwSfH0+sw+J1ptBoRQY1Or7wNi4xMvLM//ZigrIz8dqTSE1dQx1dWt7ZPPHq2jUZAoV1QE+L/2C4rJ6yso1ZWVRaquteK25ZNrz8Tg8hEItGXl9fUvmp5TJlJp+iE2xuelK12Yz0wMB8Ac0Pp9GR4/i/tnmg/Ri8O6FiAMCXgh6IeIgNbOO1Kw63Bl12BxRLFEHSttRkRR08VACtWk0NJjVZGVHcQ/aimXiJ2SkNGC3W7DbFA6bDY/DQ5ozjTSnl1RbOo5oBvZIBhbtwOMxP/q0NJP5RCJm/0IhOFgTZFvlDnbWbKMiWEK6x0lehpf+2R7y07PIcw0m3doPpRRpaU0Zo8aREqS2PkJtXZSa2gh1gQbqwlXUhQ7SEK4l05FHnnMIqSobpRSpqS1X9E3BoPVjDJ9PU34wRLWvhrCznNpIGZWNlfRL7cfonNHkuHOO2Ix8JBphZ/VONpVvospfxZCMIQzLHMYA7wAsytI8jy/soy5QR22gltpALXarncHpg8lwZcTdRiQaoaKxgv31+znQcID99fvZV7ePSl8lue5cCtMLKUwvxOv0UuWrotJXSZWviin9pzApf9Jh6wtFQgQjQVIdqW3Ga63ZV7+PnVU7UUphVVasFisWZUGhsCgLNouNQemDSHO2bbghHA1TWluKL9xyW2C32ClML8RuPfxZUFRHqfJVUdFYQXljORZlYUbBDGyWttlvfbCe94vfJ8WWQo47h9zUXFJsKdQH65uHrJQs2mab3a83g8I/gO8ppZ4FvgTU9Eh5ArR9VyE/HwCPZwpVVct6ZPNdEY6GqWiswG6xk+3OPmx6VEfZWbWT3TW72VOzh93Ve6ioryUYjBIKaUJhTShkIRy0EApaaQwEKPeXUhkupiZaijd0EoMOXk1ayaU0VHmortFU6m1UZ79JIPd9yPsMcjaD9ZDLm9a/OZ8bgh6wR1HZGpWrsYS9WIMZWIKZ2MLppOR58djT6O/wgjVAIxU0qnLqLRWErNUELdUEVDVaRbDjxqU8pFg9ZDr7kZfan4FpA8hyp7Ovrox9dfs40LiXimAJNeHydo9dQ2xoT39Pf8bljEKhWLNvDbWB2o7/GYHYEOO2u3EH3Lir3aTUp2C32glGggTCAQKRAGUNZUR1FFyYAaA2NsS4bC4GpQ0iqqNU+6upCdQQjnZwKdmKx+GhX2o/wtEwwUiQUCSERjdndAC+sA9fyEdER9pdT6YrkxHZIxiYNpACbwEF3gIiOkJxTTEldSXsqdnDlootBCKBw5Z1WB3YLXYCkUCH6fY6vBSmF2K1WJuPT0OwgUpfpTlGcdYbjHR8ezZz4Ey+M/U7nD/yfFbuXskLm17gtS9eoyZQwwDvAEZkjWBo5lBKaktYt38dFY2day4/x53D8MzheBwedlbvZE/Nnrj7ZlVWhmcNZ1T2KNx2N8W1xRTXFLO3bu9hxzvXncvFoy9m/rj5VPureW7Dc7z+xettAk08C2ct5M4z7+xUuo+V0sfykKszK1bqGeA0IAc4APwasANorR9W5jLhAUwNpUbgWq316iOtd9q0aXr16iPO1rH33oNTT4WlS+GsswAoLr6P7dt/xMkn78PpzO/a+o9RQ7CBj/d+zKriVXxY+iFlDWXmxx0N4Q/7qWis4KDvYPP8o7PHMtZ9Ghk1p1BSu49twZWUWt8lYD3YdsVBN2iLGQCUBhUBSwQidqgrgJpBWBr7owatIpKxDUs4lazqM2j0fkajcxcAaXogBbYihrgmMtw7noLsTHJyFLk5CrszTHlDOfvrD7C/7gCN4QasFgsWZUFrTX2onmp/dfPQdPVYF6zDaXWS484hx51DtjubrJQsMpwZZLgysFvtNAQbqA/WUxuspazBBIG9dXupCdTQL7UfA7wDzOAZwOCMwRSmFzLAO4BwNGy2EagjEAmQ5kzD6/DidXqxKEtzxtkQamDbwW1sqdzC5orNRKIRpg2Y1jxkuDLQWhPVUcLRMHXBuub01wRqmvepyldFY6jRZLxhH6FICKfNicPqwGl1ku/JZ0TWCE7KOonC9EJC0VDzFWBZQxm7q3ezu2Y3xbXF2Cw20p3ppDvT8Tg82Cw2rBYrVmXFbXeTmZJJVkoWHoeH/fX72VW9i51VO6n0VWK32rFbzKCUQmuNRqO1JsWegtvuJsWWQpozjX6p/eiX2o+slCz21e9jc8VmtlRsYVvVNkprSymtK20Ojtkp2QxMG8jAtIGMzhnN2NyxjM0dS1ZKFrurd7Ojagc7qnYQjoZx2Vy4bC5S7CnNxzzNmUYgHGB3zW52Ve+iuLYYrTVOmxOn1Ynb7qZfaj/yUvPI8+SRl5pHf29/8j3m7rPGX0NxbTF7avZQF6gj251Ndko2HoeHJVuX8PCah9lcsbn5tM9KyeLCURcyPHM426q28UXlF+yo2kGBt4BJeZOYlD+JEVkjUEoR1VEi0QhRHW0+VoFIgD01e9h2cBvbq7bTEGxgaOZQhmUMY2jmUDwOT/O2fCFf8zm0pXILgXCAgWkDGZQ+iIHegeR58prP8dpALS9seoFXt7xKQ8hcpuSl5nHp2EuZN2oeCtV8V+EP+/E4PHgcHrwOL6NzRjMmd8wx5S9KqTVa62lHnC9RQSFRuiUobN1qHs49+SR84xsAVFevZN26OUyYsITs7HOOabWRaITPyz5nf/1+GoINNIQaqA3UUlpbSnFtMSW1JRz0HcQf9jcP4WjYnJA6QmOosfkqqcA5Em90MOGgnXDAQTjgwBrIwRbIxervR1VjDeWpK2HQe+CsB0BVD8ddNofAjipQAAAgAElEQVTM+i+T7xzG4IxChucOpF+Ws/lxQkpKy+ONpoLL1gWYWmveL36fJz59gje3v0lRfhFnDz+bs4afxfCs4V077t1MesvrGfXBeizKgtvu7u2kdEhrzcrdK1m+azmzB89m9uDZhz2i6Ut8IR/LdizD6/Aye/BsrBbrkRfqgs4Ghb57xBIpTlMXHk8RAPX1a48qKJQ1lPHK5ldYtmMZb+98u82VfBO7xU5BWgGD0gYxPGs4KbYUCLuor3Hib7Dh91nxNVpprPJS8ekMytZ+iVKfeTSkVEstipSUlgK9MZlQNPznTCwK4xr8OeOH9KMws6ALB4XY9hSzCmcxq3DWkWfuZRIQekbrK+K+TCnFnCFzmDNkTm8npVNS7CnMGzWvt5NxmOQMCl6vKQVsFRRstjRSUkZ0urB5U/km7l11L0999hSBSIAB3gFcMPICzhx2JsMzh5PqSCXVnorX6SXLlcMnay0sWQKrVsEHn7bpJhowGf3AgXDaNJhxMcyYYW5mcnLa1uY4nA2QzoGEEN0jOYOCUoe91QymsLmu7sO4i0R1lA1lG1i5eyWvbX2NN7a9gcvm4pqia7hh+g1M6DehzZVraSksXwbLlsEbb5g60krBxIlw9tnm74QJUFhokpKefviLMEII0dOSMyhA3KDg9U6hvPw5QqGD2GyZbKncwls73uLtnW+zcvfK5kdDg9IGcftpt/Pdad8lNzUXMFUu330X/t//M4FgS+yVh6wsmDsXzj3XBIOcnB7dSyGEOCrJGxQKCmDz5jajPJ7JaA0PrLqDe9cspqTWtJw6NGMoF466kDmDzfPKwemDm+8KNmyARYtMMNi3zzz3/8pX4Prrzd+JE3v+NXUhhDhWyRsURo6E115rfqsZIGo/iTs2wfLy+5gzeA63zr6VM4adwbDMYYctvmED/OY3Jhg4HHDOObBggXkT0nN8lMsJIcRhkjcojB5tXjPdtQtOOolP9n3CZYsvY2cV3DRhEv9z8b+a38xsrbQUfvITeP5587boLbfAj35k3j4VQojjXfIGhVGjzN/NmynOdXLK46eQ6crkr7NPYXRK+WEBQWt46inTIFUwaBql+vGPJRgIIU4sEhS2bOFe29sEwgFWXrsSS83T7Nr1a8LhOmw2L2Cqj15/Pbz6KpxyCjz+OJx0Ui+mXQghEiR5g0JWFuTmUvnFOhYFXuTrE77OsMxhVEQmA5r6+k/JyDiFrVvhtNNMq4v33mvuFDp+b0AIIY5fyRsUAEaN4kH/ShpDjfzXrP8CTLVUgPr6T6ioOIXTTzePiz74ACYd3gijEEKcUJK6smTD6GHc37+YC0ZewPh+4wFwOPpjt+excWMxp59u2jF/+20JCEKI5JDUdwqPDauhMqhZOOmG5nFKKerrz+bqq39EKGQCwsSJvZhIIYToQUkbFEKREPfwPqfshi/XZrSZduedt1BXl8qKFY0UFfXtliGFEKI7Je3jo2fXP8ueYDkL36PNm81LlsA774zi6qtvY+jQvt/pjhBCdKekDQp/Wf0XxuaM5dxdtuaGioJB8yLayJGaSy99ksrK13o5lUII0bOSMijsr9/PByUfcPn4y1HDT2q+U3jgAfjiC/jjHxV5eWdQWfkaOk7XgEIIcaJKyqDw2hevodFcOPpC09zFli2UlcHtt5s2jM49F7KzLyAY3N/p/hWEEOJEkJRB4ZUtrzAkYwgT+k0wbzZv28Yvb4nS2GheUANiva9ZqKx8tVfTKoQQPSnpgkJDsIG3drzFhaMuNM1fjx5NcSiPRx9TfO975sYBwG7PJj39y1KuIIRIKkkXFN7c/ib+sL+lb9RRo3iZi9Ba8d3vtp03O/t86uvXEgiU9nxChRCiFyRdUHhlyytkuDI4tfBUMyIWFMb0q2TkyLbzZmefD0Bl5es9nEohhOgdSRUUwtEwr33xGueNOA+71Q5Apc7iHeZwUf/D+2Z2u8ficg2VcgUhRNJIqqDwfvH7VPoquXDUhc3jXn8dIti4mJcOm18pRXb2+VRVvUUk0tiTSRVCiF6RVEHhlc2v4LA6mHvS3OZxL78MBe4qppb+I+4y2dnnE436qa5e3lPJFEKIXpM0QUFrzStbXuErQ7+C12k6z2lshDfegIuKdmGpKDOdJhwiI2MOVquH8vLD7ySEEOJEkzRBYWP5RrZXbW/z6GjZMvD54KK5fjMi1txFaxaLk379Lqes7GmCwbKeSq4QQvSKpAkKWyq3kOZMa6mKCrz0EmRkwJxLcsyIDRviLjto0M1EowFKSu7viaQKIUSvSZqg8LUxX6Pi5goGeAcAEA6bPpfPPx/so4fD4MHwwgtxl3W7R5KT8zX27n2QcLiuJ5MthBA9KmmCAtBcDRXgvfdMEcJFFwEWC1x5Jbz5JuzfH3fZwsKfEQ5Xs2/foh5KrRBC9LykCgqtvfwyOJ1w9tmxEd/4BkSj8MwzcedPS5tORsZXKC6+l2g00HMJFUKIHpS0QeH992HWLPB4YiNGj4Zp0+DJJ9tdprBwIcHgXg4ceLpnEimEED0soUFBKTVXKbVFKbVNKbUwzvRrlFLlSql1seE/EpmeJlrDxo0wbtwhE77xDVi3Dtavj7tcZuaZeDyT2bPnbulnQQhxQkpYUFBKWYEHgXOAscAVSqmxcWZ9TmtdFBseTVR6WisuhoYGGHtoai6/HKxWeOqpuMsppSgsXIjPt0XuFoQQJ6RE3inMALZprXdorYPAs8CFR1imR2zcaP4eFhT69YO5c+HppyESibtsbu6leL0z2L79ZkKh6sQmVAghelgig0IBUNzqe0ls3KEuUUp9ppRarJQaFG9FSqnrlVKrlVKry8vLu5ywdoMCmEdIpaWwYkXcZZWyMHLkQ4RC5eza9asup0UIIfqS3i5ofhUYorWeCCwDnog3k9Z6kdZ6mtZ6Wm5ubpc3unEj5OZCTk6cifPmQVpau4+QALzeqRQU3EBp6UPSXacQ4oSSyKBQCrS+8h8YG9dMa12ptW6q3/koMDWB6Wm2aVM7dwkAKSkwfz4sXgy7drW7jiFD7sBuz+WLL74rhc5CiBNGIoPCx8AIpdRQpZQDuBxo0xSpUqp/q6/zgE0JTA/QUvOo3aAAsHAh2O1w4YVQXx93Frs9g+HD76Gu7iP27euR8nEhhEi4hAUFrXUY+B6wFJPZP6+13qCU+o1SqqkBoh8opTYopT4FfgBck6j0NNm/H6qrjxAUTjoJnn3WVE295hrzUlsceXlXkpFxGtu3/5T6+s8Tkl4hhOhJCS1T0Fov0VqP1FoP11r/LjbuVq31P2Kff661Hqe1nqS1Pl1rvTmR6YEjFDK3dvbZ8Ic/mPaQ7rgj7ixKKUaPfhKr1cvnn58rfTkLIY57vV3Q3OM6HRQAfvQj+OY34bbbTBlDHC7XICZMeJ1wuJrPPjuPcLi229IqhBA9LSmDQkYG5OV1Ymal4H//F2bONA3mvf123Nm83iLGjVtMQ8N6NmyYTzQa6t5ECyFED0nKoDB2rMnvO8XlMh05jxxpCp5XrYo7W1bW2Ywa9b9UVb3Jpk1XEY0Guy/RQgjRQ5I2KByVrCzTrHZ+Ppx7Lnz6adzZ+vf/NsOH30N5+fOsX38xkYiv6wkWQogelFRBobwcKiqOISgA9O8Pb71lmlU96yzYuTPubIMG/YSRI/+Xgwf/yWefnSNlDEKI40pSBYVNsbcgjikoAAwZYjp2DgTgiisgFL/sYMCA6xkz5mlqat7j00/PIBCI33GPEEL0NUkVFI6q5lF7Ro+GRx6BDz+EX/6y3dny8q5g/PiXaGjYwNq106U5DCHEcSHpgoLHAwMHdnFF8+fDf/4n3H03LF3a7mw5ORcwefK/AcUnn5xCWdlzXdywEEIkVtIFhaOqedSRP/4Rxo83raru29fubF7vZKZO/RiPZwobN17O9u0/k5pJQog+KymDQrdISYHnnjNtI11+uem1px0ORx5FRW/Tv//1FBffzdq1J9PQkPBmnoQQ4qglTVCoqjIX9GPGdONKx46FRx+F996Dr3zFVG1qh8XiZNSo/2XcuBfw+3ezZs0USkoekBZWhRB9StIEhS7XPGrP179u2kf67DM45RTYvbvD2XNzv8b06evJyDidbdu+z0cfjaak5E/Si5sQok9ImqCwd69pDbvbgwLARReZl9sOHICTT4aHHzY9t+3da9rqPoTTmc+ECa8zduyz2O05bNt2E6tWFfDFFzfg9+9JQAKFEKJzlI6TafVl06ZN06tXrz6mZcNhsFq7qaA5nvXr4fzz294t5OTA738P117b7obr6tZQWvoABw48DSj697+OwYNvwekckKCECiF6ndbmfaeTToLf/jbhm1NKrdFaTzvifMkUFHpENAolJfDFF7BliymMfvdd0xT3I4/AoLjdUAPg9+9h9+7fsn//4yhlIz//GvLzv43XOxWVsEgmhOgVr75quv8F+Ne/4PTTE7o5CQp9RTQKDz1kenOzWODGG80LcMOGmaF/fzO+FZ9vB7t3/46ysmeIRn2kpk4kP/8aPJ4iXK5CnM5BWCyOXtohIUSXRaMwebKptaiUuWv47DNwuxO2SQkKfc3OnfCd75hmMlofc7vd3D0MHgzjxsF3v9tc8BEO13DgwDPs2/co9fVrWq1M4fFMZsSI+0lP+7K54sjMhFmzDgswQiSVAwfg8cfhe98zb6p2l48/Nnf+X/969/zGnn3WPDp6+mkYMMDcJdx8s3khNkEkKPRVgQDs2QM7dphh924z7NkDn3wCPp9pifWnP4XTTmsuh/D5duH378Dv34Pfv4v9+x8nWraHSfcV4lkRK5weNMicaFddBRMmdD2t5eWQm9v19YjjTyQCNTWmLvfBg1BQYDKvviwchjPPhHfegQsugJdeMoWIrdXUQFpa5wsWGxpMczZ/+pO5mJs92wSdYcO6ls6xY02z/OvWmSBz/fXw17+a5nOmToXly+G++8z2X3wR0tOPfXsxnQ0KaK2Pq2Hq1Kn6hFVervVvfqN1bq7WoLXVqrXLpXVamtb5+VovWKD1M89oXV2tw28t0aE8j47Y0dt/4NH7/uds7Ttjgo7arGbZP/2pa2m54w6znu9/X+twuHv2T/SeNWu0vukmrRct0nrTJq2j0fbn/dOftLbbzf+/9TBjhtb//d9m+c6orta6pKT96WVlWu/cqfWBA1rX1nb9PPvVr0w6v/Y18/eHP2yZFo227Ndpp2m9bduR1/fmm1oPGWLWdcMN5tilpWntdmv95z9rHYkcvkw0qvV772m9dKnWoVD89T76qFnnyy+3jKuq0rp/f61HjtR64kQzPSfHpHfOHK19vqM6FPEAq3Un8thez+SPdjihg0KTxkatH3tM61/8QuubbzY/5m98Q+t+/cy/zG7X2mLReuRIXf/uM/qTT87Q77yTqpcvR7/3MrrsVPMjrvnhXO33lR799m+/3WxnwgTz97zzzI82EQKB9n88x7NwWOuDBzvOfHtCebnW//mfWitlLjKaMvjcXK1vvFHr+vq28z/xhJl+9tla//GP5vsrr5hgMGNGy/I33KC139922YoKs86pU7XOzGyZ9wc/aDtvJKL13XdrbbO1DTrp6Vr/7ndaNzQc/X4uW2b28ZprzPcf/cis889/1rqmRuv58833OXNMxp6SovW99x4eiPbu1fqee7SeNMnMP2KE1itXtkzfs8ccG9B62DCtb71V6y1bzDn83HNaT5/esj8DBmj985+b6U3neWOj1oMGmWN56Lnx8sstv7u//tUEgr//3ezXRRd1+XciQeFEFA6bq5Cbb9b6Zz/Tuq6ueVI0GtE+3y5dUfFPveOLX+kD8zK0Bl0yD/3RqnF647KzdMnjl+iq/16g6566XQc+X6mjweDh27jtNnNaXH212d5f/mIyk4kTtd69u/v2Zf16k4F4vVo7HGb9X/+6ySwqKjpedssWM29mptZnnqn1nXdq/dFHHV9pJupuJxDQetcurf/9b/MD/ulPtZ49W+vU1JYA3pQJ3H57x8HV79f6oYdMpnDppVovX965oLJ3r9b/+IfJoK65Ruvvftek46c/NcfIajUXFlVV5tg9+qjWV1xhMptx47TeuNGs57XXzLxf+crhGX6T4uKWDHfGjJZz4sUXtc7LMxn9WWeZNNx9twkeoPW0aVpv326C1Hnn6eYr+scf1/rBB7X+wx+0vuCClsz0kUc6nwnu22e2PXZsS5ALh7WeN89cPA0ZYvbr7rvN8SwubknDSSeZjHzKFK3HjzfzN+3bAw+YTPxQ0ai5Yz/zTHMMQeusrJYg8vDD5nhccEHbQNx6WLas/f/lof/zP//ZLPPtb3fpIqOzQUHKFE5UWhP86fU47n2UqNOCJXB4cxpRBwQKUiA7C0vuAGzWdKyvvWXeqXjkkZbnsW++aVqGDYVMFbqvfx3mzgVHBzWg9uyB99+Hf/8b1q4Fm810jp2RAbt2wcqVZvnLLjPPqtevN8OePWaeX/7SFBY6nc37w/bt8N//DU8+acZfdJGpsbF+vZln5Ej4859NJ0hNNm2C73/fPKMdPtw0Yjh+vNlGJGKGUAhqa6G62gx5eaYV3PbKZSoq4G9/M8+WP/+8bcUBh8PUKpk+3fS/UV5u2lfZudNUTc7NhV/9yjxDdjpNLZSDB+H55+HOO0115qlTzfwHD5pnz9/8pun9z+EwQ3m5KfTcssU06NXUIKPFYmqzBQLQ2GjKp844wzybHjfu8P1Ytsz0Pd7YaAo5f/970w7MihXg9bb/vwXznPuaa0x6Zs2Cf/wDiorMMSkqajvvyy+becEU/paXw733wg03HP5s/733TFo++MDsy4UXwsUXm/K1HTtM2t55B7ZtM+eUzQalpbB/vykMbr2fDQ2mALe42FQNnz27ZZrW8Pe/m/+jUuZct1pbGrkcNarj/W9SWmoKjT/+2LSBdsEFbcsx9u41x6qmxvyvtTblM9/61tG9MPXrX8NvfgO33gq339755VqRgmZh/N//mRN27Fj0mDEEC70Edn9M+LNVsGE9amcx6uBBbDUaex1UznFy4OczSPWOJzV1PF7vdDyeSVi27TKFbc8/bzLFzEw49VSYOBEmTTI/4DVrTBB4/32TuQGkpsKUKeaH0pTpulwm8Fx77eEF2Rs2mEzhn/+EoUNNhrBhgymELyszGekNN5gqvv36mWUOHDAZ3O23m8zi0kvhjjtMBnXvvSYjuvpqk6b162HrVvMDbS0lxQSK9HRT8O/zwZw5Zlu5uebHX1oKq1ebDDAYNBn/OeeYAv6CAtMm+6hR7QfLjz6Cn/3MZGz9+pljUl5uCh7BNJNy222mHS2/32RkDz5otnmo9HSzrdGjW4JQUZE53k20PnLGs3evyczefde8RPXvf7cc1yP54gu45BITnG691eyb3R5/3p07TSWIgwdNJjplSvvr1drUqHvqKViyxAQtm63lOA0Y0BKww2Hzv/zhD00AOVQwaKa7XJ3bp75Ka7jlFnO8px25rDgeCQqi06LREA0Nn1Fb+yH19Z/S0LCBhob1RCI1ACjlwOOZjMczCadlAGkfVpP62gYcn+5GfbGtbQZbWAhf/rIZZs0yQcNmO/pELVtmgsPGjebqb/Jkk+ldconJgOMJBOAPf4Df/c5kqmACz113tc3oAgEzvenq0GZrm5kdPGhqgjz0kLmraa1fP5O5ffvbx1bDS2tz5/X44yZY5eWZdU6ebIJsvEy8osKkORg0fzMzzTLd9UJjOGwy4K9+9eg7GwkETA2l/Pwjz9v08ORoqnT6fOZceOcdcxdz2mnmjk9e5jxqEhREl2itCQSKqa39iLq6j6it/ZCGhg2Ew5Wt5lKk2afQr2ISaXUDiUwaTXRAJhDBZsvG45mA1Zra3iY6kwjzeOdog8quXeYx0te+ZgLTsYpETGYEJhAVFHRv3XchepAEBZEQkYifYLAUv383NTXvcvDgMmprPwAicea24HaPJDV1AkrZiEQaiUZ9WCxO0tNnk5l5Bh7PJJRquXLUWkuTHkIkgAQF0WPC4Rrq6z8HQCkrSlkJBvdRX7+OurpPaGzcACgslhSsVjehUBU+3xYAbLZs7PYcIpEawuFqotEgKSkn4fFMJDV1IqmpY3G5hpOSMhyb7QiFn0KIdnU2KBzDw14h2rLZ0snIOOWw8Tk5cQr+YgKBUqqq/kV19XIikQZstnRstnSUstPYuIX6+nWUl78AtFy02O25uN2jcbvHkpo6FqezgECgFL9/F37/LiwWN17vNNLSpuPxFHXt0ZUQSUruFESfFYk00Nj4BT7fdvz+7fh822hs3Bwr26hqns9iScHlGkIkUkcgEKv1hMJq9TQPNlsGTmcBDkcBTudA7PZsbLY0rNY0LBZnLLjsxO/fBSjS0maQljYTt3scFotcO4njn9wpiOOe1ZqK1zsZr3dym/Faa0KhMgKB0lgGn9tcDhEI7KOubjX19Z8QDlcRiTQQidQTCh3E59tGdfUKwuH2e7lzOAagdZD9+x8DwGJxk5IyDIdjQCyo9Mduz8Jmy8Juz0IpJxClqVtVp3MALtcw7PaMw9YdClVSW/shtbUf4PNtIzPzTHJzL8Fm63q7NkJ0F7lTEEknEmkgFKoiEqklHK4lGvXhdBbgdBZitbrQWuP376S29kPq6j7C799FIFBKIFBKMLgfOHK/2jZbBg5HPlqHiUaDRKMBQqEDsakW7PZcQqEDKOUkJ+cC0tNPIRoNEI36iEb9OBwDcLvH4HaPxuHIjxXu78Ln2xkrrHfFBifRaIBIpIFotAFQbe6IHI78w+50TBnQOqLREGlp0yUoJQm5UxCiHVZraoflDUopUlKGkZIyjLy8K9pM0zpKJFJHKFRFOHyQaDQYqz1lAaIEAiX4/Tvx+XYQCpWjlB2l7FgsdlyuYaSlnYzXOw2rNZW6uo85cOBvlJU9S3n54lbbt6F1uLv2FqezAJerEJstk4aGjfj921vvLW73WNLSvoTLNQS7PRe7PQebLaNVrTBFJFJHMLifYPAAoVAFFksKNlsGNls6dnsWDkd+8wBWotHGWG0zf5vU2GxpOBz9m+/stI5QW/shlZWv0ti4Ga93Gunpp+D1Tsdq7VrfAoHAXurq1qB1CK93Bi5X++9gNF0IWK1pOBw5Xdru8S6hdwpKqbnAnwAr8KjW+q5DpjuBJ4GpQCWwQGu9q6N1yp2CONFEo2HC4Wqs1hQsFhdgIRg8QGPjJhobNxMM7sPpHITLNZSUlKFYrR4iEXNHEY36sVhczYFO6wjB4N7YnU0JgUAxfv9u/P49hEIVuN2j8Xqn4PFMQSlr7HHWKurqPiIUquhUeq1WL9GoH61Dx7S/TY/knM6B1NWtjm3Xiss1GL9/B2ACo93eD1Cx7wqrNR27PRu7PRur1RsLWoqmgKx1GK3DhMO11Nd/QjC4t812HY4BeL3TcTr7N5c1RaMB6upWU1e3OlZOpfB6p5OdfS6ZmV/FavXS9HhQ60jzNrQOxR5N1hGJ1BKNBrDZMrHbc7Dbs4lEGmhoWE9Dw3p8vq2kpIwkM/NMMjNPx27Pjv3fQ4RCFUSjjbF1m6GpBp+pxt2A378Dn28nfv8OMjJOJzf34mM67r1eJVUpZQW+AL4KlAAfA1dorTe2mucGYKLW+jtKqcuBi7XWCzparwQFIRIjGg0SClUSClXEyl00YBpJs1o9sTuBflgsDrTWRKN+wuEawuFKgsH9BAL7Yo/XNFarG4vFHQtyLe+dhMOV+HzbY5UHdpOaOp7s7PPJypqL3Z5BKHSQ2tpV1NS8RzBY3jp1hMPVsfRVEonUtUpftDkTVcqGxZKCxzMRr3caXu/0WPD7KPbG/prY8vVEoz7AisczITbvNILBMg4eXEJt7Ye0rvl2rGy2LFJSTqKxcVMszQqns5BwuLq5xYDOslq9FBb+jMGDf3FMaekLQeFk4Dat9dmx7z8H0Frf2WqepbF5VimlbMB+IFd3kCgJCkKI7mCuzKNYLIe31xQMVlBb+2+i0VAs4JhHhOZxoAk+5u7Mi81marCZoFVBMFiOxeIiNXUcDkc+Simi0RB1daupqnqbxsbN2O1ZsbuKXKzW1Oagduhdj8XiwuUahss1FLs9u0svdvaFMoUCoLjV9xLgS+3No7UOK6VqgGygzX2sUup64HqAwsLCRKVXCJFEmh7TxONw5HT4nk08dns2KSnD406zWOykp59MevrJR53OnnZcdOirtV6ktZ6mtZ6WK91DCiFEwiQyKJQCg1p9HxgbF3ee2OOjdEyBsxBCiF6QyKDwMTBCKTVUKeUALgf+ccg8/wCujn2+FPhXR+UJQgghEithZQqxMoLvAUsxVVIf01pvUEr9BtMt3D+AvwJPKaW2AQcxgUMIIUQvSejLa1rrJcCSQ8bd2uqzH5ifyDQIIYTovOOioFkIIUTPkKAghBCimQQFIYQQzY67VlKVUuXA7mNcPIdDXowTh5Fj1DE5Pkcmx6hjvXV8Bmutj/ii13EXFLpCKbW6M695JzM5Rh2T43Nkcow61tePjzw+EkII0UyCghBCiGbJFhQW9XYCjgNyjDomx+fI5Bh1rE8fn6QqUxBCCNGxZLtTEEII0YGkCQpKqblKqS1KqW1KqYW9nZ7eppQapJRarpTaqJTaoJT6YWx8llJqmVJqa+xvZm+ntbcppaxKqU+UUq/Fvg9VSn0YO5eeizX4mJSUUhlKqcVKqc1KqU1KqZPlHGpLKfWj2G9svVLqGaWUqy+fQ0kRFGJdgz4InAOMBa5QSo3t3VT1ujDwE631WGAmcGPsmCwE3tZajwDejn1Pdj8ENrX6/nvgj1rrk4Aq4Nu9kqq+4U/AG1rr0cAkzHGScyhGKVUA/ACYprUej2kc9HL68DmUFEEBmAFs01rv0FoHgWeBo+tW6QSjtd6ntV4b+1yH+TEXYI7LE7HZngAu6p0U9g1KqYHAecCjse8K+AqwODZL0h4jpVQ6MBvT2jFa6/dLikMAAAPCSURBVKDWuho5hw5lA1Jifca4gX304XMoWYJCvK5BC3opLX2OUmoIMBn4EMjTWu+LTdoP5PVSsvqK+4D/AqKx79lAtdY6HPuezOfSUKAceDz2eO1RpVQqcg4101qXAvcAezDBoAZYQx8+h5IlKIh2KKU8wAvATVrr2tbTYh0eJW31NKXU+UCZ1npNb6elj7IBU4C/aK0nAw0c8qhIziGViblzGgoMAFKBub2aqCNIlqDQma5Bk45Syo4JCE9rrV+MjT6glOofm94fKOut9PUBs4B5SqldmEeOX8E8Q8+IPQqA5D6XSoASrfWHse+LMUFCzqEWZwI7tdblWusQ8CLmvOqz51CyBIXOdA2aVGLPxv8KbNJa39tqUusuUq8GXunptPUVWuufa60Haq2HYM6Zf2mtrwSWY7qPhSQ+Rlrr/UCxUmpUbNQZwEbkHGptDzBTKeWO/eaajlGfPYeS5uU1pdS5mOfDTV2D/q6Xk9SrlFKnAO8Cn9PyvPwWTLnC80AhpjXa/9/e3bvcGMdxHH9/JCJKisVAWKS4SxlIKavB4KE8DMpmMSgpEv+ASTESgxS7GO4yCHkajCaTgZSBxNfw+50rburWXe771P1+bed3fufquuo653M9nOv7PVhVH+ZkJcdIkt3A6aram2Q97cxhJfACOFpVX+dy/eZKkgnaTfhFwFvgOO1g032oS3IROET7x98L4ATtHsJY7kPzJhQkSdObL5ePJEn/wFCQJA0MBUnSwFCQJA0MBUnSwFCQZlGS3aNqq9I4MhQkSQNDQfqLJEeTPEnyMsm13lPhc5LLvTb+wySr+tyJJI+TvE5yb9Q/IMnGJA+SvEryPMmGvvhlv/QguNWfdJXGgqEgTZFkE+0J1J1VNQF8B47Qipk9q6rNwCRwoX/kBnCmqrbQnhAfjd8CrlTVVmAHrUomtIq0p2i9PdbTauFIY2Hh9FOkeWcPsA142g/il9CKuv0Abvc5N4G7vafAiqqa7OPXgTtJlgNrquoeQFV9AejLe1JV7/rrl8A64NH/3yxpeoaC9KcA16vq7G+Dyfkp82ZaI+bXGjff8XuoMeLlI+lPD4H9SVbD0Ld6Le37MqpseRh4VFWfgI9JdvXxY8Bk72b3Lsm+vozFSZbO6lZIM+ARijRFVb1Jcg64n2QB8A04SWsis72/95523wFa6eOr/Ud/VCkUWkBcS3KpL+PALG6GNCNWSZX+UZLPVbVsrtdD+p+8fCRJGnimIEkaeKYgSRoYCpKkgaEgSRoYCpKkgaEgSRoYCpKkwU+x8P1ROy0EXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1711 - acc: 0.9487\n",
      "Loss: 0.17107631326696582 Accuracy: 0.948702\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0771 - acc: 0.3923\n",
      "Epoch 00001: val_loss improved from inf to 0.85990, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/001-0.8599.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 2.0771 - acc: 0.3923 - val_loss: 0.8599 - val_acc: 0.7414\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9626 - acc: 0.6977\n",
      "Epoch 00002: val_loss improved from 0.85990 to 0.47734, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/002-0.4773.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.9625 - acc: 0.6977 - val_loss: 0.4773 - val_acc: 0.8637\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6431 - acc: 0.7985\n",
      "Epoch 00003: val_loss improved from 0.47734 to 0.34233, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/003-0.3423.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.6432 - acc: 0.7985 - val_loss: 0.3423 - val_acc: 0.8991\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8462\n",
      "Epoch 00004: val_loss improved from 0.34233 to 0.28617, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/004-0.2862.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.4971 - acc: 0.8462 - val_loss: 0.2862 - val_acc: 0.9166\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8747\n",
      "Epoch 00005: val_loss improved from 0.28617 to 0.25923, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/005-0.2592.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.4029 - acc: 0.8747 - val_loss: 0.2592 - val_acc: 0.9220\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8944\n",
      "Epoch 00006: val_loss improved from 0.25923 to 0.22745, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/006-0.2275.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.3412 - acc: 0.8944 - val_loss: 0.2275 - val_acc: 0.9334\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9077\n",
      "Epoch 00007: val_loss improved from 0.22745 to 0.20626, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/007-0.2063.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.2939 - acc: 0.9076 - val_loss: 0.2063 - val_acc: 0.9390\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9179\n",
      "Epoch 00008: val_loss improved from 0.20626 to 0.19091, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/008-0.1909.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.2605 - acc: 0.9179 - val_loss: 0.1909 - val_acc: 0.9441\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9276\n",
      "Epoch 00009: val_loss did not improve from 0.19091\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.2328 - acc: 0.9276 - val_loss: 0.2250 - val_acc: 0.9327\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9338\n",
      "Epoch 00010: val_loss improved from 0.19091 to 0.16830, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/010-0.1683.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.2114 - acc: 0.9338 - val_loss: 0.1683 - val_acc: 0.9506\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9410\n",
      "Epoch 00011: val_loss did not improve from 0.16830\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1901 - acc: 0.9410 - val_loss: 0.1704 - val_acc: 0.9476\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9432\n",
      "Epoch 00012: val_loss did not improve from 0.16830\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1807 - acc: 0.9432 - val_loss: 0.1710 - val_acc: 0.9497\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9469\n",
      "Epoch 00013: val_loss improved from 0.16830 to 0.16207, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/013-0.1621.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1647 - acc: 0.9469 - val_loss: 0.1621 - val_acc: 0.9490\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9520\n",
      "Epoch 00014: val_loss improved from 0.16207 to 0.15335, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/014-0.1533.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1492 - acc: 0.9520 - val_loss: 0.1533 - val_acc: 0.9564\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9582\n",
      "Epoch 00015: val_loss improved from 0.15335 to 0.14134, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/015-0.1413.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1319 - acc: 0.9582 - val_loss: 0.1413 - val_acc: 0.9585\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9609\n",
      "Epoch 00016: val_loss improved from 0.14134 to 0.12943, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/016-0.1294.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1221 - acc: 0.9609 - val_loss: 0.1294 - val_acc: 0.9611\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9620\n",
      "Epoch 00017: val_loss did not improve from 0.12943\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1190 - acc: 0.9620 - val_loss: 0.1512 - val_acc: 0.9553\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9664\n",
      "Epoch 00018: val_loss did not improve from 0.12943\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1067 - acc: 0.9664 - val_loss: 0.1419 - val_acc: 0.9595\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9700\n",
      "Epoch 00019: val_loss did not improve from 0.12943\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0987 - acc: 0.9699 - val_loss: 0.1574 - val_acc: 0.9574\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9677\n",
      "Epoch 00020: val_loss improved from 0.12943 to 0.12344, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/020-0.1234.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1009 - acc: 0.9677 - val_loss: 0.1234 - val_acc: 0.9651\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9741\n",
      "Epoch 00021: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0846 - acc: 0.9740 - val_loss: 0.1547 - val_acc: 0.9536\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9718\n",
      "Epoch 00022: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0913 - acc: 0.9718 - val_loss: 0.1289 - val_acc: 0.9630\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9734\n",
      "Epoch 00023: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0825 - acc: 0.9734 - val_loss: 0.1416 - val_acc: 0.9620\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9790\n",
      "Epoch 00024: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0678 - acc: 0.9791 - val_loss: 0.1445 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9798\n",
      "Epoch 00025: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0666 - acc: 0.9797 - val_loss: 0.1653 - val_acc: 0.9506\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9756\n",
      "Epoch 00026: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0770 - acc: 0.9756 - val_loss: 0.1356 - val_acc: 0.9595\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9805\n",
      "Epoch 00027: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0620 - acc: 0.9805 - val_loss: 0.1415 - val_acc: 0.9613\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9817\n",
      "Epoch 00028: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0613 - acc: 0.9817 - val_loss: 0.1368 - val_acc: 0.9588\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9817\n",
      "Epoch 00029: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0583 - acc: 0.9817 - val_loss: 0.1462 - val_acc: 0.9609\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9847\n",
      "Epoch 00030: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0500 - acc: 0.9847 - val_loss: 0.1264 - val_acc: 0.9634\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9853\n",
      "Epoch 00031: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0484 - acc: 0.9853 - val_loss: 0.1339 - val_acc: 0.9623\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9869\n",
      "Epoch 00032: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0439 - acc: 0.9869 - val_loss: 0.1404 - val_acc: 0.9634\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9872\n",
      "Epoch 00033: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0425 - acc: 0.9872 - val_loss: 0.1318 - val_acc: 0.9651\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9868\n",
      "Epoch 00034: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0431 - acc: 0.9868 - val_loss: 0.1487 - val_acc: 0.9611\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9894\n",
      "Epoch 00035: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0359 - acc: 0.9894 - val_loss: 0.1828 - val_acc: 0.9546\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9882\n",
      "Epoch 00036: val_loss did not improve from 0.12344\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0377 - acc: 0.9882 - val_loss: 0.1951 - val_acc: 0.9546\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9859\n",
      "Epoch 00037: val_loss improved from 0.12344 to 0.11990, saving model to model/checkpoint/1D_CNN_custom_he-uniform_DO_BN_9_conv_checkpoint/037-0.1199.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0442 - acc: 0.9859 - val_loss: 0.1199 - val_acc: 0.9667\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9911\n",
      "Epoch 00038: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0301 - acc: 0.9911 - val_loss: 0.1272 - val_acc: 0.9634\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9899\n",
      "Epoch 00039: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0332 - acc: 0.9899 - val_loss: 0.1694 - val_acc: 0.9567\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9906\n",
      "Epoch 00040: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0312 - acc: 0.9906 - val_loss: 0.2150 - val_acc: 0.9441\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9908\n",
      "Epoch 00041: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.1637 - val_acc: 0.9595\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9927\n",
      "Epoch 00042: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0249 - acc: 0.9927 - val_loss: 0.1426 - val_acc: 0.9613\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 00043: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.1682 - val_acc: 0.9576\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9935\n",
      "Epoch 00044: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0233 - acc: 0.9935 - val_loss: 0.1598 - val_acc: 0.9599\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9905\n",
      "Epoch 00045: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0293 - acc: 0.9905 - val_loss: 0.1485 - val_acc: 0.9625\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9930\n",
      "Epoch 00046: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0239 - acc: 0.9930 - val_loss: 0.1842 - val_acc: 0.9571\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9929\n",
      "Epoch 00047: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0231 - acc: 0.9929 - val_loss: 0.1650 - val_acc: 0.9606\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9921\n",
      "Epoch 00048: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0252 - acc: 0.9921 - val_loss: 0.1596 - val_acc: 0.9611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9937\n",
      "Epoch 00049: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0212 - acc: 0.9937 - val_loss: 0.1523 - val_acc: 0.9648\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9944\n",
      "Epoch 00050: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.1838 - val_acc: 0.9550\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 0.0305 - acc: 0.9908 - val_loss: 0.2263 - val_acc: 0.9511\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9917\n",
      "Epoch 00052: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0272 - acc: 0.9917 - val_loss: 0.1723 - val_acc: 0.9618\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9960\n",
      "Epoch 00053: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0149 - acc: 0.9960 - val_loss: 0.1838 - val_acc: 0.9595\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0160 - acc: 0.9955 - val_loss: 0.1463 - val_acc: 0.9655\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00055: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0164 - acc: 0.9954 - val_loss: 0.2474 - val_acc: 0.9495\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9950\n",
      "Epoch 00056: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0177 - acc: 0.9950 - val_loss: 0.2080 - val_acc: 0.9548\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9894\n",
      "Epoch 00057: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.0343 - acc: 0.9894 - val_loss: 0.1354 - val_acc: 0.9674\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9962\n",
      "Epoch 00058: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0137 - acc: 0.9963 - val_loss: 0.1235 - val_acc: 0.9669\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9951\n",
      "Epoch 00059: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0162 - acc: 0.9951 - val_loss: 0.1565 - val_acc: 0.9632\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 00060: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0145 - acc: 0.9958 - val_loss: 0.1761 - val_acc: 0.9606\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9933\n",
      "Epoch 00061: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.1464 - val_acc: 0.9646\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9945\n",
      "Epoch 00062: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0195 - acc: 0.9945 - val_loss: 0.1557 - val_acc: 0.9655\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.1643 - val_acc: 0.9618\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9960\n",
      "Epoch 00064: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0148 - acc: 0.9960 - val_loss: 0.1662 - val_acc: 0.9627\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0146 - acc: 0.9959 - val_loss: 0.1743 - val_acc: 0.9576\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9907\n",
      "Epoch 00066: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0287 - acc: 0.9907 - val_loss: 0.1459 - val_acc: 0.9646\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00067: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0106 - acc: 0.9970 - val_loss: 0.1801 - val_acc: 0.9623\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9951\n",
      "Epoch 00068: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 0.1535 - val_acc: 0.9681\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9970\n",
      "Epoch 00069: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0108 - acc: 0.9970 - val_loss: 0.1512 - val_acc: 0.9662\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9963\n",
      "Epoch 00070: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0127 - acc: 0.9963 - val_loss: 0.1669 - val_acc: 0.9618\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9964\n",
      "Epoch 00071: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0122 - acc: 0.9964 - val_loss: 0.1528 - val_acc: 0.9648\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9962\n",
      "Epoch 00072: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0126 - acc: 0.9962 - val_loss: 0.1636 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 00073: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0137 - acc: 0.9959 - val_loss: 0.1813 - val_acc: 0.9606\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9958\n",
      "Epoch 00074: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.1605 - val_acc: 0.9653\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9968\n",
      "Epoch 00075: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0107 - acc: 0.9968 - val_loss: 0.1530 - val_acc: 0.9639\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9983\n",
      "Epoch 00076: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0077 - acc: 0.9983 - val_loss: 0.1616 - val_acc: 0.9646\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9943\n",
      "Epoch 00077: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.1742 - val_acc: 0.9620\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9930\n",
      "Epoch 00078: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0229 - acc: 0.9930 - val_loss: 0.1359 - val_acc: 0.9672\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9954\n",
      "Epoch 00079: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0158 - acc: 0.9954 - val_loss: 0.1567 - val_acc: 0.9669\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 00080: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.1454 - val_acc: 0.9688\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9986\n",
      "Epoch 00081: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0069 - acc: 0.9986 - val_loss: 0.1910 - val_acc: 0.9641\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 00082: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0135 - acc: 0.9960 - val_loss: 0.1792 - val_acc: 0.9644\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9964\n",
      "Epoch 00083: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.1748 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9938\n",
      "Epoch 00084: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0209 - acc: 0.9938 - val_loss: 0.1595 - val_acc: 0.9660\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 00085: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0163 - acc: 0.9950 - val_loss: 0.1364 - val_acc: 0.9697\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 00086: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0060 - acc: 0.9985 - val_loss: 0.1498 - val_acc: 0.9653\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 00087: val_loss did not improve from 0.11990\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0098 - acc: 0.9974 - val_loss: 0.1490 - val_acc: 0.9690\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2010 - acc: 0.9464\n",
      "Loss: 0.20103070230681647 Accuracy: 0.94641745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_he-uniform_DO_BN'\n",
    "\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_3_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 943us/sample - loss: 1.5318 - acc: 0.5458\n",
      "Loss: 1.531757095991513 Accuracy: 0.54579437\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 810us/sample - loss: 0.9896 - acc: 0.7171\n",
      "Loss: 0.9895950038608856 Accuracy: 0.71713394\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 839us/sample - loss: 0.7483 - acc: 0.7832\n",
      "Loss: 0.7482678103422202 Accuracy: 0.78317755\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 894us/sample - loss: 0.4655 - acc: 0.8719\n",
      "Loss: 0.46552579213525647 Accuracy: 0.8718588\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 922us/sample - loss: 0.2775 - acc: 0.9259\n",
      "Loss: 0.277451082336816 Accuracy: 0.9258567\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_70 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 941us/sample - loss: 0.1711 - acc: 0.9487\n",
      "Loss: 0.17107631326696582 Accuracy: 0.948702\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_78 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 957us/sample - loss: 0.2010 - acc: 0.9464\n",
      "Loss: 0.20103070230681647 Accuracy: 0.94641745\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_he-uniform_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 903us/sample - loss: 2.3832 - acc: 0.6033\n",
      "Loss: 2.3831908380502482 Accuracy: 0.6033229\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 886us/sample - loss: 1.2962 - acc: 0.7329\n",
      "Loss: 1.2961588117811416 Accuracy: 0.73291796\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 995us/sample - loss: 0.8583 - acc: 0.8019\n",
      "Loss: 0.8583122308380507 Accuracy: 0.80186915\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4962 - acc: 0.8802\n",
      "Loss: 0.49623646093986984 Accuracy: 0.8801662\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3382 - acc: 0.9252\n",
      "Loss: 0.33818601191141534 Accuracy: 0.92523366\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_70 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1844 - acc: 0.9531\n",
      "Loss: 0.1844073685557269 Accuracy: 0.95306337\n",
      "\n",
      "1D_CNN_custom_he-uniform_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_78 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2094 - acc: 0.9529\n",
      "Loss: 0.2093510176634067 Accuracy: 0.95285565\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base)+'_last', 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
