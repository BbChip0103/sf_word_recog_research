{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,504\n",
      "Trainable params: 8,195,376\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,740,464\n",
      "Trainable params: 2,740,208\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 926,256\n",
      "Trainable params: 925,872\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5574 - acc: 0.2844\n",
      "Epoch 00001: val_loss improved from inf to 2.13498, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/001-2.1350.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.5576 - acc: 0.2844 - val_loss: 2.1350 - val_acc: 0.3301\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7624 - acc: 0.4623\n",
      "Epoch 00002: val_loss improved from 2.13498 to 1.47457, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/002-1.4746.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.7624 - acc: 0.4622 - val_loss: 1.4746 - val_acc: 0.5404\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4928 - acc: 0.5393\n",
      "Epoch 00003: val_loss improved from 1.47457 to 1.25697, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/003-1.2570.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.4928 - acc: 0.5394 - val_loss: 1.2570 - val_acc: 0.6157\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3232 - acc: 0.5890\n",
      "Epoch 00004: val_loss improved from 1.25697 to 1.19906, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/004-1.1991.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.3232 - acc: 0.5890 - val_loss: 1.1991 - val_acc: 0.6394\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2017 - acc: 0.6282\n",
      "Epoch 00005: val_loss did not improve from 1.19906\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.2017 - acc: 0.6283 - val_loss: 1.2011 - val_acc: 0.6264\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1032 - acc: 0.6540\n",
      "Epoch 00006: val_loss did not improve from 1.19906\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.1032 - acc: 0.6540 - val_loss: 1.2610 - val_acc: 0.6254\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0164 - acc: 0.6802\n",
      "Epoch 00007: val_loss improved from 1.19906 to 1.13526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/007-1.1353.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.0165 - acc: 0.6801 - val_loss: 1.1353 - val_acc: 0.6525\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9464 - acc: 0.7040\n",
      "Epoch 00008: val_loss did not improve from 1.13526\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9466 - acc: 0.7040 - val_loss: 1.1869 - val_acc: 0.6480\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8845 - acc: 0.7218\n",
      "Epoch 00009: val_loss did not improve from 1.13526\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.8845 - acc: 0.7218 - val_loss: 1.1445 - val_acc: 0.6629\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8256 - acc: 0.7400\n",
      "Epoch 00010: val_loss improved from 1.13526 to 1.12898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/010-1.1290.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.8255 - acc: 0.7400 - val_loss: 1.1290 - val_acc: 0.6643\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7543\n",
      "Epoch 00011: val_loss did not improve from 1.12898\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7846 - acc: 0.7543 - val_loss: 1.1375 - val_acc: 0.6634\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7683\n",
      "Epoch 00012: val_loss improved from 1.12898 to 1.11729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/012-1.1173.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7381 - acc: 0.7683 - val_loss: 1.1173 - val_acc: 0.6753\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6975 - acc: 0.7790\n",
      "Epoch 00013: val_loss improved from 1.11729 to 1.10720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/013-1.1072.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6978 - acc: 0.7789 - val_loss: 1.1072 - val_acc: 0.6809\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.7911\n",
      "Epoch 00014: val_loss improved from 1.10720 to 1.10185, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/014-1.1018.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6639 - acc: 0.7911 - val_loss: 1.1018 - val_acc: 0.6846\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.7981\n",
      "Epoch 00015: val_loss did not improve from 1.10185\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6327 - acc: 0.7980 - val_loss: 1.1914 - val_acc: 0.6543\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.8102\n",
      "Epoch 00016: val_loss did not improve from 1.10185\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6026 - acc: 0.8101 - val_loss: 1.1025 - val_acc: 0.6844\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5787 - acc: 0.8146\n",
      "Epoch 00017: val_loss did not improve from 1.10185\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5788 - acc: 0.8146 - val_loss: 1.1874 - val_acc: 0.6592\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.8252\n",
      "Epoch 00018: val_loss improved from 1.10185 to 1.10111, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv_checkpoint/018-1.1011.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5506 - acc: 0.8252 - val_loss: 1.1011 - val_acc: 0.6851\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8284\n",
      "Epoch 00019: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5352 - acc: 0.8284 - val_loss: 1.1049 - val_acc: 0.6937\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.8366\n",
      "Epoch 00020: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5104 - acc: 0.8366 - val_loss: 1.1026 - val_acc: 0.6953\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8426\n",
      "Epoch 00021: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4930 - acc: 0.8426 - val_loss: 1.1069 - val_acc: 0.6976\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8449\n",
      "Epoch 00022: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4817 - acc: 0.8449 - val_loss: 1.1445 - val_acc: 0.6790\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8538\n",
      "Epoch 00023: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4570 - acc: 0.8538 - val_loss: 1.1317 - val_acc: 0.6937\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.8571\n",
      "Epoch 00024: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4434 - acc: 0.8572 - val_loss: 1.2143 - val_acc: 0.6636\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8587\n",
      "Epoch 00025: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4386 - acc: 0.8587 - val_loss: 1.1278 - val_acc: 0.6937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8666\n",
      "Epoch 00026: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4137 - acc: 0.8666 - val_loss: 1.1839 - val_acc: 0.6844\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8676\n",
      "Epoch 00027: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4078 - acc: 0.8676 - val_loss: 1.1016 - val_acc: 0.7098\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8698\n",
      "Epoch 00028: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4009 - acc: 0.8698 - val_loss: 1.1638 - val_acc: 0.6925\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8714\n",
      "Epoch 00029: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3932 - acc: 0.8714 - val_loss: 1.1434 - val_acc: 0.7004\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8758\n",
      "Epoch 00030: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3832 - acc: 0.8758 - val_loss: 1.1268 - val_acc: 0.6988\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8819\n",
      "Epoch 00031: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3679 - acc: 0.8819 - val_loss: 1.1322 - val_acc: 0.7039\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8832\n",
      "Epoch 00032: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3623 - acc: 0.8832 - val_loss: 1.1922 - val_acc: 0.6867\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8844\n",
      "Epoch 00033: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3588 - acc: 0.8843 - val_loss: 1.1537 - val_acc: 0.6997\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8864\n",
      "Epoch 00034: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3451 - acc: 0.8863 - val_loss: 1.1729 - val_acc: 0.6981\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8889\n",
      "Epoch 00035: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3379 - acc: 0.8889 - val_loss: 1.1678 - val_acc: 0.6965\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8936\n",
      "Epoch 00036: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3288 - acc: 0.8935 - val_loss: 1.1508 - val_acc: 0.7126\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8932\n",
      "Epoch 00037: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3276 - acc: 0.8932 - val_loss: 1.1630 - val_acc: 0.7051\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8964\n",
      "Epoch 00038: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3167 - acc: 0.8964 - val_loss: 1.1221 - val_acc: 0.7156\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8974\n",
      "Epoch 00039: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3119 - acc: 0.8974 - val_loss: 1.1315 - val_acc: 0.7191\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8995\n",
      "Epoch 00040: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3087 - acc: 0.8994 - val_loss: 1.1667 - val_acc: 0.7011\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8992\n",
      "Epoch 00041: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3057 - acc: 0.8992 - val_loss: 1.2619 - val_acc: 0.6883\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9046\n",
      "Epoch 00042: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2939 - acc: 0.9046 - val_loss: 1.2181 - val_acc: 0.7023\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9054\n",
      "Epoch 00043: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2923 - acc: 0.9053 - val_loss: 1.2941 - val_acc: 0.6758\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9061\n",
      "Epoch 00044: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2894 - acc: 0.9061 - val_loss: 1.1367 - val_acc: 0.7181\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9077\n",
      "Epoch 00045: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2815 - acc: 0.9077 - val_loss: 1.2165 - val_acc: 0.6925\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9109\n",
      "Epoch 00046: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2760 - acc: 0.9109 - val_loss: 1.1845 - val_acc: 0.7133\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9123\n",
      "Epoch 00047: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2644 - acc: 0.9123 - val_loss: 1.2319 - val_acc: 0.7067\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9160\n",
      "Epoch 00048: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2583 - acc: 0.9160 - val_loss: 1.2689 - val_acc: 0.6995\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9144\n",
      "Epoch 00049: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2617 - acc: 0.9144 - val_loss: 1.2833 - val_acc: 0.6888\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9169\n",
      "Epoch 00050: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2544 - acc: 0.9168 - val_loss: 1.2010 - val_acc: 0.7093\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9161\n",
      "Epoch 00051: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2537 - acc: 0.9161 - val_loss: 1.3238 - val_acc: 0.6855\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9164\n",
      "Epoch 00052: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2518 - acc: 0.9164 - val_loss: 1.2824 - val_acc: 0.6935\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9200\n",
      "Epoch 00053: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2468 - acc: 0.9199 - val_loss: 1.2768 - val_acc: 0.7030\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9201\n",
      "Epoch 00054: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2418 - acc: 0.9201 - val_loss: 1.2809 - val_acc: 0.7037\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9205\n",
      "Epoch 00055: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2445 - acc: 0.9205 - val_loss: 1.4250 - val_acc: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9226\n",
      "Epoch 00056: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2378 - acc: 0.9225 - val_loss: 1.1695 - val_acc: 0.7242\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9240\n",
      "Epoch 00057: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2338 - acc: 0.9240 - val_loss: 1.2630 - val_acc: 0.7093\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9219\n",
      "Epoch 00058: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2393 - acc: 0.9219 - val_loss: 1.1750 - val_acc: 0.7174\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9270\n",
      "Epoch 00059: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2249 - acc: 0.9270 - val_loss: 1.2525 - val_acc: 0.7140\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9273\n",
      "Epoch 00060: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2229 - acc: 0.9273 - val_loss: 1.3243 - val_acc: 0.7021\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9283\n",
      "Epoch 00061: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2208 - acc: 0.9283 - val_loss: 1.2628 - val_acc: 0.7086\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9289\n",
      "Epoch 00062: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2183 - acc: 0.9288 - val_loss: 1.5597 - val_acc: 0.6667\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9303\n",
      "Epoch 00063: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2151 - acc: 0.9303 - val_loss: 1.3005 - val_acc: 0.7025\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9309\n",
      "Epoch 00064: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2148 - acc: 0.9309 - val_loss: 1.2302 - val_acc: 0.7207\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9310\n",
      "Epoch 00065: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2093 - acc: 0.9310 - val_loss: 1.2510 - val_acc: 0.7156\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9342\n",
      "Epoch 00066: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2079 - acc: 0.9342 - val_loss: 1.2348 - val_acc: 0.7216\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9348\n",
      "Epoch 00067: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2047 - acc: 0.9348 - val_loss: 1.2022 - val_acc: 0.7237\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9332\n",
      "Epoch 00068: val_loss did not improve from 1.10111\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.2117 - acc: 0.9332 - val_loss: 1.2722 - val_acc: 0.7095\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 858us/sample - loss: 1.2528 - acc: 0.6415\n",
      "Loss: 1.2527855815174425 Accuracy: 0.6415369\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3458 - acc: 0.3262\n",
      "Epoch 00001: val_loss improved from inf to 1.86573, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/001-1.8657.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 2.3457 - acc: 0.3262 - val_loss: 1.8657 - val_acc: 0.3799\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6285 - acc: 0.5033\n",
      "Epoch 00002: val_loss improved from 1.86573 to 1.44014, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/002-1.4401.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.6286 - acc: 0.5033 - val_loss: 1.4401 - val_acc: 0.5465\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3903 - acc: 0.5732\n",
      "Epoch 00003: val_loss improved from 1.44014 to 1.24682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/003-1.2468.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.3902 - acc: 0.5732 - val_loss: 1.2468 - val_acc: 0.6028\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2506 - acc: 0.6144\n",
      "Epoch 00004: val_loss improved from 1.24682 to 1.06216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/004-1.0622.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.2506 - acc: 0.6144 - val_loss: 1.0622 - val_acc: 0.6695\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1571 - acc: 0.6419\n",
      "Epoch 00005: val_loss did not improve from 1.06216\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.1570 - acc: 0.6419 - val_loss: 1.1187 - val_acc: 0.6625\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0581 - acc: 0.6699\n",
      "Epoch 00006: val_loss did not improve from 1.06216\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.0582 - acc: 0.6698 - val_loss: 1.3187 - val_acc: 0.5905\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9858 - acc: 0.6914\n",
      "Epoch 00007: val_loss did not improve from 1.06216\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.9859 - acc: 0.6913 - val_loss: 1.1055 - val_acc: 0.6627\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.7120\n",
      "Epoch 00008: val_loss improved from 1.06216 to 1.03050, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/008-1.0305.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.9241 - acc: 0.7120 - val_loss: 1.0305 - val_acc: 0.6860\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8663 - acc: 0.7303\n",
      "Epoch 00009: val_loss did not improve from 1.03050\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.8664 - acc: 0.7302 - val_loss: 1.2519 - val_acc: 0.6343\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7460\n",
      "Epoch 00010: val_loss did not improve from 1.03050\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.8146 - acc: 0.7460 - val_loss: 1.0550 - val_acc: 0.6825\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7627 - acc: 0.7618\n",
      "Epoch 00011: val_loss improved from 1.03050 to 1.03017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/011-1.0302.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.7626 - acc: 0.7619 - val_loss: 1.0302 - val_acc: 0.6965\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7336 - acc: 0.7689\n",
      "Epoch 00012: val_loss did not improve from 1.03017\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.7337 - acc: 0.7689 - val_loss: 1.0753 - val_acc: 0.6869\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7829\n",
      "Epoch 00013: val_loss improved from 1.03017 to 0.96817, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/013-0.9682.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6926 - acc: 0.7829 - val_loss: 0.9682 - val_acc: 0.7123\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6656 - acc: 0.7905\n",
      "Epoch 00014: val_loss did not improve from 0.96817\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6657 - acc: 0.7905 - val_loss: 0.9793 - val_acc: 0.7016\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8016\n",
      "Epoch 00015: val_loss improved from 0.96817 to 0.90974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/015-0.9097.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6313 - acc: 0.8016 - val_loss: 0.9097 - val_acc: 0.7342\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8132\n",
      "Epoch 00016: val_loss did not improve from 0.90974\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5982 - acc: 0.8131 - val_loss: 0.9809 - val_acc: 0.7025\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8155\n",
      "Epoch 00017: val_loss did not improve from 0.90974\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5867 - acc: 0.8155 - val_loss: 0.9577 - val_acc: 0.7268\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.8268\n",
      "Epoch 00018: val_loss did not improve from 0.90974\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5529 - acc: 0.8268 - val_loss: 1.0210 - val_acc: 0.6997\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5343 - acc: 0.8322\n",
      "Epoch 00019: val_loss did not improve from 0.90974\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5345 - acc: 0.8322 - val_loss: 0.9328 - val_acc: 0.7256\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8345\n",
      "Epoch 00020: val_loss improved from 0.90974 to 0.90384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/020-0.9038.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5214 - acc: 0.8345 - val_loss: 0.9038 - val_acc: 0.7407\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.8402\n",
      "Epoch 00021: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5049 - acc: 0.8403 - val_loss: 0.9880 - val_acc: 0.7177\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8436\n",
      "Epoch 00022: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4906 - acc: 0.8437 - val_loss: 1.4872 - val_acc: 0.5924\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8517\n",
      "Epoch 00023: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4650 - acc: 0.8517 - val_loss: 1.9000 - val_acc: 0.5399\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8528\n",
      "Epoch 00024: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4623 - acc: 0.8528 - val_loss: 0.9594 - val_acc: 0.7342\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8565\n",
      "Epoch 00025: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4469 - acc: 0.8565 - val_loss: 0.9128 - val_acc: 0.7361\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.8616\n",
      "Epoch 00026: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4354 - acc: 0.8616 - val_loss: 1.2797 - val_acc: 0.6431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.8624\n",
      "Epoch 00027: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4231 - acc: 0.8624 - val_loss: 1.1824 - val_acc: 0.6834\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8661\n",
      "Epoch 00028: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4173 - acc: 0.8661 - val_loss: 0.9334 - val_acc: 0.7356\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8699\n",
      "Epoch 00029: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4043 - acc: 0.8699 - val_loss: 0.9167 - val_acc: 0.7428\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8740\n",
      "Epoch 00030: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3908 - acc: 0.8740 - val_loss: 0.9730 - val_acc: 0.7384\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8767\n",
      "Epoch 00031: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3831 - acc: 0.8767 - val_loss: 0.9060 - val_acc: 0.7536\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8792\n",
      "Epoch 00032: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3716 - acc: 0.8793 - val_loss: 1.0770 - val_acc: 0.7116\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8807\n",
      "Epoch 00033: val_loss did not improve from 0.90384\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3689 - acc: 0.8806 - val_loss: 0.9807 - val_acc: 0.7235\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8849\n",
      "Epoch 00034: val_loss improved from 0.90384 to 0.90127, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/034-0.9013.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3598 - acc: 0.8849 - val_loss: 0.9013 - val_acc: 0.7559\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8833\n",
      "Epoch 00035: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3559 - acc: 0.8833 - val_loss: 0.9628 - val_acc: 0.7382\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8890\n",
      "Epoch 00036: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3404 - acc: 0.8890 - val_loss: 0.9338 - val_acc: 0.7496\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8878\n",
      "Epoch 00037: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3403 - acc: 0.8878 - val_loss: 0.9391 - val_acc: 0.7503\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8904\n",
      "Epoch 00038: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3388 - acc: 0.8904 - val_loss: 0.9521 - val_acc: 0.7405\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8951\n",
      "Epoch 00039: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3269 - acc: 0.8951 - val_loss: 0.9459 - val_acc: 0.7531\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8960\n",
      "Epoch 00040: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3203 - acc: 0.8959 - val_loss: 1.1086 - val_acc: 0.7128\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8960\n",
      "Epoch 00041: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3189 - acc: 0.8960 - val_loss: 1.0123 - val_acc: 0.7286\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.8995\n",
      "Epoch 00042: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3063 - acc: 0.8995 - val_loss: 0.9478 - val_acc: 0.7487\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.9002\n",
      "Epoch 00043: val_loss did not improve from 0.90127\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3048 - acc: 0.9002 - val_loss: 1.0537 - val_acc: 0.7263\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9022\n",
      "Epoch 00044: val_loss improved from 0.90127 to 0.89846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv_checkpoint/044-0.8985.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3021 - acc: 0.9022 - val_loss: 0.8985 - val_acc: 0.7605\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9027\n",
      "Epoch 00045: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2966 - acc: 0.9028 - val_loss: 0.9556 - val_acc: 0.7545\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9039\n",
      "Epoch 00046: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2947 - acc: 0.9039 - val_loss: 1.0113 - val_acc: 0.7340\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9053\n",
      "Epoch 00047: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2894 - acc: 0.9053 - val_loss: 1.5275 - val_acc: 0.6401\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9086\n",
      "Epoch 00048: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2814 - acc: 0.9085 - val_loss: 0.9975 - val_acc: 0.7426\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9088\n",
      "Epoch 00049: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2778 - acc: 0.9087 - val_loss: 0.9794 - val_acc: 0.7538\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9096\n",
      "Epoch 00050: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2728 - acc: 0.9095 - val_loss: 1.1008 - val_acc: 0.7247\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9121\n",
      "Epoch 00051: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2704 - acc: 0.9121 - val_loss: 1.4117 - val_acc: 0.6650\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9158\n",
      "Epoch 00052: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2600 - acc: 0.9158 - val_loss: 1.0339 - val_acc: 0.7419\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9140\n",
      "Epoch 00053: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2618 - acc: 0.9140 - val_loss: 1.0055 - val_acc: 0.7501\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9155\n",
      "Epoch 00054: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2598 - acc: 0.9155 - val_loss: 1.0664 - val_acc: 0.7249\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9154\n",
      "Epoch 00055: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2547 - acc: 0.9153 - val_loss: 0.9495 - val_acc: 0.7549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9184\n",
      "Epoch 00056: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2452 - acc: 0.9184 - val_loss: 0.9565 - val_acc: 0.7570\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9179\n",
      "Epoch 00057: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2534 - acc: 0.9179 - val_loss: 1.0069 - val_acc: 0.7524\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9185\n",
      "Epoch 00058: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2477 - acc: 0.9185 - val_loss: 1.1413 - val_acc: 0.7205\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9198\n",
      "Epoch 00059: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2426 - acc: 0.9198 - val_loss: 1.0515 - val_acc: 0.7463\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9192\n",
      "Epoch 00060: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2439 - acc: 0.9191 - val_loss: 0.9819 - val_acc: 0.7631\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9205\n",
      "Epoch 00061: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2408 - acc: 0.9205 - val_loss: 0.9453 - val_acc: 0.7680\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9219\n",
      "Epoch 00062: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2370 - acc: 0.9219 - val_loss: 0.9945 - val_acc: 0.7563\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9247\n",
      "Epoch 00063: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2289 - acc: 0.9247 - val_loss: 0.9853 - val_acc: 0.7584\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9279\n",
      "Epoch 00064: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2245 - acc: 0.9278 - val_loss: 1.6048 - val_acc: 0.6546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9232\n",
      "Epoch 00065: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2363 - acc: 0.9232 - val_loss: 1.0457 - val_acc: 0.7454\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9285\n",
      "Epoch 00066: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2207 - acc: 0.9284 - val_loss: 1.0719 - val_acc: 0.7454\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9276\n",
      "Epoch 00067: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2212 - acc: 0.9276 - val_loss: 1.0688 - val_acc: 0.7480\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9284\n",
      "Epoch 00068: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2201 - acc: 0.9284 - val_loss: 1.1095 - val_acc: 0.7340\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9265\n",
      "Epoch 00069: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2218 - acc: 0.9265 - val_loss: 1.0253 - val_acc: 0.7515\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9314\n",
      "Epoch 00070: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2125 - acc: 0.9314 - val_loss: 1.0966 - val_acc: 0.7480\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9312\n",
      "Epoch 00071: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2105 - acc: 0.9312 - val_loss: 1.0273 - val_acc: 0.7577\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9334\n",
      "Epoch 00072: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2080 - acc: 0.9334 - val_loss: 1.3385 - val_acc: 0.6965\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9319\n",
      "Epoch 00073: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2079 - acc: 0.9319 - val_loss: 1.1000 - val_acc: 0.7438\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9317\n",
      "Epoch 00074: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2054 - acc: 0.9317 - val_loss: 0.9664 - val_acc: 0.7710\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9328\n",
      "Epoch 00075: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2018 - acc: 0.9328 - val_loss: 0.9712 - val_acc: 0.7671\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9349\n",
      "Epoch 00076: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1937 - acc: 0.9348 - val_loss: 1.0653 - val_acc: 0.7522\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9327\n",
      "Epoch 00077: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2033 - acc: 0.9327 - val_loss: 1.0211 - val_acc: 0.7577\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9361\n",
      "Epoch 00078: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1962 - acc: 0.9361 - val_loss: 1.0914 - val_acc: 0.7442\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9355\n",
      "Epoch 00079: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1964 - acc: 0.9355 - val_loss: 1.0267 - val_acc: 0.7629\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9374\n",
      "Epoch 00080: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1905 - acc: 0.9374 - val_loss: 1.0956 - val_acc: 0.7468\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9375\n",
      "Epoch 00081: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1933 - acc: 0.9375 - val_loss: 1.1477 - val_acc: 0.7403\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9384\n",
      "Epoch 00082: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1917 - acc: 0.9383 - val_loss: 1.1401 - val_acc: 0.7512\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9370\n",
      "Epoch 00083: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1913 - acc: 0.9369 - val_loss: 1.0440 - val_acc: 0.7505\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9390\n",
      "Epoch 00084: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1844 - acc: 0.9390 - val_loss: 1.1503 - val_acc: 0.7454\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9392\n",
      "Epoch 00085: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1867 - acc: 0.9392 - val_loss: 1.0647 - val_acc: 0.7519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9368\n",
      "Epoch 00086: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1865 - acc: 0.9368 - val_loss: 1.0769 - val_acc: 0.7510\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9399\n",
      "Epoch 00087: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1840 - acc: 0.9399 - val_loss: 1.0116 - val_acc: 0.7664\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9415\n",
      "Epoch 00088: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1776 - acc: 0.9414 - val_loss: 1.0357 - val_acc: 0.7633\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9385\n",
      "Epoch 00089: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1855 - acc: 0.9385 - val_loss: 1.1161 - val_acc: 0.7454\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9395\n",
      "Epoch 00090: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1821 - acc: 0.9395 - val_loss: 1.0541 - val_acc: 0.7573\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9429\n",
      "Epoch 00091: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1742 - acc: 0.9429 - val_loss: 1.0486 - val_acc: 0.7694\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9423\n",
      "Epoch 00092: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1744 - acc: 0.9422 - val_loss: 1.1380 - val_acc: 0.7517\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9437\n",
      "Epoch 00093: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1751 - acc: 0.9437 - val_loss: 1.0468 - val_acc: 0.7619\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9453\n",
      "Epoch 00094: val_loss did not improve from 0.89846\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1664 - acc: 0.9453 - val_loss: 1.0605 - val_acc: 0.7591\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 846us/sample - loss: 1.0359 - acc: 0.7200\n",
      "Loss: 1.0359347320172398 Accuracy: 0.7200415\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5680 - acc: 0.2723\n",
      "Epoch 00001: val_loss improved from inf to 1.83169, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/001-1.8317.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 2.5680 - acc: 0.2722 - val_loss: 1.8317 - val_acc: 0.3823\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7681 - acc: 0.4553\n",
      "Epoch 00002: val_loss improved from 1.83169 to 1.55601, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/002-1.5560.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.7681 - acc: 0.4553 - val_loss: 1.5560 - val_acc: 0.5192\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4804 - acc: 0.5443\n",
      "Epoch 00003: val_loss improved from 1.55601 to 1.24828, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/003-1.2483.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.4803 - acc: 0.5444 - val_loss: 1.2483 - val_acc: 0.6084\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3378 - acc: 0.5861\n",
      "Epoch 00004: val_loss improved from 1.24828 to 1.15040, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/004-1.1504.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.3378 - acc: 0.5861 - val_loss: 1.1504 - val_acc: 0.6268\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2290 - acc: 0.6158\n",
      "Epoch 00005: val_loss improved from 1.15040 to 1.07503, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/005-1.0750.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.2290 - acc: 0.6158 - val_loss: 1.0750 - val_acc: 0.6678\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1546 - acc: 0.6417\n",
      "Epoch 00006: val_loss did not improve from 1.07503\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.1546 - acc: 0.6417 - val_loss: 1.0891 - val_acc: 0.6604\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0846 - acc: 0.6639\n",
      "Epoch 00007: val_loss improved from 1.07503 to 1.02090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/007-1.0209.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0846 - acc: 0.6639 - val_loss: 1.0209 - val_acc: 0.6739\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0274 - acc: 0.6804\n",
      "Epoch 00008: val_loss did not improve from 1.02090\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.0278 - acc: 0.6803 - val_loss: 1.1923 - val_acc: 0.6271\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9772 - acc: 0.6985\n",
      "Epoch 00009: val_loss improved from 1.02090 to 0.93533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/009-0.9353.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9773 - acc: 0.6985 - val_loss: 0.9353 - val_acc: 0.7121\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7127\n",
      "Epoch 00010: val_loss improved from 0.93533 to 0.90413, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/010-0.9041.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9355 - acc: 0.7127 - val_loss: 0.9041 - val_acc: 0.7298\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8903 - acc: 0.7224\n",
      "Epoch 00011: val_loss did not improve from 0.90413\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8906 - acc: 0.7224 - val_loss: 0.9129 - val_acc: 0.7226\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8609 - acc: 0.7321\n",
      "Epoch 00012: val_loss improved from 0.90413 to 0.85762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/012-0.8576.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8612 - acc: 0.7320 - val_loss: 0.8576 - val_acc: 0.7473\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7448\n",
      "Epoch 00013: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8316 - acc: 0.7447 - val_loss: 1.4562 - val_acc: 0.5758\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7499\n",
      "Epoch 00014: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8057 - acc: 0.7499 - val_loss: 0.8884 - val_acc: 0.7247\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7767 - acc: 0.7611\n",
      "Epoch 00015: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7770 - acc: 0.7610 - val_loss: 0.9228 - val_acc: 0.7347\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7690\n",
      "Epoch 00016: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7547 - acc: 0.7689 - val_loss: 0.9439 - val_acc: 0.7116\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7755\n",
      "Epoch 00017: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7341 - acc: 0.7755 - val_loss: 0.9283 - val_acc: 0.7386\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7787\n",
      "Epoch 00018: val_loss did not improve from 0.85762\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7201 - acc: 0.7787 - val_loss: 0.8773 - val_acc: 0.7340\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7843\n",
      "Epoch 00019: val_loss improved from 0.85762 to 0.84423, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/019-0.8442.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6958 - acc: 0.7843 - val_loss: 0.8442 - val_acc: 0.7531\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.7911\n",
      "Epoch 00020: val_loss improved from 0.84423 to 0.75739, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/020-0.7574.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6809 - acc: 0.7910 - val_loss: 0.7574 - val_acc: 0.7743\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.7940\n",
      "Epoch 00021: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6667 - acc: 0.7940 - val_loss: 0.7847 - val_acc: 0.7645\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.7989\n",
      "Epoch 00022: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6542 - acc: 0.7988 - val_loss: 0.8320 - val_acc: 0.7468\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.8005\n",
      "Epoch 00023: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6410 - acc: 0.8005 - val_loss: 0.9008 - val_acc: 0.7503\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8082\n",
      "Epoch 00024: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6242 - acc: 0.8082 - val_loss: 0.7929 - val_acc: 0.7626\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6152 - acc: 0.8099\n",
      "Epoch 00025: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6153 - acc: 0.8099 - val_loss: 0.7809 - val_acc: 0.7722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.8137\n",
      "Epoch 00026: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6013 - acc: 0.8136 - val_loss: 0.9217 - val_acc: 0.7328\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5914 - acc: 0.8163\n",
      "Epoch 00027: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5915 - acc: 0.8162 - val_loss: 0.7693 - val_acc: 0.7738\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5807 - acc: 0.8189\n",
      "Epoch 00028: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5807 - acc: 0.8189 - val_loss: 0.8045 - val_acc: 0.7717\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.8224\n",
      "Epoch 00029: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5767 - acc: 0.8224 - val_loss: 0.9669 - val_acc: 0.7093\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.8232\n",
      "Epoch 00030: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5708 - acc: 0.8232 - val_loss: 0.9481 - val_acc: 0.7221\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8279\n",
      "Epoch 00031: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5521 - acc: 0.8279 - val_loss: 0.7661 - val_acc: 0.7831\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8303\n",
      "Epoch 00032: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5428 - acc: 0.8303 - val_loss: 0.7870 - val_acc: 0.7743\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8323\n",
      "Epoch 00033: val_loss did not improve from 0.75739\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5435 - acc: 0.8323 - val_loss: 0.7688 - val_acc: 0.7745\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8356\n",
      "Epoch 00034: val_loss improved from 0.75739 to 0.75535, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/034-0.7554.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5285 - acc: 0.8355 - val_loss: 0.7554 - val_acc: 0.7843\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8382\n",
      "Epoch 00035: val_loss did not improve from 0.75535\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5210 - acc: 0.8382 - val_loss: 0.9112 - val_acc: 0.7431\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8396\n",
      "Epoch 00036: val_loss improved from 0.75535 to 0.74600, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/036-0.7460.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5133 - acc: 0.8396 - val_loss: 0.7460 - val_acc: 0.7892\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8419\n",
      "Epoch 00037: val_loss did not improve from 0.74600\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5120 - acc: 0.8418 - val_loss: 0.7667 - val_acc: 0.7808\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8429\n",
      "Epoch 00038: val_loss did not improve from 0.74600\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4983 - acc: 0.8429 - val_loss: 0.8669 - val_acc: 0.7556\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8468\n",
      "Epoch 00039: val_loss did not improve from 0.74600\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4937 - acc: 0.8468 - val_loss: 0.7971 - val_acc: 0.7761\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8494\n",
      "Epoch 00040: val_loss did not improve from 0.74600\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4861 - acc: 0.8494 - val_loss: 0.7951 - val_acc: 0.7724\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.8477\n",
      "Epoch 00041: val_loss improved from 0.74600 to 0.72927, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/041-0.7293.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4836 - acc: 0.8476 - val_loss: 0.7293 - val_acc: 0.7864\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8512\n",
      "Epoch 00042: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4747 - acc: 0.8512 - val_loss: 0.8225 - val_acc: 0.7640\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8551\n",
      "Epoch 00043: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4626 - acc: 0.8551 - val_loss: 0.7501 - val_acc: 0.7859\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8538\n",
      "Epoch 00044: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4630 - acc: 0.8538 - val_loss: 0.7360 - val_acc: 0.7915\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.8584\n",
      "Epoch 00045: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4558 - acc: 0.8584 - val_loss: 0.7965 - val_acc: 0.7710\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8563\n",
      "Epoch 00046: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4531 - acc: 0.8563 - val_loss: 0.7478 - val_acc: 0.7850\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8612\n",
      "Epoch 00047: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4444 - acc: 0.8612 - val_loss: 0.8568 - val_acc: 0.7491\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8617\n",
      "Epoch 00048: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4410 - acc: 0.8617 - val_loss: 0.7338 - val_acc: 0.7934\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.8610\n",
      "Epoch 00049: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4393 - acc: 0.8609 - val_loss: 0.8076 - val_acc: 0.7561\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8626\n",
      "Epoch 00050: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4319 - acc: 0.8627 - val_loss: 0.7319 - val_acc: 0.7929\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.8657\n",
      "Epoch 00051: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4210 - acc: 0.8656 - val_loss: 0.8896 - val_acc: 0.7598\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8658\n",
      "Epoch 00052: val_loss did not improve from 0.72927\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4226 - acc: 0.8658 - val_loss: 0.7946 - val_acc: 0.7810\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8663\n",
      "Epoch 00053: val_loss improved from 0.72927 to 0.71398, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv_checkpoint/053-0.7140.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4156 - acc: 0.8663 - val_loss: 0.7140 - val_acc: 0.7985\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8712\n",
      "Epoch 00054: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4055 - acc: 0.8712 - val_loss: 0.7796 - val_acc: 0.7869\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8695\n",
      "Epoch 00055: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4114 - acc: 0.8695 - val_loss: 0.7457 - val_acc: 0.7906\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8727\n",
      "Epoch 00056: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3988 - acc: 0.8727 - val_loss: 0.7357 - val_acc: 0.7941\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8730\n",
      "Epoch 00057: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3971 - acc: 0.8731 - val_loss: 0.7530 - val_acc: 0.7959\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8752\n",
      "Epoch 00058: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3889 - acc: 0.8752 - val_loss: 0.7342 - val_acc: 0.7934\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8764\n",
      "Epoch 00059: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3902 - acc: 0.8764 - val_loss: 0.7904 - val_acc: 0.7848\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8771\n",
      "Epoch 00060: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3854 - acc: 0.8771 - val_loss: 0.7634 - val_acc: 0.7838\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8775\n",
      "Epoch 00061: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3838 - acc: 0.8775 - val_loss: 0.7676 - val_acc: 0.7897\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8775\n",
      "Epoch 00062: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3792 - acc: 0.8775 - val_loss: 0.7625 - val_acc: 0.7969\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8791\n",
      "Epoch 00063: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3768 - acc: 0.8791 - val_loss: 0.7955 - val_acc: 0.7848\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8821\n",
      "Epoch 00064: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3719 - acc: 0.8821 - val_loss: 0.9135 - val_acc: 0.7475\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8856\n",
      "Epoch 00065: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3602 - acc: 0.8855 - val_loss: 0.7531 - val_acc: 0.7962\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8841\n",
      "Epoch 00066: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3642 - acc: 0.8840 - val_loss: 0.7738 - val_acc: 0.7932\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8826\n",
      "Epoch 00067: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3660 - acc: 0.8826 - val_loss: 0.7889 - val_acc: 0.7850\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8844\n",
      "Epoch 00068: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3618 - acc: 0.8843 - val_loss: 0.7383 - val_acc: 0.7952\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8875\n",
      "Epoch 00069: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3522 - acc: 0.8875 - val_loss: 0.7717 - val_acc: 0.7918\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8873\n",
      "Epoch 00070: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3504 - acc: 0.8873 - val_loss: 0.8196 - val_acc: 0.7838\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8874\n",
      "Epoch 00071: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3495 - acc: 0.8874 - val_loss: 0.7725 - val_acc: 0.7950\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8889\n",
      "Epoch 00072: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3419 - acc: 0.8890 - val_loss: 0.8071 - val_acc: 0.7887\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8908\n",
      "Epoch 00073: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3387 - acc: 0.8908 - val_loss: 0.8439 - val_acc: 0.7775\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8892\n",
      "Epoch 00074: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3421 - acc: 0.8892 - val_loss: 0.7733 - val_acc: 0.7878\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8910\n",
      "Epoch 00075: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3314 - acc: 0.8909 - val_loss: 0.7767 - val_acc: 0.7829\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8937\n",
      "Epoch 00076: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3306 - acc: 0.8937 - val_loss: 0.7386 - val_acc: 0.8020\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8926\n",
      "Epoch 00077: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3323 - acc: 0.8926 - val_loss: 0.7943 - val_acc: 0.7945\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8924\n",
      "Epoch 00078: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3301 - acc: 0.8924 - val_loss: 0.9223 - val_acc: 0.7510\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8952\n",
      "Epoch 00079: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3215 - acc: 0.8952 - val_loss: 0.9011 - val_acc: 0.7701\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8943\n",
      "Epoch 00080: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3256 - acc: 0.8944 - val_loss: 0.8415 - val_acc: 0.7759\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8984\n",
      "Epoch 00081: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3136 - acc: 0.8984 - val_loss: 0.7977 - val_acc: 0.7864\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8952\n",
      "Epoch 00082: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3215 - acc: 0.8953 - val_loss: 0.8120 - val_acc: 0.7890\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8973\n",
      "Epoch 00083: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3181 - acc: 0.8973 - val_loss: 0.7711 - val_acc: 0.7999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.8998\n",
      "Epoch 00084: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3096 - acc: 0.8997 - val_loss: 0.7914 - val_acc: 0.7943\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8980\n",
      "Epoch 00085: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3103 - acc: 0.8980 - val_loss: 0.7534 - val_acc: 0.8022\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.8992\n",
      "Epoch 00086: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3098 - acc: 0.8991 - val_loss: 0.8123 - val_acc: 0.7955\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9002\n",
      "Epoch 00087: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3062 - acc: 0.9002 - val_loss: 0.7863 - val_acc: 0.7901\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9039\n",
      "Epoch 00088: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2938 - acc: 0.9039 - val_loss: 1.0647 - val_acc: 0.7403\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9029\n",
      "Epoch 00089: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2999 - acc: 0.9029 - val_loss: 0.8208 - val_acc: 0.7838\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9037\n",
      "Epoch 00090: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2933 - acc: 0.9037 - val_loss: 0.7683 - val_acc: 0.7936\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9042\n",
      "Epoch 00091: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2924 - acc: 0.9042 - val_loss: 0.7641 - val_acc: 0.7957\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9044\n",
      "Epoch 00092: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2912 - acc: 0.9044 - val_loss: 0.8733 - val_acc: 0.7873\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9051\n",
      "Epoch 00093: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2919 - acc: 0.9050 - val_loss: 0.8855 - val_acc: 0.7717\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9055\n",
      "Epoch 00094: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2902 - acc: 0.9054 - val_loss: 0.8563 - val_acc: 0.7827\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9059\n",
      "Epoch 00095: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2886 - acc: 0.9058 - val_loss: 0.8916 - val_acc: 0.7766\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9080\n",
      "Epoch 00096: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2855 - acc: 0.9081 - val_loss: 0.8388 - val_acc: 0.7880\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9083\n",
      "Epoch 00097: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2819 - acc: 0.9083 - val_loss: 0.8399 - val_acc: 0.7887\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9085\n",
      "Epoch 00098: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2775 - acc: 0.9085 - val_loss: 0.8321 - val_acc: 0.7843\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9097\n",
      "Epoch 00099: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2741 - acc: 0.9097 - val_loss: 0.9534 - val_acc: 0.7629\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9110\n",
      "Epoch 00100: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2706 - acc: 0.9109 - val_loss: 0.8991 - val_acc: 0.7703\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9081\n",
      "Epoch 00101: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2800 - acc: 0.9081 - val_loss: 0.7731 - val_acc: 0.7999\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9084\n",
      "Epoch 00102: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2755 - acc: 0.9084 - val_loss: 0.8287 - val_acc: 0.7824\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9123\n",
      "Epoch 00103: val_loss did not improve from 0.71398\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2669 - acc: 0.9123 - val_loss: 0.8353 - val_acc: 0.7817\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 939us/sample - loss: 0.8208 - acc: 0.7657\n",
      "Loss: 0.8207525815795391 Accuracy: 0.7657321\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7542 - acc: 0.2239\n",
      "Epoch 00001: val_loss improved from inf to 1.77089, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/001-1.7709.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 2.7540 - acc: 0.2239 - val_loss: 1.7709 - val_acc: 0.4519\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8983 - acc: 0.4021\n",
      "Epoch 00002: val_loss improved from 1.77089 to 1.42526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/002-1.4253.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.8983 - acc: 0.4021 - val_loss: 1.4253 - val_acc: 0.5458\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5905 - acc: 0.4949\n",
      "Epoch 00003: val_loss improved from 1.42526 to 1.15726, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/003-1.1573.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.5906 - acc: 0.4949 - val_loss: 1.1573 - val_acc: 0.6371\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4027 - acc: 0.5568\n",
      "Epoch 00004: val_loss did not improve from 1.15726\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.4027 - acc: 0.5568 - val_loss: 1.2551 - val_acc: 0.6122\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2907 - acc: 0.5909\n",
      "Epoch 00005: val_loss improved from 1.15726 to 0.99474, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/005-0.9947.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.2907 - acc: 0.5909 - val_loss: 0.9947 - val_acc: 0.7009\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1898 - acc: 0.6230\n",
      "Epoch 00006: val_loss did not improve from 0.99474\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.1899 - acc: 0.6230 - val_loss: 1.0828 - val_acc: 0.6660\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1239 - acc: 0.6485\n",
      "Epoch 00007: val_loss improved from 0.99474 to 0.97932, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/007-0.9793.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.1240 - acc: 0.6484 - val_loss: 0.9793 - val_acc: 0.7018\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0597 - acc: 0.6685\n",
      "Epoch 00008: val_loss improved from 0.97932 to 0.95860, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/008-0.9586.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.0598 - acc: 0.6684 - val_loss: 0.9586 - val_acc: 0.7042\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0108 - acc: 0.6846\n",
      "Epoch 00009: val_loss improved from 0.95860 to 0.90525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/009-0.9053.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.0108 - acc: 0.6846 - val_loss: 0.9053 - val_acc: 0.7177\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9584 - acc: 0.7027\n",
      "Epoch 00010: val_loss improved from 0.90525 to 0.83895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/010-0.8390.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9592 - acc: 0.7026 - val_loss: 0.8390 - val_acc: 0.7559\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9189 - acc: 0.7185\n",
      "Epoch 00011: val_loss did not improve from 0.83895\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9190 - acc: 0.7185 - val_loss: 0.8836 - val_acc: 0.7386\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8902 - acc: 0.7246\n",
      "Epoch 00012: val_loss did not improve from 0.83895\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8902 - acc: 0.7246 - val_loss: 0.8603 - val_acc: 0.7533\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8528 - acc: 0.7353\n",
      "Epoch 00013: val_loss improved from 0.83895 to 0.83519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/013-0.8352.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8529 - acc: 0.7353 - val_loss: 0.8352 - val_acc: 0.7440\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7447\n",
      "Epoch 00014: val_loss did not improve from 0.83519\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8301 - acc: 0.7447 - val_loss: 0.8427 - val_acc: 0.7347\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7550\n",
      "Epoch 00015: val_loss improved from 0.83519 to 0.80713, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/015-0.8071.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7973 - acc: 0.7550 - val_loss: 0.8071 - val_acc: 0.7664\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7810 - acc: 0.7599\n",
      "Epoch 00016: val_loss did not improve from 0.80713\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7811 - acc: 0.7598 - val_loss: 0.8336 - val_acc: 0.7449\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7590 - acc: 0.7670\n",
      "Epoch 00017: val_loss did not improve from 0.80713\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7592 - acc: 0.7670 - val_loss: 0.9146 - val_acc: 0.7198\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7390 - acc: 0.7748\n",
      "Epoch 00018: val_loss did not improve from 0.80713\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7390 - acc: 0.7747 - val_loss: 0.8641 - val_acc: 0.7405\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7174 - acc: 0.7818\n",
      "Epoch 00019: val_loss improved from 0.80713 to 0.74883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/019-0.7488.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7176 - acc: 0.7817 - val_loss: 0.7488 - val_acc: 0.7750\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6983 - acc: 0.7856\n",
      "Epoch 00020: val_loss improved from 0.74883 to 0.72730, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/020-0.7273.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6986 - acc: 0.7856 - val_loss: 0.7273 - val_acc: 0.7866\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7905\n",
      "Epoch 00021: val_loss improved from 0.72730 to 0.66434, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/021-0.6643.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6829 - acc: 0.7905 - val_loss: 0.6643 - val_acc: 0.8078\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6728 - acc: 0.7959\n",
      "Epoch 00022: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6729 - acc: 0.7958 - val_loss: 0.6790 - val_acc: 0.8020\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6587 - acc: 0.7975\n",
      "Epoch 00023: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6586 - acc: 0.7975 - val_loss: 0.6952 - val_acc: 0.7901\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6406 - acc: 0.8042\n",
      "Epoch 00024: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6406 - acc: 0.8042 - val_loss: 0.7096 - val_acc: 0.7932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.8058\n",
      "Epoch 00025: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6362 - acc: 0.8057 - val_loss: 0.7055 - val_acc: 0.7980\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6259 - acc: 0.8086\n",
      "Epoch 00026: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6258 - acc: 0.8086 - val_loss: 0.6739 - val_acc: 0.8053\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.8137\n",
      "Epoch 00027: val_loss did not improve from 0.66434\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6097 - acc: 0.8137 - val_loss: 0.7003 - val_acc: 0.7925\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.8172\n",
      "Epoch 00028: val_loss improved from 0.66434 to 0.63486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/028-0.6349.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6026 - acc: 0.8172 - val_loss: 0.6349 - val_acc: 0.8123\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5918 - acc: 0.8183\n",
      "Epoch 00029: val_loss did not improve from 0.63486\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5918 - acc: 0.8183 - val_loss: 0.6762 - val_acc: 0.8020\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8259\n",
      "Epoch 00030: val_loss did not improve from 0.63486\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5751 - acc: 0.8259 - val_loss: 0.6780 - val_acc: 0.8078\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8283\n",
      "Epoch 00031: val_loss improved from 0.63486 to 0.62530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/031-0.6253.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5637 - acc: 0.8282 - val_loss: 0.6253 - val_acc: 0.8190\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8307\n",
      "Epoch 00032: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5584 - acc: 0.8307 - val_loss: 0.6476 - val_acc: 0.8109\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.8315\n",
      "Epoch 00033: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5554 - acc: 0.8315 - val_loss: 0.6700 - val_acc: 0.8046\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8326\n",
      "Epoch 00034: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5443 - acc: 0.8326 - val_loss: 0.6331 - val_acc: 0.8167\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8402\n",
      "Epoch 00035: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5262 - acc: 0.8401 - val_loss: 0.6494 - val_acc: 0.8255\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.8380\n",
      "Epoch 00036: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5254 - acc: 0.8380 - val_loss: 0.6660 - val_acc: 0.8104\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.8417\n",
      "Epoch 00037: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5178 - acc: 0.8417 - val_loss: 0.6448 - val_acc: 0.8085\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8443\n",
      "Epoch 00038: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5071 - acc: 0.8443 - val_loss: 0.8222 - val_acc: 0.7864\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8481\n",
      "Epoch 00039: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5022 - acc: 0.8481 - val_loss: 0.6443 - val_acc: 0.8206\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8475\n",
      "Epoch 00040: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4929 - acc: 0.8475 - val_loss: 0.6304 - val_acc: 0.8202\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4854 - acc: 0.8517\n",
      "Epoch 00041: val_loss did not improve from 0.62530\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4856 - acc: 0.8517 - val_loss: 0.6502 - val_acc: 0.8234\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8535\n",
      "Epoch 00042: val_loss improved from 0.62530 to 0.59917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/042-0.5992.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4785 - acc: 0.8535 - val_loss: 0.5992 - val_acc: 0.8265\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8567\n",
      "Epoch 00043: val_loss did not improve from 0.59917\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4698 - acc: 0.8567 - val_loss: 0.6262 - val_acc: 0.8155\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8576\n",
      "Epoch 00044: val_loss did not improve from 0.59917\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4668 - acc: 0.8577 - val_loss: 0.6677 - val_acc: 0.8164\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8602\n",
      "Epoch 00045: val_loss improved from 0.59917 to 0.58348, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/045-0.5835.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4626 - acc: 0.8602 - val_loss: 0.5835 - val_acc: 0.8334\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8601\n",
      "Epoch 00046: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4538 - acc: 0.8602 - val_loss: 0.6349 - val_acc: 0.8241\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8658\n",
      "Epoch 00047: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4397 - acc: 0.8658 - val_loss: 0.6234 - val_acc: 0.8286\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8616\n",
      "Epoch 00048: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4458 - acc: 0.8616 - val_loss: 0.6428 - val_acc: 0.8311\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8628\n",
      "Epoch 00049: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4384 - acc: 0.8628 - val_loss: 0.5907 - val_acc: 0.8374\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8691\n",
      "Epoch 00050: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4267 - acc: 0.8691 - val_loss: 0.6008 - val_acc: 0.8304\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8704\n",
      "Epoch 00051: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4275 - acc: 0.8704 - val_loss: 0.6230 - val_acc: 0.8211\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8679\n",
      "Epoch 00052: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4233 - acc: 0.8679 - val_loss: 0.6727 - val_acc: 0.8141\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8739\n",
      "Epoch 00053: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4113 - acc: 0.8739 - val_loss: 0.6432 - val_acc: 0.8244\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8751\n",
      "Epoch 00054: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4028 - acc: 0.8751 - val_loss: 0.6117 - val_acc: 0.8267\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8738\n",
      "Epoch 00055: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4086 - acc: 0.8738 - val_loss: 0.6137 - val_acc: 0.8330\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8758\n",
      "Epoch 00056: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4015 - acc: 0.8758 - val_loss: 0.6499 - val_acc: 0.8295\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8790\n",
      "Epoch 00057: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3911 - acc: 0.8790 - val_loss: 0.7209 - val_acc: 0.7964\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8805\n",
      "Epoch 00058: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3850 - acc: 0.8805 - val_loss: 0.5843 - val_acc: 0.8449\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8810\n",
      "Epoch 00059: val_loss did not improve from 0.58348\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3821 - acc: 0.8810 - val_loss: 0.6267 - val_acc: 0.8244\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8803\n",
      "Epoch 00060: val_loss improved from 0.58348 to 0.57983, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/060-0.5798.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3815 - acc: 0.8803 - val_loss: 0.5798 - val_acc: 0.8428\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8835\n",
      "Epoch 00061: val_loss improved from 0.57983 to 0.57393, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv_checkpoint/061-0.5739.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3769 - acc: 0.8835 - val_loss: 0.5739 - val_acc: 0.8435\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8816\n",
      "Epoch 00062: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3762 - acc: 0.8815 - val_loss: 0.6832 - val_acc: 0.8132\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8855\n",
      "Epoch 00063: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3729 - acc: 0.8855 - val_loss: 0.6521 - val_acc: 0.8181\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8857\n",
      "Epoch 00064: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3638 - acc: 0.8856 - val_loss: 0.5793 - val_acc: 0.8437\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8883\n",
      "Epoch 00065: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3562 - acc: 0.8883 - val_loss: 0.6142 - val_acc: 0.8432\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8886\n",
      "Epoch 00066: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3515 - acc: 0.8886 - val_loss: 0.6458 - val_acc: 0.8344\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8881\n",
      "Epoch 00067: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3571 - acc: 0.8881 - val_loss: 0.5897 - val_acc: 0.8397\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8896\n",
      "Epoch 00068: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3544 - acc: 0.8896 - val_loss: 0.6681 - val_acc: 0.8288\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8910\n",
      "Epoch 00069: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3441 - acc: 0.8910 - val_loss: 0.6756 - val_acc: 0.8199\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8919\n",
      "Epoch 00070: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3446 - acc: 0.8919 - val_loss: 0.6570 - val_acc: 0.8265\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8967\n",
      "Epoch 00071: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3321 - acc: 0.8967 - val_loss: 0.5884 - val_acc: 0.8407\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8937\n",
      "Epoch 00072: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3374 - acc: 0.8937 - val_loss: 0.6647 - val_acc: 0.8206\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8945\n",
      "Epoch 00073: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3319 - acc: 0.8945 - val_loss: 0.5843 - val_acc: 0.8425\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8972\n",
      "Epoch 00074: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3249 - acc: 0.8972 - val_loss: 0.6194 - val_acc: 0.8300\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8983\n",
      "Epoch 00075: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3221 - acc: 0.8983 - val_loss: 0.6260 - val_acc: 0.8381\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8979\n",
      "Epoch 00076: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3235 - acc: 0.8979 - val_loss: 0.6293 - val_acc: 0.8404\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9008\n",
      "Epoch 00077: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3138 - acc: 0.9008 - val_loss: 0.6507 - val_acc: 0.8402\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9014\n",
      "Epoch 00078: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3111 - acc: 0.9014 - val_loss: 0.6030 - val_acc: 0.8383\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.8997\n",
      "Epoch 00079: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3125 - acc: 0.8997 - val_loss: 0.6990 - val_acc: 0.8192\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9056\n",
      "Epoch 00080: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3058 - acc: 0.9056 - val_loss: 0.6586 - val_acc: 0.8316\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9040\n",
      "Epoch 00081: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3041 - acc: 0.9040 - val_loss: 0.6311 - val_acc: 0.8372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9064\n",
      "Epoch 00082: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2961 - acc: 0.9064 - val_loss: 0.7237 - val_acc: 0.8260\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9051\n",
      "Epoch 00083: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2938 - acc: 0.9051 - val_loss: 0.7124 - val_acc: 0.8206\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9041\n",
      "Epoch 00084: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2986 - acc: 0.9041 - val_loss: 0.6784 - val_acc: 0.8321\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9079\n",
      "Epoch 00085: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2911 - acc: 0.9079 - val_loss: 0.7569 - val_acc: 0.8097\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9089\n",
      "Epoch 00086: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2876 - acc: 0.9089 - val_loss: 0.6676 - val_acc: 0.8279\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9093\n",
      "Epoch 00087: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2875 - acc: 0.9093 - val_loss: 0.6765 - val_acc: 0.8297\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9098\n",
      "Epoch 00088: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2832 - acc: 0.9097 - val_loss: 0.6665 - val_acc: 0.8302\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9095\n",
      "Epoch 00089: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2863 - acc: 0.9094 - val_loss: 0.6402 - val_acc: 0.8383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9110\n",
      "Epoch 00090: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2760 - acc: 0.9110 - val_loss: 0.6395 - val_acc: 0.8372\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9143\n",
      "Epoch 00091: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2717 - acc: 0.9144 - val_loss: 0.6129 - val_acc: 0.8446\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9131\n",
      "Epoch 00092: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2737 - acc: 0.9131 - val_loss: 0.6219 - val_acc: 0.8421\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9127\n",
      "Epoch 00093: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2720 - acc: 0.9127 - val_loss: 0.6389 - val_acc: 0.8383\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9132\n",
      "Epoch 00094: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2693 - acc: 0.9132 - val_loss: 0.6283 - val_acc: 0.8334\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9158\n",
      "Epoch 00095: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2649 - acc: 0.9157 - val_loss: 0.6237 - val_acc: 0.8386\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9131\n",
      "Epoch 00096: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2714 - acc: 0.9130 - val_loss: 0.6739 - val_acc: 0.8393\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9172\n",
      "Epoch 00097: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2622 - acc: 0.9172 - val_loss: 0.6291 - val_acc: 0.8362\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9162\n",
      "Epoch 00098: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2596 - acc: 0.9162 - val_loss: 0.6938 - val_acc: 0.8302\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9170\n",
      "Epoch 00099: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2586 - acc: 0.9169 - val_loss: 0.6399 - val_acc: 0.8449\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9187\n",
      "Epoch 00100: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2540 - acc: 0.9187 - val_loss: 0.6158 - val_acc: 0.8388\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9201\n",
      "Epoch 00101: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2515 - acc: 0.9201 - val_loss: 0.6553 - val_acc: 0.8307\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9203\n",
      "Epoch 00102: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2458 - acc: 0.9203 - val_loss: 0.6972 - val_acc: 0.8169\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9187\n",
      "Epoch 00103: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2509 - acc: 0.9187 - val_loss: 0.6617 - val_acc: 0.8362\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9229\n",
      "Epoch 00104: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2414 - acc: 0.9229 - val_loss: 0.7334 - val_acc: 0.8192\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9187\n",
      "Epoch 00105: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2495 - acc: 0.9186 - val_loss: 0.6463 - val_acc: 0.8404\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9221\n",
      "Epoch 00106: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2431 - acc: 0.9221 - val_loss: 0.6351 - val_acc: 0.8451\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9240\n",
      "Epoch 00107: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2342 - acc: 0.9240 - val_loss: 0.6734 - val_acc: 0.8274\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9272\n",
      "Epoch 00108: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2309 - acc: 0.9272 - val_loss: 0.6024 - val_acc: 0.8477\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9233\n",
      "Epoch 00109: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2376 - acc: 0.9233 - val_loss: 0.7122 - val_acc: 0.8300\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9236\n",
      "Epoch 00110: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2356 - acc: 0.9236 - val_loss: 0.6795 - val_acc: 0.8316\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9242\n",
      "Epoch 00111: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2328 - acc: 0.9242 - val_loss: 0.6532 - val_acc: 0.8360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 931us/sample - loss: 0.6737 - acc: 0.8104\n",
      "Loss: 0.6737026598594642 Accuracy: 0.8103842\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6825 - acc: 0.2348\n",
      "Epoch 00001: val_loss improved from inf to 1.90843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/001-1.9084.hdf5\n",
      "36805/36805 [==============================] - 120s 3ms/sample - loss: 2.6826 - acc: 0.2348 - val_loss: 1.9084 - val_acc: 0.4265\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9271 - acc: 0.3925\n",
      "Epoch 00002: val_loss improved from 1.90843 to 1.34104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/002-1.3410.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.9271 - acc: 0.3924 - val_loss: 1.3410 - val_acc: 0.5882\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5870 - acc: 0.4957\n",
      "Epoch 00003: val_loss improved from 1.34104 to 1.08948, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/003-1.0895.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.5871 - acc: 0.4956 - val_loss: 1.0895 - val_acc: 0.6599\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3605 - acc: 0.5678\n",
      "Epoch 00004: val_loss improved from 1.08948 to 0.99894, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/004-0.9989.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.3606 - acc: 0.5678 - val_loss: 0.9989 - val_acc: 0.6865\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2067 - acc: 0.6126\n",
      "Epoch 00005: val_loss improved from 0.99894 to 0.88702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/005-0.8870.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.2069 - acc: 0.6126 - val_loss: 0.8870 - val_acc: 0.7289\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0891 - acc: 0.6556\n",
      "Epoch 00006: val_loss did not improve from 0.88702\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0891 - acc: 0.6556 - val_loss: 0.9248 - val_acc: 0.7284\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9929 - acc: 0.6877\n",
      "Epoch 00007: val_loss improved from 0.88702 to 0.76514, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/007-0.7651.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9932 - acc: 0.6876 - val_loss: 0.7651 - val_acc: 0.7671\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9065 - acc: 0.7170\n",
      "Epoch 00008: val_loss did not improve from 0.76514\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9066 - acc: 0.7170 - val_loss: 0.8012 - val_acc: 0.7624\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8368 - acc: 0.7391\n",
      "Epoch 00009: val_loss improved from 0.76514 to 0.61763, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/009-0.6176.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8369 - acc: 0.7391 - val_loss: 0.6176 - val_acc: 0.8325\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7762 - acc: 0.7640\n",
      "Epoch 00010: val_loss improved from 0.61763 to 0.56387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/010-0.5639.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7762 - acc: 0.7640 - val_loss: 0.5639 - val_acc: 0.8323\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7800\n",
      "Epoch 00011: val_loss improved from 0.56387 to 0.55914, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/011-0.5591.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7215 - acc: 0.7799 - val_loss: 0.5591 - val_acc: 0.8418\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6771 - acc: 0.7931\n",
      "Epoch 00012: val_loss improved from 0.55914 to 0.51086, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/012-0.5109.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6774 - acc: 0.7930 - val_loss: 0.5109 - val_acc: 0.8519\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.8065\n",
      "Epoch 00013: val_loss did not improve from 0.51086\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6376 - acc: 0.8065 - val_loss: 0.5420 - val_acc: 0.8390\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5920 - acc: 0.8206\n",
      "Epoch 00014: val_loss improved from 0.51086 to 0.45983, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/014-0.4598.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5919 - acc: 0.8206 - val_loss: 0.4598 - val_acc: 0.8777\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8287\n",
      "Epoch 00015: val_loss improved from 0.45983 to 0.45387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/015-0.4539.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5683 - acc: 0.8287 - val_loss: 0.4539 - val_acc: 0.8693\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8388\n",
      "Epoch 00016: val_loss improved from 0.45387 to 0.41336, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/016-0.4134.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5347 - acc: 0.8387 - val_loss: 0.4134 - val_acc: 0.8826\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8459\n",
      "Epoch 00017: val_loss improved from 0.41336 to 0.40479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/017-0.4048.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5077 - acc: 0.8458 - val_loss: 0.4048 - val_acc: 0.8887\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.8538\n",
      "Epoch 00018: val_loss improved from 0.40479 to 0.38994, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/018-0.3899.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4835 - acc: 0.8537 - val_loss: 0.3899 - val_acc: 0.8912\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.8593\n",
      "Epoch 00019: val_loss improved from 0.38994 to 0.37552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/019-0.3755.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4645 - acc: 0.8593 - val_loss: 0.3755 - val_acc: 0.8917\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.8678\n",
      "Epoch 00020: val_loss improved from 0.37552 to 0.36057, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/020-0.3606.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4416 - acc: 0.8678 - val_loss: 0.3606 - val_acc: 0.9012\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8691\n",
      "Epoch 00021: val_loss did not improve from 0.36057\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4295 - acc: 0.8691 - val_loss: 0.3796 - val_acc: 0.8942\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8764\n",
      "Epoch 00022: val_loss did not improve from 0.36057\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4072 - acc: 0.8764 - val_loss: 0.3727 - val_acc: 0.8970\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8799\n",
      "Epoch 00023: val_loss improved from 0.36057 to 0.35246, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/023-0.3525.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3951 - acc: 0.8799 - val_loss: 0.3525 - val_acc: 0.8973\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8842\n",
      "Epoch 00024: val_loss did not improve from 0.35246\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3832 - acc: 0.8842 - val_loss: 0.3625 - val_acc: 0.8991\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8874\n",
      "Epoch 00025: val_loss improved from 0.35246 to 0.30946, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/025-0.3095.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3711 - acc: 0.8875 - val_loss: 0.3095 - val_acc: 0.9161\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8909\n",
      "Epoch 00026: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3585 - acc: 0.8909 - val_loss: 0.3401 - val_acc: 0.9047\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8949\n",
      "Epoch 00027: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3492 - acc: 0.8949 - val_loss: 0.3363 - val_acc: 0.9043\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8971\n",
      "Epoch 00028: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3382 - acc: 0.8970 - val_loss: 0.3331 - val_acc: 0.9038\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8986\n",
      "Epoch 00029: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3312 - acc: 0.8986 - val_loss: 0.3279 - val_acc: 0.9113\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.9019\n",
      "Epoch 00030: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3221 - acc: 0.9018 - val_loss: 0.3157 - val_acc: 0.9119\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9031\n",
      "Epoch 00031: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3143 - acc: 0.9030 - val_loss: 0.3550 - val_acc: 0.8984\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.9046\n",
      "Epoch 00032: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3111 - acc: 0.9046 - val_loss: 0.3254 - val_acc: 0.9099\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9103\n",
      "Epoch 00033: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2936 - acc: 0.9103 - val_loss: 0.3100 - val_acc: 0.9140\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9116\n",
      "Epoch 00034: val_loss did not improve from 0.30946\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2905 - acc: 0.9115 - val_loss: 0.3437 - val_acc: 0.9036\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9134\n",
      "Epoch 00035: val_loss improved from 0.30946 to 0.29740, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/035-0.2974.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2834 - acc: 0.9134 - val_loss: 0.2974 - val_acc: 0.9175\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9151\n",
      "Epoch 00036: val_loss did not improve from 0.29740\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2749 - acc: 0.9151 - val_loss: 0.3003 - val_acc: 0.9150\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9181\n",
      "Epoch 00037: val_loss did not improve from 0.29740\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2660 - acc: 0.9181 - val_loss: 0.3201 - val_acc: 0.9138\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9185\n",
      "Epoch 00038: val_loss improved from 0.29740 to 0.29615, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/038-0.2962.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2616 - acc: 0.9184 - val_loss: 0.2962 - val_acc: 0.9143\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9179\n",
      "Epoch 00039: val_loss did not improve from 0.29615\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2667 - acc: 0.9179 - val_loss: 0.3028 - val_acc: 0.9106\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9210\n",
      "Epoch 00040: val_loss improved from 0.29615 to 0.29095, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/040-0.2910.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2574 - acc: 0.9209 - val_loss: 0.2910 - val_acc: 0.9168\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9237\n",
      "Epoch 00041: val_loss did not improve from 0.29095\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2438 - acc: 0.9237 - val_loss: 0.3023 - val_acc: 0.9164\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9240\n",
      "Epoch 00042: val_loss did not improve from 0.29095\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2435 - acc: 0.9240 - val_loss: 0.3161 - val_acc: 0.9173\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9262\n",
      "Epoch 00043: val_loss improved from 0.29095 to 0.27825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/043-0.2783.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2359 - acc: 0.9262 - val_loss: 0.2783 - val_acc: 0.9222\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9283\n",
      "Epoch 00044: val_loss improved from 0.27825 to 0.25666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv_checkpoint/044-0.2567.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2342 - acc: 0.9283 - val_loss: 0.2567 - val_acc: 0.9264\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9284\n",
      "Epoch 00045: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2264 - acc: 0.9283 - val_loss: 0.2675 - val_acc: 0.9252\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9280\n",
      "Epoch 00046: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2262 - acc: 0.9280 - val_loss: 0.3095 - val_acc: 0.9166\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9306\n",
      "Epoch 00047: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2217 - acc: 0.9306 - val_loss: 0.3179 - val_acc: 0.9161\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9343\n",
      "Epoch 00048: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2110 - acc: 0.9342 - val_loss: 0.2947 - val_acc: 0.9147\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9340\n",
      "Epoch 00049: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2125 - acc: 0.9340 - val_loss: 0.2791 - val_acc: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9362\n",
      "Epoch 00050: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2059 - acc: 0.9362 - val_loss: 0.2950 - val_acc: 0.9157\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9369\n",
      "Epoch 00051: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2015 - acc: 0.9369 - val_loss: 0.2696 - val_acc: 0.9234\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9353\n",
      "Epoch 00052: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2018 - acc: 0.9353 - val_loss: 0.3007 - val_acc: 0.9133\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9376\n",
      "Epoch 00053: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1984 - acc: 0.9376 - val_loss: 0.3038 - val_acc: 0.9164\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9401\n",
      "Epoch 00054: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1869 - acc: 0.9401 - val_loss: 0.2766 - val_acc: 0.9238\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9413\n",
      "Epoch 00055: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1822 - acc: 0.9413 - val_loss: 0.3072 - val_acc: 0.9171\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9434\n",
      "Epoch 00056: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1803 - acc: 0.9434 - val_loss: 0.2755 - val_acc: 0.9203\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9436\n",
      "Epoch 00057: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1788 - acc: 0.9436 - val_loss: 0.3184 - val_acc: 0.9178\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9430\n",
      "Epoch 00058: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1739 - acc: 0.9430 - val_loss: 0.3028 - val_acc: 0.9164\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9471\n",
      "Epoch 00059: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1699 - acc: 0.9471 - val_loss: 0.3020 - val_acc: 0.9196\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9477\n",
      "Epoch 00060: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1621 - acc: 0.9477 - val_loss: 0.2960 - val_acc: 0.9201\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9467\n",
      "Epoch 00061: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1646 - acc: 0.9467 - val_loss: 0.3302 - val_acc: 0.9119\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9482\n",
      "Epoch 00062: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1613 - acc: 0.9482 - val_loss: 0.3254 - val_acc: 0.9117\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9472\n",
      "Epoch 00063: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1623 - acc: 0.9472 - val_loss: 0.2863 - val_acc: 0.9185\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9493\n",
      "Epoch 00064: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1599 - acc: 0.9493 - val_loss: 0.3065 - val_acc: 0.9192\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9491\n",
      "Epoch 00065: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1573 - acc: 0.9491 - val_loss: 0.2982 - val_acc: 0.9178\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9504\n",
      "Epoch 00066: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1547 - acc: 0.9504 - val_loss: 0.3279 - val_acc: 0.9143\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9524\n",
      "Epoch 00067: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1482 - acc: 0.9524 - val_loss: 0.2986 - val_acc: 0.9145\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9517\n",
      "Epoch 00068: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1477 - acc: 0.9517 - val_loss: 0.2891 - val_acc: 0.9215\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9543\n",
      "Epoch 00069: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1419 - acc: 0.9543 - val_loss: 0.3028 - val_acc: 0.9185\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9556\n",
      "Epoch 00070: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1392 - acc: 0.9556 - val_loss: 0.3188 - val_acc: 0.9113\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9514\n",
      "Epoch 00071: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1438 - acc: 0.9514 - val_loss: 0.3093 - val_acc: 0.9189\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9555\n",
      "Epoch 00072: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1366 - acc: 0.9555 - val_loss: 0.2834 - val_acc: 0.9280\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9575\n",
      "Epoch 00073: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1303 - acc: 0.9575 - val_loss: 0.3130 - val_acc: 0.9224\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9569\n",
      "Epoch 00074: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1322 - acc: 0.9568 - val_loss: 0.2943 - val_acc: 0.9217\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9563\n",
      "Epoch 00075: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1354 - acc: 0.9562 - val_loss: 0.3372 - val_acc: 0.9106\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9573\n",
      "Epoch 00076: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1321 - acc: 0.9573 - val_loss: 0.3260 - val_acc: 0.9180\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9601\n",
      "Epoch 00077: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1262 - acc: 0.9601 - val_loss: 0.2990 - val_acc: 0.9189\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9605\n",
      "Epoch 00078: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1218 - acc: 0.9605 - val_loss: 0.3363 - val_acc: 0.9192\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9563\n",
      "Epoch 00079: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1329 - acc: 0.9562 - val_loss: 0.3368 - val_acc: 0.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9598\n",
      "Epoch 00080: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1235 - acc: 0.9597 - val_loss: 0.3024 - val_acc: 0.9227\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9608\n",
      "Epoch 00081: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1204 - acc: 0.9608 - val_loss: 0.3274 - val_acc: 0.9115\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9621\n",
      "Epoch 00082: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1169 - acc: 0.9621 - val_loss: 0.3416 - val_acc: 0.9157\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9552\n",
      "Epoch 00083: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1359 - acc: 0.9553 - val_loss: 0.3188 - val_acc: 0.9182\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9654\n",
      "Epoch 00084: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1086 - acc: 0.9654 - val_loss: 0.3135 - val_acc: 0.9231\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9638\n",
      "Epoch 00085: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1098 - acc: 0.9638 - val_loss: 0.3401 - val_acc: 0.9173\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9664\n",
      "Epoch 00086: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1071 - acc: 0.9664 - val_loss: 0.3188 - val_acc: 0.9194\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9657\n",
      "Epoch 00087: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1030 - acc: 0.9657 - val_loss: 0.3542 - val_acc: 0.9129\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9651\n",
      "Epoch 00088: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1110 - acc: 0.9650 - val_loss: 0.3983 - val_acc: 0.9106\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9611\n",
      "Epoch 00089: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1175 - acc: 0.9611 - val_loss: 0.3122 - val_acc: 0.9210\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9660\n",
      "Epoch 00090: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1058 - acc: 0.9660 - val_loss: 0.3372 - val_acc: 0.9231\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9668\n",
      "Epoch 00091: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1020 - acc: 0.9667 - val_loss: 0.3279 - val_acc: 0.9208\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9659\n",
      "Epoch 00092: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1032 - acc: 0.9659 - val_loss: 0.3258 - val_acc: 0.9187\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9679\n",
      "Epoch 00093: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0992 - acc: 0.9679 - val_loss: 0.3301 - val_acc: 0.9210\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9691\n",
      "Epoch 00094: val_loss did not improve from 0.25666\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0953 - acc: 0.9690 - val_loss: 0.3609 - val_acc: 0.9143\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3358 - acc: 0.9070\n",
      "Loss: 0.3357766230282135 Accuracy: 0.90695745\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4075 - acc: 0.3226\n",
      "Epoch 00001: val_loss improved from inf to 2.04603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/001-2.0460.hdf5\n",
      "36805/36805 [==============================] - 131s 4ms/sample - loss: 2.4079 - acc: 0.3226 - val_loss: 2.0460 - val_acc: 0.3431\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4159 - acc: 0.5649\n",
      "Epoch 00002: val_loss improved from 2.04603 to 0.83573, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/002-0.8357.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.4163 - acc: 0.5649 - val_loss: 0.8357 - val_acc: 0.7445\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6693\n",
      "Epoch 00003: val_loss improved from 0.83573 to 0.60420, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/003-0.6042.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0697 - acc: 0.6693 - val_loss: 0.6042 - val_acc: 0.8300\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7404\n",
      "Epoch 00004: val_loss improved from 0.60420 to 0.49364, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/004-0.4936.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8342 - acc: 0.7404 - val_loss: 0.4936 - val_acc: 0.8619\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.7868\n",
      "Epoch 00005: val_loss improved from 0.49364 to 0.45691, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/005-0.4569.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6923 - acc: 0.7868 - val_loss: 0.4569 - val_acc: 0.8684\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.8192\n",
      "Epoch 00006: val_loss improved from 0.45691 to 0.37078, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/006-0.3708.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5819 - acc: 0.8192 - val_loss: 0.3708 - val_acc: 0.8938\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8423\n",
      "Epoch 00007: val_loss improved from 0.37078 to 0.31646, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/007-0.3165.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5069 - acc: 0.8422 - val_loss: 0.3165 - val_acc: 0.9110\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8606\n",
      "Epoch 00008: val_loss improved from 0.31646 to 0.30087, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/008-0.3009.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4522 - acc: 0.8606 - val_loss: 0.3009 - val_acc: 0.9131\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8747\n",
      "Epoch 00009: val_loss improved from 0.30087 to 0.29928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/009-0.2993.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4048 - acc: 0.8747 - val_loss: 0.2993 - val_acc: 0.9147\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8856\n",
      "Epoch 00010: val_loss improved from 0.29928 to 0.27401, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/010-0.2740.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3704 - acc: 0.8856 - val_loss: 0.2740 - val_acc: 0.9229\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8949\n",
      "Epoch 00011: val_loss improved from 0.27401 to 0.27102, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/011-0.2710.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3425 - acc: 0.8949 - val_loss: 0.2710 - val_acc: 0.9238\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.9002\n",
      "Epoch 00012: val_loss did not improve from 0.27102\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3216 - acc: 0.9002 - val_loss: 0.2962 - val_acc: 0.9180\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9083\n",
      "Epoch 00013: val_loss improved from 0.27102 to 0.23860, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/013-0.2386.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2974 - acc: 0.9084 - val_loss: 0.2386 - val_acc: 0.9306\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9146\n",
      "Epoch 00014: val_loss improved from 0.23860 to 0.22621, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/014-0.2262.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2785 - acc: 0.9146 - val_loss: 0.2262 - val_acc: 0.9352\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9202\n",
      "Epoch 00015: val_loss improved from 0.22621 to 0.21083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/015-0.2108.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2607 - acc: 0.9202 - val_loss: 0.2108 - val_acc: 0.9376\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9253\n",
      "Epoch 00016: val_loss did not improve from 0.21083\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2441 - acc: 0.9253 - val_loss: 0.2472 - val_acc: 0.9336\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9278\n",
      "Epoch 00017: val_loss did not improve from 0.21083\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2345 - acc: 0.9278 - val_loss: 0.2268 - val_acc: 0.9334\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9301\n",
      "Epoch 00018: val_loss improved from 0.21083 to 0.19475, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/018-0.1947.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2266 - acc: 0.9301 - val_loss: 0.1947 - val_acc: 0.9436\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9354\n",
      "Epoch 00019: val_loss did not improve from 0.19475\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2073 - acc: 0.9354 - val_loss: 0.2400 - val_acc: 0.9285\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9374\n",
      "Epoch 00020: val_loss did not improve from 0.19475\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2016 - acc: 0.9374 - val_loss: 0.2438 - val_acc: 0.9334\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9399\n",
      "Epoch 00021: val_loss did not improve from 0.19475\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1924 - acc: 0.9399 - val_loss: 0.2196 - val_acc: 0.9385\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9439\n",
      "Epoch 00022: val_loss improved from 0.19475 to 0.19302, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/022-0.1930.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1807 - acc: 0.9439 - val_loss: 0.1930 - val_acc: 0.9455\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9444\n",
      "Epoch 00023: val_loss did not improve from 0.19302\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1757 - acc: 0.9444 - val_loss: 0.2085 - val_acc: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9461\n",
      "Epoch 00024: val_loss did not improve from 0.19302\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1734 - acc: 0.9461 - val_loss: 0.2006 - val_acc: 0.9394\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9483\n",
      "Epoch 00025: val_loss did not improve from 0.19302\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1629 - acc: 0.9482 - val_loss: 0.2172 - val_acc: 0.9378\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9491\n",
      "Epoch 00026: val_loss improved from 0.19302 to 0.17690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv_checkpoint/026-0.1769.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1666 - acc: 0.9490 - val_loss: 0.1769 - val_acc: 0.9453\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9527\n",
      "Epoch 00027: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1502 - acc: 0.9527 - val_loss: 0.1905 - val_acc: 0.9469\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9532\n",
      "Epoch 00028: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1458 - acc: 0.9532 - val_loss: 0.1845 - val_acc: 0.9467\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9554\n",
      "Epoch 00029: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1399 - acc: 0.9553 - val_loss: 0.1888 - val_acc: 0.9474\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9557\n",
      "Epoch 00030: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1371 - acc: 0.9557 - val_loss: 0.2216 - val_acc: 0.9387\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9594\n",
      "Epoch 00031: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1303 - acc: 0.9594 - val_loss: 0.1831 - val_acc: 0.9485\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9619\n",
      "Epoch 00032: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1215 - acc: 0.9619 - val_loss: 0.1811 - val_acc: 0.9509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9626\n",
      "Epoch 00033: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1189 - acc: 0.9625 - val_loss: 0.2110 - val_acc: 0.9399\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9626\n",
      "Epoch 00034: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1230 - acc: 0.9626 - val_loss: 0.1814 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9658\n",
      "Epoch 00035: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1090 - acc: 0.9658 - val_loss: 0.1782 - val_acc: 0.9495\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9649\n",
      "Epoch 00036: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1086 - acc: 0.9649 - val_loss: 0.1819 - val_acc: 0.9457\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9654\n",
      "Epoch 00037: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1068 - acc: 0.9654 - val_loss: 0.1837 - val_acc: 0.9464\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9680\n",
      "Epoch 00038: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1012 - acc: 0.9680 - val_loss: 0.2009 - val_acc: 0.9441\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9678\n",
      "Epoch 00039: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1021 - acc: 0.9678 - val_loss: 0.2007 - val_acc: 0.9446\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9688\n",
      "Epoch 00040: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0986 - acc: 0.9688 - val_loss: 0.1944 - val_acc: 0.9432\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9717\n",
      "Epoch 00041: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0897 - acc: 0.9717 - val_loss: 0.1948 - val_acc: 0.9483\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9734\n",
      "Epoch 00042: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0860 - acc: 0.9733 - val_loss: 0.2164 - val_acc: 0.9418\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9715\n",
      "Epoch 00043: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0916 - acc: 0.9715 - val_loss: 0.2126 - val_acc: 0.9392\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9751\n",
      "Epoch 00044: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0778 - acc: 0.9751 - val_loss: 0.2022 - val_acc: 0.9464\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9756\n",
      "Epoch 00045: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0778 - acc: 0.9755 - val_loss: 0.2066 - val_acc: 0.9434\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9691\n",
      "Epoch 00046: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0959 - acc: 0.9691 - val_loss: 0.2077 - val_acc: 0.9411\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9771\n",
      "Epoch 00047: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0742 - acc: 0.9771 - val_loss: 0.2038 - val_acc: 0.9460\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9782\n",
      "Epoch 00048: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0707 - acc: 0.9782 - val_loss: 0.1947 - val_acc: 0.9483\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9785\n",
      "Epoch 00049: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0680 - acc: 0.9785 - val_loss: 0.2108 - val_acc: 0.9427\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9782\n",
      "Epoch 00050: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0677 - acc: 0.9782 - val_loss: 0.2189 - val_acc: 0.9420\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9783\n",
      "Epoch 00051: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0681 - acc: 0.9783 - val_loss: 0.2146 - val_acc: 0.9439\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9776\n",
      "Epoch 00052: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0741 - acc: 0.9776 - val_loss: 0.2255 - val_acc: 0.9385\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9779\n",
      "Epoch 00053: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0675 - acc: 0.9779 - val_loss: 0.2028 - val_acc: 0.9450\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9798\n",
      "Epoch 00054: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0621 - acc: 0.9798 - val_loss: 0.2029 - val_acc: 0.9457\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9818\n",
      "Epoch 00055: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0567 - acc: 0.9818 - val_loss: 0.2276 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9816\n",
      "Epoch 00056: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0592 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.9425\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9826\n",
      "Epoch 00057: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0566 - acc: 0.9826 - val_loss: 0.2323 - val_acc: 0.9399\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9809\n",
      "Epoch 00058: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0617 - acc: 0.9809 - val_loss: 0.2229 - val_acc: 0.9399\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9831\n",
      "Epoch 00059: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0524 - acc: 0.9831 - val_loss: 0.2263 - val_acc: 0.9429\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9829\n",
      "Epoch 00060: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0559 - acc: 0.9829 - val_loss: 0.2272 - val_acc: 0.9429\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9790\n",
      "Epoch 00061: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0645 - acc: 0.9790 - val_loss: 0.2259 - val_acc: 0.9464\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9854\n",
      "Epoch 00062: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0481 - acc: 0.9854 - val_loss: 0.2316 - val_acc: 0.9429\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9856\n",
      "Epoch 00063: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0471 - acc: 0.9856 - val_loss: 0.2394 - val_acc: 0.9422\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9859\n",
      "Epoch 00064: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0458 - acc: 0.9859 - val_loss: 0.2353 - val_acc: 0.9457\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9851\n",
      "Epoch 00065: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0473 - acc: 0.9851 - val_loss: 0.2537 - val_acc: 0.9390\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9861\n",
      "Epoch 00066: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0445 - acc: 0.9861 - val_loss: 0.2254 - val_acc: 0.9457\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9856\n",
      "Epoch 00067: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0460 - acc: 0.9856 - val_loss: 0.2580 - val_acc: 0.9413\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9860\n",
      "Epoch 00068: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0445 - acc: 0.9860 - val_loss: 0.2262 - val_acc: 0.9448\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9826\n",
      "Epoch 00069: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0531 - acc: 0.9826 - val_loss: 0.2208 - val_acc: 0.9464\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9868\n",
      "Epoch 00070: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0403 - acc: 0.9868 - val_loss: 0.2346 - val_acc: 0.9462\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9887\n",
      "Epoch 00071: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0380 - acc: 0.9887 - val_loss: 0.2173 - val_acc: 0.9490\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9871\n",
      "Epoch 00072: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0407 - acc: 0.9871 - val_loss: 0.2434 - val_acc: 0.9434\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9835\n",
      "Epoch 00073: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0515 - acc: 0.9835 - val_loss: 0.2316 - val_acc: 0.9420\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9899\n",
      "Epoch 00074: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0345 - acc: 0.9899 - val_loss: 0.2508 - val_acc: 0.9385\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9886\n",
      "Epoch 00075: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.2425 - val_acc: 0.9460\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9877\n",
      "Epoch 00076: val_loss did not improve from 0.17690\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0398 - acc: 0.9877 - val_loss: 0.2443 - val_acc: 0.9448\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2746 - acc: 0.9236\n",
      "Loss: 0.2745589104768272 Accuracy: 0.9235722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 914us/sample - loss: 1.2528 - acc: 0.6415\n",
      "Loss: 1.2527855815174425 Accuracy: 0.6415369\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 788us/sample - loss: 1.0359 - acc: 0.7200\n",
      "Loss: 1.0359347320172398 Accuracy: 0.7200415\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 861us/sample - loss: 0.8208 - acc: 0.7657\n",
      "Loss: 0.8207525815795391 Accuracy: 0.7657321\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 880us/sample - loss: 0.6737 - acc: 0.8104\n",
      "Loss: 0.6737026598594642 Accuracy: 0.8103842\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 936us/sample - loss: 0.3358 - acc: 0.9070\n",
      "Loss: 0.3357766230282135 Accuracy: 0.90695745\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 945us/sample - loss: 0.2746 - acc: 0.9236\n",
      "Loss: 0.2745589104768272 Accuracy: 0.9235722\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 906us/sample - loss: 1.4996 - acc: 0.6550\n",
      "Loss: 1.4996208312479258 Accuracy: 0.65503633\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 945us/sample - loss: 1.1705 - acc: 0.7275\n",
      "Loss: 1.1704648270166305 Accuracy: 0.7275182\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 967us/sample - loss: 0.9509 - acc: 0.7537\n",
      "Loss: 0.9508819115867496 Accuracy: 0.75368637\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7341 - acc: 0.8174\n",
      "Loss: 0.7340780897056326 Accuracy: 0.81744546\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4539 - acc: 0.8916\n",
      "Loss: 0.45389406679822897 Accuracy: 0.8915888\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3392 - acc: 0.9242\n",
      "Loss: 0.3392076714001093 Accuracy: 0.92419523\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
