{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3735 - acc: 0.2478\n",
      "Epoch 00001: val_loss improved from inf to 2.01200, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/001-2.0120.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 2.3734 - acc: 0.2479 - val_loss: 2.0120 - val_acc: 0.3753\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8500 - acc: 0.4309\n",
      "Epoch 00002: val_loss improved from 2.01200 to 1.76180, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/002-1.7618.hdf5\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 1.8501 - acc: 0.4309 - val_loss: 1.7618 - val_acc: 0.4454\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5935 - acc: 0.5117\n",
      "Epoch 00003: val_loss improved from 1.76180 to 1.60145, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/003-1.6015.hdf5\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 1.5935 - acc: 0.5116 - val_loss: 1.6015 - val_acc: 0.4983\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4261 - acc: 0.5612\n",
      "Epoch 00004: val_loss improved from 1.60145 to 1.54723, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/004-1.5472.hdf5\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 1.4260 - acc: 0.5613 - val_loss: 1.5472 - val_acc: 0.5206\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3040 - acc: 0.5964\n",
      "Epoch 00005: val_loss improved from 1.54723 to 1.50679, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/005-1.5068.hdf5\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 1.3039 - acc: 0.5964 - val_loss: 1.5068 - val_acc: 0.5309\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2069 - acc: 0.6277\n",
      "Epoch 00006: val_loss improved from 1.50679 to 1.48629, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/006-1.4863.hdf5\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 1.2069 - acc: 0.6277 - val_loss: 1.4863 - val_acc: 0.5455\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6506\n",
      "Epoch 00007: val_loss improved from 1.48629 to 1.45991, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/007-1.4599.hdf5\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 1.1302 - acc: 0.6506 - val_loss: 1.4599 - val_acc: 0.5502\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 0.6721\n",
      "Epoch 00008: val_loss improved from 1.45991 to 1.44930, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/008-1.4493.hdf5\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 1.0613 - acc: 0.6721 - val_loss: 1.4493 - val_acc: 0.5486\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.6894\n",
      "Epoch 00009: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 1.0003 - acc: 0.6894 - val_loss: 1.4682 - val_acc: 0.5455\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9448 - acc: 0.7067\n",
      "Epoch 00010: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.9447 - acc: 0.7068 - val_loss: 1.4778 - val_acc: 0.5479\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.7205\n",
      "Epoch 00011: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.8988 - acc: 0.7204 - val_loss: 1.4619 - val_acc: 0.5544\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7338\n",
      "Epoch 00012: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.8565 - acc: 0.7338 - val_loss: 1.4527 - val_acc: 0.5618\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7429\n",
      "Epoch 00013: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.8149 - acc: 0.7429 - val_loss: 1.4758 - val_acc: 0.5618\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7579\n",
      "Epoch 00014: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.7758 - acc: 0.7579 - val_loss: 1.4745 - val_acc: 0.5681\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7389 - acc: 0.7663\n",
      "Epoch 00015: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.7389 - acc: 0.7663 - val_loss: 1.4991 - val_acc: 0.5597\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7797\n",
      "Epoch 00016: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.7061 - acc: 0.7797 - val_loss: 1.5088 - val_acc: 0.5586\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.7880\n",
      "Epoch 00017: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.6758 - acc: 0.7880 - val_loss: 1.5293 - val_acc: 0.5660\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.7954\n",
      "Epoch 00018: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.6466 - acc: 0.7954 - val_loss: 1.5194 - val_acc: 0.5693\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6118 - acc: 0.8087\n",
      "Epoch 00019: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.6119 - acc: 0.8087 - val_loss: 1.5687 - val_acc: 0.5625\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8128\n",
      "Epoch 00020: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 611us/sample - loss: 0.5903 - acc: 0.8128 - val_loss: 1.5518 - val_acc: 0.5702\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8205\n",
      "Epoch 00021: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.5671 - acc: 0.8204 - val_loss: 1.5785 - val_acc: 0.5723\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8249\n",
      "Epoch 00022: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.5484 - acc: 0.8249 - val_loss: 1.5840 - val_acc: 0.5674\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8323\n",
      "Epoch 00023: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.5268 - acc: 0.8323 - val_loss: 1.6325 - val_acc: 0.5663\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8362\n",
      "Epoch 00024: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.5088 - acc: 0.8362 - val_loss: 1.6466 - val_acc: 0.5658\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8425\n",
      "Epoch 00025: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4898 - acc: 0.8425 - val_loss: 1.6352 - val_acc: 0.5763\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8483\n",
      "Epoch 00026: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4705 - acc: 0.8483 - val_loss: 1.6478 - val_acc: 0.5802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8572\n",
      "Epoch 00027: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.4493 - acc: 0.8572 - val_loss: 1.6700 - val_acc: 0.5756\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8604\n",
      "Epoch 00028: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.4373 - acc: 0.8604 - val_loss: 1.6849 - val_acc: 0.5765\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8669\n",
      "Epoch 00029: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.4199 - acc: 0.8669 - val_loss: 1.7049 - val_acc: 0.5751\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8705\n",
      "Epoch 00030: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4009 - acc: 0.8705 - val_loss: 1.7889 - val_acc: 0.5681\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8740\n",
      "Epoch 00031: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3912 - acc: 0.8741 - val_loss: 1.7474 - val_acc: 0.5840\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8790\n",
      "Epoch 00032: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.3788 - acc: 0.8790 - val_loss: 1.7647 - val_acc: 0.5800\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8835\n",
      "Epoch 00033: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.3639 - acc: 0.8835 - val_loss: 1.7561 - val_acc: 0.5842\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8865\n",
      "Epoch 00034: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3536 - acc: 0.8865 - val_loss: 1.7978 - val_acc: 0.5779\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8892\n",
      "Epoch 00035: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.3471 - acc: 0.8893 - val_loss: 1.8275 - val_acc: 0.5823\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8928\n",
      "Epoch 00036: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 0.3367 - acc: 0.8928 - val_loss: 1.8428 - val_acc: 0.5823\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8939\n",
      "Epoch 00037: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3271 - acc: 0.8940 - val_loss: 1.8238 - val_acc: 0.5924\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9017\n",
      "Epoch 00038: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.3069 - acc: 0.9017 - val_loss: 1.8186 - val_acc: 0.5886\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9025\n",
      "Epoch 00039: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.3042 - acc: 0.9025 - val_loss: 1.8626 - val_acc: 0.5802\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9070\n",
      "Epoch 00040: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2900 - acc: 0.9070 - val_loss: 1.9247 - val_acc: 0.5786\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9082\n",
      "Epoch 00041: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.2937 - acc: 0.9082 - val_loss: 1.8953 - val_acc: 0.5840\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9104\n",
      "Epoch 00042: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.2804 - acc: 0.9104 - val_loss: 1.9258 - val_acc: 0.5777\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9133\n",
      "Epoch 00043: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 607us/sample - loss: 0.2722 - acc: 0.9133 - val_loss: 1.9462 - val_acc: 0.5807\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9148\n",
      "Epoch 00044: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2695 - acc: 0.9148 - val_loss: 1.9455 - val_acc: 0.5856\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9146\n",
      "Epoch 00045: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2671 - acc: 0.9146 - val_loss: 1.9179 - val_acc: 0.5868\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9183\n",
      "Epoch 00046: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.2581 - acc: 0.9182 - val_loss: 1.9540 - val_acc: 0.5896\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9196\n",
      "Epoch 00047: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2534 - acc: 0.9197 - val_loss: 1.9579 - val_acc: 0.5912\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9228\n",
      "Epoch 00048: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 607us/sample - loss: 0.2451 - acc: 0.9228 - val_loss: 2.0164 - val_acc: 0.5854\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9242\n",
      "Epoch 00049: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 609us/sample - loss: 0.2374 - acc: 0.9242 - val_loss: 2.0253 - val_acc: 0.5870\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9264\n",
      "Epoch 00050: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.2336 - acc: 0.9264 - val_loss: 2.0225 - val_acc: 0.5889\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9253\n",
      "Epoch 00051: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2355 - acc: 0.9253 - val_loss: 2.0127 - val_acc: 0.5928\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9291\n",
      "Epoch 00052: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2285 - acc: 0.9291 - val_loss: 2.0311 - val_acc: 0.5935\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9322\n",
      "Epoch 00053: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2186 - acc: 0.9322 - val_loss: 2.0257 - val_acc: 0.5931\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9323\n",
      "Epoch 00054: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2173 - acc: 0.9323 - val_loss: 2.0371 - val_acc: 0.5830\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9312\n",
      "Epoch 00055: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2176 - acc: 0.9312 - val_loss: 2.0086 - val_acc: 0.5996\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9343\n",
      "Epoch 00056: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.2067 - acc: 0.9343 - val_loss: 2.0403 - val_acc: 0.5952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9355\n",
      "Epoch 00057: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2072 - acc: 0.9355 - val_loss: 2.0605 - val_acc: 0.5947\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9381\n",
      "Epoch 00058: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2007 - acc: 0.9381 - val_loss: 2.0634 - val_acc: 0.5961\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 323us/sample - loss: 1.5276 - acc: 0.5169\n",
      "Loss: 1.5275790819126498 Accuracy: 0.5169263\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3194 - acc: 0.2541\n",
      "Epoch 00001: val_loss improved from inf to 1.82589, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/001-1.8259.hdf5\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 2.3193 - acc: 0.2541 - val_loss: 1.8259 - val_acc: 0.4228\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7313 - acc: 0.4477\n",
      "Epoch 00002: val_loss improved from 1.82589 to 1.61547, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/002-1.6155.hdf5\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 1.7312 - acc: 0.4477 - val_loss: 1.6155 - val_acc: 0.4885\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5481 - acc: 0.5094\n",
      "Epoch 00003: val_loss improved from 1.61547 to 1.46045, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/003-1.4604.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.5481 - acc: 0.5094 - val_loss: 1.4604 - val_acc: 0.5434\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4206 - acc: 0.5555\n",
      "Epoch 00004: val_loss improved from 1.46045 to 1.37365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/004-1.3737.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.4205 - acc: 0.5555 - val_loss: 1.3737 - val_acc: 0.5884\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.5879\n",
      "Epoch 00005: val_loss improved from 1.37365 to 1.30498, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/005-1.3050.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.3265 - acc: 0.5878 - val_loss: 1.3050 - val_acc: 0.6056\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2574 - acc: 0.6093\n",
      "Epoch 00006: val_loss improved from 1.30498 to 1.25890, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/006-1.2589.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.2573 - acc: 0.6094 - val_loss: 1.2589 - val_acc: 0.6215\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1923 - acc: 0.6346\n",
      "Epoch 00007: val_loss improved from 1.25890 to 1.23930, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/007-1.2393.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.1923 - acc: 0.6346 - val_loss: 1.2393 - val_acc: 0.6184\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1423 - acc: 0.6492\n",
      "Epoch 00008: val_loss improved from 1.23930 to 1.19953, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/008-1.1995.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.1423 - acc: 0.6491 - val_loss: 1.1995 - val_acc: 0.6341\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6632\n",
      "Epoch 00009: val_loss improved from 1.19953 to 1.18560, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/009-1.1856.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.0949 - acc: 0.6632 - val_loss: 1.1856 - val_acc: 0.6252\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.6754\n",
      "Epoch 00010: val_loss improved from 1.18560 to 1.15145, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/010-1.1515.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.0568 - acc: 0.6753 - val_loss: 1.1515 - val_acc: 0.6406\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0131 - acc: 0.6885\n",
      "Epoch 00011: val_loss did not improve from 1.15145\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.0131 - acc: 0.6885 - val_loss: 1.1635 - val_acc: 0.6401\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9762 - acc: 0.7025\n",
      "Epoch 00012: val_loss improved from 1.15145 to 1.11850, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/012-1.1185.hdf5\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.9763 - acc: 0.7025 - val_loss: 1.1185 - val_acc: 0.6564\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9351 - acc: 0.7148\n",
      "Epoch 00013: val_loss improved from 1.11850 to 1.11414, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/013-1.1141.hdf5\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 0.9352 - acc: 0.7148 - val_loss: 1.1141 - val_acc: 0.6557\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7231\n",
      "Epoch 00014: val_loss improved from 1.11414 to 1.10492, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/014-1.1049.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.9046 - acc: 0.7231 - val_loss: 1.1049 - val_acc: 0.6536\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8694 - acc: 0.7332\n",
      "Epoch 00015: val_loss did not improve from 1.10492\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.8694 - acc: 0.7332 - val_loss: 1.1103 - val_acc: 0.6599\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7408\n",
      "Epoch 00016: val_loss improved from 1.10492 to 1.06307, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/016-1.0631.hdf5\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.8406 - acc: 0.7408 - val_loss: 1.0631 - val_acc: 0.6720\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8084 - acc: 0.7523\n",
      "Epoch 00017: val_loss did not improve from 1.06307\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.8084 - acc: 0.7522 - val_loss: 1.0775 - val_acc: 0.6737\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7807 - acc: 0.7609\n",
      "Epoch 00018: val_loss did not improve from 1.06307\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.7807 - acc: 0.7609 - val_loss: 1.0790 - val_acc: 0.6748\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7712\n",
      "Epoch 00019: val_loss improved from 1.06307 to 1.05903, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/019-1.0590.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.7461 - acc: 0.7712 - val_loss: 1.0590 - val_acc: 0.6862\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7226 - acc: 0.7757\n",
      "Epoch 00020: val_loss improved from 1.05903 to 1.04501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/020-1.0450.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.7226 - acc: 0.7757 - val_loss: 1.0450 - val_acc: 0.6897\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7831\n",
      "Epoch 00021: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.6940 - acc: 0.7830 - val_loss: 1.0715 - val_acc: 0.6827\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7914\n",
      "Epoch 00022: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.6733 - acc: 0.7914 - val_loss: 1.0683 - val_acc: 0.6799\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.7985\n",
      "Epoch 00023: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.6482 - acc: 0.7985 - val_loss: 1.0499 - val_acc: 0.6895\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6238 - acc: 0.8049\n",
      "Epoch 00024: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.6238 - acc: 0.8049 - val_loss: 1.0702 - val_acc: 0.6918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8090\n",
      "Epoch 00025: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.6104 - acc: 0.8090 - val_loss: 1.0478 - val_acc: 0.6942\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5846 - acc: 0.8160\n",
      "Epoch 00026: val_loss improved from 1.04501 to 1.03054, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/026-1.0305.hdf5\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.5845 - acc: 0.8160 - val_loss: 1.0305 - val_acc: 0.7028\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.8222\n",
      "Epoch 00027: val_loss improved from 1.03054 to 1.02962, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/027-1.0296.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5649 - acc: 0.8223 - val_loss: 1.0296 - val_acc: 0.7009\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8264\n",
      "Epoch 00028: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.5526 - acc: 0.8264 - val_loss: 1.0669 - val_acc: 0.6925\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8305\n",
      "Epoch 00029: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5372 - acc: 0.8305 - val_loss: 1.0426 - val_acc: 0.7014\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.8323\n",
      "Epoch 00030: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.5252 - acc: 0.8323 - val_loss: 1.0487 - val_acc: 0.7056\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8422\n",
      "Epoch 00031: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5036 - acc: 0.8422 - val_loss: 1.0437 - val_acc: 0.7060\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8461\n",
      "Epoch 00032: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4848 - acc: 0.8461 - val_loss: 1.0659 - val_acc: 0.7070\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8474\n",
      "Epoch 00033: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.4770 - acc: 0.8474 - val_loss: 1.0696 - val_acc: 0.7079\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8507\n",
      "Epoch 00034: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.4630 - acc: 0.8508 - val_loss: 1.0721 - val_acc: 0.7074\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8562\n",
      "Epoch 00035: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.4500 - acc: 0.8562 - val_loss: 1.0715 - val_acc: 0.7112\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8574\n",
      "Epoch 00036: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4406 - acc: 0.8574 - val_loss: 1.0688 - val_acc: 0.7191\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8593\n",
      "Epoch 00037: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.4347 - acc: 0.8593 - val_loss: 1.0967 - val_acc: 0.7051\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8630\n",
      "Epoch 00038: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.4221 - acc: 0.8630 - val_loss: 1.1282 - val_acc: 0.7056\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8674\n",
      "Epoch 00039: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.4114 - acc: 0.8674 - val_loss: 1.0924 - val_acc: 0.7049\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8679\n",
      "Epoch 00040: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.4033 - acc: 0.8680 - val_loss: 1.0979 - val_acc: 0.7093\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8732\n",
      "Epoch 00041: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.3894 - acc: 0.8732 - val_loss: 1.0964 - val_acc: 0.7072\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8730\n",
      "Epoch 00042: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3855 - acc: 0.8730 - val_loss: 1.0912 - val_acc: 0.7142\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8752\n",
      "Epoch 00043: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3762 - acc: 0.8752 - val_loss: 1.1153 - val_acc: 0.7056\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8797\n",
      "Epoch 00044: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.3696 - acc: 0.8797 - val_loss: 1.1092 - val_acc: 0.7133\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8811\n",
      "Epoch 00045: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3631 - acc: 0.8812 - val_loss: 1.1130 - val_acc: 0.7102\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8858\n",
      "Epoch 00046: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3468 - acc: 0.8858 - val_loss: 1.0952 - val_acc: 0.7216\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8882\n",
      "Epoch 00047: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.3437 - acc: 0.8882 - val_loss: 1.1370 - val_acc: 0.7147\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8891\n",
      "Epoch 00048: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3390 - acc: 0.8891 - val_loss: 1.1277 - val_acc: 0.7184\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8897\n",
      "Epoch 00049: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3383 - acc: 0.8897 - val_loss: 1.1329 - val_acc: 0.7202\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8926\n",
      "Epoch 00050: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.3265 - acc: 0.8926 - val_loss: 1.1125 - val_acc: 0.7244\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.8935\n",
      "Epoch 00051: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.3185 - acc: 0.8935 - val_loss: 1.1299 - val_acc: 0.7163\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8944\n",
      "Epoch 00052: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 0.3179 - acc: 0.8943 - val_loss: 1.1238 - val_acc: 0.7237\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8982\n",
      "Epoch 00053: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.3127 - acc: 0.8982 - val_loss: 1.1180 - val_acc: 0.7242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9007\n",
      "Epoch 00054: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.3059 - acc: 0.9007 - val_loss: 1.1377 - val_acc: 0.7212\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9010\n",
      "Epoch 00055: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2990 - acc: 0.9010 - val_loss: 1.1552 - val_acc: 0.7195\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9022\n",
      "Epoch 00056: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2953 - acc: 0.9022 - val_loss: 1.1468 - val_acc: 0.7212\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9037\n",
      "Epoch 00057: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.2912 - acc: 0.9037 - val_loss: 1.1276 - val_acc: 0.7310\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9045\n",
      "Epoch 00058: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.2886 - acc: 0.9045 - val_loss: 1.1601 - val_acc: 0.7195\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9078\n",
      "Epoch 00059: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2829 - acc: 0.9078 - val_loss: 1.2020 - val_acc: 0.7205\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9059\n",
      "Epoch 00060: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.2799 - acc: 0.9059 - val_loss: 1.1723 - val_acc: 0.7258\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9090\n",
      "Epoch 00061: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2709 - acc: 0.9090 - val_loss: 1.1544 - val_acc: 0.7312\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9111\n",
      "Epoch 00062: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2726 - acc: 0.9110 - val_loss: 1.1645 - val_acc: 0.7256\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9100\n",
      "Epoch 00063: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.2676 - acc: 0.9100 - val_loss: 1.1643 - val_acc: 0.7300\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9138\n",
      "Epoch 00064: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 0.2603 - acc: 0.9138 - val_loss: 1.1658 - val_acc: 0.7244\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9151\n",
      "Epoch 00065: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2591 - acc: 0.9151 - val_loss: 1.1761 - val_acc: 0.7284\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9160\n",
      "Epoch 00066: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2547 - acc: 0.9159 - val_loss: 1.1859 - val_acc: 0.7265\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9139\n",
      "Epoch 00067: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2558 - acc: 0.9139 - val_loss: 1.1527 - val_acc: 0.7279\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9183\n",
      "Epoch 00068: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2474 - acc: 0.9184 - val_loss: 1.1637 - val_acc: 0.7321\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9206\n",
      "Epoch 00069: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2424 - acc: 0.9206 - val_loss: 1.2327 - val_acc: 0.7275\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9214\n",
      "Epoch 00070: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2413 - acc: 0.9214 - val_loss: 1.2002 - val_acc: 0.7310\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9223\n",
      "Epoch 00071: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2376 - acc: 0.9223 - val_loss: 1.1923 - val_acc: 0.7338\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9214\n",
      "Epoch 00072: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2345 - acc: 0.9214 - val_loss: 1.2038 - val_acc: 0.7331\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9217\n",
      "Epoch 00073: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2321 - acc: 0.9217 - val_loss: 1.2106 - val_acc: 0.7291\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9224\n",
      "Epoch 00074: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2340 - acc: 0.9224 - val_loss: 1.1947 - val_acc: 0.7300\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9258\n",
      "Epoch 00075: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2260 - acc: 0.9258 - val_loss: 1.2039 - val_acc: 0.7317\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9259\n",
      "Epoch 00076: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2229 - acc: 0.9259 - val_loss: 1.2297 - val_acc: 0.7307\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9249\n",
      "Epoch 00077: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2243 - acc: 0.9248 - val_loss: 1.2062 - val_acc: 0.7393\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmS2Tfd9YE0BZEiBAQBAFq4K44YKIPtpqXfr4PNZqbX2kalvb2mrVti619WetW7Vu4Fq1KJbFBdSwoyBrIAvZ93W28/vjZAMCBMgwk8z3/Xpdkszcufc7k3C+955z7vcqrTVCCCEEgCXQAQghhAgekhSEEEJ0kKQghBCigyQFIYQQHSQpCCGE6CBJQQghRAdJCkIIITpIUhBCCNFBkoIQQogOtkAHcLSSkpJ0RkZGoMMQQog+Zc2aNRVa6+QjrdfnkkJGRgZ5eXmBDkMIIfoUpdSenqwn3UdCCCE6SFIQQgjRQZKCEEKIDn1uTKE7brebwsJCWlpaAh1Kn+V0Ohk0aBB2uz3QoQghAqhfJIXCwkKio6PJyMhAKRXocPocrTWVlZUUFhaSmZkZ6HCEEAHUL7qPWlpaSExMlIRwjJRSJCYmypmWEKJ/JAVAEsJxks9PCAH9KCkcidfbTGtrET6fJ9ChCCFE0AqZpODzteBy7UNrV69vu6amhr/85S/H9NrzzjuPmpqaHq9/77338vDDDx/TvoQQ4khCJikoZcbUte79M4XDJQWP5/D7e//994mLi+v1mIQQ4lhIUugFCxcuZOfOneTk5HDHHXewfPlyTj/9dObOncuYMWMAuPjii5k0aRJZWVk89dRTHa/NyMigoqKC/Px8Ro8ezY033khWVhazZ8+mubn5sPtdv349U6dOZdy4cVxyySVUV1cD8NhjjzFmzBjGjRvHFVdcAcCKFSvIyckhJyeHCRMmUF9f3+ufgxCi7+sXU1K72r79Nhoa1nfzjMbrbcBicaLU0c3Fj4rK4aSTHjnk8w888ACbN29m/Xqz3+XLl7N27Vo2b97cMcXzmWeeISEhgebmZiZPnsy8efNITEw8IPbtvPzyy/ztb3/j8ssvZ/HixVx99dWH3O/3vvc9Hn/8cWbOnMkvfvELfvWrX/HII4/wwAMPsHv3bsLCwjq6ph5++GGeeOIJpk+fTkNDA06n86g+AyFEaAiZMwVon12jT8jepkyZst+c/8cee4zx48czdepUCgoK2L59+0GvyczMJCcnB4BJkyaRn59/yO3X1tZSU1PDzJkzAbjmmmtYuXIlAOPGjeOqq67ixRdfxGYzeX/69OncfvvtPPbYY9TU1HQ8LoQQXfW7luFwR/T19Wux25NxOgf7PY7IyMiO75cvX87SpUtZtWoVERERnHHGGd1eExAWFtbxvdVqPWL30aG89957rFy5knfffZff/va3bNq0iYULF3L++efz/vvvM336dJYsWcKoUaOOaftCiP4rhM4UzLiCP8YUoqOjD9tHX1tbS3x8PBEREWzdupXVq1cf9z5jY2OJj4/nk08+AeAf//gHM2fOxOfzUVBQwHe+8x1+//vfU1tbS0NDAzt37mTs2LHceeedTJ48ma1btx53DEKI/qffnSkcjlJWvySFxMREpk+fTnZ2Nueeey7nn3/+fs/PmTOHJ598ktGjRzNy5EimTp3aK/t9/vnnuemmm2hqamLYsGE8++yzeL1err76ampra9Fa86Mf/Yi4uDh+/vOfs2zZMiwWC1lZWZx77rm9EoMQon9RWp+YPvbekpubqw+8yc6WLVsYPXr0EV/b1PQtWmsiI6XbpDs9/RyFEH2PUmqN1jr3SOuFXPcRyBXNQghxKCGXFPzRfSSEEP1FiCUFK1p76WtdZkIIcaKEWFKwYa5T8AU6FCGECEohlRTaJ1tJF5IQQnQvpJKCP+sfCSFEfxBiScEKBEdSiIqKOqrHhRDiRAixpNB+puANcCRCCBGcQjQp9O6ZwsKFC3niiSc6fm6/EU5DQwNnnXUWEydOZOzYsbz99ts93qbWmjvuuIPs7GzGjh3Lq6++CsC+ffuYMWMGOTk5ZGdn88knn+D1ern22ms71v3Tn/7Uq+9PCBE6+l+Zi9tug/Xdlc42dVLDvfVYVBhYHD3fZk4OPHLoQnsLFizgtttu4+abbwbgtddeY8mSJTidTt58801iYmKoqKhg6tSpzJ07t0f3Q37jjTdYv349GzZsoKKigsmTJzNjxgz++c9/cs4553D33Xfj9Xppampi/fr1FBUVsXnzZoCjupObEEJ01f+SwmGojn979zqFCRMmUFZWRnFxMeXl5cTHxzN48GDcbjd33XUXK1euxGKxUFRURGlpKWlpaUfc5qeffsqVV16J1WolNTWVmTNn8tVXXzF58mSuu+463G43F198MTk5OQwbNoxdu3Zxyy23cP755zN79uxefX9CiNDR/5LCYY7oAVoaNmK1RhMennnY9Y7W/PnzWbRoESUlJSxYsACAl156ifLyctasWYPdbicjI6PbktlHY8aMGaxcuZL33nuPa6+9lttvv53vfe97bNiwgSVLlvDkk0/y2muv8cwzz/TG2xJChJiQGlMA/5W6WLBgAa+88gqLFi1i/vz5gCmZnZKSgt1uZ9myZezZs6fH2zv99NN59dVX8Xq9lJeXs3LlSqZMmcKePXtITU3lxhtv5IYbbmDt2rVUVFTg8/mYN28e9913H2vXru319yeECA3970zhCPyVFLKysqivr2fgwIGkp6cDcNVVV3HhhRcyduxYcnNzj+qmNpdccgmrVq1i/PjxKKV48MEHSUtL4/nnn+ehhx7CbrcTFRXFCy+8QFFREd///vfx+cyV2vfff3+vvz8hRGgIqdLZAM3NO/F6m4iKGuuP8Po0KZ0tRP8lpbMPwZwpyHUKQgjRnZBMCuCRSqlCCNGNEE0KclWzEEJ0x29JQSk1WCm1TCn1jVLqa6XUrd2so5RSjymldiilNiqlJvorns59Wtu+C3z9IyGECDb+nH3kAX6itV6rlIoG1iilPtJaf9NlnXOBk9qWU4C/tn31IzlTEEKIQ/HbmYLWep/Wem3b9/XAFmDgAatdBLygjdVAnFIq3V8xgZTPFkKIwzkhYwpKqQxgAvDFAU8NBAq6/FzIwYkDpdQPlFJ5Sqm88vLy44yl95NCTU0Nf/nLX47pteedd57UKhJCBA2/JwWlVBSwGLhNa113LNvQWj+ltc7VWucmJycfZzy9f0+FwyUFj+fw+3n//feJi4vrtViEEOJ4+DUpKKXsmITwktb6jW5WKQIGd/l5UNtjfoyp98cUFi5cyM6dO8nJyeGOO+5g+fLlnH766cydO5cxY8YAcPHFFzNp0iSysrJ46qmnOl6bkZFBRUUF+fn5jB49mhtvvJGsrCxmz55Nc3PzQft69913OeWUU5gwYQJnn302paWlADQ0NPD973+fsWPHMm7cOBYvXgzAv//9byZOnMj48eM566yzeu09CyH6J78NNCtTH/rvwBat9R8Psdo7wA+VUq9gBphrtdb7jme/h6mc3R4ZXu9IlLJj6WFKPELlbB544AE2b97M+rYdL1++nLVr17J582YyM03hvWeeeYaEhASam5uZPHky8+bNIzExcb/tbN++nZdffpm//e1vXH755SxevJirr756v3VOO+00Vq9ejVKKp59+mgcffJA//OEP/OY3vyE2NpZNmzYBUF1dTXl5OTfeeCMrV64kMzOTqqqqnr1hIUTI8ufso+nAd4FNSqn2ZvouYAiA1vpJ4H3gPGAH0AR834/xdNH75bMPNGXKlI6EAPDYY4/x5ptvAlBQUMD27dsPSgqZmZnk5OQAMGnSJPLz8w/abmFhIQsWLGDfvn24XK6OfSxdupRXXnmlY734+HjeffddZsyY0bFOQkJCr75HIUT/47ekoLX+lPZbGBx6HQ3c3Jv7PULlbAAaGwtQykpExMm9uev9REZGdny/fPlyli5dyqpVq4iIiOCMM87otoR2WFhYx/dWq7Xb7qNbbrmF22+/nblz57J8+XLuvfdev8QvhAhNIXdFM5jB5t4cU4iOjqa+vv6Qz9fW1hIfH09ERARbt25l9erVx7yv2tpaBg40E7Sef/75jsdnzZq13y1Bq6urmTp1KitXrmT37t0A0n0khDiiEE0KvVs+OzExkenTp5Odnc0dd9xx0PNz5szB4/EwevRoFi5cyNSpU495X/feey/z589n0qRJJCUldTx+zz33UF1dTXZ2NuPHj2fZsmUkJyfz1FNPcemllzJ+/PiOm/8IIcShhFzpbICWlr243ZVER0/o7fD6NCmdLUT/JaWzD8NMS/VKpVQhhDhAiCaF3r+ATQgh+oMQTQpSFE8IIboT4klBzhSEEKKrkE4Kck8FIYTYX0gmBZAxBSGE6E5oJQWfD7QOijGFqKiogO1bCCEOJXSSQlUVrF0LLpfMPhJCiEMInaTQXleosRFTwLX3rmpeuHDhfiUm7r33Xh5++GEaGho466yzmDhxImPHjuXtt98+4rYOVWK7uxLYhyqXLYQQx8qfVVID4rZ/38b6kkPUzq6vhzUOCAvD621EKQsWS/gRt5mTlsMjcw5daW/BggXcdttt3Hyzqe332muvsWTJEpxOJ2+++SYxMTFUVFQwdepU5s6d25aUutddiW2fz9dtCezuymULIcTx6HdJ4bCsVjOuAByhgOtRmTBhAmVlZRQXF1NeXk58fDyDBw/G7XZz1113sXLlSiwWC0VFRZSWlpKWlnbIbXVXYru8vLzbEtjdlcsWQojj0e+SwuGO6Nmzx4wt5OTQ1LwDrd1ERo7plf3Onz+fRYsWUVJS0lF47qWXXqK8vJw1a9Zgt9vJyMjotmR2u56W2BZCCH8JnTEFgIgI8HqhtbXXK6UuWLCAV155hUWLFjF//nzAlLlOSUnBbrezbNky9uzZc9htHKrE9qFKYHdXLlsIIY5HaCWF9hvfNDW13VOh95JCVlYW9fX1DBw4kPT0dACuuuoq8vLyGDt2LC+88AKjRo067DYOVWL7UCWwuyuXLYQQxyO0Smf7fLBuHaSm0ppsweUqJipqIkqFVm48FCmdLUT/JaWzu2OxmC6kxsaguIBNCCGCTWglBTBJoakJ1VHqwh3ggIQQInj0m6TQ426wtsFmi8ecKXi9TX6Mqu/oa92IQgj/6BdJwel0UllZ2bOGrW2w2dLsAax4vQ3+Da4P0FpTWVmJ0+kMdChCiADrF9cpDBo0iMLCQsrLy4+8stZQWQkuF65IN1pXERbW7P8gg5zT6WTQoEGBDkMIEWD9IinY7faOq3175LrrwOlkz/Nz2L37Z5x6ahkOR7L/AhRCiD6iX3QfHbXcXFizhtjoaQDU1X0e4ICEECI4hG5SaGggel8cSjmorf0s0BEJIURQCM2kMGkSANb1m4mOnkRt7acBDkgIIYJDaCaF0aMhPBzy8oiNnU59/Rq8Xik8J4QQoZkUbDbIyWlLCqehtYv6+rwjv04IIfq50EwKYMYV1q0jJvIUAOlCEkIIQj0pNDbi2FFOePhI6upksFkIIUI3KZxxhvm6dCmxsdOprf0MrX2HfYkQQvR3oZsUhgyBUaPgww+JjZ2Ox1NNU9PWQEclhBABFbpJAWD2bFixgtiwyQByvYIQIuRJUmhuJnxNCXZ7sgw2CyFCXmgnhZkzwW5HffQRMTGnypmCECLkhXZSiIqC6dPbxhVOo6VlJ62t+wIdlRBCBExoJwWAc86BDRtIdE8AoLLy3QAHJIQQgeO3pKCUekYpVaaU2nyI589QStUqpda3Lb/wVyyHNXs2ABGfFxMePoKystcCEoYQQgQDf54pPAfMOcI6n2itc9qWX/sxlkPLyYGkJNRHH5GcfDk1NctwuXpwsx4hhOiH/JYUtNYrgSp/bb/XWCwwaxZ8+CHJifMAHxUVbwY6KiGECIhAjylMU0ptUEp9oJTKOtRKSqkfKKXylFJ5Pbrl5tGaPRtKS4nabSE8fATl5a/3/j6EEKIPCGRSWAsM1VqPBx4H3jrUilrrp7TWuVrr3ORkP9w2c9YsgI4upOpq6UISQoSmgCUFrXWd1rqh7fv3AbtSKikgwQwcCFlZpgspeT7glS4kIURIClhSUEqlKaVU2/dT2mKpDFQ8nHMOfPIJUZaTpAtJCBGy/Dkl9WVgFTBSKVWolLpeKXWTUuqmtlUuAzYrpTYAjwFXaK21v+I5ogsvhNZW1D//SXLy/LYupIqAhSOEEIGgAtkOH4vc3Fydl+eHu6RpDaecAuXl1K95jTUbp3DyyU8xYMCNvb8vIYQ4wZRSa7TWuUdaL9Czj4KHUvCLX0B+PlFvb5YuJCFESJKk0NX558OECaj77yc5fh7V1f/B5SoLdFRCCHHCSFLoSim45x7Yvp0BnyYAXoqLnwp0VEIIccJIUjjQxRdDdjbOh58jIW4ORUWP4/W2BDoqIYQ4ISQpHMhigbvvhi1bGLZuCm53GaWlLwY6KiGEOCEkKXRn/nwYOZLIR94iKiKHwsI/orUv0FEJIYTfSVLojtUK99yD2riRk1ZPoalpC1VVHwQ6KiGE8DtJCofyX/8F06YR85s3iGxJp6DgD4GOSAgh/E6SwqFYLPDkk6jqakY/P5iammXU168JdFRCCOFXPUoKSqlblVIxyvi7UmqtUmq2v4MLuHHj4Mc/JuqVL4n/OlzOFoQQ/V5PzxSu01rXAbOBeOC7wAN+iyqY3HsvDBnCqEcjKC9+lebmnYGOSAgh/KanSUG1fT0P+IfW+usuj/VvkZHw+OOEba9kyGIb27f/kL5WL0oIIXqqp0lhjVLqQ0xSWKKUigZCZ47m3Llw8cUMfU7TmvdvyssXBzoiIYTwi54mheuBhcBkrXUTYAe+77eogtGf/4xKSGb8nTYKVv4Qj6c+0BEJIUSv62lSmAZ8q7WuUUpdDdwD1PovrCA0cCBqyRJsnnDG3FpKQd5PAx2REEL0up4mhb8CTUqp8cBPgJ3AC36LKlhlZ2N5fwlh1TaSvvsUDUWfBjoiIYToVT1NCp62u6JdBPxZa/0EEO2/sILYtGn4Xn+ZyHzQF56H3r0r0BEJIUSv6WlSqFdK/QwzFfU9pZQFM64QkmznX0bdn/+XyE31cNJJcNVVsH59oMMSQojj1tOksABoxVyvUAIMAh7yW1R9QOwP/szW92ZQNN+KfvcdmDAB5syB4uJAhyaE8IcQmYreo6TQlgheAmKVUhcALVrr0BtT6EIpxbAZL7D7f8PY/N5k9O9+B599BqedBrukS0mIfqWsDIYOhSuvhPoTMPNw2za47jq4/35oavL//rroaZmLy4EvgfnA5cAXSqnL/BlYX+B0DmXYsAeo9C6j5Np0+PhjqK01ieHrrwMdnhCit/z0p7BvH7z2GkyeDJs3+2c/TU3m7o9jx8LLL8Ndd8HIkfD88+A7MZeG9bT76G7MNQrXaK2/B0wBfu6/sPqOAQP+h9jY09m588e0jh8MK1aYJ2bMgK++CmxwQvQlNTUnrOE7KsuWwT/+AXfeCf/5jznwmzLFPHYoTU3w4Yfw8MPmiH/aNEhKgjPPhBdegIaG/dcvLzdJICsLfvtbWLAA8vNh+XJIS4Nrr4WJE00sfqZ6UrJBKbVJaz22y88WYEPXx06U3NxcnZeXd6J3e1hNTdvIyxtPQsIcsrLeQO3aBbNmQWmp+eVedhmcdRaEhQU6VCGC07vvmv8np54Kzz1numqCQWsrjB8PLpc5+w8Ph5IS0420fDmMGAE5OWadrCz49lv46CP49FPzGoDUVBg9GoYNMweNO3ea8jmXXgoeD6xeDbt3m3XHjIG//AVmzuyMweeDV1+Fn/0MbroJFi48preilFqjtc494opa6yMumEHlJcC1bcsHwO978treXiZNmqSD0Z49D+ply9AFBY+ZB4qKtP7e97SOidEatI6N1fqaa7Tety+gcQoRdN54Q2u7XeusLK2josz/mX/8Q2ufr/f35fNpvW2b1s8+q/Wtt2q9YsXh17/vPvP/97339n/c7db6sce0njdP6xEjzDrty7hxWv/0p1ovWaJ1VdXB+//0U61vuEHr6GitBw4023jwQROLy3XoWJqbzXKMgDzdk/a+JyuZ7TEP+GPbcklPX9fbS7AmBZ/PozdunKuXLbPoioouf0AtLeYP6vvf1zo83PwRfPFF4AIVIpi8/rrWNpvWU6dqXVOj9c6dWk+fbpqm+fO1Liw8vu17PFqvWaP1n/6k9SWXaJ2S0tl4WyxaW61aP/RQ9wlo506tnU6tL7vsyPupqzP/r4/moM8fSe8wepoUetR9FEyCsfuoncfTwPr1M2hu3s6ECZ8RFTVu/xU2bICLLzYDVk8+afoJwfQ/rlhhBq/+538gKuqExy7ECffqq+Yan6lT4f33ISbGPO71wu9/D7/8pWm+L70UbrnFTOBQCvbuNd0zq1dDY6N5rH1pbYXmZrPU18O6dVBXZ7abmQmnnw7Tp5tl0CC44QZYtAjmzYNnn4XoaGhpMdv+xS/M67duhYEDA/c59ZKedh8dNikopeqB7lZQgNZaxxx7iMcmmJMCQGtrEWvWTEEpKxMnfklYWNr+K1RWmnGGjz82t/ysrDQJoaXFPH/WWfCvf4HTeeKDF+JYeDzwzjtmoDQpyQyITpwI2dndj6P5fHDffeZeJaedBu+9ZxrjA+3aBX/9K/z971BdbfrlGxtNUgDTLx8X17XjxuwvPNwsERFmFs+MGSYZDBp08D60hj/+0Qwijxhh1vnsM/P/0Wo1B2833NCrH1eg9OqYQjAtwdp91FVd3Rq9YkWEzsubrD2exoNXcLu1vv1282c8cqTWt91m+h+ffto8dtFFh+9bFKI7J7g7QhcWav3LX2o9YID5u01LM2Nn7U20w6H1FVdonZfX+Zrycq3POcc8f/XVWjc0HHk/jY3m/8bMmaZL6dFHtV671vw/6i3Llmk9bJjWY8easYZ33jHdWf0I0n0UWBUVb7N58yUkJl5AVtYbWCy2g1dqaTn4jOCJJ+CHP4SrrzZzky1yG+2gVVQEAwaYbotAW77cnIFGR5vumKlTITfXHFnv2WOWkhI491xzf5AD/65aWszZa2SkKd1yuPe1eTM8+KA5M/B64ZxzTLfneeeZo+tdu2DNGvjkE/M3XF8PZ5xhZuzcd5+Zlff443DjjcHx2YUIOVMIAoWFT+hly9Bbt96ofUdzFPfb35ojqRtv1Hr9eq3Lyk78UaA4tJYWrW+5xfyOFizQuqkpsPEsXWomMYwapfWll2qdnq71/p0qZlC1fSZcVpbWL71kjrR37ND6jju0Tkraf/2ICHPUfMkl5vknn9R60SKtL7ig8/lbbzWvP5yaGq0ffljrQYPM6zIy9j9zECcMcqYQHHbtupu9e3/H0KG/JDPz3p69SGszF/nBBzsfczhMf+fEieYo8JRTTH9pRYWZ45yfbwbXrrvOHO0J/9ixwxyRr11rjrr//W9zhevbb5uLjI5FSws8+qjp224/Uj/pJDOvvbHRHFmXlpp+9Vmz4PrrTd89mAukLrrIrP/xx5CcbP5+CgtNjLGxZs7/oEHmqPzVV+F3v4NvvjHz50tLzdH9RReZvnObDbZvN+9z+3Yzp37nzs4594mJ8KMfwc03m+97yu2Gzz838/nj4o7tcxLHpVcGmoNRX0sKWmu+/fY6Skqe4+ST/x8DBvygpy80Mx927TJF9oqLTeP/1VedF7p057zz4K23wB6yRWz95+WX4b//2zSczz1numHeesvMoElMNBdgjRsHVVXmd7R3r+mmaR/4jIw03TKpqeZxrWHxYvi//zPrn3MOxMebxnj79s5ZM/Hx5jVhYWYGW1gYXHGFOTi47TYzAPvRR52J4kh8PjMw/Oyz5iDjhhsOP7vG6zVdZQUF5kItOejokyQpBBGfz83mzRdRVbWEkSP/Rnr6dce3wdJS+OILc7SXlgYZGWb5979N3+6118Izz0h/bW8pKzNTIl97zUxl/Oc/YciQzufXrYMLLzQzyRyOzsb8UBwO83qHw/wOs7PhT3+Cs8/uXEdrc2YQFWXWa/f112bc6YUXzFnExIkmISQk9O57Fv2OJIUg4/U2snnzpVRXf0hm5n0MGXIXyh+N9q9+Zab63XWXqaHSLj/fNB61tZ3zuJUyg3/Z2b0fR3+gteluueUW09D/8pfmqN7WzaSB4mLz2Tscpttn2LDOxNH+eTc0mG6d9oHfsjJzxH/99d1v83Bqa+GDD0y5dumOET0gSSEI+Xwuvv32ekpLX2TAgP/lpJMeQylr7+5Ea1Mf5amnTN+x02kati++2H89u910I3i95ih34UJTd+ZQ23z5ZfjDH8zrkpJMd0lammnUJkw48I2abpU33zQXHl188dGftbhcZhtnn31sR8EeDyxZYrpwSktNA1xWZhrT2lrTyLeXQLbZTL+63W4uoIqLM102lZWmANqUKebMKyvr6OMQIkhIUghSWvvYtetnFBQ8SFLSpYwZ808sll4ulOf1muJib71lfp4wwQyOXnoppKebRGGzmUbviSfgscfM96efDtdcYxrx9kHEHTtMl9TSpaa/PDXVDG5XVpors91u87pbb4Xzz4fXXzc14LdsMftpaTGN6v33mwqRPVFba64w/fhjkxB+/evOvvwj8flMDD//uemXb5eYCCkppsGPjTWNf/sFU16vSSIul0kUNTWm66alxQyo/vjHJmkI0YfJlNQgV1DwiF62DL1x44Xa623t/R00NZmiX9u2HXndhgZzQdCwYWbaoNWq9axZZsphWJiZyvjEE6aOTFfV1Wa6YUaG7rhYCbTOztb6n/80UzeffrpzOuLZZ2v9wQdae72HjqWgwEyFtNm0fuABrc8807x2zBit337bXOT3xBPmgr9587T+7/82U3hffNHsc/z4zhgWL9a6uLh3L3ISoo+itwviBcvSX5KC1p3XMWzaNE97vUHQcPl8pnjYz37WWflx/nxT8fVwPB6t33xT6+uvNw33gY1+c7PWf/xjZzGyzEyt779f65KS/dfbuNEUDIyO1vrDDztjevNNrYcP1wfNox858uD59cOGmQRxYAITIsT1NCn4rftIKfUMcAFQprU+aCRTmVEAXHS1AAAgAElEQVTWR4HzgCbgWq312iNtt693Hx2ooOARdu78MSkpVzB69Iu9P8ZwrLQ23Sjx8b23TZfLjDM8+aS5AhfM9Eq73QzQNjSY8Yr33zfz2btqbTUDq/HxZk5+enrnOEVTk5kuWVFhuqpkOq4QBwn4mIJSagbQALxwiKRwHnALJimcAjyqtT7lSNvtb0kBYO/e37Nr10JSU7/HqFHPBE9i8KetW80c/fp6kyxcLtOY//jH+0/3FEL0ip4mhaOcB9dzWuuVSqmMw6xyESZhaGC1UipOKZWutd7nr5iC1ZAhd+LztZKf/0u83jpGj34JqzUi0GH516hRcPfdgY5CCHGAQFZbGwgUdPm5sO2xkJSR8QtGjHiUioq3Wb/+DFyu0kCHJIQIQX2iBKdS6gdKqTylVF55eXmgw/GbQYN+RHb2mzQ2bmbt2qk0Nm4JdEhCiBATyKRQBAzu8vOgtscOorV+Smudq7XOTU5OPiHBBUpS0kXk5KzA621m3bpTqa1dFeiQhBAhJJBJ4R3ge8qYCtSG4nhCd2JiJjNx4mrs9mQ2bpxNTc3KQIckhAgRfksKSqmXgVXASKVUoVLqeqXUTUqpm9pWeR/YBewA/gb8r79i6YvCwzPIyVlBWNhgNm6cQ1XV0kCHJIQIAVLmIsi5XGVs2HA2TU3byM5+g8TE8wIdkhCiDwr4lFTROxyOFHJylrFhw2w2b76Y4cMfYuDAH/mnwqoQIiDcbnMbDper8/J8MOW+HA5zjafDYRZ/36FXkkIfYLcnMn78x2zd+l127LiNqqoPGTXqWRyOlECHJkRQ83rNxfAtLWZxuUztQ7fbfHU6zS0roqIgIsLUQSwq6ryvVXW1ub6yvahua6t57YGLy2W+Wiym4bbbzaKUqdHYvng8nYvbbWo/lpeb4gE9cccd+9+Q0R8kKfQRdnsc2dnvUFz8F3bs+AlffTWO0aOfJyHhnECHJsQRad1ZiLa9QXa5TKNbUWGWqipzZBwZaZaICFP5pLLSPFdVZRrnhobOxWLpPIq228322u9eWlZmEkFviIoyRXWdzs4Gv31p37fTaRp+t9skELfbvNZiMYtS5v3ZbCZmmw1GjDCVXZKTTSFfp9O8pr0joP1zam01X085Ys2H4ydJoQ9RSjFw4M3Exs5ky5Yr2bhxDhkZv2Lo0J9Ld5I4Jl0b6/YGu+vRb0UFlJSYpazMNLp1dZ23pPB4Oo+Cvd7OBqzr0Xn7PYZ8vuOL1WIxFc+7HtlDZ+wul6mKnppqLphPSelsyNuX9ga8vXFubTUNeHuSiY83d0wdONB8TUw0CcrfXTbBRJJCHxQVlc3EiV+ybdtN5Of/ksbGrxk16tn+XxpD4PGY21gUFppujurqzga6ru7g7o32rgqvt7O7orraHHXX1Jj1e0op0yjHxJjGNzraNLI2W+fRcHv/d/vSfnvq8PD9G+X2JT6+855NCQkmzsZG00A3NZnGv/25mJjQapwDRZJCH2W1hjNq1HNERo5l167/o7l5J9nZb+F0Dgp0aKIbLS2mEfd6TePa3rg1Nnb2V9fW7t/g79tnGsf2o+2mJtP/3N0Rt1KmkW4vOtt1sVo7j4xjYmDMGNPIxsWZxrp9ALO7bpH2G+ylppoujqO9a6joe+RX3IcppRgy5KdERIxiy5YrWbt2MqNGvUBCwqxAh9avud2mcS4t7RyQLCoyXSytrabhbx/gLCoyt2PedxSXZTqdnd0XaWn7H22npcHgwTBokFknIcEctYdaF4fwH0kK/UBS0gVMmLCKr7++jI0bZzNgwP8ybNjvsdmiAh1aUGtqgvx82L3bLHv3miP39j7x1lazTkNDZ5dGebkZ+OxOUpJpuK3Wzls+DxgA554LGRmmMXc4zJF++7TDyMjOO4PGxJhGPz7+6G9pLURvkaTQT0RFZZObu47du++msPARqqqWMGrUc8TFnRbo0E6IlhbYuLHziL2kxDTgTU37D3hWVZnHKypMQ9+Vw2H6sLv2iUdGmseSkyEz03xNSTHdKSkpnYOSaWlybx/RP0hS6Ees1nBGjPgjSUkXs3XrtaxfP4OhQ39BRsbP+/yNe5qbTXdN+1F7Y6Np4L/8Ej77DPLyzOyTdkqZrpWoqP1nnyQnmz71pCSzDB1qGvvMTNPQyxG6CHWSFPqhuLgZ5OZuZPv2m9mz51fU1n7KmDEv4XCkBjq0btXWwvbtZikuNv3v7V9LSszX2truX+twQG4u3HorTJtmumnS0mRQVIhjJf9t+imbLaqt+2gm27ffTF5eDmPGvEJc3MyAxNM+lXLrVvjmG7Ns2QLbtpkzgK6cTtMtk54O2dlw9tnm+9RU0+/efmFT+0ya9gt+hBDHT5JCP6aUIj39OqKjc/n668tYv/5MBg/+KRkZ92K1hvfqvrQ2Uym3boVvvzWNffuRf3v/ftfaiwkJpkG/4AI4+WSzjBhhZtXExko3jhCBIkkhBERFjWPSpDXs2PFjCgoepKLiLUaOfJq4uNOPaXs1NbBunVk2b+488q+v77pP09BnZMDUqeZIPz0dRo40ySA5WRp+IYKRlM4OMVVVS9m27UZaWvIZMOBmhg9/8JBXQvt8sGuXafg3bTLL2rWwc2fnOqmpkJVlGvoxY0x5gZEjTQKQRl+I4CGls0W3EhLOJjd3E7t330NR0WPU1a0iLe0d8vIG8sUXZq5+UZFZCgs7C4opZWboTJwIN9xgvk6caGbwCCH6D0kKIcbjgS1bosjLe4QVK37MihVN5OcPBMw8+/YrZSdNgosuMkf/2dnmbCAyMsDBCyH8TpJCP+d2w+rV8NFHsGyZ6f5pajLPRUcPZerUBs499w+MGfMO559/DZmZ1wU2YCFEQElS6Ee8XjMGsHGjWdasgRUrOuvO5+bCjTear7m5ZiDYYonC7b6Ob775iD17rqeu7hWGD3+IqKjxgX47QvQpWmu/lLAvaywjrziPNcVrmDJwCueM8O89VCQp9GENDeYs4NNP4ZNP4IsvOks3WCym0f/ud2HWLPjOd0xVzO7Y7fGMG/ceRUV/Jj//1+TlTSAt7VoyM39DWNjAE/eGxAnj8rqobKokOTIZm6X7ZqDZ3Uyzpxmvz4tXe9FaEx0WTaQ9sseNX6OrkS+KvuCzvZ+xZt8ahscPZ9bwWcwYOoMIe/cTHLw+L8X1xeTX5JNfk8/umt3k1+RTUFdAvDOeYfHDyIzLZGjcUBpdjRTVF1FcX0xJQ0nH88PihzE0bigtnhZKGkoobSiltLEUq7IS6Ygk0h5JpCMSr89Ls6e5472WN5ZT3FDMvvp9lDaWEu2IZlDMIAbFDGJA9ABaPa2UN5WbpbGcquYqalpqqG6ppqalBq01NoutY0kITyAtKq1jsSorLq8Lt8+Ny+ui2dNMk7uJZrf5qpTCYXXgsDqwKivbKrdRUFcAgEJx1+l3+T0pyOyjPsLlgq++Mkf/a9ea5ZtvzNmBxQLjx8P06Wbwd9w4MxYQfgyXIrjd1ezd+zsKCx9DKSuZmb9l0KBbUSo0SnDWtNSwoWQD60vWs6NqB6cMOoULT76QWGdsr+7H4/NQVFdEenQ6Dquj23UaXY2E2cL2a7RrWmr4z+7/sHTXUj4v+JzkyGTGJI1hTPIYTk48mVZvK6UNpZQ1llHWWEa9q55GdyMNrgYaXA2UNpSyr2EfFU0VADhtTsaljmNC2gSyU7LZV7+PTWWb2FS2ifya/G7jslvsJIQnkBCeQJQjigh7BBH2CJw2J26fu6OBq2utY2vFVrzai0IxImEEe2r34PK6cFgdTBs0jfjweNxe00C2eFoori9mb+1e3D73fvscED2AQTGDqG6uJr8m/6Dn7RY7qVGpVDZV0uxpPubfi8PqID0qnfTodFIjU6l31VNYV0hBbUHHdqMcUSRHJJMcmUxieCLx4fHEO+OJDYvFarHi8Xnw+Dy4vC6qmqsoaSjpWDQau8WOw+rAbrUTbgvv+PzC7eForTsShtvrZmjcUCYPmEzugFwmpE0gOiz6mN9bT2cfSVIIYvv2wQcfwHvvmTGB9usAUlPNQPDEiSYRnHqqubq3NzU372bHjh9RWfkvYmNPZ9SoZwkPH96r+6htqcVmsRFuD8fSw6RT31rPvoZ9HUdyLq+r40jW6/Pi9rk7Gpn2/1zti8fnQaGwWqxY22pBVTVXUdpojiKL64sprCvs2Fe4LZxmTzMOq4PZw2dz8ciLSYxI7NifT/uwKmvHUaFSirLGMorriymqK6K0sRSH1UG0I5rosGjCrGHsrtnN1+Vfs61yW0fjOC51HLnpuYxNHUtRXREbyzayoWRDxxFiTFgMCeEJRNgj2FqxFZ/2EWmP5NTBp1LdUs2W8i00uhsP+qycNicxYTFE2iOJckQR6YgkJTKFAVEDSI9OJzE8kV3Vu1hbspZ1+9ZR21qLVVkZmTSSsSljyUrOIiYspuPzUkpR31pPZXMlVc1VVDVX0ehu7DjSbfY0Y7fYOxq5CHsE2SnZTB88nWmDpxHnjKPJ3cQnez5h6a6lrNizglZva0cj6bA6SI9OJzMuk4y4DIbGDiUzPpMhsUNw2jovW28/k9hTu4coRxQDoweSGJGIRVnQWlPWWMau6l3sqd1DhD2i4yg9JTIFrTUNrgYa3Y00uhqxWWw4bU7C7eE4bU6iHdHdngVpraltrcVpc+4XS18iSaGP2rMHFi+G1183XUNgZgSddx7MmdN5IdiJoLWmtPQFtm+/Fa3dDB/+IAMG3NSj4no+7aPR1Uhda13HUt5UzvqS9eQV55FXnEdRfVHH+hH2CGLCYshKzmJS+iQmpk9kVNIotlVu46vir/iy6EvWlayjrrXuqN+HzWLDbrF3HHG3JxCNJjE8kZTIFFKjUkmNTGVM8hhy0nLIScshJTKFL4u+ZNE3i1j0zSL21O7p8T4TwxNJi0rD7XNT31pPg6uBJncTQ+OGMiZ5DGOSxjAsfhi7qneRt898HnWtdViVldHJoxmXOo6s5Cw8Pg+VTZVUtVRR11rH+NTxzBo2i1MGndJxhuHTPgpqC9hWuY1IRySpkamkRKYQ5YjqcTeP1pri+mKSIpIIs4Ud9Wcsgp8khT5Ca1MD6O234a23TNVPgAkTYN48uPBCGDvWvxeCNbmbWLlnJZ/t/Yx9Dfs6+kvrWuvIjM/k5PjBxLk/IVFvJi06gzHD7mDEoOuxWOx8U/4NqwpW8Xnh56zdt5aqZtN41bfWo+n+b2tk4khyB+QyLnUcYLpJGt2NVDVXsbF0I5vKNuHydpY8dVgd5KTlMCl9EhlxGfud3jttzo4jWavFut+pud1ix2619/gs5HC01myt2EqrtxWLsmBVVizKgk/7cPvceHwevD4vyZHJDIgecNRHkz7to7CukNTIVGmUhV9IUghiLpcp9/zeeyYZ7NhhHp882SSCyy6D4cfZU+P1edlasZW61jqa3E0dS6u3lVZPK63eVqqaq1iev5zPCj7D5XVhVVZSIlNIjkwmOSKZKEcUu6p38W3lt/s10gAKcFhttHo9gDkynjJwCmlRacSExRATFkO0I5pYZ2zHz3HOOLKSs47YP+/yuvi67Gu2VmzlpMSTGJc67pD97kKInpGkEGRKS+Gdd+D992HpUjNzyG6HM8+Eiy82ZwQDj3OiT1ljGUt2LOGDHR/w4c4PqWw+xC3Cumjvjpg1fBanDTmt2xkhHp/HJIeKb6loqqCo8jP2lH1AXXMxo+IHcuGE+5mUcbVfpuMJIXqHJIUgUFICb7xhxgdWrjS1hIYMMbdnPPdckxCiu5lMUNdax6ubX+W97e/h076OgUy71U6UPYrosGiiHFHYLXZ21+xme9V2tlVuo6ShBICUyBTmjJjD2ZlnkxKZ0jGzIdxmBtPCbGGEWcOIsEcQ6Ti2y5S11lRWvsOOHT+mpWU3ycnzGT78YZzOIcfzkQkh/ESSQoBUVJiB4ldeMReOaQ2jR8P8+aZrqH18wOPzsLF0Ix6fp6M/vKKpghc3vsjr37xOk7uJYfHDiA2LxePzdMykaXQ1dsyeAJMATk48mZMTTmZU0ijOzDyTCekTeqUfvSe83mYKCh5i7977AcWQIXcyePAdhyyyJ4QIDEkKJ9iqVfDrX5upo17tZuAZHzDo9OWcM2kkF+VOZmzKWGwWG2v3reXFjS/y8uaXKW0sPWg70Y5orsy+kusmXMeUgVMO2SXj9XlxeV2E23v3vgjHqqVlDzt33kF5+euEhQ1m2LDfk5JyhXQpCREkJCn4WX5NPh/v+hhPcySLX0zgo3cSSExxkXHBK+yKeJlqVwU2iw2PzwzEhlnDSI1KZW/tXhxWBxecfAHzRs8jJiwGn/bh9XlxWB2ckXHGMXfpBIOampXs2HEbDQ3riImZxoAB/01i4oXY7QmBDk2IkCZJwQ8aXA0s/mYxz214juX5y7tdJ8waxtyRc7lm/DXMHj6bgroCvioy8+x31+zmnOHnMD9rPgnh/beR1NpLSclz5Of/itbWAsBKXNxMkpPnkZZ2rXQtCREAkhR60ZriNTyZ9yQvb36ZRncjKbbhuL68lprP5zH9NLj5p1VEJVXR6m3lrMyziA+PP6HxBSutNfX1eVRUvElFxZs0NW0lLGwoJ530OElJFwY6PCFCitxk5zh5fB5e2PACf837K3nFeYTbwpnsvJId715H8epTyc1V/PoFc5WxdJt3TylFTMxkYmImM2zY76ipWcG2bTezefNcEhPnMmLEo4SHZwQ6TCFEF6FR5ewo+bSPa9+6luvfuZ5mdzP3n/440z4vZuVP/05q63TeeUfx5ZdmWqkkhJ6Li5tJbu46hg17kOrqpXz11Ri2b7+Vlpael48QQviXnCkcQGvNze/dzEubXuK+79zHGda7uOIKRXk5PPkk/OAHkgiOh8ViZ8iQO0hJuYLdu39OcfFfKCp6gpSUKxgy5A65j4MQASZJoQutNXcuvZMn1zzJ/516J+F5d3PGneaCs88/N1VJRe9wOgczevRzZGb+hsLCR9i37ynKyl4iImIUCQnnkZh4PrGxp2GxSHkLIU4kGWju4rcrf8s9y+7hf3L/h8gVT/DwQ4pLLoFnn4XY3i2nLw7gdldTWvoPKivfo6ZmOVq7sFqjSUq6hNTU7xIf/50eVWcVQnRPZh8dpefXP8+1b1/Ld8d9lzHbnuNnCy3cfDM8/rh0F51oHk8DNTXLqKh4m/LyRXi9tTgcA0hN/S8SE+cSEzMNyyHuFiaE6F5QJAWl1BzgUcAKPK21fuCA568FHgLaC+v/WWv99OG26Y+ksKFkA1P/PpVpg6axwPUhN/3AxpVXwosvmruaicDxeluorHyX0tJ/UFX1AVp7sFpjiY8/m8TEc0lJuQKrte9e7CfEiRLwpKDMuf42YBZQCHwFXKm1/qbLOtcCuVrrH/Z0u72dFGpaash9KpdmTzP3DVrHDVemMGuWqWjqkO7soOJ211BT8zGVlR9QVfVvXK4i7PYUhgy5kwED/gerNThKfggRjHqaFPx5HDwF2KG13qW1dgGvABf5cX9Hzad9XPPWNeyp3cMvR7/OTd9NYcoUU9BOEkLwsdvjSE6ex6hRTzNtWgE5OZ8QFTWOnTt/whdfDKOw8HG83qZAhylEn+bPpDAQKOjyc2HbYweap5TaqJRapJQa7Md4DvLQZw/xzrfv8LuZf+DBW04lPd3c+CZSeiOCnlKKuLjTGD/+I3JyVhAePpIdO37EqlVD2LXrHlpb9wU6RCH6pED3mL8LZGitxwEfAc93t5JS6gdKqTylVF55eXmv7PjTvZ9y13/u4vKsy9n1yi3s2gXPPw8J/bckUb8VFzeDCROWk5Ozkri4Gezd+ztWrx7Kli3fZd++56ivX4fP1xroMIXoE/w5pjANuFdrfU7bzz8D0Frff4j1rUCV1vqwkz97Y0yhxdPC+CfH4/K6+ONJG7n0/Gh+8hN4+OHj2qwIEs3NOyksfIySkmfxeusBUMpGRMQYEhLOJTn5MqKjJ0lZbxFSgmGg2YYZaD4LM7voK+C/tNZfd1knXWu9r+37S4A7tdZTD7fd3kgKd318F/d/ej+LL/qIH553NvHxsGYNOI/uXusiyGntpalpO42NG2ho2Eh9/Zdt10B4cDozSE6+jKSki4mJmSrXQIh+L+AF8bTWHqXUD4ElmCmpz2itv1ZK/RrI01q/A/xIKTUX8ABVwLX+iqfdun3rePCzB/l+zvd5/fdnU14O//qXJIT+SCkrkZGjiIwcRUrKAgDc7koqKt6hvHwRhYWPUlDwMHZ7MomJF5KUNJfY2BnY7VLlVoSukLp4zePzcMrTp1BUV8RT47Zw0Tnx/OY3cM89vRyk6BM8nloqKz+gsvIdKivfx+utBSAiYhQxMVOJiZlKRMRowsOH43Cko07QLU6F8IeAnykEoz+u+iNr961l0fxFfPFSPFYr3H57oKMSgWKzxZKaegWpqVfg87morf2currPqatbTWXlvygpea5jXYslnPDwEcTFnUFi4gXExc3EYgkLXPBC+EnInClsr9zOuCfHcd5J57H48sWcdRbU1JixBCEOpLWmpSWf5ubtNDfvpLl5B01N31BTsxyfrwWLJZKEhFnEx59NXNwZRESMkYFrEdTkTOEAu6p3MSB6AH8+9894vfDll3DNNYGOSgQrpRTh4ZmEh2fu97jX20RNzTIqK/9FZeX7VFS8BYDdnkxc3BlER08mOnoiUVET5L7Uok8KmaRwzohz+PaH32Kz2Ni0CRoaYOph5zkJcTCrNYLExPNJTDy/7WxiNzU1y9uWFZSXv96xbljYUBITzyUpaR5xcWdIET/RJ4TUX6mt7T/l6tXmZ0kK4niYs4lhhIcPIz39OgBcrgoaGtbR0LCOurovKCl5geLiJ7HZEklKmktExGgcjjQcjlQcjnQiIkbKPSNEUAmppNBu1SpITIThwwMdiehvHI4kEhJmkZAwCzDdTVVVSygvX0x5+RsdM5zaKRVGdPTEttlOpxAZmU14+AgZxBYBE5JJYfVqc5Yg44LC36zWCJKTLyE5+RK01ni9DbhcJbhcJbS2FlJfv4a6utUUF/+VwsI/tb3KQnj4MCIiRhEePpKIiPZlFA5HSkDfj+j/Qi4p1NTAli1w1VWBjkSEGqUUNls0Nls0EREnAZCaeiUAPp+bxsbNNDVtoalpa9uyhaqqj9C6s25TTMxU0tKuJyVlATZbdEDeh+jfQi4pfPml+SrjCSKYWCx2oqMnEB09Yb/HtfbS0rKXpqZvaWhYR2npP9i27UZ27LiV5OT5REZmY7cnYrcnYLMlEhY2gLCwgdL9JI5ZyCWF1atNt9HkyYGORIgjU8raMTU2MXEOQ4YsbBvA/jtlZa9SWtptYWHs9pS25BCOUjaUsmGxOIiOziUhYQ7R0afIbCjRrZC5eK3duedCYSFs2tSLQQkRAO1jFG53JR5PJW53Ba2txbS2FtLaWkBra1FbyXAvWnvwehtoaNgI+NpuaXoWMTHTiIoaT1RUDg5HcqDfkvAjuXitGz4ffPEFzJsX6EiEOH5dxyggo0evcburqa7+mOrqJVRVfUhFxRsdzzkcaVitsW1nFlaUshMWlk5Y2FCczqE4nRlERJxMePjJcuvTfiykksL27VBdDdOmBToSIQLDbo8nJeUyUlIuA0zV2IaGDTQ0rKexcTNebyNamzMLrV20tBRQW/spHk9Nl60onM5MIiJGExk5hoiIMURGjiYiYjQ2W0xg3pjoNSGVFOSiNSH2Z7cnEh9/JvHxZx52PY+nlpaWfJqavqWx8Zu2WVJbqK7+CHMLdsNqjcHhSMFuT2m7QC+tbUnH4UhvGxCPxWqNxWaLw2aL8vdbFEcp5JJCTAyMGhXoSIToW2y22Laxh/H7Pe7zeWhp2U1T0zc0NW2ltXUfbncZLlcpTU3bqKlZicdTeZjtxuF0DiM8fDhO5zAcjtS2xJGA3Z5IRMRoub/FCRZSSWHVKjjlFLBIWXwheoXFYiMi4qS26y4u6nYdn8/VccGex1ONx1OLx1ODx1NNS8teWlp20dCwgYqKt9DafdDrw8NPJiZmCtHRk7FYwvF6G/H5mvD5mrHZEnE6hxAWNgSncyh2e5JUqz1OIZMUGhrMjKO77w50JEKEFovFgdM5BKdzyGHX09qH11uP212J212F211GQ8MG6uu/pLr6Y0pLXzzgFQrYf/akzRZPZGR225KFzZaIxeLEYgnDYnFitycTFjYQmy1OkschhExSyMszs49kkFmI4KSUBZstFpstlvDwYQAkJp4HmOm3Llcp4MViicRqjUApOx5PFS0te2lt3UtLSz6NjVtobNxMaek/D6oz1ZXF4sThSAcsaN2Kz9eK1m6czkyioiYSHT2J6OiJhIefHHIJJGSSgtZw2mkwZUqgIxFCHC2lFGFhaQc9bq7mTuzmSnCNy7UPj6cWn68Vn68Fn68Zt7uc1tZiXK5iWluLAY3FEoZSDpSy0dy8jYqKNykp+XvHtqzWqLbuqSFYrTEdZx0WixObLb7LwHoKdnsSNpu5wryvVr8NmaTwne/AJ58EOgohxIlgksgAwsIGHPVrtda0tu6lvn4tLS27u5yJ7MXr3d12ZtGCz9eCx1PLgV1Y7azWKGy2OKzWmLYZV9EdFxGacZFGwsIGERWVQ1TUBKKicggLG4LdHo9S1uP8BI5dyCQFIYToCaVU28V6Q4+4rtZe3O5KXK4y3O7StvGQyo6rzM2geh1ebx0eTy1K2bDZ4gkLG4TFEk5Ly2727XsWn+/P+23XTNdNABQ+X3NHEho8+HYyM3/jp3fetm+/bl0IIfoxpaw4HCltJc2zj2kbWvtobt5JQ8MGXK5i3O6qtrIlVYDCYnFitYZjsTiJiTm1V+PvjiQFIYQIIKUsXab1Bp7M2BdCCNFBkoIQQogOkhSEEEJ0kKQghBCigyQFIYQQHSQpCCGE6CBJQQghRAdJCkIIIToorbuv2xGslFLlwJ5jfHkSUBe1wNsAAAYmSURBVNGL4fS2YI8Pgj9Gie/4SHzHJ5jjG6q1Tj7SSn0uKRwPpVSe1jo30HEcSrDHB8Efo8R3fCS+4xPs8fWEdB8JIYToIElBCCFEh1BLCk8FOoAjCPb4IPhjlPiOj8R3fII9viMKqTEFIYQQhxdqZwpCCCEOI2SSglJqjlLqW6XUDqXUwiCI5xmlVJlSanOXxxKUUh8ppba3fY0PYHyDlVLLlFLfKKW+VkrdGkwxKqWcSqkvlVIb2uL7VdvjmUqpL9p+z68qpQJ6o1yllFUptU4p9a9gi08pla+U2qSUWq+Uymt7LCh+v22xxCmlFimltiqltiilpgVLfEqpkW2fW/tSp5S6LVjiOx4hkRSUueHpE8C5wBjgSqXUmMBGxXPAnAMeWwh8rLU+Cfi47edA8QA/0VqPAaYCN7d9ZsESYytwptZ6PJADzFFKTQV+D/xJaz0CqAauD1B87W4FtnT5Odji+47WOqfLNMpg+f0CPAr8W2s9ChiP+RyDIj6t9bdtn1sOMAloAt4MlviOi9a63y/ANGBJl59/BvwsCOLKADZ3+flbIL3t+3Tg20DH2CW2t4FZwRgjEAGsBU7BXDhk6+73HoC4BmEahjOBfwEqyOLLB5IOeCwofr9ALLCbtnHPYIvvgJhmA58Fa3xHu4TEmQIwECjo8nNh22PBJlVrva/t+xIgNZDBtFNKZQATgC8IohjbumbWA2XAR8BOoEZr7WlbJdC/50eA/wN8bT8nElzxaeBDpdQapdQP2h4Llt9vJlAOPNvW/fa0UioyiOLr6grg5bbvgzG+oxIqSaHP0eZQI+BTw5RSUcBi4DatdV3X5wIdo9baq83p+yBgCjAqULEcSCl1AVCmtV4T6FgO4zSt9URMt+rNSqkZXZ8M8O/XBkwE/qq1ngA0ckBXTKD//gDaxoTmAq8f+FwwxHcsQiUpFAGDu/w8qO2xYFOqlEoHaPtaFshglFJ2TEJ4Sev/3979vHhRx3Ecf75CEtNwC+xSUFgREYgnD1kgeMpDdDCiTCQ6dukW0i/oDyg6BHnwYCQahYZ0dIsFD6Vim5lCRQRtVEJo5KEIe3X4vL/T11VQF9zvB/b1gGFnPjM7vIfZ7/c985md98cHqrmrGAFsnwc+o3XHTElaVqsmeZ43Ao9L+hHYT+tCept+4sP2z/XzLK0/fAP9nN85YM72F7X8ES1J9BLfyGPACdu/1XJv8V23pZIUjgH3139+3Ey73Ts04Ziu5BCwo+Z30PrxJ0KSgN3AGdtvjq3qIkZJayRN1fwK2vOOM7TksHXS8dneafsu2/fQ/t4+tb2tl/gkrZR062ie1i9+ik7Or+1fgZ8kPVBNm4HTdBLfmKf5v+sI+ovv+k36ocZiTcAW4Ftav/PLHcSzD/gF+Id2VfQ8rc95GvgOOAzcPsH4HqHd+p4EZmva0kuMwDrgy4rvFPBata8FjgLf027pl3dwrjcBn/QUX8XxVU3fjD4TvZzfimU9cLzO8cfAbZ3FtxL4HVg91tZNfAud8kZzREQMlkr3UUREXIMkhYiIGCQpRETEIEkhIiIGSQoRETFIUohYRJI2jSqmRvQoSSEiIgZJChFXIOnZGq9hVtKuKr53QdJbNX7DtKQ1te16SZ9LOinp4KiGvqT7JB2uMR9OSLq3dr9qbJyAvfX2eEQXkhQi5pH0IPAUsNGt4N5FYBvtDdbjth8CZoDX61feA16yvQ74eqx9L/CO25gPD9PeYIdWcfZF2tgea2l1kiK6sOzqm0QsOZtpA6ccq4v4FbTCZv8CH9Q27wMHJK0GpmzPVPse4MOqK3Sn7YMAtv8CqP0dtT1Xy7O0cTWO3PjDiri6JIWIywnYY3vnJY3Sq/O2W2iNmL/H5i+Sz2F0JN1HEZebBrZKugOGcYvvpn1eRhVOnwGO2P4DOCfp0WrfDszY/hOYk/RE7WO5pFsW9SgiFiBXKBHz2D4t6RXaqGQ30SrZvkAb6GVDrTtLe+4ArUTyu/Wl/wPwXLVvB3ZJeqP28eQiHkbEgqRKasQ1knTB9qpJxxFxI6X7KCIiBrlTiIiIQe4UIiJikKQQERGDJIWIiBgkKURExCBJISIiBkkKEREx+A9R+NCW9axl7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 335us/sample - loss: 1.1507 - acc: 0.6656\n",
      "Loss: 1.150674240363845 Accuracy: 0.66562825\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3854 - acc: 0.2131\n",
      "Epoch 00001: val_loss improved from inf to 1.80858, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/001-1.8086.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 2.3853 - acc: 0.2131 - val_loss: 1.8086 - val_acc: 0.4172\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7117 - acc: 0.4407\n",
      "Epoch 00002: val_loss improved from 1.80858 to 1.50537, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/002-1.5054.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 1.7117 - acc: 0.4407 - val_loss: 1.5054 - val_acc: 0.5227\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5012 - acc: 0.5214\n",
      "Epoch 00003: val_loss improved from 1.50537 to 1.38713, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/003-1.3871.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.5011 - acc: 0.5215 - val_loss: 1.3871 - val_acc: 0.5586\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3636 - acc: 0.5700\n",
      "Epoch 00004: val_loss improved from 1.38713 to 1.25903, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/004-1.2590.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.3636 - acc: 0.5699 - val_loss: 1.2590 - val_acc: 0.6191\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2606 - acc: 0.6055\n",
      "Epoch 00005: val_loss improved from 1.25903 to 1.18096, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/005-1.1810.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.2605 - acc: 0.6055 - val_loss: 1.1810 - val_acc: 0.6408\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1682 - acc: 0.6372\n",
      "Epoch 00006: val_loss improved from 1.18096 to 1.10013, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/006-1.1001.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.1683 - acc: 0.6372 - val_loss: 1.1001 - val_acc: 0.6755\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6616\n",
      "Epoch 00007: val_loss improved from 1.10013 to 1.06895, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/007-1.0689.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.0985 - acc: 0.6616 - val_loss: 1.0689 - val_acc: 0.6713\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0426 - acc: 0.6772\n",
      "Epoch 00008: val_loss improved from 1.06895 to 1.00095, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/008-1.0009.hdf5\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 1.0425 - acc: 0.6772 - val_loss: 1.0009 - val_acc: 0.7000\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9903 - acc: 0.6971\n",
      "Epoch 00009: val_loss did not improve from 1.00095\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.9907 - acc: 0.6971 - val_loss: 1.0388 - val_acc: 0.6879\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9519 - acc: 0.7093\n",
      "Epoch 00010: val_loss improved from 1.00095 to 0.94815, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/010-0.9482.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.9519 - acc: 0.7094 - val_loss: 0.9482 - val_acc: 0.7140\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9038 - acc: 0.7242\n",
      "Epoch 00011: val_loss improved from 0.94815 to 0.93063, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/011-0.9306.hdf5\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.9037 - acc: 0.7242 - val_loss: 0.9306 - val_acc: 0.7165\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7340\n",
      "Epoch 00012: val_loss improved from 0.93063 to 0.92876, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/012-0.9288.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.8720 - acc: 0.7341 - val_loss: 0.9288 - val_acc: 0.7205\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.7481\n",
      "Epoch 00013: val_loss improved from 0.92876 to 0.88534, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/013-0.8853.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.8311 - acc: 0.7481 - val_loss: 0.8853 - val_acc: 0.7382\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7979 - acc: 0.7567\n",
      "Epoch 00014: val_loss improved from 0.88534 to 0.85750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/014-0.8575.hdf5\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.7981 - acc: 0.7566 - val_loss: 0.8575 - val_acc: 0.7410\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7665\n",
      "Epoch 00015: val_loss did not improve from 0.85750\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.7705 - acc: 0.7665 - val_loss: 0.8702 - val_acc: 0.7421\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7711\n",
      "Epoch 00016: val_loss improved from 0.85750 to 0.81078, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/016-0.8108.hdf5\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.7492 - acc: 0.7711 - val_loss: 0.8108 - val_acc: 0.7589\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.7823\n",
      "Epoch 00017: val_loss improved from 0.81078 to 0.80756, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/017-0.8076.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.7180 - acc: 0.7822 - val_loss: 0.8076 - val_acc: 0.7636\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7886\n",
      "Epoch 00018: val_loss did not improve from 0.80756\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.6932 - acc: 0.7886 - val_loss: 0.8129 - val_acc: 0.7580\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.7962\n",
      "Epoch 00019: val_loss did not improve from 0.80756\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6688 - acc: 0.7962 - val_loss: 0.8249 - val_acc: 0.7582\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.8040\n",
      "Epoch 00020: val_loss improved from 0.80756 to 0.80272, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/020-0.8027.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.6476 - acc: 0.8040 - val_loss: 0.8027 - val_acc: 0.7629\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8080\n",
      "Epoch 00021: val_loss improved from 0.80272 to 0.78100, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/021-0.7810.hdf5\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.6301 - acc: 0.8080 - val_loss: 0.7810 - val_acc: 0.7678\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.8164\n",
      "Epoch 00022: val_loss improved from 0.78100 to 0.77646, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/022-0.7765.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.6053 - acc: 0.8164 - val_loss: 0.7765 - val_acc: 0.7757\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.8224\n",
      "Epoch 00023: val_loss did not improve from 0.77646\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5834 - acc: 0.8224 - val_loss: 0.7907 - val_acc: 0.7785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8286\n",
      "Epoch 00024: val_loss improved from 0.77646 to 0.76112, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/024-0.7611.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.5637 - acc: 0.8286 - val_loss: 0.7611 - val_acc: 0.7764\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.8321\n",
      "Epoch 00025: val_loss did not improve from 0.76112\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5508 - acc: 0.8321 - val_loss: 0.7778 - val_acc: 0.7799\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8364\n",
      "Epoch 00026: val_loss improved from 0.76112 to 0.74828, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/026-0.7483.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5345 - acc: 0.8364 - val_loss: 0.7483 - val_acc: 0.7852\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8404\n",
      "Epoch 00027: val_loss did not improve from 0.74828\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5146 - acc: 0.8404 - val_loss: 0.7486 - val_acc: 0.7892\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8435\n",
      "Epoch 00028: val_loss improved from 0.74828 to 0.73654, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/028-0.7365.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5035 - acc: 0.8435 - val_loss: 0.7365 - val_acc: 0.7964\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8495\n",
      "Epoch 00029: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.4850 - acc: 0.8494 - val_loss: 0.7521 - val_acc: 0.7887\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8541\n",
      "Epoch 00030: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.4734 - acc: 0.8541 - val_loss: 0.7445 - val_acc: 0.7885\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8580\n",
      "Epoch 00031: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.4581 - acc: 0.8580 - val_loss: 0.7430 - val_acc: 0.7855\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8605\n",
      "Epoch 00032: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.4503 - acc: 0.8605 - val_loss: 0.7490 - val_acc: 0.7876\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8636\n",
      "Epoch 00033: val_loss improved from 0.73654 to 0.72818, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/033-0.7282.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4353 - acc: 0.8636 - val_loss: 0.7282 - val_acc: 0.7927\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8669\n",
      "Epoch 00034: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.4206 - acc: 0.8671 - val_loss: 0.7594 - val_acc: 0.7897\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8693\n",
      "Epoch 00035: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4115 - acc: 0.8693 - val_loss: 0.7426 - val_acc: 0.7936\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8726\n",
      "Epoch 00036: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4047 - acc: 0.8725 - val_loss: 0.7423 - val_acc: 0.7925\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8746\n",
      "Epoch 00037: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3989 - acc: 0.8746 - val_loss: 0.7388 - val_acc: 0.7906\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8770\n",
      "Epoch 00038: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3855 - acc: 0.8771 - val_loss: 0.7394 - val_acc: 0.7950\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8836\n",
      "Epoch 00039: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3714 - acc: 0.8836 - val_loss: 0.7319 - val_acc: 0.7959\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8832\n",
      "Epoch 00040: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3670 - acc: 0.8832 - val_loss: 0.7420 - val_acc: 0.7966\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8839\n",
      "Epoch 00041: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3621 - acc: 0.8839 - val_loss: 0.7668 - val_acc: 0.7941\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8895\n",
      "Epoch 00042: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3462 - acc: 0.8894 - val_loss: 0.7482 - val_acc: 0.7969\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8924\n",
      "Epoch 00043: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3366 - acc: 0.8925 - val_loss: 0.7549 - val_acc: 0.7925\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8923\n",
      "Epoch 00044: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3371 - acc: 0.8923 - val_loss: 0.7408 - val_acc: 0.7962\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8934\n",
      "Epoch 00045: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3285 - acc: 0.8934 - val_loss: 0.7572 - val_acc: 0.7887\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8988\n",
      "Epoch 00046: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3177 - acc: 0.8988 - val_loss: 0.7480 - val_acc: 0.7922\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8982\n",
      "Epoch 00047: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3162 - acc: 0.8982 - val_loss: 0.7587 - val_acc: 0.7906\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9002\n",
      "Epoch 00048: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3062 - acc: 0.9002 - val_loss: 0.7541 - val_acc: 0.7943\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9044\n",
      "Epoch 00049: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3016 - acc: 0.9044 - val_loss: 0.7453 - val_acc: 0.7992\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9040\n",
      "Epoch 00050: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2978 - acc: 0.9040 - val_loss: 0.7473 - val_acc: 0.8011\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9071\n",
      "Epoch 00051: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2889 - acc: 0.9071 - val_loss: 0.8072 - val_acc: 0.7927\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9056\n",
      "Epoch 00052: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.2884 - acc: 0.9056 - val_loss: 0.7608 - val_acc: 0.7934\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9086\n",
      "Epoch 00053: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2819 - acc: 0.9086 - val_loss: 0.7828 - val_acc: 0.7955\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9114\n",
      "Epoch 00054: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2748 - acc: 0.9114 - val_loss: 0.7594 - val_acc: 0.8039\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9118\n",
      "Epoch 00055: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2715 - acc: 0.9119 - val_loss: 0.7829 - val_acc: 0.7964\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9135\n",
      "Epoch 00056: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2651 - acc: 0.9135 - val_loss: 0.7605 - val_acc: 0.8015\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9155\n",
      "Epoch 00057: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2621 - acc: 0.9155 - val_loss: 0.7667 - val_acc: 0.8069\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9171\n",
      "Epoch 00058: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2597 - acc: 0.9170 - val_loss: 0.7667 - val_acc: 0.7962\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9165\n",
      "Epoch 00059: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2533 - acc: 0.9165 - val_loss: 0.7645 - val_acc: 0.8099\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9186\n",
      "Epoch 00060: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2499 - acc: 0.9185 - val_loss: 0.7663 - val_acc: 0.8006\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9211\n",
      "Epoch 00061: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2445 - acc: 0.9212 - val_loss: 0.7953 - val_acc: 0.8036\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9216\n",
      "Epoch 00062: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2430 - acc: 0.9216 - val_loss: 0.8217 - val_acc: 0.7997\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9220\n",
      "Epoch 00063: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2424 - acc: 0.9220 - val_loss: 0.7776 - val_acc: 0.8006\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9218\n",
      "Epoch 00064: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2377 - acc: 0.9218 - val_loss: 0.7773 - val_acc: 0.8088\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9248\n",
      "Epoch 00065: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2299 - acc: 0.9248 - val_loss: 0.7982 - val_acc: 0.8085\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9248\n",
      "Epoch 00066: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2307 - acc: 0.9248 - val_loss: 0.7929 - val_acc: 0.8053\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9268\n",
      "Epoch 00067: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2250 - acc: 0.9268 - val_loss: 0.7924 - val_acc: 0.8078\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9279\n",
      "Epoch 00068: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2229 - acc: 0.9279 - val_loss: 0.8041 - val_acc: 0.8113\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9273\n",
      "Epoch 00069: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2212 - acc: 0.9273 - val_loss: 0.8143 - val_acc: 0.8069\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9286\n",
      "Epoch 00070: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2211 - acc: 0.9287 - val_loss: 0.8196 - val_acc: 0.8074\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9295\n",
      "Epoch 00071: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2145 - acc: 0.9295 - val_loss: 0.8046 - val_acc: 0.8057\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9320\n",
      "Epoch 00072: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2129 - acc: 0.9320 - val_loss: 0.8038 - val_acc: 0.8048\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9311\n",
      "Epoch 00073: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2065 - acc: 0.9311 - val_loss: 0.8512 - val_acc: 0.8055\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9306\n",
      "Epoch 00074: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2089 - acc: 0.9306 - val_loss: 0.8092 - val_acc: 0.8123\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9321\n",
      "Epoch 00075: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2083 - acc: 0.9322 - val_loss: 0.8253 - val_acc: 0.8071\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9326\n",
      "Epoch 00076: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2016 - acc: 0.9326 - val_loss: 0.8203 - val_acc: 0.8071\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9329\n",
      "Epoch 00077: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2045 - acc: 0.9329 - val_loss: 0.8280 - val_acc: 0.8134\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9335\n",
      "Epoch 00078: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2008 - acc: 0.9335 - val_loss: 0.7871 - val_acc: 0.8118\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9347\n",
      "Epoch 00079: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.1982 - acc: 0.9347 - val_loss: 0.8503 - val_acc: 0.8088\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9351\n",
      "Epoch 00080: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.1949 - acc: 0.9351 - val_loss: 0.8262 - val_acc: 0.8130\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9376\n",
      "Epoch 00081: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.1929 - acc: 0.9376 - val_loss: 0.7923 - val_acc: 0.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9364\n",
      "Epoch 00082: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.1941 - acc: 0.9363 - val_loss: 0.8059 - val_acc: 0.8106\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9362\n",
      "Epoch 00083: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.1890 - acc: 0.9362 - val_loss: 0.8167 - val_acc: 0.8164\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 329us/sample - loss: 0.8196 - acc: 0.7605\n",
      "Loss: 0.8195533891457016 Accuracy: 0.76054\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4982 - acc: 0.1763\n",
      "Epoch 00001: val_loss improved from inf to 1.92088, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/001-1.9209.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.4982 - acc: 0.1763 - val_loss: 1.9209 - val_acc: 0.3760\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7969 - acc: 0.4131\n",
      "Epoch 00002: val_loss improved from 1.92088 to 1.55590, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/002-1.5559.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 1.7968 - acc: 0.4131 - val_loss: 1.5559 - val_acc: 0.5066\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5721 - acc: 0.4904\n",
      "Epoch 00003: val_loss improved from 1.55590 to 1.37877, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/003-1.3788.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 1.5721 - acc: 0.4904 - val_loss: 1.3788 - val_acc: 0.5765\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4449 - acc: 0.5346\n",
      "Epoch 00004: val_loss improved from 1.37877 to 1.33987, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/004-1.3399.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.4449 - acc: 0.5347 - val_loss: 1.3399 - val_acc: 0.5884\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3538 - acc: 0.5642\n",
      "Epoch 00005: val_loss improved from 1.33987 to 1.21457, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/005-1.2146.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.3538 - acc: 0.5642 - val_loss: 1.2146 - val_acc: 0.6282\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2725 - acc: 0.5951\n",
      "Epoch 00006: val_loss improved from 1.21457 to 1.17436, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/006-1.1744.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 1.2724 - acc: 0.5952 - val_loss: 1.1744 - val_acc: 0.6518\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2047 - acc: 0.6218\n",
      "Epoch 00007: val_loss improved from 1.17436 to 1.08483, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/007-1.0848.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.2047 - acc: 0.6218 - val_loss: 1.0848 - val_acc: 0.6765\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1362 - acc: 0.6424\n",
      "Epoch 00008: val_loss improved from 1.08483 to 1.00837, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/008-1.0084.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 1.1362 - acc: 0.6424 - val_loss: 1.0084 - val_acc: 0.6916\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0793 - acc: 0.6645\n",
      "Epoch 00009: val_loss improved from 1.00837 to 0.94569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/009-0.9457.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.0795 - acc: 0.6644 - val_loss: 0.9457 - val_acc: 0.7172\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0263 - acc: 0.6803\n",
      "Epoch 00010: val_loss improved from 0.94569 to 0.90257, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/010-0.9026.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 1.0262 - acc: 0.6803 - val_loss: 0.9026 - val_acc: 0.7307\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9851 - acc: 0.6926\n",
      "Epoch 00011: val_loss improved from 0.90257 to 0.86548, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/011-0.8655.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9851 - acc: 0.6926 - val_loss: 0.8655 - val_acc: 0.7396\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9433 - acc: 0.7071\n",
      "Epoch 00012: val_loss improved from 0.86548 to 0.82906, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/012-0.8291.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.9432 - acc: 0.7072 - val_loss: 0.8291 - val_acc: 0.7510\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7207\n",
      "Epoch 00013: val_loss improved from 0.82906 to 0.79616, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/013-0.7962.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9046 - acc: 0.7207 - val_loss: 0.7962 - val_acc: 0.7710\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7314\n",
      "Epoch 00014: val_loss improved from 0.79616 to 0.76465, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/014-0.7647.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.8661 - acc: 0.7314 - val_loss: 0.7647 - val_acc: 0.7738\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7488\n",
      "Epoch 00015: val_loss did not improve from 0.76465\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.8236 - acc: 0.7488 - val_loss: 0.7698 - val_acc: 0.7724\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7574\n",
      "Epoch 00016: val_loss improved from 0.76465 to 0.72178, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/016-0.7218.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.7936 - acc: 0.7575 - val_loss: 0.7218 - val_acc: 0.7908\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.7655\n",
      "Epoch 00017: val_loss improved from 0.72178 to 0.69566, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/017-0.6957.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.7649 - acc: 0.7655 - val_loss: 0.6957 - val_acc: 0.8018\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7750\n",
      "Epoch 00018: val_loss improved from 0.69566 to 0.66264, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/018-0.6626.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.7398 - acc: 0.7749 - val_loss: 0.6626 - val_acc: 0.8062\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7154 - acc: 0.7824\n",
      "Epoch 00019: val_loss improved from 0.66264 to 0.66075, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/019-0.6607.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.7154 - acc: 0.7825 - val_loss: 0.6607 - val_acc: 0.8097\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7902\n",
      "Epoch 00020: val_loss improved from 0.66075 to 0.64051, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/020-0.6405.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6931 - acc: 0.7902 - val_loss: 0.6405 - val_acc: 0.8143\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6669 - acc: 0.7987\n",
      "Epoch 00021: val_loss did not improve from 0.64051\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6669 - acc: 0.7986 - val_loss: 0.6441 - val_acc: 0.8167\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.8055\n",
      "Epoch 00022: val_loss improved from 0.64051 to 0.59966, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/022-0.5997.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6462 - acc: 0.8055 - val_loss: 0.5997 - val_acc: 0.8255\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.8096\n",
      "Epoch 00023: val_loss did not improve from 0.59966\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6276 - acc: 0.8096 - val_loss: 0.6061 - val_acc: 0.8253\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8131\n",
      "Epoch 00024: val_loss improved from 0.59966 to 0.57283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/024-0.5728.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6090 - acc: 0.8132 - val_loss: 0.5728 - val_acc: 0.8390\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8223\n",
      "Epoch 00025: val_loss did not improve from 0.57283\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.5935 - acc: 0.8223 - val_loss: 0.5785 - val_acc: 0.8332\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.8223\n",
      "Epoch 00026: val_loss improved from 0.57283 to 0.55860, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/026-0.5586.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.5823 - acc: 0.8223 - val_loss: 0.5586 - val_acc: 0.8404\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.8281\n",
      "Epoch 00027: val_loss improved from 0.55860 to 0.55577, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/027-0.5558.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5640 - acc: 0.8281 - val_loss: 0.5558 - val_acc: 0.8418\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8332\n",
      "Epoch 00028: val_loss improved from 0.55577 to 0.53802, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/028-0.5380.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.5494 - acc: 0.8332 - val_loss: 0.5380 - val_acc: 0.8474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8364\n",
      "Epoch 00029: val_loss did not improve from 0.53802\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5383 - acc: 0.8364 - val_loss: 0.5398 - val_acc: 0.8439\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8407\n",
      "Epoch 00030: val_loss improved from 0.53802 to 0.50454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/030-0.5045.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.5219 - acc: 0.8407 - val_loss: 0.5045 - val_acc: 0.8588\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8437\n",
      "Epoch 00031: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.5113 - acc: 0.8437 - val_loss: 0.5583 - val_acc: 0.8386\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8475\n",
      "Epoch 00032: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5007 - acc: 0.8475 - val_loss: 0.5179 - val_acc: 0.8519\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8518\n",
      "Epoch 00033: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4875 - acc: 0.8518 - val_loss: 0.5166 - val_acc: 0.8535\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8553\n",
      "Epoch 00034: val_loss improved from 0.50454 to 0.49462, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/034-0.4946.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4775 - acc: 0.8553 - val_loss: 0.4946 - val_acc: 0.8602\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.8550\n",
      "Epoch 00035: val_loss did not improve from 0.49462\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4722 - acc: 0.8550 - val_loss: 0.4988 - val_acc: 0.8558\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8598\n",
      "Epoch 00036: val_loss improved from 0.49462 to 0.48582, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/036-0.4858.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4620 - acc: 0.8598 - val_loss: 0.4858 - val_acc: 0.8677\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8626\n",
      "Epoch 00037: val_loss improved from 0.48582 to 0.47951, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/037-0.4795.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4531 - acc: 0.8626 - val_loss: 0.4795 - val_acc: 0.8686\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8652\n",
      "Epoch 00038: val_loss improved from 0.47951 to 0.47656, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/038-0.4766.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4397 - acc: 0.8652 - val_loss: 0.4766 - val_acc: 0.8686\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8683\n",
      "Epoch 00039: val_loss improved from 0.47656 to 0.45949, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/039-0.4595.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4334 - acc: 0.8683 - val_loss: 0.4595 - val_acc: 0.8768\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8696\n",
      "Epoch 00040: val_loss improved from 0.45949 to 0.45348, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/040-0.4535.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4293 - acc: 0.8696 - val_loss: 0.4535 - val_acc: 0.8698\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8710\n",
      "Epoch 00041: val_loss improved from 0.45348 to 0.44869, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/041-0.4487.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.4217 - acc: 0.8710 - val_loss: 0.4487 - val_acc: 0.8754\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8730\n",
      "Epoch 00042: val_loss improved from 0.44869 to 0.44419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/042-0.4442.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4123 - acc: 0.8730 - val_loss: 0.4442 - val_acc: 0.8779\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8758\n",
      "Epoch 00043: val_loss improved from 0.44419 to 0.43576, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/043-0.4358.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4040 - acc: 0.8758 - val_loss: 0.4358 - val_acc: 0.8826\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8774\n",
      "Epoch 00044: val_loss improved from 0.43576 to 0.43255, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/044-0.4326.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3969 - acc: 0.8774 - val_loss: 0.4326 - val_acc: 0.8812\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8796\n",
      "Epoch 00045: val_loss did not improve from 0.43255\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.3907 - acc: 0.8797 - val_loss: 0.4336 - val_acc: 0.8866\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8855\n",
      "Epoch 00046: val_loss improved from 0.43255 to 0.42838, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/046-0.4284.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3786 - acc: 0.8855 - val_loss: 0.4284 - val_acc: 0.8863\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8844\n",
      "Epoch 00047: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.3770 - acc: 0.8844 - val_loss: 0.4377 - val_acc: 0.8805\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8851\n",
      "Epoch 00048: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.3760 - acc: 0.8851 - val_loss: 0.4316 - val_acc: 0.8842\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8845\n",
      "Epoch 00049: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3747 - acc: 0.8845 - val_loss: 0.4445 - val_acc: 0.8761\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8913\n",
      "Epoch 00050: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3577 - acc: 0.8913 - val_loss: 0.4672 - val_acc: 0.8812\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8881\n",
      "Epoch 00051: val_loss improved from 0.42838 to 0.41929, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/051-0.4193.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.3575 - acc: 0.8881 - val_loss: 0.4193 - val_acc: 0.8915\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8920\n",
      "Epoch 00052: val_loss improved from 0.41929 to 0.41435, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/052-0.4143.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3484 - acc: 0.8919 - val_loss: 0.4143 - val_acc: 0.8894\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8936\n",
      "Epoch 00053: val_loss did not improve from 0.41435\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3478 - acc: 0.8935 - val_loss: 0.4181 - val_acc: 0.8884\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8933\n",
      "Epoch 00054: val_loss improved from 0.41435 to 0.41050, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/054-0.4105.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3415 - acc: 0.8933 - val_loss: 0.4105 - val_acc: 0.8859\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8956\n",
      "Epoch 00055: val_loss did not improve from 0.41050\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3340 - acc: 0.8956 - val_loss: 0.4116 - val_acc: 0.8912\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8943\n",
      "Epoch 00056: val_loss improved from 0.41050 to 0.40054, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/056-0.4005.hdf5\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.3366 - acc: 0.8942 - val_loss: 0.4005 - val_acc: 0.8882\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.8997\n",
      "Epoch 00057: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.3210 - acc: 0.8997 - val_loss: 0.4258 - val_acc: 0.8901\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9011\n",
      "Epoch 00058: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.3196 - acc: 0.9011 - val_loss: 0.4510 - val_acc: 0.8761\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9003\n",
      "Epoch 00059: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.3208 - acc: 0.9003 - val_loss: 0.4011 - val_acc: 0.8898\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9026\n",
      "Epoch 00060: val_loss improved from 0.40054 to 0.39376, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/060-0.3938.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3148 - acc: 0.9026 - val_loss: 0.3938 - val_acc: 0.8968\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9032\n",
      "Epoch 00061: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3106 - acc: 0.9032 - val_loss: 0.3989 - val_acc: 0.8966\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9043\n",
      "Epoch 00062: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.3063 - acc: 0.9043 - val_loss: 0.4545 - val_acc: 0.8812\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9055\n",
      "Epoch 00063: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3018 - acc: 0.9055 - val_loss: 0.3943 - val_acc: 0.8963\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9048\n",
      "Epoch 00064: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2994 - acc: 0.9047 - val_loss: 0.3965 - val_acc: 0.8940\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9084\n",
      "Epoch 00065: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2934 - acc: 0.9084 - val_loss: 0.4059 - val_acc: 0.8921\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9079\n",
      "Epoch 00066: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2915 - acc: 0.9079 - val_loss: 0.4108 - val_acc: 0.8928\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9105\n",
      "Epoch 00067: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.2873 - acc: 0.9106 - val_loss: 0.3983 - val_acc: 0.8973\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9118\n",
      "Epoch 00068: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.2825 - acc: 0.9118 - val_loss: 0.3966 - val_acc: 0.8968\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9141\n",
      "Epoch 00069: val_loss improved from 0.39376 to 0.38444, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/069-0.3844.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2759 - acc: 0.9141 - val_loss: 0.3844 - val_acc: 0.9026\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9129\n",
      "Epoch 00070: val_loss did not improve from 0.38444\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2745 - acc: 0.9129 - val_loss: 0.3890 - val_acc: 0.9022\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9139\n",
      "Epoch 00071: val_loss improved from 0.38444 to 0.37927, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/071-0.3793.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2691 - acc: 0.9139 - val_loss: 0.3793 - val_acc: 0.9031\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9125\n",
      "Epoch 00072: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2763 - acc: 0.9124 - val_loss: 0.3864 - val_acc: 0.9045\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9158\n",
      "Epoch 00073: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2649 - acc: 0.9158 - val_loss: 0.3866 - val_acc: 0.9012\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9165\n",
      "Epoch 00074: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2597 - acc: 0.9165 - val_loss: 0.3824 - val_acc: 0.9036\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9161\n",
      "Epoch 00075: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2643 - acc: 0.9161 - val_loss: 0.3922 - val_acc: 0.8994\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9187\n",
      "Epoch 00076: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2567 - acc: 0.9187 - val_loss: 0.4135 - val_acc: 0.8947\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9177\n",
      "Epoch 00077: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2572 - acc: 0.9177 - val_loss: 0.3899 - val_acc: 0.9015\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9217\n",
      "Epoch 00078: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2436 - acc: 0.9217 - val_loss: 0.3860 - val_acc: 0.9019\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9233\n",
      "Epoch 00079: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2447 - acc: 0.9234 - val_loss: 0.3882 - val_acc: 0.9052\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9209\n",
      "Epoch 00080: val_loss improved from 0.37927 to 0.37398, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/080-0.3740.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2458 - acc: 0.9209 - val_loss: 0.3740 - val_acc: 0.9068\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9230\n",
      "Epoch 00081: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2408 - acc: 0.9230 - val_loss: 0.3764 - val_acc: 0.9089\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9260\n",
      "Epoch 00082: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.2362 - acc: 0.9259 - val_loss: 0.3826 - val_acc: 0.9008\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9252\n",
      "Epoch 00083: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2345 - acc: 0.9253 - val_loss: 0.4144 - val_acc: 0.8980\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9252\n",
      "Epoch 00084: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2337 - acc: 0.9252 - val_loss: 0.3756 - val_acc: 0.9052\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9248\n",
      "Epoch 00085: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2327 - acc: 0.9247 - val_loss: 0.3845 - val_acc: 0.9059\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9260\n",
      "Epoch 00086: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2319 - acc: 0.9260 - val_loss: 0.3752 - val_acc: 0.9064\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9263\n",
      "Epoch 00087: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2270 - acc: 0.9263 - val_loss: 0.3889 - val_acc: 0.9078\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00088: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.3780 - val_acc: 0.9073\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9279\n",
      "Epoch 00089: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2243 - acc: 0.9279 - val_loss: 0.3914 - val_acc: 0.9073\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9291\n",
      "Epoch 00090: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.2185 - acc: 0.9291 - val_loss: 0.3916 - val_acc: 0.9054\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9272\n",
      "Epoch 00091: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2217 - acc: 0.9272 - val_loss: 0.3837 - val_acc: 0.9054\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9291\n",
      "Epoch 00092: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2167 - acc: 0.9291 - val_loss: 0.4016 - val_acc: 0.8949\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9310\n",
      "Epoch 00093: val_loss improved from 0.37398 to 0.36884, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/093-0.3688.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2137 - acc: 0.9310 - val_loss: 0.3688 - val_acc: 0.9103\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9311\n",
      "Epoch 00094: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2130 - acc: 0.9311 - val_loss: 0.3812 - val_acc: 0.9110\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9321\n",
      "Epoch 00095: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2124 - acc: 0.9322 - val_loss: 0.3725 - val_acc: 0.9094\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9314\n",
      "Epoch 00096: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2088 - acc: 0.9314 - val_loss: 0.3792 - val_acc: 0.9066\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9306\n",
      "Epoch 00097: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2104 - acc: 0.9306 - val_loss: 0.3828 - val_acc: 0.9101\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9343\n",
      "Epoch 00098: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2037 - acc: 0.9343 - val_loss: 0.3864 - val_acc: 0.9071\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9338\n",
      "Epoch 00099: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2050 - acc: 0.9338 - val_loss: 0.3705 - val_acc: 0.9099\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9346\n",
      "Epoch 00100: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2011 - acc: 0.9346 - val_loss: 0.3848 - val_acc: 0.9047\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9318\n",
      "Epoch 00101: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2109 - acc: 0.9318 - val_loss: 0.3694 - val_acc: 0.9110\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9348\n",
      "Epoch 00102: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1950 - acc: 0.9348 - val_loss: 0.3724 - val_acc: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9357\n",
      "Epoch 00103: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1932 - acc: 0.9357 - val_loss: 0.3702 - val_acc: 0.9106\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9361\n",
      "Epoch 00104: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1938 - acc: 0.9361 - val_loss: 0.3793 - val_acc: 0.9045\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9367\n",
      "Epoch 00105: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1945 - acc: 0.9367 - val_loss: 0.3843 - val_acc: 0.9099\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9361\n",
      "Epoch 00106: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1951 - acc: 0.9361 - val_loss: 0.3823 - val_acc: 0.9078\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9378\n",
      "Epoch 00107: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1902 - acc: 0.9378 - val_loss: 0.3907 - val_acc: 0.9073\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9379\n",
      "Epoch 00108: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1868 - acc: 0.9379 - val_loss: 0.3877 - val_acc: 0.9080\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9377\n",
      "Epoch 00109: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1860 - acc: 0.9376 - val_loss: 0.3907 - val_acc: 0.9092\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9381\n",
      "Epoch 00110: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1893 - acc: 0.9381 - val_loss: 0.3703 - val_acc: 0.9124\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9405\n",
      "Epoch 00111: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1835 - acc: 0.9405 - val_loss: 0.3780 - val_acc: 0.9096\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9393\n",
      "Epoch 00112: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1837 - acc: 0.9393 - val_loss: 0.3782 - val_acc: 0.9145\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9419\n",
      "Epoch 00113: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1800 - acc: 0.9419 - val_loss: 0.3852 - val_acc: 0.9092\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9400\n",
      "Epoch 00114: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1826 - acc: 0.9400 - val_loss: 0.3818 - val_acc: 0.9103\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9415\n",
      "Epoch 00115: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1780 - acc: 0.9415 - val_loss: 0.3898 - val_acc: 0.9110\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9404\n",
      "Epoch 00116: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1804 - acc: 0.9404 - val_loss: 0.3832 - val_acc: 0.9145\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9421\n",
      "Epoch 00117: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1753 - acc: 0.9420 - val_loss: 0.3842 - val_acc: 0.9138\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9423\n",
      "Epoch 00118: val_loss improved from 0.36884 to 0.36817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/118-0.3682.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1752 - acc: 0.9423 - val_loss: 0.3682 - val_acc: 0.9145\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9438\n",
      "Epoch 00119: val_loss did not improve from 0.36817\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1721 - acc: 0.9438 - val_loss: 0.3894 - val_acc: 0.9087\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9425\n",
      "Epoch 00120: val_loss did not improve from 0.36817\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1773 - acc: 0.9425 - val_loss: 0.3703 - val_acc: 0.9180\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9439\n",
      "Epoch 00121: val_loss improved from 0.36817 to 0.36709, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/121-0.3671.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1720 - acc: 0.9439 - val_loss: 0.3671 - val_acc: 0.9119\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9437\n",
      "Epoch 00122: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1665 - acc: 0.9437 - val_loss: 0.3883 - val_acc: 0.9096\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9428\n",
      "Epoch 00123: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1688 - acc: 0.9428 - val_loss: 0.3850 - val_acc: 0.9115\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9458\n",
      "Epoch 00124: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1628 - acc: 0.9458 - val_loss: 0.3902 - val_acc: 0.9122\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9449\n",
      "Epoch 00125: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1705 - acc: 0.9448 - val_loss: 0.3801 - val_acc: 0.9110\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9434\n",
      "Epoch 00126: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1676 - acc: 0.9434 - val_loss: 0.4010 - val_acc: 0.9101\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9486\n",
      "Epoch 00127: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3774 - val_acc: 0.9164\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9459\n",
      "Epoch 00128: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1613 - acc: 0.9459 - val_loss: 0.3732 - val_acc: 0.9143\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9482\n",
      "Epoch 00129: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1609 - acc: 0.9482 - val_loss: 0.3757 - val_acc: 0.9161\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9464\n",
      "Epoch 00130: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1629 - acc: 0.9464 - val_loss: 0.3781 - val_acc: 0.9101\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9479\n",
      "Epoch 00131: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1554 - acc: 0.9479 - val_loss: 0.3805 - val_acc: 0.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9482\n",
      "Epoch 00132: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1572 - acc: 0.9482 - val_loss: 0.3851 - val_acc: 0.9150\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9476\n",
      "Epoch 00133: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1578 - acc: 0.9476 - val_loss: 0.3912 - val_acc: 0.9106\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9457\n",
      "Epoch 00134: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1598 - acc: 0.9457 - val_loss: 0.3834 - val_acc: 0.9140\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9494\n",
      "Epoch 00135: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1540 - acc: 0.9494 - val_loss: 0.3759 - val_acc: 0.9124\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9485\n",
      "Epoch 00136: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1555 - acc: 0.9485 - val_loss: 0.3793 - val_acc: 0.9173\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9496\n",
      "Epoch 00137: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1513 - acc: 0.9496 - val_loss: 0.3677 - val_acc: 0.9150\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9504\n",
      "Epoch 00138: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1480 - acc: 0.9504 - val_loss: 0.3893 - val_acc: 0.9159\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9485\n",
      "Epoch 00139: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1528 - acc: 0.9485 - val_loss: 0.3799 - val_acc: 0.9113\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9508\n",
      "Epoch 00140: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1504 - acc: 0.9508 - val_loss: 0.3707 - val_acc: 0.9152\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9511\n",
      "Epoch 00141: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1442 - acc: 0.9511 - val_loss: 0.3838 - val_acc: 0.9129\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9545\n",
      "Epoch 00142: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1431 - acc: 0.9545 - val_loss: 0.3782 - val_acc: 0.9192\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9522\n",
      "Epoch 00143: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1485 - acc: 0.9522 - val_loss: 0.3870 - val_acc: 0.9147\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9523\n",
      "Epoch 00144: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1452 - acc: 0.9523 - val_loss: 0.3786 - val_acc: 0.9147\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9500\n",
      "Epoch 00145: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1476 - acc: 0.9500 - val_loss: 0.3833 - val_acc: 0.9140\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9536\n",
      "Epoch 00146: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1395 - acc: 0.9536 - val_loss: 0.3756 - val_acc: 0.9159\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9511\n",
      "Epoch 00147: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1423 - acc: 0.9511 - val_loss: 0.3833 - val_acc: 0.9154\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9531\n",
      "Epoch 00148: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1447 - acc: 0.9531 - val_loss: 0.3776 - val_acc: 0.9173\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9527\n",
      "Epoch 00149: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1439 - acc: 0.9527 - val_loss: 0.3875 - val_acc: 0.9150\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9543\n",
      "Epoch 00150: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1400 - acc: 0.9543 - val_loss: 0.3912 - val_acc: 0.9126\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9534\n",
      "Epoch 00151: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1408 - acc: 0.9534 - val_loss: 0.3881 - val_acc: 0.9136\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9528\n",
      "Epoch 00152: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1398 - acc: 0.9528 - val_loss: 0.3823 - val_acc: 0.9171\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9557\n",
      "Epoch 00153: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1332 - acc: 0.9557 - val_loss: 0.3863 - val_acc: 0.9154\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9546\n",
      "Epoch 00154: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1357 - acc: 0.9547 - val_loss: 0.3914 - val_acc: 0.9152\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9554\n",
      "Epoch 00155: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1363 - acc: 0.9554 - val_loss: 0.3822 - val_acc: 0.9173\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9523\n",
      "Epoch 00156: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1381 - acc: 0.9523 - val_loss: 0.3860 - val_acc: 0.9129\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9540\n",
      "Epoch 00157: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1327 - acc: 0.9539 - val_loss: 0.4208 - val_acc: 0.9099\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9548\n",
      "Epoch 00158: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1349 - acc: 0.9548 - val_loss: 0.4135 - val_acc: 0.9143\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9554\n",
      "Epoch 00159: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1336 - acc: 0.9554 - val_loss: 0.3696 - val_acc: 0.9182\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9573\n",
      "Epoch 00160: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1282 - acc: 0.9573 - val_loss: 0.3739 - val_acc: 0.9220\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9565\n",
      "Epoch 00161: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1284 - acc: 0.9565 - val_loss: 0.4090 - val_acc: 0.9157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9541\n",
      "Epoch 00162: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1360 - acc: 0.9541 - val_loss: 0.3691 - val_acc: 0.9171\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9562\n",
      "Epoch 00163: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1344 - acc: 0.9562 - val_loss: 0.3825 - val_acc: 0.9196\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9557\n",
      "Epoch 00164: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1318 - acc: 0.9557 - val_loss: 0.3897 - val_acc: 0.9173\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9590\n",
      "Epoch 00165: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1246 - acc: 0.9590 - val_loss: 0.3822 - val_acc: 0.9180\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9581\n",
      "Epoch 00166: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1272 - acc: 0.9581 - val_loss: 0.3957 - val_acc: 0.9152\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9570\n",
      "Epoch 00167: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1276 - acc: 0.9570 - val_loss: 0.3862 - val_acc: 0.9131\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9567\n",
      "Epoch 00168: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.1251 - acc: 0.9567 - val_loss: 0.3831 - val_acc: 0.9154\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9575\n",
      "Epoch 00169: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1244 - acc: 0.9575 - val_loss: 0.3865 - val_acc: 0.9199\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9581\n",
      "Epoch 00170: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1247 - acc: 0.9581 - val_loss: 0.3876 - val_acc: 0.9147\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9566\n",
      "Epoch 00171: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1268 - acc: 0.9566 - val_loss: 0.3805 - val_acc: 0.9203\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 348us/sample - loss: 0.4329 - acc: 0.8893\n",
      "Loss: 0.4329061113166413 Accuracy: 0.8893043\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5627 - acc: 0.1510\n",
      "Epoch 00001: val_loss improved from inf to 2.01433, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/001-2.0143.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 2.5627 - acc: 0.1510 - val_loss: 2.0143 - val_acc: 0.3364\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8914 - acc: 0.3705\n",
      "Epoch 00002: val_loss improved from 2.01433 to 1.52395, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/002-1.5239.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 1.8914 - acc: 0.3705 - val_loss: 1.5239 - val_acc: 0.5178\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5867 - acc: 0.4679\n",
      "Epoch 00003: val_loss improved from 1.52395 to 1.30286, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/003-1.3029.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 1.5867 - acc: 0.4679 - val_loss: 1.3029 - val_acc: 0.5907\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4251 - acc: 0.5273\n",
      "Epoch 00004: val_loss improved from 1.30286 to 1.18139, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/004-1.1814.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.4251 - acc: 0.5272 - val_loss: 1.1814 - val_acc: 0.6375\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3094 - acc: 0.5677\n",
      "Epoch 00005: val_loss improved from 1.18139 to 1.08469, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/005-1.0847.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.3096 - acc: 0.5676 - val_loss: 1.0847 - val_acc: 0.6713\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2136 - acc: 0.6044\n",
      "Epoch 00006: val_loss improved from 1.08469 to 1.03232, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/006-1.0323.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 1.2131 - acc: 0.6046 - val_loss: 1.0323 - val_acc: 0.6969\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1271 - acc: 0.6386\n",
      "Epoch 00007: val_loss improved from 1.03232 to 0.94146, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/007-0.9415.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 1.1270 - acc: 0.6386 - val_loss: 0.9415 - val_acc: 0.7226\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0500 - acc: 0.6671\n",
      "Epoch 00008: val_loss improved from 0.94146 to 0.86306, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/008-0.8631.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 1.0500 - acc: 0.6671 - val_loss: 0.8631 - val_acc: 0.7386\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.6922\n",
      "Epoch 00009: val_loss improved from 0.86306 to 0.79546, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/009-0.7955.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9832 - acc: 0.6922 - val_loss: 0.7955 - val_acc: 0.7706\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7127\n",
      "Epoch 00010: val_loss improved from 0.79546 to 0.74712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/010-0.7471.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.9160 - acc: 0.7127 - val_loss: 0.7471 - val_acc: 0.7852\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8617 - acc: 0.7359\n",
      "Epoch 00011: val_loss improved from 0.74712 to 0.68827, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/011-0.6883.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.8617 - acc: 0.7359 - val_loss: 0.6883 - val_acc: 0.8022\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8063 - acc: 0.7517\n",
      "Epoch 00012: val_loss improved from 0.68827 to 0.64109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/012-0.6411.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.8063 - acc: 0.7517 - val_loss: 0.6411 - val_acc: 0.8188\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.7675\n",
      "Epoch 00013: val_loss improved from 0.64109 to 0.60197, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/013-0.6020.hdf5\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.7615 - acc: 0.7675 - val_loss: 0.6020 - val_acc: 0.8283\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7184 - acc: 0.7815\n",
      "Epoch 00014: val_loss improved from 0.60197 to 0.56504, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/014-0.5650.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.7185 - acc: 0.7816 - val_loss: 0.5650 - val_acc: 0.8430\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7887\n",
      "Epoch 00015: val_loss improved from 0.56504 to 0.55159, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/015-0.5516.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6827 - acc: 0.7887 - val_loss: 0.5516 - val_acc: 0.8451\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.8006\n",
      "Epoch 00016: val_loss improved from 0.55159 to 0.53894, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/016-0.5389.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.6518 - acc: 0.8005 - val_loss: 0.5389 - val_acc: 0.8479\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8120\n",
      "Epoch 00017: val_loss improved from 0.53894 to 0.49715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/017-0.4971.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.6171 - acc: 0.8120 - val_loss: 0.4971 - val_acc: 0.8595\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5944 - acc: 0.8180\n",
      "Epoch 00018: val_loss improved from 0.49715 to 0.46630, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/018-0.4663.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5944 - acc: 0.8180 - val_loss: 0.4663 - val_acc: 0.8707\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8271\n",
      "Epoch 00019: val_loss did not improve from 0.46630\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.5682 - acc: 0.8271 - val_loss: 0.4863 - val_acc: 0.8528\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.8322\n",
      "Epoch 00020: val_loss improved from 0.46630 to 0.42246, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/020-0.4225.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5468 - acc: 0.8322 - val_loss: 0.4225 - val_acc: 0.8835\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8390\n",
      "Epoch 00021: val_loss improved from 0.42246 to 0.40320, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/021-0.4032.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5279 - acc: 0.8390 - val_loss: 0.4032 - val_acc: 0.8870\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8466\n",
      "Epoch 00022: val_loss did not improve from 0.40320\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.5027 - acc: 0.8467 - val_loss: 0.4291 - val_acc: 0.8786\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8499\n",
      "Epoch 00023: val_loss improved from 0.40320 to 0.38296, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/023-0.3830.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4919 - acc: 0.8498 - val_loss: 0.3830 - val_acc: 0.8942\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8528\n",
      "Epoch 00024: val_loss improved from 0.38296 to 0.36440, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/024-0.3644.hdf5\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.4788 - acc: 0.8528 - val_loss: 0.3644 - val_acc: 0.8984\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8579\n",
      "Epoch 00025: val_loss did not improve from 0.36440\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4585 - acc: 0.8579 - val_loss: 0.3737 - val_acc: 0.8952\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8634\n",
      "Epoch 00026: val_loss improved from 0.36440 to 0.34750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/026-0.3475.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4468 - acc: 0.8634 - val_loss: 0.3475 - val_acc: 0.9036\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8679\n",
      "Epoch 00027: val_loss did not improve from 0.34750\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.4283 - acc: 0.8679 - val_loss: 0.3527 - val_acc: 0.9040\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8716\n",
      "Epoch 00028: val_loss improved from 0.34750 to 0.34621, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/028-0.3462.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.4220 - acc: 0.8716 - val_loss: 0.3462 - val_acc: 0.9008\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8732\n",
      "Epoch 00029: val_loss improved from 0.34621 to 0.30986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/029-0.3099.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4113 - acc: 0.8732 - val_loss: 0.3099 - val_acc: 0.9108\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8766\n",
      "Epoch 00030: val_loss did not improve from 0.30986\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.4008 - acc: 0.8766 - val_loss: 0.3108 - val_acc: 0.9124\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8811\n",
      "Epoch 00031: val_loss did not improve from 0.30986\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3879 - acc: 0.8811 - val_loss: 0.3312 - val_acc: 0.9061\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8818\n",
      "Epoch 00032: val_loss improved from 0.30986 to 0.30608, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/032-0.3061.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3816 - acc: 0.8818 - val_loss: 0.3061 - val_acc: 0.9138\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8873\n",
      "Epoch 00033: val_loss improved from 0.30608 to 0.28891, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/033-0.2889.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3650 - acc: 0.8873 - val_loss: 0.2889 - val_acc: 0.9213\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8855\n",
      "Epoch 00034: val_loss improved from 0.28891 to 0.28709, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/034-0.2871.hdf5\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.3652 - acc: 0.8855 - val_loss: 0.2871 - val_acc: 0.9159\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8887\n",
      "Epoch 00035: val_loss improved from 0.28709 to 0.27911, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/035-0.2791.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3571 - acc: 0.8887 - val_loss: 0.2791 - val_acc: 0.9276\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8933\n",
      "Epoch 00036: val_loss improved from 0.27911 to 0.26817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/036-0.2682.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3446 - acc: 0.8933 - val_loss: 0.2682 - val_acc: 0.9259\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8924\n",
      "Epoch 00037: val_loss did not improve from 0.26817\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3416 - acc: 0.8924 - val_loss: 0.2922 - val_acc: 0.9133\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8942\n",
      "Epoch 00038: val_loss did not improve from 0.26817\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3404 - acc: 0.8942 - val_loss: 0.2689 - val_acc: 0.9271\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8973\n",
      "Epoch 00039: val_loss improved from 0.26817 to 0.25545, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/039-0.2555.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3327 - acc: 0.8973 - val_loss: 0.2555 - val_acc: 0.9304\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8998\n",
      "Epoch 00040: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3264 - acc: 0.8998 - val_loss: 0.2773 - val_acc: 0.9213\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9004\n",
      "Epoch 00041: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3151 - acc: 0.9003 - val_loss: 0.2633 - val_acc: 0.9262\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9007\n",
      "Epoch 00042: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3150 - acc: 0.9007 - val_loss: 0.2714 - val_acc: 0.9238\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9031\n",
      "Epoch 00043: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3101 - acc: 0.9031 - val_loss: 0.2589 - val_acc: 0.9280\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9050\n",
      "Epoch 00044: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.3035 - acc: 0.9050 - val_loss: 0.2605 - val_acc: 0.9252\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9052\n",
      "Epoch 00045: val_loss improved from 0.25545 to 0.24513, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/045-0.2451.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2997 - acc: 0.9052 - val_loss: 0.2451 - val_acc: 0.9327\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9069\n",
      "Epoch 00046: val_loss did not improve from 0.24513\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2915 - acc: 0.9068 - val_loss: 0.2478 - val_acc: 0.9320\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9097\n",
      "Epoch 00047: val_loss did not improve from 0.24513\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.2889 - acc: 0.9097 - val_loss: 0.2487 - val_acc: 0.9308\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9108\n",
      "Epoch 00048: val_loss improved from 0.24513 to 0.23944, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/048-0.2394.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2837 - acc: 0.9108 - val_loss: 0.2394 - val_acc: 0.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9106\n",
      "Epoch 00049: val_loss did not improve from 0.23944\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2801 - acc: 0.9106 - val_loss: 0.2418 - val_acc: 0.9306\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9120\n",
      "Epoch 00050: val_loss improved from 0.23944 to 0.23385, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/050-0.2338.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2790 - acc: 0.9120 - val_loss: 0.2338 - val_acc: 0.9324\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9146\n",
      "Epoch 00051: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2745 - acc: 0.9147 - val_loss: 0.2506 - val_acc: 0.9299\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9139\n",
      "Epoch 00052: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2713 - acc: 0.9139 - val_loss: 0.2390 - val_acc: 0.9304\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9158\n",
      "Epoch 00053: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2661 - acc: 0.9158 - val_loss: 0.2343 - val_acc: 0.9334\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9171\n",
      "Epoch 00054: val_loss improved from 0.23385 to 0.23209, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/054-0.2321.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2619 - acc: 0.9171 - val_loss: 0.2321 - val_acc: 0.9329\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9170\n",
      "Epoch 00055: val_loss improved from 0.23209 to 0.23034, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/055-0.2303.hdf5\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.2606 - acc: 0.9170 - val_loss: 0.2303 - val_acc: 0.9355\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9179\n",
      "Epoch 00056: val_loss did not improve from 0.23034\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2582 - acc: 0.9179 - val_loss: 0.2352 - val_acc: 0.9338\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9185\n",
      "Epoch 00057: val_loss did not improve from 0.23034\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2543 - acc: 0.9185 - val_loss: 0.2367 - val_acc: 0.9313\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9191\n",
      "Epoch 00058: val_loss improved from 0.23034 to 0.22321, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/058-0.2232.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2514 - acc: 0.9192 - val_loss: 0.2232 - val_acc: 0.9362\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9213\n",
      "Epoch 00059: val_loss did not improve from 0.22321\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2490 - acc: 0.9213 - val_loss: 0.2372 - val_acc: 0.9331\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9209\n",
      "Epoch 00060: val_loss improved from 0.22321 to 0.22080, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/060-0.2208.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2453 - acc: 0.9208 - val_loss: 0.2208 - val_acc: 0.9394\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.2455 - acc: 0.9220 - val_loss: 0.2264 - val_acc: 0.9355\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9228\n",
      "Epoch 00062: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2412 - acc: 0.9229 - val_loss: 0.2248 - val_acc: 0.9376\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9249\n",
      "Epoch 00063: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2341 - acc: 0.9250 - val_loss: 0.2217 - val_acc: 0.9390\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9265\n",
      "Epoch 00064: val_loss improved from 0.22080 to 0.21875, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/064-0.2188.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2303 - acc: 0.9265 - val_loss: 0.2188 - val_acc: 0.9357\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9234\n",
      "Epoch 00065: val_loss did not improve from 0.21875\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2349 - acc: 0.9234 - val_loss: 0.2201 - val_acc: 0.9387\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9269\n",
      "Epoch 00066: val_loss improved from 0.21875 to 0.21768, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/066-0.2177.hdf5\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2253 - acc: 0.9269 - val_loss: 0.2177 - val_acc: 0.9413\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9286\n",
      "Epoch 00067: val_loss improved from 0.21768 to 0.20772, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/067-0.2077.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2238 - acc: 0.9287 - val_loss: 0.2077 - val_acc: 0.9411\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9270\n",
      "Epoch 00068: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2234 - acc: 0.9269 - val_loss: 0.2122 - val_acc: 0.9427\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9286\n",
      "Epoch 00069: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2209 - acc: 0.9286 - val_loss: 0.2138 - val_acc: 0.9385\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9282\n",
      "Epoch 00070: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2191 - acc: 0.9281 - val_loss: 0.2383 - val_acc: 0.9322\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9284\n",
      "Epoch 00071: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2214 - acc: 0.9284 - val_loss: 0.2198 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9320\n",
      "Epoch 00072: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.2141 - acc: 0.9320 - val_loss: 0.2322 - val_acc: 0.9376\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9314\n",
      "Epoch 00073: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2129 - acc: 0.9314 - val_loss: 0.2127 - val_acc: 0.9385\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9308\n",
      "Epoch 00074: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2101 - acc: 0.9309 - val_loss: 0.2234 - val_acc: 0.9362\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9312\n",
      "Epoch 00075: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2087 - acc: 0.9313 - val_loss: 0.2111 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9325\n",
      "Epoch 00076: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2065 - acc: 0.9325 - val_loss: 0.2315 - val_acc: 0.9383\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9346\n",
      "Epoch 00077: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2022 - acc: 0.9346 - val_loss: 0.2162 - val_acc: 0.9394\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9349\n",
      "Epoch 00078: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1998 - acc: 0.9350 - val_loss: 0.2113 - val_acc: 0.9420\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9350\n",
      "Epoch 00079: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2016 - acc: 0.9350 - val_loss: 0.2167 - val_acc: 0.9397\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9346\n",
      "Epoch 00080: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2025 - acc: 0.9346 - val_loss: 0.2119 - val_acc: 0.9399\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9356\n",
      "Epoch 00081: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1976 - acc: 0.9356 - val_loss: 0.2107 - val_acc: 0.9434\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9360\n",
      "Epoch 00082: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1941 - acc: 0.9360 - val_loss: 0.2206 - val_acc: 0.9392\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9354\n",
      "Epoch 00083: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1976 - acc: 0.9354 - val_loss: 0.2175 - val_acc: 0.9387\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9371\n",
      "Epoch 00084: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1946 - acc: 0.9371 - val_loss: 0.2093 - val_acc: 0.9422\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9355\n",
      "Epoch 00085: val_loss improved from 0.20772 to 0.19980, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/085-0.1998.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1950 - acc: 0.9355 - val_loss: 0.1998 - val_acc: 0.9474\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9380\n",
      "Epoch 00086: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1872 - acc: 0.9380 - val_loss: 0.2007 - val_acc: 0.9434\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9374\n",
      "Epoch 00087: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1920 - acc: 0.9375 - val_loss: 0.2002 - val_acc: 0.9460\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9388\n",
      "Epoch 00088: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1839 - acc: 0.9388 - val_loss: 0.2107 - val_acc: 0.9418\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9389\n",
      "Epoch 00089: val_loss improved from 0.19980 to 0.19565, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/089-0.1957.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1870 - acc: 0.9388 - val_loss: 0.1957 - val_acc: 0.9453\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9391\n",
      "Epoch 00090: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1837 - acc: 0.9391 - val_loss: 0.2035 - val_acc: 0.9441\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9408\n",
      "Epoch 00091: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1807 - acc: 0.9408 - val_loss: 0.1984 - val_acc: 0.9443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9395\n",
      "Epoch 00092: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1833 - acc: 0.9395 - val_loss: 0.2052 - val_acc: 0.9462\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9403\n",
      "Epoch 00093: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1841 - acc: 0.9403 - val_loss: 0.2197 - val_acc: 0.9404\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9416\n",
      "Epoch 00094: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1742 - acc: 0.9416 - val_loss: 0.2035 - val_acc: 0.9429\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9420\n",
      "Epoch 00095: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1740 - acc: 0.9420 - val_loss: 0.2092 - val_acc: 0.9425\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9431\n",
      "Epoch 00096: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1704 - acc: 0.9431 - val_loss: 0.2097 - val_acc: 0.9439\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9431\n",
      "Epoch 00097: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1727 - acc: 0.9432 - val_loss: 0.2025 - val_acc: 0.9429\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9415\n",
      "Epoch 00098: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1753 - acc: 0.9415 - val_loss: 0.2012 - val_acc: 0.9446\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9436\n",
      "Epoch 00099: val_loss improved from 0.19565 to 0.19551, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/099-0.1955.hdf5\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1711 - acc: 0.9436 - val_loss: 0.1955 - val_acc: 0.9467\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9424\n",
      "Epoch 00100: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1703 - acc: 0.9424 - val_loss: 0.2062 - val_acc: 0.9413\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9440\n",
      "Epoch 00101: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1696 - acc: 0.9440 - val_loss: 0.1987 - val_acc: 0.9446\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9432\n",
      "Epoch 00102: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1694 - acc: 0.9432 - val_loss: 0.2009 - val_acc: 0.9474\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9438\n",
      "Epoch 00103: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1732 - acc: 0.9438 - val_loss: 0.2215 - val_acc: 0.9406\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9443\n",
      "Epoch 00104: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1688 - acc: 0.9443 - val_loss: 0.2039 - val_acc: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9459\n",
      "Epoch 00105: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1630 - acc: 0.9459 - val_loss: 0.2151 - val_acc: 0.9439\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9471\n",
      "Epoch 00106: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1614 - acc: 0.9471 - val_loss: 0.2079 - val_acc: 0.9427\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9468\n",
      "Epoch 00107: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1584 - acc: 0.9468 - val_loss: 0.2131 - val_acc: 0.9404\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9450\n",
      "Epoch 00108: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1617 - acc: 0.9450 - val_loss: 0.2157 - val_acc: 0.9422\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9468\n",
      "Epoch 00109: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1601 - acc: 0.9469 - val_loss: 0.2030 - val_acc: 0.9464\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9466\n",
      "Epoch 00110: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1569 - acc: 0.9466 - val_loss: 0.2011 - val_acc: 0.9432\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9480\n",
      "Epoch 00111: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1569 - acc: 0.9481 - val_loss: 0.2107 - val_acc: 0.9408\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9451\n",
      "Epoch 00112: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1596 - acc: 0.9451 - val_loss: 0.2029 - val_acc: 0.9460\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9458\n",
      "Epoch 00113: val_loss improved from 0.19551 to 0.19370, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/113-0.1937.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1601 - acc: 0.9458 - val_loss: 0.1937 - val_acc: 0.9464\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9496\n",
      "Epoch 00114: val_loss did not improve from 0.19370\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1522 - acc: 0.9496 - val_loss: 0.1983 - val_acc: 0.9453\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9483\n",
      "Epoch 00115: val_loss did not improve from 0.19370\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1536 - acc: 0.9483 - val_loss: 0.1970 - val_acc: 0.9450\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9493\n",
      "Epoch 00116: val_loss improved from 0.19370 to 0.19354, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/116-0.1935.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1531 - acc: 0.9494 - val_loss: 0.1935 - val_acc: 0.9476\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9490\n",
      "Epoch 00117: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1536 - acc: 0.9489 - val_loss: 0.2059 - val_acc: 0.9432\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9488\n",
      "Epoch 00118: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1526 - acc: 0.9488 - val_loss: 0.2040 - val_acc: 0.9492\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9520\n",
      "Epoch 00119: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1447 - acc: 0.9520 - val_loss: 0.2139 - val_acc: 0.9415\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9509\n",
      "Epoch 00120: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1477 - acc: 0.9509 - val_loss: 0.2111 - val_acc: 0.9485\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9510\n",
      "Epoch 00121: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1455 - acc: 0.9510 - val_loss: 0.2132 - val_acc: 0.9446\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9514\n",
      "Epoch 00122: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1449 - acc: 0.9514 - val_loss: 0.2065 - val_acc: 0.9453\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9505\n",
      "Epoch 00123: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1475 - acc: 0.9505 - val_loss: 0.2156 - val_acc: 0.9427\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9505\n",
      "Epoch 00124: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1482 - acc: 0.9505 - val_loss: 0.2158 - val_acc: 0.9413\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9523\n",
      "Epoch 00125: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1428 - acc: 0.9523 - val_loss: 0.2002 - val_acc: 0.9455\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9508\n",
      "Epoch 00126: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1456 - acc: 0.9508 - val_loss: 0.2037 - val_acc: 0.9446\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9514\n",
      "Epoch 00127: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1410 - acc: 0.9514 - val_loss: 0.1969 - val_acc: 0.9485\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9527\n",
      "Epoch 00128: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1413 - acc: 0.9527 - val_loss: 0.2015 - val_acc: 0.9467\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9520\n",
      "Epoch 00129: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1396 - acc: 0.9520 - val_loss: 0.2161 - val_acc: 0.9455\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9538\n",
      "Epoch 00130: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1388 - acc: 0.9538 - val_loss: 0.2039 - val_acc: 0.9464\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9545\n",
      "Epoch 00131: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1371 - acc: 0.9545 - val_loss: 0.2129 - val_acc: 0.9434\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9537\n",
      "Epoch 00132: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1358 - acc: 0.9537 - val_loss: 0.2085 - val_acc: 0.9462\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9544\n",
      "Epoch 00133: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1336 - acc: 0.9544 - val_loss: 0.2040 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9554\n",
      "Epoch 00134: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1312 - acc: 0.9554 - val_loss: 0.2001 - val_acc: 0.9471\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9548\n",
      "Epoch 00135: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1348 - acc: 0.9548 - val_loss: 0.2001 - val_acc: 0.9476\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9530\n",
      "Epoch 00136: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1356 - acc: 0.9530 - val_loss: 0.2340 - val_acc: 0.9418\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9555\n",
      "Epoch 00137: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1326 - acc: 0.9555 - val_loss: 0.2072 - val_acc: 0.9427\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9564\n",
      "Epoch 00138: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1291 - acc: 0.9564 - val_loss: 0.2153 - val_acc: 0.9464\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9561\n",
      "Epoch 00139: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1291 - acc: 0.9561 - val_loss: 0.2087 - val_acc: 0.9455\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9557\n",
      "Epoch 00140: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1330 - acc: 0.9556 - val_loss: 0.2057 - val_acc: 0.9481\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9563\n",
      "Epoch 00141: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1287 - acc: 0.9563 - val_loss: 0.2101 - val_acc: 0.9469\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9564\n",
      "Epoch 00142: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1274 - acc: 0.9564 - val_loss: 0.2133 - val_acc: 0.9439\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9567\n",
      "Epoch 00143: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1266 - acc: 0.9567 - val_loss: 0.2070 - val_acc: 0.9474\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9572\n",
      "Epoch 00144: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1264 - acc: 0.9572 - val_loss: 0.2086 - val_acc: 0.9499\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9569\n",
      "Epoch 00145: val_loss improved from 0.19354 to 0.19160, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/145-0.1916.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1289 - acc: 0.9569 - val_loss: 0.1916 - val_acc: 0.9469\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9579\n",
      "Epoch 00146: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1244 - acc: 0.9579 - val_loss: 0.2080 - val_acc: 0.9455\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9565\n",
      "Epoch 00147: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1242 - acc: 0.9565 - val_loss: 0.2065 - val_acc: 0.9455\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9577\n",
      "Epoch 00148: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1240 - acc: 0.9577 - val_loss: 0.2085 - val_acc: 0.9441\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9586\n",
      "Epoch 00149: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1206 - acc: 0.9585 - val_loss: 0.2072 - val_acc: 0.9481\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9575\n",
      "Epoch 00150: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1241 - acc: 0.9575 - val_loss: 0.2207 - val_acc: 0.9478\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9578\n",
      "Epoch 00151: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1222 - acc: 0.9578 - val_loss: 0.2093 - val_acc: 0.9464\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9586\n",
      "Epoch 00152: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1192 - acc: 0.9585 - val_loss: 0.2090 - val_acc: 0.9450\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9578\n",
      "Epoch 00153: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1208 - acc: 0.9578 - val_loss: 0.1988 - val_acc: 0.9497\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9595\n",
      "Epoch 00154: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1198 - acc: 0.9595 - val_loss: 0.2036 - val_acc: 0.9474\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9591\n",
      "Epoch 00155: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1190 - acc: 0.9591 - val_loss: 0.2021 - val_acc: 0.9492\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9590\n",
      "Epoch 00156: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1202 - acc: 0.9590 - val_loss: 0.2211 - val_acc: 0.9462\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9595\n",
      "Epoch 00157: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1175 - acc: 0.9595 - val_loss: 0.1986 - val_acc: 0.9518\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9590\n",
      "Epoch 00158: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1171 - acc: 0.9590 - val_loss: 0.1965 - val_acc: 0.9499\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9589\n",
      "Epoch 00159: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1172 - acc: 0.9589 - val_loss: 0.1919 - val_acc: 0.9478\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9612\n",
      "Epoch 00160: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1153 - acc: 0.9612 - val_loss: 0.2241 - val_acc: 0.9448\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9607\n",
      "Epoch 00161: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1163 - acc: 0.9607 - val_loss: 0.2306 - val_acc: 0.9420\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9605\n",
      "Epoch 00162: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1157 - acc: 0.9605 - val_loss: 0.2083 - val_acc: 0.9448\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9606\n",
      "Epoch 00163: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1137 - acc: 0.9606 - val_loss: 0.1964 - val_acc: 0.9504\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9616\n",
      "Epoch 00164: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1121 - acc: 0.9616 - val_loss: 0.1958 - val_acc: 0.9513\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9610\n",
      "Epoch 00165: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1145 - acc: 0.9610 - val_loss: 0.2110 - val_acc: 0.9457\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9617\n",
      "Epoch 00166: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1102 - acc: 0.9617 - val_loss: 0.2388 - val_acc: 0.9476\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9619\n",
      "Epoch 00167: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1114 - acc: 0.9619 - val_loss: 0.1971 - val_acc: 0.9499\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9628\n",
      "Epoch 00168: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1097 - acc: 0.9628 - val_loss: 0.2216 - val_acc: 0.9415\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9625\n",
      "Epoch 00169: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1079 - acc: 0.9625 - val_loss: 0.2236 - val_acc: 0.9439\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9639\n",
      "Epoch 00170: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1052 - acc: 0.9639 - val_loss: 0.2106 - val_acc: 0.9460\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9634\n",
      "Epoch 00171: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1049 - acc: 0.9634 - val_loss: 0.2105 - val_acc: 0.9471\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9629\n",
      "Epoch 00172: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1065 - acc: 0.9629 - val_loss: 0.2162 - val_acc: 0.9490\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9633\n",
      "Epoch 00173: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1077 - acc: 0.9633 - val_loss: 0.2090 - val_acc: 0.9513\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9635\n",
      "Epoch 00174: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1090 - acc: 0.9635 - val_loss: 0.2306 - val_acc: 0.9490\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9628\n",
      "Epoch 00175: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1070 - acc: 0.9628 - val_loss: 0.2135 - val_acc: 0.9476\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9635\n",
      "Epoch 00176: val_loss improved from 0.19160 to 0.19119, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/176-0.1912.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1039 - acc: 0.9635 - val_loss: 0.1912 - val_acc: 0.9490\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9632\n",
      "Epoch 00177: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1078 - acc: 0.9632 - val_loss: 0.2006 - val_acc: 0.9474\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9632\n",
      "Epoch 00178: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1062 - acc: 0.9632 - val_loss: 0.2161 - val_acc: 0.9441\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9660\n",
      "Epoch 00179: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0995 - acc: 0.9660 - val_loss: 0.2192 - val_acc: 0.9448\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9635\n",
      "Epoch 00180: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1056 - acc: 0.9635 - val_loss: 0.2047 - val_acc: 0.9483\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9646\n",
      "Epoch 00181: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1015 - acc: 0.9646 - val_loss: 0.2119 - val_acc: 0.9481\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9650\n",
      "Epoch 00182: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1032 - acc: 0.9650 - val_loss: 0.2060 - val_acc: 0.9504\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9644\n",
      "Epoch 00183: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1025 - acc: 0.9644 - val_loss: 0.2131 - val_acc: 0.9488\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9648\n",
      "Epoch 00184: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1028 - acc: 0.9648 - val_loss: 0.2085 - val_acc: 0.9509\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9653\n",
      "Epoch 00185: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1013 - acc: 0.9653 - val_loss: 0.2259 - val_acc: 0.9499\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9668\n",
      "Epoch 00186: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0985 - acc: 0.9668 - val_loss: 0.2174 - val_acc: 0.9488\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9640\n",
      "Epoch 00187: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1020 - acc: 0.9641 - val_loss: 0.2263 - val_acc: 0.9490\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9661\n",
      "Epoch 00188: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0979 - acc: 0.9661 - val_loss: 0.2011 - val_acc: 0.9529\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9652\n",
      "Epoch 00189: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1005 - acc: 0.9652 - val_loss: 0.1973 - val_acc: 0.9509\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9659\n",
      "Epoch 00190: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0984 - acc: 0.9659 - val_loss: 0.2019 - val_acc: 0.9509\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9670\n",
      "Epoch 00191: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0963 - acc: 0.9670 - val_loss: 0.2207 - val_acc: 0.9483\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9658\n",
      "Epoch 00192: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0981 - acc: 0.9658 - val_loss: 0.2159 - val_acc: 0.9481\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9655\n",
      "Epoch 00193: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0988 - acc: 0.9655 - val_loss: 0.2236 - val_acc: 0.9469\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9668\n",
      "Epoch 00194: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0972 - acc: 0.9668 - val_loss: 0.2179 - val_acc: 0.9478\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9662\n",
      "Epoch 00195: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0955 - acc: 0.9662 - val_loss: 0.2219 - val_acc: 0.9457\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9670\n",
      "Epoch 00196: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0956 - acc: 0.9670 - val_loss: 0.2033 - val_acc: 0.9513\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9671\n",
      "Epoch 00197: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.0952 - acc: 0.9671 - val_loss: 0.2193 - val_acc: 0.9483\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9668\n",
      "Epoch 00198: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.0957 - acc: 0.9667 - val_loss: 0.2257 - val_acc: 0.9467\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9670\n",
      "Epoch 00199: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.0981 - acc: 0.9670 - val_loss: 0.2125 - val_acc: 0.9483\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9671\n",
      "Epoch 00200: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.0929 - acc: 0.9671 - val_loss: 0.2113 - val_acc: 0.9471\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9679\n",
      "Epoch 00201: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0916 - acc: 0.9679 - val_loss: 0.2242 - val_acc: 0.9467\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9668\n",
      "Epoch 00202: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0952 - acc: 0.9668 - val_loss: 0.2188 - val_acc: 0.9509\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9697\n",
      "Epoch 00203: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0905 - acc: 0.9697 - val_loss: 0.2132 - val_acc: 0.9495\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9681\n",
      "Epoch 00204: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0946 - acc: 0.9681 - val_loss: 0.2167 - val_acc: 0.9506\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9674\n",
      "Epoch 00205: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0944 - acc: 0.9674 - val_loss: 0.2236 - val_acc: 0.9488\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9699\n",
      "Epoch 00206: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0866 - acc: 0.9699 - val_loss: 0.2122 - val_acc: 0.9502\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9682\n",
      "Epoch 00207: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0913 - acc: 0.9682 - val_loss: 0.2175 - val_acc: 0.9520\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9691\n",
      "Epoch 00208: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0904 - acc: 0.9691 - val_loss: 0.2135 - val_acc: 0.9511\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9692\n",
      "Epoch 00209: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0883 - acc: 0.9692 - val_loss: 0.2141 - val_acc: 0.9495\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9690\n",
      "Epoch 00210: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0885 - acc: 0.9689 - val_loss: 0.2174 - val_acc: 0.9497\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9693\n",
      "Epoch 00211: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0874 - acc: 0.9692 - val_loss: 0.2180 - val_acc: 0.9492\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9702\n",
      "Epoch 00212: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0862 - acc: 0.9702 - val_loss: 0.2175 - val_acc: 0.9476\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9692\n",
      "Epoch 00213: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.0878 - acc: 0.9692 - val_loss: 0.2140 - val_acc: 0.9471\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9693\n",
      "Epoch 00214: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0880 - acc: 0.9693 - val_loss: 0.2141 - val_acc: 0.9520\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9694\n",
      "Epoch 00215: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0892 - acc: 0.9694 - val_loss: 0.2124 - val_acc: 0.9509\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9708\n",
      "Epoch 00216: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0847 - acc: 0.9708 - val_loss: 0.2103 - val_acc: 0.9483\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9695\n",
      "Epoch 00217: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0878 - acc: 0.9695 - val_loss: 0.2335 - val_acc: 0.9478\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9708\n",
      "Epoch 00218: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0885 - acc: 0.9708 - val_loss: 0.2161 - val_acc: 0.9481\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9701\n",
      "Epoch 00219: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.0872 - acc: 0.9701 - val_loss: 0.2157 - val_acc: 0.9485\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9703\n",
      "Epoch 00220: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.0862 - acc: 0.9703 - val_loss: 0.2375 - val_acc: 0.9490\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9690\n",
      "Epoch 00221: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.0880 - acc: 0.9690 - val_loss: 0.2284 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9710\n",
      "Epoch 00222: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0860 - acc: 0.9710 - val_loss: 0.2038 - val_acc: 0.9513\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9723\n",
      "Epoch 00223: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.0850 - acc: 0.9723 - val_loss: 0.2124 - val_acc: 0.9506\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9715\n",
      "Epoch 00224: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.0817 - acc: 0.9715 - val_loss: 0.2395 - val_acc: 0.9457\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9718\n",
      "Epoch 00225: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.0810 - acc: 0.9718 - val_loss: 0.2175 - val_acc: 0.9513\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9717\n",
      "Epoch 00226: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.0844 - acc: 0.9717 - val_loss: 0.2304 - val_acc: 0.9478\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 372us/sample - loss: 0.2330 - acc: 0.9373\n",
      "Loss: 0.2330334084025301 Accuracy: 0.93727934\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5511 - acc: 0.1581\n",
      "Epoch 00001: val_loss improved from inf to 1.98775, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/001-1.9877.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 2.5510 - acc: 0.1581 - val_loss: 1.9877 - val_acc: 0.3659\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8731 - acc: 0.3815\n",
      "Epoch 00002: val_loss improved from 1.98775 to 1.40025, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/002-1.4002.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.8730 - acc: 0.3816 - val_loss: 1.4002 - val_acc: 0.5761\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4865 - acc: 0.5078\n",
      "Epoch 00003: val_loss improved from 1.40025 to 1.14365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/003-1.1437.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 1.4864 - acc: 0.5078 - val_loss: 1.1437 - val_acc: 0.6378\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2878 - acc: 0.5734\n",
      "Epoch 00004: val_loss improved from 1.14365 to 0.95705, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/004-0.9571.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 1.2877 - acc: 0.5734 - val_loss: 0.9571 - val_acc: 0.7060\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1456 - acc: 0.6211\n",
      "Epoch 00005: val_loss improved from 0.95705 to 0.85468, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/005-0.8547.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.1455 - acc: 0.6211 - val_loss: 0.8547 - val_acc: 0.7307\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0564 - acc: 0.6527\n",
      "Epoch 00006: val_loss improved from 0.85468 to 0.79047, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/006-0.7905.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.0565 - acc: 0.6527 - val_loss: 0.7905 - val_acc: 0.7517\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.6804\n",
      "Epoch 00007: val_loss improved from 0.79047 to 0.71186, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/007-0.7119.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.9770 - acc: 0.6804 - val_loss: 0.7119 - val_acc: 0.7880\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9119 - acc: 0.7030\n",
      "Epoch 00008: val_loss improved from 0.71186 to 0.65937, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/008-0.6594.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9118 - acc: 0.7031 - val_loss: 0.6594 - val_acc: 0.7983\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8469 - acc: 0.7257\n",
      "Epoch 00009: val_loss improved from 0.65937 to 0.61510, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/009-0.6151.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.8469 - acc: 0.7256 - val_loss: 0.6151 - val_acc: 0.8062\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7486\n",
      "Epoch 00010: val_loss improved from 0.61510 to 0.56573, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/010-0.5657.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.7870 - acc: 0.7486 - val_loss: 0.5657 - val_acc: 0.8288\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7620\n",
      "Epoch 00011: val_loss improved from 0.56573 to 0.52099, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/011-0.5210.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7443 - acc: 0.7620 - val_loss: 0.5210 - val_acc: 0.8470\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7009 - acc: 0.7754\n",
      "Epoch 00012: val_loss improved from 0.52099 to 0.48750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/012-0.4875.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7009 - acc: 0.7753 - val_loss: 0.4875 - val_acc: 0.8591\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.7876\n",
      "Epoch 00013: val_loss improved from 0.48750 to 0.46318, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/013-0.4632.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.6659 - acc: 0.7876 - val_loss: 0.4632 - val_acc: 0.8675\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8014\n",
      "Epoch 00014: val_loss improved from 0.46318 to 0.45067, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/014-0.4507.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.6255 - acc: 0.8013 - val_loss: 0.4507 - val_acc: 0.8570\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8077\n",
      "Epoch 00015: val_loss improved from 0.45067 to 0.39757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/015-0.3976.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.6028 - acc: 0.8077 - val_loss: 0.3976 - val_acc: 0.8833\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8213\n",
      "Epoch 00016: val_loss improved from 0.39757 to 0.37791, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/016-0.3779.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.5616 - acc: 0.8213 - val_loss: 0.3779 - val_acc: 0.8873\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8280\n",
      "Epoch 00017: val_loss did not improve from 0.37791\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5450 - acc: 0.8279 - val_loss: 0.3926 - val_acc: 0.8826\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.8333\n",
      "Epoch 00018: val_loss improved from 0.37791 to 0.36620, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/018-0.3662.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5230 - acc: 0.8333 - val_loss: 0.3662 - val_acc: 0.8868\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8416\n",
      "Epoch 00019: val_loss improved from 0.36620 to 0.33511, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/019-0.3351.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5001 - acc: 0.8417 - val_loss: 0.3351 - val_acc: 0.8994\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8468\n",
      "Epoch 00020: val_loss improved from 0.33511 to 0.30998, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/020-0.3100.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4795 - acc: 0.8468 - val_loss: 0.3100 - val_acc: 0.9031\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8531\n",
      "Epoch 00021: val_loss did not improve from 0.30998\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4611 - acc: 0.8531 - val_loss: 0.3100 - val_acc: 0.9108\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8605\n",
      "Epoch 00022: val_loss did not improve from 0.30998\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4421 - acc: 0.8605 - val_loss: 0.3113 - val_acc: 0.9047\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8609\n",
      "Epoch 00023: val_loss improved from 0.30998 to 0.27188, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/023-0.2719.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4362 - acc: 0.8609 - val_loss: 0.2719 - val_acc: 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8682\n",
      "Epoch 00024: val_loss improved from 0.27188 to 0.27084, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/024-0.2708.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4164 - acc: 0.8682 - val_loss: 0.2708 - val_acc: 0.9222\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8715\n",
      "Epoch 00025: val_loss did not improve from 0.27084\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.4022 - acc: 0.8715 - val_loss: 0.2839 - val_acc: 0.9145\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8759\n",
      "Epoch 00026: val_loss improved from 0.27084 to 0.25191, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/026-0.2519.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3899 - acc: 0.8759 - val_loss: 0.2519 - val_acc: 0.9299\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8788\n",
      "Epoch 00027: val_loss improved from 0.25191 to 0.25031, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/027-0.2503.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3783 - acc: 0.8788 - val_loss: 0.2503 - val_acc: 0.9285\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8813\n",
      "Epoch 00028: val_loss improved from 0.25031 to 0.23283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/028-0.2328.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3729 - acc: 0.8813 - val_loss: 0.2328 - val_acc: 0.9341\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8848\n",
      "Epoch 00029: val_loss did not improve from 0.23283\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3640 - acc: 0.8848 - val_loss: 0.2347 - val_acc: 0.9324\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8864\n",
      "Epoch 00030: val_loss improved from 0.23283 to 0.21914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/030-0.2191.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3534 - acc: 0.8864 - val_loss: 0.2191 - val_acc: 0.9373\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8897\n",
      "Epoch 00031: val_loss did not improve from 0.21914\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3478 - acc: 0.8897 - val_loss: 0.2553 - val_acc: 0.9241\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8919\n",
      "Epoch 00032: val_loss improved from 0.21914 to 0.21756, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/032-0.2176.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3367 - acc: 0.8919 - val_loss: 0.2176 - val_acc: 0.9369\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8952\n",
      "Epoch 00033: val_loss did not improve from 0.21756\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.3299 - acc: 0.8952 - val_loss: 0.2198 - val_acc: 0.9317\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8983\n",
      "Epoch 00034: val_loss did not improve from 0.21756\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3195 - acc: 0.8983 - val_loss: 0.2180 - val_acc: 0.9322\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9000\n",
      "Epoch 00035: val_loss improved from 0.21756 to 0.21137, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/035-0.2114.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3129 - acc: 0.9000 - val_loss: 0.2114 - val_acc: 0.9394\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9027\n",
      "Epoch 00036: val_loss improved from 0.21137 to 0.20871, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/036-0.2087.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3049 - acc: 0.9027 - val_loss: 0.2087 - val_acc: 0.9394\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9047\n",
      "Epoch 00037: val_loss improved from 0.20871 to 0.19285, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/037-0.1929.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2999 - acc: 0.9047 - val_loss: 0.1929 - val_acc: 0.9462\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9064\n",
      "Epoch 00038: val_loss improved from 0.19285 to 0.19003, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/038-0.1900.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2960 - acc: 0.9064 - val_loss: 0.1900 - val_acc: 0.9453\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9061\n",
      "Epoch 00039: val_loss improved from 0.19003 to 0.18984, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/039-0.1898.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2932 - acc: 0.9061 - val_loss: 0.1898 - val_acc: 0.9448\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9106\n",
      "Epoch 00040: val_loss improved from 0.18984 to 0.18550, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/040-0.1855.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2811 - acc: 0.9106 - val_loss: 0.1855 - val_acc: 0.9434\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9120\n",
      "Epoch 00041: val_loss did not improve from 0.18550\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2761 - acc: 0.9120 - val_loss: 0.1897 - val_acc: 0.9450\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9105\n",
      "Epoch 00042: val_loss did not improve from 0.18550\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2759 - acc: 0.9105 - val_loss: 0.1878 - val_acc: 0.9441\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9154\n",
      "Epoch 00043: val_loss improved from 0.18550 to 0.17579, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/043-0.1758.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2643 - acc: 0.9154 - val_loss: 0.1758 - val_acc: 0.9488\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9168\n",
      "Epoch 00044: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2605 - acc: 0.9168 - val_loss: 0.1840 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9171\n",
      "Epoch 00045: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2585 - acc: 0.9171 - val_loss: 0.1811 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9196\n",
      "Epoch 00046: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2544 - acc: 0.9196 - val_loss: 0.1810 - val_acc: 0.9478\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9183\n",
      "Epoch 00047: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2493 - acc: 0.9183 - val_loss: 0.1761 - val_acc: 0.9481\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9186\n",
      "Epoch 00048: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2482 - acc: 0.9186 - val_loss: 0.1866 - val_acc: 0.9434\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9202\n",
      "Epoch 00049: val_loss improved from 0.17579 to 0.17262, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/049-0.1726.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2485 - acc: 0.9202 - val_loss: 0.1726 - val_acc: 0.9509\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9210\n",
      "Epoch 00050: val_loss improved from 0.17262 to 0.16472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/050-0.1647.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2413 - acc: 0.9210 - val_loss: 0.1647 - val_acc: 0.9539\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9255\n",
      "Epoch 00051: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2333 - acc: 0.9255 - val_loss: 0.1859 - val_acc: 0.9455\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9264\n",
      "Epoch 00052: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2276 - acc: 0.9264 - val_loss: 0.1706 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9258\n",
      "Epoch 00053: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2253 - acc: 0.9258 - val_loss: 0.1902 - val_acc: 0.9446\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9271\n",
      "Epoch 00054: val_loss improved from 0.16472 to 0.16094, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/054-0.1609.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2268 - acc: 0.9271 - val_loss: 0.1609 - val_acc: 0.9522\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9287\n",
      "Epoch 00055: val_loss improved from 0.16094 to 0.15707, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/055-0.1571.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2183 - acc: 0.9287 - val_loss: 0.1571 - val_acc: 0.9527\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9274\n",
      "Epoch 00056: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2206 - acc: 0.9274 - val_loss: 0.1581 - val_acc: 0.9536\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9296\n",
      "Epoch 00057: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2156 - acc: 0.9295 - val_loss: 0.1646 - val_acc: 0.9520\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9287\n",
      "Epoch 00058: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2197 - acc: 0.9288 - val_loss: 0.1863 - val_acc: 0.9455\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9327\n",
      "Epoch 00059: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2067 - acc: 0.9328 - val_loss: 0.1592 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9305\n",
      "Epoch 00060: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2108 - acc: 0.9305 - val_loss: 0.1675 - val_acc: 0.9511\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9308\n",
      "Epoch 00061: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2092 - acc: 0.9309 - val_loss: 0.1622 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9326\n",
      "Epoch 00062: val_loss improved from 0.15707 to 0.15235, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/062-0.1523.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2032 - acc: 0.9326 - val_loss: 0.1523 - val_acc: 0.9585\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9352\n",
      "Epoch 00063: val_loss did not improve from 0.15235\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1982 - acc: 0.9352 - val_loss: 0.1541 - val_acc: 0.9555\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9341\n",
      "Epoch 00064: val_loss improved from 0.15235 to 0.15001, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/064-0.1500.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1983 - acc: 0.9341 - val_loss: 0.1500 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9366\n",
      "Epoch 00065: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1941 - acc: 0.9366 - val_loss: 0.1532 - val_acc: 0.9569\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9367\n",
      "Epoch 00066: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1904 - acc: 0.9366 - val_loss: 0.1591 - val_acc: 0.9518\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9376\n",
      "Epoch 00067: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1913 - acc: 0.9376 - val_loss: 0.1575 - val_acc: 0.9534\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9367\n",
      "Epoch 00068: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1911 - acc: 0.9367 - val_loss: 0.1573 - val_acc: 0.9532\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9385\n",
      "Epoch 00069: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1844 - acc: 0.9385 - val_loss: 0.1545 - val_acc: 0.9536\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9391\n",
      "Epoch 00070: val_loss improved from 0.15001 to 0.14389, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/070-0.1439.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1862 - acc: 0.9391 - val_loss: 0.1439 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9393\n",
      "Epoch 00071: val_loss did not improve from 0.14389\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1815 - acc: 0.9393 - val_loss: 0.1569 - val_acc: 0.9543\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9417\n",
      "Epoch 00072: val_loss improved from 0.14389 to 0.14257, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/072-0.1426.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1781 - acc: 0.9416 - val_loss: 0.1426 - val_acc: 0.9590\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9399\n",
      "Epoch 00073: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1800 - acc: 0.9399 - val_loss: 0.1442 - val_acc: 0.9574\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9413\n",
      "Epoch 00074: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1771 - acc: 0.9413 - val_loss: 0.1470 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9438\n",
      "Epoch 00075: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1704 - acc: 0.9437 - val_loss: 0.1524 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9423\n",
      "Epoch 00076: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1735 - acc: 0.9423 - val_loss: 0.1500 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9435\n",
      "Epoch 00077: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1712 - acc: 0.9435 - val_loss: 0.1472 - val_acc: 0.9585\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9464\n",
      "Epoch 00078: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1665 - acc: 0.9464 - val_loss: 0.1431 - val_acc: 0.9564\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9461\n",
      "Epoch 00079: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1646 - acc: 0.9461 - val_loss: 0.1519 - val_acc: 0.9576\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9455\n",
      "Epoch 00080: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1650 - acc: 0.9455 - val_loss: 0.1452 - val_acc: 0.9574\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9464\n",
      "Epoch 00081: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1632 - acc: 0.9463 - val_loss: 0.1455 - val_acc: 0.9550\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9466\n",
      "Epoch 00082: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1584 - acc: 0.9465 - val_loss: 0.1646 - val_acc: 0.9513\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9463\n",
      "Epoch 00083: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1609 - acc: 0.9463 - val_loss: 0.1492 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9471\n",
      "Epoch 00084: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1554 - acc: 0.9471 - val_loss: 0.1631 - val_acc: 0.9522\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9469\n",
      "Epoch 00085: val_loss improved from 0.14257 to 0.14189, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/085-0.1419.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1594 - acc: 0.9469 - val_loss: 0.1419 - val_acc: 0.9597\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9484\n",
      "Epoch 00086: val_loss improved from 0.14189 to 0.13715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/086-0.1371.hdf5\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1517 - acc: 0.9484 - val_loss: 0.1371 - val_acc: 0.9604\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9495\n",
      "Epoch 00087: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1505 - acc: 0.9495 - val_loss: 0.1415 - val_acc: 0.9583\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9508\n",
      "Epoch 00088: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1471 - acc: 0.9508 - val_loss: 0.1487 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9508\n",
      "Epoch 00089: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1473 - acc: 0.9508 - val_loss: 0.1435 - val_acc: 0.9590\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9502\n",
      "Epoch 00090: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1498 - acc: 0.9503 - val_loss: 0.1416 - val_acc: 0.9616\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9509\n",
      "Epoch 00091: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1419 - acc: 0.9509 - val_loss: 0.1481 - val_acc: 0.9592\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9508\n",
      "Epoch 00092: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1468 - acc: 0.9508 - val_loss: 0.1376 - val_acc: 0.9597\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9526\n",
      "Epoch 00093: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1435 - acc: 0.9526 - val_loss: 0.1398 - val_acc: 0.9588\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9515\n",
      "Epoch 00094: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1457 - acc: 0.9515 - val_loss: 0.1485 - val_acc: 0.9597\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9532\n",
      "Epoch 00095: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1397 - acc: 0.9531 - val_loss: 0.1462 - val_acc: 0.9581\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9531\n",
      "Epoch 00096: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1391 - acc: 0.9530 - val_loss: 0.1447 - val_acc: 0.9578\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9524\n",
      "Epoch 00097: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1374 - acc: 0.9525 - val_loss: 0.1502 - val_acc: 0.9583\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9556\n",
      "Epoch 00098: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1329 - acc: 0.9556 - val_loss: 0.1407 - val_acc: 0.9606\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9534\n",
      "Epoch 00099: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1385 - acc: 0.9534 - val_loss: 0.1447 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9521\n",
      "Epoch 00100: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1390 - acc: 0.9521 - val_loss: 0.1400 - val_acc: 0.9581\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9559\n",
      "Epoch 00101: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1308 - acc: 0.9559 - val_loss: 0.1475 - val_acc: 0.9592\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9553\n",
      "Epoch 00102: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1311 - acc: 0.9553 - val_loss: 0.1439 - val_acc: 0.9613\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9554\n",
      "Epoch 00103: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1319 - acc: 0.9554 - val_loss: 0.1462 - val_acc: 0.9578\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9571\n",
      "Epoch 00104: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1285 - acc: 0.9571 - val_loss: 0.1429 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9539\n",
      "Epoch 00105: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1357 - acc: 0.9539 - val_loss: 0.1398 - val_acc: 0.9609\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9577\n",
      "Epoch 00106: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1242 - acc: 0.9577 - val_loss: 0.1444 - val_acc: 0.9592\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9586\n",
      "Epoch 00107: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1218 - acc: 0.9586 - val_loss: 0.1393 - val_acc: 0.9590\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9584\n",
      "Epoch 00108: val_loss improved from 0.13715 to 0.13528, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/108-0.1353.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1253 - acc: 0.9584 - val_loss: 0.1353 - val_acc: 0.9639\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9586\n",
      "Epoch 00109: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1210 - acc: 0.9586 - val_loss: 0.1584 - val_acc: 0.9546\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9574\n",
      "Epoch 00110: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1228 - acc: 0.9574 - val_loss: 0.1426 - val_acc: 0.9595\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9590\n",
      "Epoch 00111: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1201 - acc: 0.9590 - val_loss: 0.1438 - val_acc: 0.9602\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9586\n",
      "Epoch 00112: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1216 - acc: 0.9586 - val_loss: 0.1519 - val_acc: 0.9616\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9581\n",
      "Epoch 00113: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1231 - acc: 0.9581 - val_loss: 0.1488 - val_acc: 0.9583\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9592\n",
      "Epoch 00114: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1227 - acc: 0.9592 - val_loss: 0.1402 - val_acc: 0.9627\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9587\n",
      "Epoch 00115: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1181 - acc: 0.9587 - val_loss: 0.1375 - val_acc: 0.9616\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9609\n",
      "Epoch 00116: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1137 - acc: 0.9609 - val_loss: 0.1374 - val_acc: 0.9606\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9611\n",
      "Epoch 00117: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1143 - acc: 0.9611 - val_loss: 0.1377 - val_acc: 0.9602\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9589\n",
      "Epoch 00118: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1186 - acc: 0.9589 - val_loss: 0.1396 - val_acc: 0.9618\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9619\n",
      "Epoch 00119: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1111 - acc: 0.9619 - val_loss: 0.1433 - val_acc: 0.9620\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9620\n",
      "Epoch 00120: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1126 - acc: 0.9620 - val_loss: 0.1496 - val_acc: 0.9599\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9601\n",
      "Epoch 00121: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1147 - acc: 0.9601 - val_loss: 0.1404 - val_acc: 0.9625\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9617\n",
      "Epoch 00122: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1149 - acc: 0.9617 - val_loss: 0.1374 - val_acc: 0.9630\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9624\n",
      "Epoch 00123: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1107 - acc: 0.9624 - val_loss: 0.1478 - val_acc: 0.9597\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9621\n",
      "Epoch 00124: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1104 - acc: 0.9621 - val_loss: 0.1521 - val_acc: 0.9606\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9623\n",
      "Epoch 00125: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1066 - acc: 0.9623 - val_loss: 0.1406 - val_acc: 0.9627\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9632\n",
      "Epoch 00126: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1057 - acc: 0.9632 - val_loss: 0.1477 - val_acc: 0.9613\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9644\n",
      "Epoch 00127: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1051 - acc: 0.9644 - val_loss: 0.1424 - val_acc: 0.9616\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9634\n",
      "Epoch 00128: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1056 - acc: 0.9634 - val_loss: 0.1442 - val_acc: 0.9602\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9648\n",
      "Epoch 00129: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1041 - acc: 0.9648 - val_loss: 0.1395 - val_acc: 0.9604\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9637\n",
      "Epoch 00130: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1041 - acc: 0.9637 - val_loss: 0.1442 - val_acc: 0.9630\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9640\n",
      "Epoch 00131: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1014 - acc: 0.9640 - val_loss: 0.1426 - val_acc: 0.9599\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9657\n",
      "Epoch 00132: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1021 - acc: 0.9656 - val_loss: 0.1388 - val_acc: 0.9597\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9649\n",
      "Epoch 00133: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1011 - acc: 0.9649 - val_loss: 0.1431 - val_acc: 0.9639\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9643\n",
      "Epoch 00134: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1005 - acc: 0.9644 - val_loss: 0.1514 - val_acc: 0.9585\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9649\n",
      "Epoch 00135: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1018 - acc: 0.9649 - val_loss: 0.1544 - val_acc: 0.9595\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9661\n",
      "Epoch 00136: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0973 - acc: 0.9661 - val_loss: 0.1449 - val_acc: 0.9644\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9654\n",
      "Epoch 00137: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1000 - acc: 0.9654 - val_loss: 0.1446 - val_acc: 0.9602\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9655\n",
      "Epoch 00138: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1000 - acc: 0.9655 - val_loss: 0.1465 - val_acc: 0.9627\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9670\n",
      "Epoch 00139: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0978 - acc: 0.9670 - val_loss: 0.1509 - val_acc: 0.9595\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9668\n",
      "Epoch 00140: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0939 - acc: 0.9669 - val_loss: 0.1498 - val_acc: 0.9592\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9671\n",
      "Epoch 00141: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0957 - acc: 0.9672 - val_loss: 0.1453 - val_acc: 0.9611\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9678\n",
      "Epoch 00142: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0906 - acc: 0.9678 - val_loss: 0.1500 - val_acc: 0.9620\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9679\n",
      "Epoch 00143: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0930 - acc: 0.9679 - val_loss: 0.1534 - val_acc: 0.9611\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9674\n",
      "Epoch 00144: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0953 - acc: 0.9674 - val_loss: 0.1419 - val_acc: 0.9632\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9688\n",
      "Epoch 00145: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0875 - acc: 0.9688 - val_loss: 0.1470 - val_acc: 0.9623\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9682\n",
      "Epoch 00146: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0902 - acc: 0.9682 - val_loss: 0.1514 - val_acc: 0.9604\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9672\n",
      "Epoch 00147: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0925 - acc: 0.9672 - val_loss: 0.1608 - val_acc: 0.9616\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9686\n",
      "Epoch 00148: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0905 - acc: 0.9686 - val_loss: 0.1582 - val_acc: 0.9588\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9683\n",
      "Epoch 00149: val_loss improved from 0.13528 to 0.13419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/149-0.1342.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0909 - acc: 0.9683 - val_loss: 0.1342 - val_acc: 0.9637\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9688\n",
      "Epoch 00150: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0904 - acc: 0.9688 - val_loss: 0.1595 - val_acc: 0.9623\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9691\n",
      "Epoch 00151: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0893 - acc: 0.9691 - val_loss: 0.1575 - val_acc: 0.9620\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9696\n",
      "Epoch 00152: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0872 - acc: 0.9696 - val_loss: 0.1429 - val_acc: 0.9602\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9692\n",
      "Epoch 00153: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0889 - acc: 0.9692 - val_loss: 0.1512 - val_acc: 0.9613\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9711\n",
      "Epoch 00154: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0857 - acc: 0.9711 - val_loss: 0.1436 - val_acc: 0.9620\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9697\n",
      "Epoch 00155: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0864 - acc: 0.9697 - val_loss: 0.1469 - val_acc: 0.9609\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9702\n",
      "Epoch 00156: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0851 - acc: 0.9702 - val_loss: 0.1504 - val_acc: 0.9611\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9710\n",
      "Epoch 00157: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0850 - acc: 0.9710 - val_loss: 0.1440 - val_acc: 0.9623\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9709\n",
      "Epoch 00158: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0824 - acc: 0.9709 - val_loss: 0.1467 - val_acc: 0.9616\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9710\n",
      "Epoch 00159: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0828 - acc: 0.9710 - val_loss: 0.1431 - val_acc: 0.9646\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9701\n",
      "Epoch 00160: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0868 - acc: 0.9700 - val_loss: 0.1410 - val_acc: 0.9639\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9708\n",
      "Epoch 00161: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0855 - acc: 0.9708 - val_loss: 0.1442 - val_acc: 0.9644\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9730\n",
      "Epoch 00162: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0788 - acc: 0.9730 - val_loss: 0.1488 - val_acc: 0.9613\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9714\n",
      "Epoch 00163: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0850 - acc: 0.9714 - val_loss: 0.1729 - val_acc: 0.9571\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9699\n",
      "Epoch 00164: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0890 - acc: 0.9699 - val_loss: 0.1449 - val_acc: 0.9632\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9713\n",
      "Epoch 00165: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0841 - acc: 0.9713 - val_loss: 0.1510 - val_acc: 0.9611\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9715\n",
      "Epoch 00166: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0808 - acc: 0.9715 - val_loss: 0.1595 - val_acc: 0.9592\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9744\n",
      "Epoch 00167: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0765 - acc: 0.9744 - val_loss: 0.1527 - val_acc: 0.9613\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9715\n",
      "Epoch 00168: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0795 - acc: 0.9715 - val_loss: 0.1421 - val_acc: 0.9632\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9729\n",
      "Epoch 00169: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0763 - acc: 0.9729 - val_loss: 0.1505 - val_acc: 0.9639\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9733\n",
      "Epoch 00170: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0793 - acc: 0.9733 - val_loss: 0.1492 - val_acc: 0.9641\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9728\n",
      "Epoch 00171: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0771 - acc: 0.9728 - val_loss: 0.1521 - val_acc: 0.9646\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9733\n",
      "Epoch 00172: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0757 - acc: 0.9733 - val_loss: 0.1511 - val_acc: 0.9623\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9723\n",
      "Epoch 00173: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0816 - acc: 0.9723 - val_loss: 0.1488 - val_acc: 0.9646\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9744\n",
      "Epoch 00174: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0723 - acc: 0.9744 - val_loss: 0.1600 - val_acc: 0.9618\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9724\n",
      "Epoch 00175: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0775 - acc: 0.9724 - val_loss: 0.1576 - val_acc: 0.9644\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9740\n",
      "Epoch 00176: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0740 - acc: 0.9740 - val_loss: 0.1500 - val_acc: 0.9606\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9745\n",
      "Epoch 00177: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0754 - acc: 0.9745 - val_loss: 0.1426 - val_acc: 0.9613\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9744\n",
      "Epoch 00178: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0745 - acc: 0.9744 - val_loss: 0.1513 - val_acc: 0.9620\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9735\n",
      "Epoch 00179: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0756 - acc: 0.9735 - val_loss: 0.1543 - val_acc: 0.9618\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9734\n",
      "Epoch 00180: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0762 - acc: 0.9734 - val_loss: 0.1567 - val_acc: 0.9604\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9734\n",
      "Epoch 00181: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0750 - acc: 0.9734 - val_loss: 0.1550 - val_acc: 0.9630\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9759\n",
      "Epoch 00182: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0698 - acc: 0.9759 - val_loss: 0.1568 - val_acc: 0.9604\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9757\n",
      "Epoch 00183: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.0709 - acc: 0.9757 - val_loss: 0.1579 - val_acc: 0.9609\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9747\n",
      "Epoch 00184: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0732 - acc: 0.9747 - val_loss: 0.1502 - val_acc: 0.9630\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9746\n",
      "Epoch 00185: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.0735 - acc: 0.9746 - val_loss: 0.1465 - val_acc: 0.9637\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9767\n",
      "Epoch 00186: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.0701 - acc: 0.9767 - val_loss: 0.1577 - val_acc: 0.9602\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9750\n",
      "Epoch 00187: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0707 - acc: 0.9750 - val_loss: 0.1537 - val_acc: 0.9630\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9765\n",
      "Epoch 00188: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0699 - acc: 0.9766 - val_loss: 0.1499 - val_acc: 0.9618\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9763\n",
      "Epoch 00189: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.0700 - acc: 0.9763 - val_loss: 0.1532 - val_acc: 0.9620\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9769\n",
      "Epoch 00190: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.0682 - acc: 0.9769 - val_loss: 0.1519 - val_acc: 0.9616\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9750\n",
      "Epoch 00191: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.0717 - acc: 0.9750 - val_loss: 0.1587 - val_acc: 0.9639\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9770\n",
      "Epoch 00192: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0658 - acc: 0.9770 - val_loss: 0.1565 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9763\n",
      "Epoch 00193: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0675 - acc: 0.9763 - val_loss: 0.1624 - val_acc: 0.9595\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9760\n",
      "Epoch 00194: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0679 - acc: 0.9760 - val_loss: 0.1613 - val_acc: 0.9613\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9767\n",
      "Epoch 00195: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.0670 - acc: 0.9767 - val_loss: 0.1657 - val_acc: 0.9609\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9773\n",
      "Epoch 00196: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.0660 - acc: 0.9773 - val_loss: 0.1541 - val_acc: 0.9651\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9771\n",
      "Epoch 00197: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0668 - acc: 0.9771 - val_loss: 0.1590 - val_acc: 0.9639\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9779\n",
      "Epoch 00198: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.0646 - acc: 0.9779 - val_loss: 0.1505 - val_acc: 0.9623\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9776\n",
      "Epoch 00199: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.0661 - acc: 0.9776 - val_loss: 0.1477 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 395us/sample - loss: 0.1897 - acc: 0.9477\n",
      "Loss: 0.18966707489603157 Accuracy: 0.94766355\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4064 - acc: 0.2193\n",
      "Epoch 00001: val_loss improved from inf to 1.60643, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/001-1.6064.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 2.4062 - acc: 0.2193 - val_loss: 1.6064 - val_acc: 0.5188\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4865 - acc: 0.5120\n",
      "Epoch 00002: val_loss improved from 1.60643 to 1.06528, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/002-1.0653.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 1.4864 - acc: 0.5121 - val_loss: 1.0653 - val_acc: 0.6576\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1512 - acc: 0.6211\n",
      "Epoch 00003: val_loss improved from 1.06528 to 0.83861, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/003-0.8386.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.1511 - acc: 0.6211 - val_loss: 0.8386 - val_acc: 0.7382\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9823 - acc: 0.6788\n",
      "Epoch 00004: val_loss improved from 0.83861 to 0.71781, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/004-0.7178.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.9823 - acc: 0.6788 - val_loss: 0.7178 - val_acc: 0.7803\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8711 - acc: 0.7161\n",
      "Epoch 00005: val_loss improved from 0.71781 to 0.63255, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/005-0.6326.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.8710 - acc: 0.7162 - val_loss: 0.6326 - val_acc: 0.8050\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7484\n",
      "Epoch 00006: val_loss improved from 0.63255 to 0.57700, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/006-0.5770.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.7784 - acc: 0.7484 - val_loss: 0.5770 - val_acc: 0.8230\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7114 - acc: 0.7685\n",
      "Epoch 00007: val_loss improved from 0.57700 to 0.50882, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/007-0.5088.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.7112 - acc: 0.7687 - val_loss: 0.5088 - val_acc: 0.8446\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.7840\n",
      "Epoch 00008: val_loss improved from 0.50882 to 0.45914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/008-0.4591.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.6598 - acc: 0.7841 - val_loss: 0.4591 - val_acc: 0.8570\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8041\n",
      "Epoch 00009: val_loss improved from 0.45914 to 0.43817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/009-0.4382.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.6066 - acc: 0.8041 - val_loss: 0.4382 - val_acc: 0.8637\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8168\n",
      "Epoch 00010: val_loss improved from 0.43817 to 0.39193, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/010-0.3919.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.5678 - acc: 0.8168 - val_loss: 0.3919 - val_acc: 0.8756\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8280\n",
      "Epoch 00011: val_loss improved from 0.39193 to 0.39095, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/011-0.3909.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5309 - acc: 0.8280 - val_loss: 0.3909 - val_acc: 0.8803\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.8373\n",
      "Epoch 00012: val_loss improved from 0.39095 to 0.35714, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/012-0.3571.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.5023 - acc: 0.8373 - val_loss: 0.3571 - val_acc: 0.8891\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8481\n",
      "Epoch 00013: val_loss improved from 0.35714 to 0.35282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/013-0.3528.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4719 - acc: 0.8481 - val_loss: 0.3528 - val_acc: 0.8863\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8539\n",
      "Epoch 00014: val_loss improved from 0.35282 to 0.32157, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/014-0.3216.hdf5\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.4521 - acc: 0.8539 - val_loss: 0.3216 - val_acc: 0.9015\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8612\n",
      "Epoch 00015: val_loss improved from 0.32157 to 0.28539, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/015-0.2854.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.4310 - acc: 0.8612 - val_loss: 0.2854 - val_acc: 0.9115\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8714\n",
      "Epoch 00016: val_loss did not improve from 0.28539\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.4002 - acc: 0.8714 - val_loss: 0.2857 - val_acc: 0.9101\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8749\n",
      "Epoch 00017: val_loss improved from 0.28539 to 0.27066, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/017-0.2707.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3875 - acc: 0.8750 - val_loss: 0.2707 - val_acc: 0.9171\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8825\n",
      "Epoch 00018: val_loss improved from 0.27066 to 0.25140, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/018-0.2514.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.3625 - acc: 0.8825 - val_loss: 0.2514 - val_acc: 0.9241\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8858\n",
      "Epoch 00019: val_loss improved from 0.25140 to 0.24630, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/019-0.2463.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.3527 - acc: 0.8858 - val_loss: 0.2463 - val_acc: 0.9187\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8921\n",
      "Epoch 00020: val_loss improved from 0.24630 to 0.24149, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/020-0.2415.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3379 - acc: 0.8922 - val_loss: 0.2415 - val_acc: 0.9215\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8966\n",
      "Epoch 00021: val_loss improved from 0.24149 to 0.22456, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/021-0.2246.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.3212 - acc: 0.8966 - val_loss: 0.2246 - val_acc: 0.9269\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8987\n",
      "Epoch 00022: val_loss improved from 0.22456 to 0.21376, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/022-0.2138.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3145 - acc: 0.8987 - val_loss: 0.2138 - val_acc: 0.9322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9018\n",
      "Epoch 00023: val_loss did not improve from 0.21376\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3008 - acc: 0.9018 - val_loss: 0.2251 - val_acc: 0.9278\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9063\n",
      "Epoch 00024: val_loss improved from 0.21376 to 0.20566, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/024-0.2057.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2908 - acc: 0.9063 - val_loss: 0.2057 - val_acc: 0.9350\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9091\n",
      "Epoch 00025: val_loss did not improve from 0.20566\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2828 - acc: 0.9091 - val_loss: 0.2057 - val_acc: 0.9357\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9127\n",
      "Epoch 00026: val_loss improved from 0.20566 to 0.20390, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/026-0.2039.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.2695 - acc: 0.9128 - val_loss: 0.2039 - val_acc: 0.9373\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9171\n",
      "Epoch 00027: val_loss improved from 0.20390 to 0.19999, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/027-0.2000.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.2608 - acc: 0.9172 - val_loss: 0.2000 - val_acc: 0.9383\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9163\n",
      "Epoch 00028: val_loss improved from 0.19999 to 0.18762, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/028-0.1876.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.2536 - acc: 0.9163 - val_loss: 0.1876 - val_acc: 0.9422\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9188\n",
      "Epoch 00029: val_loss improved from 0.18762 to 0.18733, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/029-0.1873.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.2483 - acc: 0.9188 - val_loss: 0.1873 - val_acc: 0.9415\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9236\n",
      "Epoch 00030: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.2342 - acc: 0.9236 - val_loss: 0.1918 - val_acc: 0.9427\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9232\n",
      "Epoch 00031: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2351 - acc: 0.9231 - val_loss: 0.1888 - val_acc: 0.9422\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00032: val_loss improved from 0.18733 to 0.17006, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/032-0.1701.hdf5\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.1701 - val_acc: 0.9446\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9291\n",
      "Epoch 00033: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2184 - acc: 0.9291 - val_loss: 0.1940 - val_acc: 0.9383\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9301\n",
      "Epoch 00034: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2135 - acc: 0.9301 - val_loss: 0.1913 - val_acc: 0.9406\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9315\n",
      "Epoch 00035: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2075 - acc: 0.9315 - val_loss: 0.1745 - val_acc: 0.9495\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9334\n",
      "Epoch 00036: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.2020 - acc: 0.9334 - val_loss: 0.1793 - val_acc: 0.9474\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9343\n",
      "Epoch 00037: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1978 - acc: 0.9344 - val_loss: 0.1706 - val_acc: 0.9492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9362\n",
      "Epoch 00038: val_loss improved from 0.17006 to 0.16171, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/038-0.1617.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1924 - acc: 0.9362 - val_loss: 0.1617 - val_acc: 0.9513\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9392\n",
      "Epoch 00039: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1857 - acc: 0.9392 - val_loss: 0.1704 - val_acc: 0.9460\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9390\n",
      "Epoch 00040: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1849 - acc: 0.9391 - val_loss: 0.1745 - val_acc: 0.9462\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9420\n",
      "Epoch 00041: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1769 - acc: 0.9420 - val_loss: 0.1803 - val_acc: 0.9476\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9420\n",
      "Epoch 00042: val_loss improved from 0.16171 to 0.16092, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/042-0.1609.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1753 - acc: 0.9420 - val_loss: 0.1609 - val_acc: 0.9525\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9439\n",
      "Epoch 00043: val_loss improved from 0.16092 to 0.15897, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/043-0.1590.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1714 - acc: 0.9439 - val_loss: 0.1590 - val_acc: 0.9506\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9449\n",
      "Epoch 00044: val_loss did not improve from 0.15897\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1657 - acc: 0.9449 - val_loss: 0.1657 - val_acc: 0.9485\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9445\n",
      "Epoch 00045: val_loss improved from 0.15897 to 0.15125, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/045-0.1512.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1677 - acc: 0.9445 - val_loss: 0.1512 - val_acc: 0.9539\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9482\n",
      "Epoch 00046: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1589 - acc: 0.9481 - val_loss: 0.1608 - val_acc: 0.9502\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9472\n",
      "Epoch 00047: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1579 - acc: 0.9472 - val_loss: 0.1540 - val_acc: 0.9541\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9486\n",
      "Epoch 00048: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1515 - acc: 0.9486 - val_loss: 0.1581 - val_acc: 0.9527\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9490\n",
      "Epoch 00049: val_loss improved from 0.15125 to 0.15035, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/049-0.1503.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1516 - acc: 0.9490 - val_loss: 0.1503 - val_acc: 0.9529\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9515\n",
      "Epoch 00050: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1458 - acc: 0.9515 - val_loss: 0.1581 - val_acc: 0.9502\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9500\n",
      "Epoch 00051: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1504 - acc: 0.9500 - val_loss: 0.1617 - val_acc: 0.9522\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9530\n",
      "Epoch 00052: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1408 - acc: 0.9530 - val_loss: 0.1586 - val_acc: 0.9527\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9530\n",
      "Epoch 00053: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1393 - acc: 0.9530 - val_loss: 0.1540 - val_acc: 0.9525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9542\n",
      "Epoch 00054: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1375 - acc: 0.9542 - val_loss: 0.1663 - val_acc: 0.9511\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9559\n",
      "Epoch 00055: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1298 - acc: 0.9559 - val_loss: 0.1575 - val_acc: 0.9534\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9555\n",
      "Epoch 00056: val_loss improved from 0.15035 to 0.15018, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/056-0.1502.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1314 - acc: 0.9555 - val_loss: 0.1502 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9572\n",
      "Epoch 00057: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1297 - acc: 0.9572 - val_loss: 0.1539 - val_acc: 0.9557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9568\n",
      "Epoch 00058: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1270 - acc: 0.9568 - val_loss: 0.1596 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9598\n",
      "Epoch 00059: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1201 - acc: 0.9597 - val_loss: 0.1650 - val_acc: 0.9520\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9590\n",
      "Epoch 00060: val_loss improved from 0.15018 to 0.14282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/060-0.1428.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1223 - acc: 0.9590 - val_loss: 0.1428 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9589\n",
      "Epoch 00061: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1210 - acc: 0.9589 - val_loss: 0.1481 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9608\n",
      "Epoch 00062: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1163 - acc: 0.9608 - val_loss: 0.1599 - val_acc: 0.9543\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9607\n",
      "Epoch 00063: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1138 - acc: 0.9607 - val_loss: 0.1477 - val_acc: 0.9571\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9617\n",
      "Epoch 00064: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1103 - acc: 0.9617 - val_loss: 0.1561 - val_acc: 0.9557\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9606\n",
      "Epoch 00065: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1139 - acc: 0.9606 - val_loss: 0.1477 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9626\n",
      "Epoch 00066: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1090 - acc: 0.9625 - val_loss: 0.1518 - val_acc: 0.9564\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9629\n",
      "Epoch 00067: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1075 - acc: 0.9629 - val_loss: 0.1513 - val_acc: 0.9581\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9648\n",
      "Epoch 00068: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1031 - acc: 0.9648 - val_loss: 0.1496 - val_acc: 0.9571\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9645\n",
      "Epoch 00069: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1032 - acc: 0.9645 - val_loss: 0.1627 - val_acc: 0.9534\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9648\n",
      "Epoch 00070: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0997 - acc: 0.9648 - val_loss: 0.1560 - val_acc: 0.9539\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9640\n",
      "Epoch 00071: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1018 - acc: 0.9640 - val_loss: 0.1477 - val_acc: 0.9606\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9661\n",
      "Epoch 00072: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0965 - acc: 0.9660 - val_loss: 0.1504 - val_acc: 0.9576\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9677\n",
      "Epoch 00073: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0953 - acc: 0.9676 - val_loss: 0.1445 - val_acc: 0.9602\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9671\n",
      "Epoch 00074: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0959 - acc: 0.9671 - val_loss: 0.1663 - val_acc: 0.9562\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9677\n",
      "Epoch 00075: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0940 - acc: 0.9677 - val_loss: 0.1522 - val_acc: 0.9592\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9687\n",
      "Epoch 00076: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0909 - acc: 0.9687 - val_loss: 0.1516 - val_acc: 0.9560\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9669\n",
      "Epoch 00077: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0927 - acc: 0.9669 - val_loss: 0.1543 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9698\n",
      "Epoch 00078: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0894 - acc: 0.9698 - val_loss: 0.1501 - val_acc: 0.9583\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9700\n",
      "Epoch 00079: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.0873 - acc: 0.9700 - val_loss: 0.1755 - val_acc: 0.9527\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9709\n",
      "Epoch 00080: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0850 - acc: 0.9709 - val_loss: 0.1482 - val_acc: 0.9571\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9706\n",
      "Epoch 00081: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0851 - acc: 0.9706 - val_loss: 0.1541 - val_acc: 0.9590\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9718\n",
      "Epoch 00082: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.0817 - acc: 0.9718 - val_loss: 0.1538 - val_acc: 0.9590\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9707\n",
      "Epoch 00083: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0864 - acc: 0.9707 - val_loss: 0.1777 - val_acc: 0.9564\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9727\n",
      "Epoch 00084: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0798 - acc: 0.9727 - val_loss: 0.1555 - val_acc: 0.9592\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9722\n",
      "Epoch 00085: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.0790 - acc: 0.9722 - val_loss: 0.1447 - val_acc: 0.9620\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9719\n",
      "Epoch 00086: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0802 - acc: 0.9719 - val_loss: 0.1616 - val_acc: 0.9567\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9737\n",
      "Epoch 00087: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0769 - acc: 0.9737 - val_loss: 0.1842 - val_acc: 0.9550\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9731\n",
      "Epoch 00088: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0791 - acc: 0.9731 - val_loss: 0.1520 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9745\n",
      "Epoch 00089: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0744 - acc: 0.9745 - val_loss: 0.1534 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9743\n",
      "Epoch 00090: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0743 - acc: 0.9743 - val_loss: 0.1589 - val_acc: 0.9567\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9740\n",
      "Epoch 00091: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0739 - acc: 0.9741 - val_loss: 0.1604 - val_acc: 0.9569\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9753\n",
      "Epoch 00092: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0725 - acc: 0.9753 - val_loss: 0.1630 - val_acc: 0.9578\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9754\n",
      "Epoch 00093: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0715 - acc: 0.9754 - val_loss: 0.1617 - val_acc: 0.9590\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9754\n",
      "Epoch 00094: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0711 - acc: 0.9754 - val_loss: 0.1632 - val_acc: 0.9597\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9770\n",
      "Epoch 00095: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0667 - acc: 0.9770 - val_loss: 0.1644 - val_acc: 0.9595\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9754\n",
      "Epoch 00096: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0704 - acc: 0.9754 - val_loss: 0.1764 - val_acc: 0.9576\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9762\n",
      "Epoch 00097: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0680 - acc: 0.9763 - val_loss: 0.1629 - val_acc: 0.9599\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9764\n",
      "Epoch 00098: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.0666 - acc: 0.9764 - val_loss: 0.1617 - val_acc: 0.9597\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9768\n",
      "Epoch 00099: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0672 - acc: 0.9768 - val_loss: 0.1497 - val_acc: 0.9590\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9776\n",
      "Epoch 00100: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.0654 - acc: 0.9776 - val_loss: 0.1595 - val_acc: 0.9585\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9775\n",
      "Epoch 00101: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0660 - acc: 0.9775 - val_loss: 0.1735 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9771\n",
      "Epoch 00102: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0654 - acc: 0.9771 - val_loss: 0.1681 - val_acc: 0.9609\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9774\n",
      "Epoch 00103: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0651 - acc: 0.9774 - val_loss: 0.1546 - val_acc: 0.9571\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9767\n",
      "Epoch 00104: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0658 - acc: 0.9767 - val_loss: 0.1566 - val_acc: 0.9599\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9784\n",
      "Epoch 00105: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0616 - acc: 0.9784 - val_loss: 0.1759 - val_acc: 0.9590\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9789\n",
      "Epoch 00106: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0623 - acc: 0.9789 - val_loss: 0.1633 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9776\n",
      "Epoch 00107: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.0616 - acc: 0.9776 - val_loss: 0.1713 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9803\n",
      "Epoch 00108: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0579 - acc: 0.9803 - val_loss: 0.1764 - val_acc: 0.9597\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9808\n",
      "Epoch 00109: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0550 - acc: 0.9808 - val_loss: 0.1728 - val_acc: 0.9599\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9809\n",
      "Epoch 00110: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0543 - acc: 0.9809 - val_loss: 0.1642 - val_acc: 0.9627\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 415us/sample - loss: 0.2135 - acc: 0.9379\n",
      "Loss: 0.21350653049761384 Accuracy: 0.9379024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 386us/sample - loss: 1.5276 - acc: 0.5169\n",
      "Loss: 1.5275790819126498 Accuracy: 0.5169263\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 413us/sample - loss: 1.1507 - acc: 0.6656\n",
      "Loss: 1.150674240363845 Accuracy: 0.66562825\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 422us/sample - loss: 0.8196 - acc: 0.7605\n",
      "Loss: 0.8195533891457016 Accuracy: 0.76054\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 433us/sample - loss: 0.4329 - acc: 0.8893\n",
      "Loss: 0.4329061113166413 Accuracy: 0.8893043\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 449us/sample - loss: 0.2330 - acc: 0.9373\n",
      "Loss: 0.2330334084025301 Accuracy: 0.93727934\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 452us/sample - loss: 0.1897 - acc: 0.9477\n",
      "Loss: 0.18966707489603157 Accuracy: 0.94766355\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 471us/sample - loss: 0.2135 - acc: 0.9379\n",
      "Loss: 0.21350653049761384 Accuracy: 0.9379024\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4032 - acc: 0.2431\n",
      "Epoch 00001: val_loss improved from inf to 2.04454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/001-2.0445.hdf5\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 2.4032 - acc: 0.2431 - val_loss: 2.0445 - val_acc: 0.3979\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8418 - acc: 0.4403\n",
      "Epoch 00002: val_loss improved from 2.04454 to 1.70078, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/002-1.7008.hdf5\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 1.8418 - acc: 0.4403 - val_loss: 1.7008 - val_acc: 0.4787\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5676 - acc: 0.5240\n",
      "Epoch 00003: val_loss improved from 1.70078 to 1.55561, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/003-1.5556.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.5676 - acc: 0.5240 - val_loss: 1.5556 - val_acc: 0.5101\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3990 - acc: 0.5753\n",
      "Epoch 00004: val_loss improved from 1.55561 to 1.49484, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/004-1.4948.hdf5\n",
      "36805/36805 [==============================] - 23s 638us/sample - loss: 1.3989 - acc: 0.5754 - val_loss: 1.4948 - val_acc: 0.5376\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2821 - acc: 0.6078\n",
      "Epoch 00005: val_loss improved from 1.49484 to 1.44730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/005-1.4473.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.2821 - acc: 0.6077 - val_loss: 1.4473 - val_acc: 0.5509\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1909 - acc: 0.6357\n",
      "Epoch 00006: val_loss improved from 1.44730 to 1.43502, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/006-1.4350.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.1909 - acc: 0.6357 - val_loss: 1.4350 - val_acc: 0.5586\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1170 - acc: 0.6579\n",
      "Epoch 00007: val_loss improved from 1.43502 to 1.40513, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/007-1.4051.hdf5\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 1.1170 - acc: 0.6578 - val_loss: 1.4051 - val_acc: 0.5670\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0474 - acc: 0.6807\n",
      "Epoch 00008: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 1.0474 - acc: 0.6807 - val_loss: 1.4250 - val_acc: 0.5577\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.6943\n",
      "Epoch 00009: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.9962 - acc: 0.6941 - val_loss: 1.4212 - val_acc: 0.5642\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.7133\n",
      "Epoch 00010: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.9373 - acc: 0.7133 - val_loss: 1.4224 - val_acc: 0.5660\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.7253\n",
      "Epoch 00011: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.8962 - acc: 0.7253 - val_loss: 1.4255 - val_acc: 0.5735\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7386\n",
      "Epoch 00012: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.8455 - acc: 0.7386 - val_loss: 1.4111 - val_acc: 0.5770\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.7510\n",
      "Epoch 00013: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 0.8018 - acc: 0.7510 - val_loss: 1.4378 - val_acc: 0.5793\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7642 - acc: 0.7643\n",
      "Epoch 00014: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 0.7642 - acc: 0.7644 - val_loss: 1.4245 - val_acc: 0.5872\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7290 - acc: 0.7735\n",
      "Epoch 00015: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.7289 - acc: 0.7735 - val_loss: 1.4568 - val_acc: 0.5856\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.7839\n",
      "Epoch 00016: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.6934 - acc: 0.7839 - val_loss: 1.4831 - val_acc: 0.5730\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6641 - acc: 0.7961\n",
      "Epoch 00017: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 0.6642 - acc: 0.7960 - val_loss: 1.4921 - val_acc: 0.5830\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.8045\n",
      "Epoch 00018: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 0.6320 - acc: 0.8046 - val_loss: 1.4713 - val_acc: 0.5837\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.8128\n",
      "Epoch 00019: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.6036 - acc: 0.8128 - val_loss: 1.5063 - val_acc: 0.5861\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8221\n",
      "Epoch 00020: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.5755 - acc: 0.8221 - val_loss: 1.4949 - val_acc: 0.5896\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8293\n",
      "Epoch 00021: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.5484 - acc: 0.8292 - val_loss: 1.5090 - val_acc: 0.5970\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8333\n",
      "Epoch 00022: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.5296 - acc: 0.8334 - val_loss: 1.5382 - val_acc: 0.5940\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.8413\n",
      "Epoch 00023: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 0.5038 - acc: 0.8413 - val_loss: 1.5459 - val_acc: 0.6033\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8461\n",
      "Epoch 00024: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.4880 - acc: 0.8461 - val_loss: 1.5585 - val_acc: 0.5935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8554\n",
      "Epoch 00025: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4618 - acc: 0.8554 - val_loss: 1.5662 - val_acc: 0.6040\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8611\n",
      "Epoch 00026: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4446 - acc: 0.8612 - val_loss: 1.6074 - val_acc: 0.5982\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8643\n",
      "Epoch 00027: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4320 - acc: 0.8643 - val_loss: 1.5932 - val_acc: 0.6007\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8718\n",
      "Epoch 00028: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4070 - acc: 0.8718 - val_loss: 1.6194 - val_acc: 0.6033\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8752\n",
      "Epoch 00029: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3935 - acc: 0.8752 - val_loss: 1.6630 - val_acc: 0.6049\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8744\n",
      "Epoch 00030: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3912 - acc: 0.8744 - val_loss: 1.6648 - val_acc: 0.6049\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8835\n",
      "Epoch 00031: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3706 - acc: 0.8835 - val_loss: 1.6621 - val_acc: 0.6028\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8864\n",
      "Epoch 00032: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3591 - acc: 0.8864 - val_loss: 1.6576 - val_acc: 0.6077\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8898\n",
      "Epoch 00033: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3479 - acc: 0.8898 - val_loss: 1.6802 - val_acc: 0.6075\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8949\n",
      "Epoch 00034: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 0.3337 - acc: 0.8949 - val_loss: 1.7046 - val_acc: 0.6038\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8983\n",
      "Epoch 00035: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.3241 - acc: 0.8984 - val_loss: 1.7238 - val_acc: 0.6080\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9009\n",
      "Epoch 00036: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3162 - acc: 0.9009 - val_loss: 1.7217 - val_acc: 0.6040\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.9070\n",
      "Epoch 00037: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.2983 - acc: 0.9070 - val_loss: 1.7641 - val_acc: 0.6005\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9098\n",
      "Epoch 00038: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.2904 - acc: 0.9098 - val_loss: 1.7719 - val_acc: 0.6068\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9098\n",
      "Epoch 00039: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.2871 - acc: 0.9098 - val_loss: 1.7516 - val_acc: 0.6138\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9133\n",
      "Epoch 00040: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.2767 - acc: 0.9134 - val_loss: 1.7869 - val_acc: 0.6066\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9145\n",
      "Epoch 00041: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.2707 - acc: 0.9145 - val_loss: 1.8168 - val_acc: 0.6028\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9177\n",
      "Epoch 00042: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 23s 638us/sample - loss: 0.2625 - acc: 0.9177 - val_loss: 1.8149 - val_acc: 0.6157\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9195\n",
      "Epoch 00043: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 0.2558 - acc: 0.9195 - val_loss: 1.8481 - val_acc: 0.6012\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9226\n",
      "Epoch 00044: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.2486 - acc: 0.9226 - val_loss: 1.8560 - val_acc: 0.6054\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9234\n",
      "Epoch 00045: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.2404 - acc: 0.9234 - val_loss: 1.8621 - val_acc: 0.6150\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9262\n",
      "Epoch 00046: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.2363 - acc: 0.9262 - val_loss: 1.8470 - val_acc: 0.6143\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9279\n",
      "Epoch 00047: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.2319 - acc: 0.9279 - val_loss: 1.8896 - val_acc: 0.6103\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9313\n",
      "Epoch 00048: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.2233 - acc: 0.9313 - val_loss: 1.9197 - val_acc: 0.6140\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9314\n",
      "Epoch 00049: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.2218 - acc: 0.9314 - val_loss: 1.8931 - val_acc: 0.6166\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9339\n",
      "Epoch 00050: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2147 - acc: 0.9339 - val_loss: 1.8887 - val_acc: 0.6143\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9363\n",
      "Epoch 00051: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.2107 - acc: 0.9363 - val_loss: 1.8995 - val_acc: 0.6115\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9361\n",
      "Epoch 00052: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.2034 - acc: 0.9361 - val_loss: 1.8823 - val_acc: 0.6161\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9397\n",
      "Epoch 00053: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.1979 - acc: 0.9397 - val_loss: 1.9376 - val_acc: 0.6143\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9404\n",
      "Epoch 00054: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.1972 - acc: 0.9404 - val_loss: 1.9455 - val_acc: 0.6161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9413\n",
      "Epoch 00055: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.1914 - acc: 0.9413 - val_loss: 1.9223 - val_acc: 0.6233\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9429\n",
      "Epoch 00056: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.1871 - acc: 0.9429 - val_loss: 1.9459 - val_acc: 0.6191\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9451\n",
      "Epoch 00057: val_loss did not improve from 1.40513\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.1829 - acc: 0.9451 - val_loss: 1.9916 - val_acc: 0.6198\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 324us/sample - loss: 1.4879 - acc: 0.5381\n",
      "Loss: 1.4878783402041855 Accuracy: 0.5381101\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3557 - acc: 0.2281\n",
      "Epoch 00001: val_loss improved from inf to 1.86757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/001-1.8676.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 2.3556 - acc: 0.2281 - val_loss: 1.8676 - val_acc: 0.3939\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7853 - acc: 0.4186\n",
      "Epoch 00002: val_loss improved from 1.86757 to 1.61590, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/002-1.6159.hdf5\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 1.7853 - acc: 0.4186 - val_loss: 1.6159 - val_acc: 0.4889\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5798 - acc: 0.5006\n",
      "Epoch 00003: val_loss improved from 1.61590 to 1.48203, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/003-1.4820.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.5798 - acc: 0.5006 - val_loss: 1.4820 - val_acc: 0.5285\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4468 - acc: 0.5491\n",
      "Epoch 00004: val_loss improved from 1.48203 to 1.38703, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/004-1.3870.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 1.4468 - acc: 0.5491 - val_loss: 1.3870 - val_acc: 0.5691\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3415 - acc: 0.5874\n",
      "Epoch 00005: val_loss improved from 1.38703 to 1.31373, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/005-1.3137.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 1.3415 - acc: 0.5874 - val_loss: 1.3137 - val_acc: 0.5975\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2601 - acc: 0.6121\n",
      "Epoch 00006: val_loss improved from 1.31373 to 1.30500, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/006-1.3050.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.2600 - acc: 0.6121 - val_loss: 1.3050 - val_acc: 0.5917\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1882 - acc: 0.6340\n",
      "Epoch 00007: val_loss improved from 1.30500 to 1.23458, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/007-1.2346.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 1.1882 - acc: 0.6339 - val_loss: 1.2346 - val_acc: 0.6175\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1291 - acc: 0.6537\n",
      "Epoch 00008: val_loss improved from 1.23458 to 1.20411, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/008-1.2041.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 1.1291 - acc: 0.6537 - val_loss: 1.2041 - val_acc: 0.6245\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0742 - acc: 0.6709\n",
      "Epoch 00009: val_loss improved from 1.20411 to 1.15816, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/009-1.1582.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.0742 - acc: 0.6709 - val_loss: 1.1582 - val_acc: 0.6424\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0181 - acc: 0.6869\n",
      "Epoch 00010: val_loss improved from 1.15816 to 1.13589, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/010-1.1359.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.0182 - acc: 0.6869 - val_loss: 1.1359 - val_acc: 0.6480\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9728 - acc: 0.7028\n",
      "Epoch 00011: val_loss improved from 1.13589 to 1.12751, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/011-1.1275.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.9727 - acc: 0.7028 - val_loss: 1.1275 - val_acc: 0.6494\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9297 - acc: 0.7156\n",
      "Epoch 00012: val_loss improved from 1.12751 to 1.10991, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/012-1.1099.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.9297 - acc: 0.7156 - val_loss: 1.1099 - val_acc: 0.6636\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8905 - acc: 0.7269\n",
      "Epoch 00013: val_loss improved from 1.10991 to 1.10971, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/013-1.1097.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.8906 - acc: 0.7269 - val_loss: 1.1097 - val_acc: 0.6597\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8518 - acc: 0.7386\n",
      "Epoch 00014: val_loss improved from 1.10971 to 1.08711, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/014-1.0871.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.8518 - acc: 0.7386 - val_loss: 1.0871 - val_acc: 0.6643\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8177 - acc: 0.7511\n",
      "Epoch 00015: val_loss improved from 1.08711 to 1.08262, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/015-1.0826.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.8176 - acc: 0.7511 - val_loss: 1.0826 - val_acc: 0.6681\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7829 - acc: 0.7612\n",
      "Epoch 00016: val_loss improved from 1.08262 to 1.05061, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/016-1.0506.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.7829 - acc: 0.7613 - val_loss: 1.0506 - val_acc: 0.6737\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7512 - acc: 0.7693\n",
      "Epoch 00017: val_loss did not improve from 1.05061\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.7512 - acc: 0.7692 - val_loss: 1.0596 - val_acc: 0.6748\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7772\n",
      "Epoch 00018: val_loss did not improve from 1.05061\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.7233 - acc: 0.7772 - val_loss: 1.0518 - val_acc: 0.6785\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6972 - acc: 0.7840\n",
      "Epoch 00019: val_loss did not improve from 1.05061\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6972 - acc: 0.7840 - val_loss: 1.0776 - val_acc: 0.6788\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6641 - acc: 0.7926\n",
      "Epoch 00020: val_loss did not improve from 1.05061\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.6641 - acc: 0.7926 - val_loss: 1.0818 - val_acc: 0.6820\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6406 - acc: 0.8017\n",
      "Epoch 00021: val_loss improved from 1.05061 to 1.04124, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/021-1.0412.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.6407 - acc: 0.8017 - val_loss: 1.0412 - val_acc: 0.6865\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.8055\n",
      "Epoch 00022: val_loss improved from 1.04124 to 1.03984, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/022-1.0398.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.6198 - acc: 0.8054 - val_loss: 1.0398 - val_acc: 0.6879\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5982 - acc: 0.8141\n",
      "Epoch 00023: val_loss did not improve from 1.03984\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5982 - acc: 0.8141 - val_loss: 1.0861 - val_acc: 0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8164\n",
      "Epoch 00024: val_loss did not improve from 1.03984\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.5775 - acc: 0.8164 - val_loss: 1.0405 - val_acc: 0.6951\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8235\n",
      "Epoch 00025: val_loss improved from 1.03984 to 1.03586, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/025-1.0359.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5578 - acc: 0.8234 - val_loss: 1.0359 - val_acc: 0.6988\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.8299\n",
      "Epoch 00026: val_loss did not improve from 1.03586\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.5390 - acc: 0.8299 - val_loss: 1.0601 - val_acc: 0.6916\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.8359\n",
      "Epoch 00027: val_loss did not improve from 1.03586\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5209 - acc: 0.8359 - val_loss: 1.0466 - val_acc: 0.6990\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8383\n",
      "Epoch 00028: val_loss did not improve from 1.03586\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.5067 - acc: 0.8383 - val_loss: 1.0830 - val_acc: 0.6932\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.8433\n",
      "Epoch 00029: val_loss did not improve from 1.03586\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4943 - acc: 0.8433 - val_loss: 1.0576 - val_acc: 0.7025\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8490\n",
      "Epoch 00030: val_loss did not improve from 1.03586\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4715 - acc: 0.8490 - val_loss: 1.0434 - val_acc: 0.7086\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.8502\n",
      "Epoch 00031: val_loss improved from 1.03586 to 1.03492, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/031-1.0349.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4653 - acc: 0.8502 - val_loss: 1.0349 - val_acc: 0.7093\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8555\n",
      "Epoch 00032: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.4528 - acc: 0.8555 - val_loss: 1.0388 - val_acc: 0.7102\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8607\n",
      "Epoch 00033: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.4354 - acc: 0.8608 - val_loss: 1.0709 - val_acc: 0.7067\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.8632\n",
      "Epoch 00034: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4297 - acc: 0.8632 - val_loss: 1.0702 - val_acc: 0.7095\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4196 - acc: 0.8645\n",
      "Epoch 00035: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4196 - acc: 0.8646 - val_loss: 1.0720 - val_acc: 0.7086\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8697\n",
      "Epoch 00036: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.4080 - acc: 0.8697 - val_loss: 1.0605 - val_acc: 0.7137\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8711\n",
      "Epoch 00037: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3985 - acc: 0.8711 - val_loss: 1.0665 - val_acc: 0.7195\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8726\n",
      "Epoch 00038: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3910 - acc: 0.8727 - val_loss: 1.0602 - val_acc: 0.7165\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8764\n",
      "Epoch 00039: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3807 - acc: 0.8764 - val_loss: 1.0791 - val_acc: 0.7116\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8811\n",
      "Epoch 00040: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3693 - acc: 0.8811 - val_loss: 1.0681 - val_acc: 0.7126\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8842\n",
      "Epoch 00041: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.3561 - acc: 0.8842 - val_loss: 1.0791 - val_acc: 0.7170\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8835\n",
      "Epoch 00042: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.3570 - acc: 0.8835 - val_loss: 1.0834 - val_acc: 0.7114\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8877\n",
      "Epoch 00043: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3446 - acc: 0.8877 - val_loss: 1.1064 - val_acc: 0.7060\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8889\n",
      "Epoch 00044: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3367 - acc: 0.8889 - val_loss: 1.0725 - val_acc: 0.7188\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8915\n",
      "Epoch 00045: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3348 - acc: 0.8915 - val_loss: 1.0753 - val_acc: 0.7200\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8920\n",
      "Epoch 00046: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3243 - acc: 0.8920 - val_loss: 1.1023 - val_acc: 0.7114\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8967\n",
      "Epoch 00047: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3181 - acc: 0.8967 - val_loss: 1.0904 - val_acc: 0.7235\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8979\n",
      "Epoch 00048: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3110 - acc: 0.8979 - val_loss: 1.1154 - val_acc: 0.7135\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.8969\n",
      "Epoch 00049: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3098 - acc: 0.8969 - val_loss: 1.0950 - val_acc: 0.7193\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.8984\n",
      "Epoch 00050: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3056 - acc: 0.8984 - val_loss: 1.1357 - val_acc: 0.7130\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9015\n",
      "Epoch 00051: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2986 - acc: 0.9015 - val_loss: 1.1147 - val_acc: 0.7130\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9038\n",
      "Epoch 00052: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2948 - acc: 0.9038 - val_loss: 1.1199 - val_acc: 0.7147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9032\n",
      "Epoch 00053: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2928 - acc: 0.9032 - val_loss: 1.1313 - val_acc: 0.7214\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9065\n",
      "Epoch 00054: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2845 - acc: 0.9065 - val_loss: 1.1051 - val_acc: 0.7298\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9074\n",
      "Epoch 00055: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2794 - acc: 0.9074 - val_loss: 1.1250 - val_acc: 0.7249\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9085\n",
      "Epoch 00056: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2724 - acc: 0.9084 - val_loss: 1.1354 - val_acc: 0.7261\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9116\n",
      "Epoch 00057: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2704 - acc: 0.9116 - val_loss: 1.1203 - val_acc: 0.7312\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9127\n",
      "Epoch 00058: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2636 - acc: 0.9127 - val_loss: 1.1427 - val_acc: 0.7135\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9143\n",
      "Epoch 00059: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 666us/sample - loss: 0.2608 - acc: 0.9143 - val_loss: 1.1268 - val_acc: 0.7249\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9142\n",
      "Epoch 00060: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2609 - acc: 0.9142 - val_loss: 1.1510 - val_acc: 0.7235\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9151\n",
      "Epoch 00061: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.2595 - acc: 0.9151 - val_loss: 1.1113 - val_acc: 0.7265\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9170\n",
      "Epoch 00062: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2534 - acc: 0.9170 - val_loss: 1.1449 - val_acc: 0.7263\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9178\n",
      "Epoch 00063: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2490 - acc: 0.9178 - val_loss: 1.1557 - val_acc: 0.7270\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9158\n",
      "Epoch 00064: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2514 - acc: 0.9158 - val_loss: 1.1725 - val_acc: 0.7240\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9186\n",
      "Epoch 00065: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2433 - acc: 0.9186 - val_loss: 1.1549 - val_acc: 0.7277\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9202\n",
      "Epoch 00066: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2404 - acc: 0.9202 - val_loss: 1.1653 - val_acc: 0.7258\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9186\n",
      "Epoch 00067: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2429 - acc: 0.9187 - val_loss: 1.1343 - val_acc: 0.7272\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9239\n",
      "Epoch 00068: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2311 - acc: 0.9239 - val_loss: 1.1757 - val_acc: 0.7289\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9233\n",
      "Epoch 00069: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2307 - acc: 0.9233 - val_loss: 1.1455 - val_acc: 0.7251\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9238\n",
      "Epoch 00070: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2274 - acc: 0.9238 - val_loss: 1.1474 - val_acc: 0.7270\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9260\n",
      "Epoch 00071: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2224 - acc: 0.9260 - val_loss: 1.1570 - val_acc: 0.7303\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9261\n",
      "Epoch 00072: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2216 - acc: 0.9262 - val_loss: 1.2113 - val_acc: 0.7219\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9279\n",
      "Epoch 00073: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2200 - acc: 0.9279 - val_loss: 1.1992 - val_acc: 0.7254\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9273\n",
      "Epoch 00074: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2180 - acc: 0.9273 - val_loss: 1.1708 - val_acc: 0.7303\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9285\n",
      "Epoch 00075: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2161 - acc: 0.9285 - val_loss: 1.1666 - val_acc: 0.7370\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9277\n",
      "Epoch 00076: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2155 - acc: 0.9277 - val_loss: 1.1967 - val_acc: 0.7268\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9310\n",
      "Epoch 00077: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2102 - acc: 0.9310 - val_loss: 1.1965 - val_acc: 0.7319\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9305\n",
      "Epoch 00078: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2097 - acc: 0.9306 - val_loss: 1.1917 - val_acc: 0.7391\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9315\n",
      "Epoch 00079: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2084 - acc: 0.9315 - val_loss: 1.1774 - val_acc: 0.7293\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9313\n",
      "Epoch 00080: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2109 - acc: 0.9313 - val_loss: 1.2279 - val_acc: 0.7254\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9302\n",
      "Epoch 00081: val_loss did not improve from 1.03492\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2092 - acc: 0.9302 - val_loss: 1.2073 - val_acc: 0.7286\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 334us/sample - loss: 1.1527 - acc: 0.6671\n",
      "Loss: 1.152688188320132 Accuracy: 0.667082\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3796 - acc: 0.2246\n",
      "Epoch 00001: val_loss improved from inf to 1.77380, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/001-1.7738.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 2.3796 - acc: 0.2246 - val_loss: 1.7738 - val_acc: 0.4340\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6847 - acc: 0.4566\n",
      "Epoch 00002: val_loss improved from 1.77380 to 1.48529, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/002-1.4853.hdf5\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 1.6846 - acc: 0.4566 - val_loss: 1.4853 - val_acc: 0.5253\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4931 - acc: 0.5226\n",
      "Epoch 00003: val_loss improved from 1.48529 to 1.37818, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/003-1.3782.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.4930 - acc: 0.5226 - val_loss: 1.3782 - val_acc: 0.5658\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3591 - acc: 0.5733\n",
      "Epoch 00004: val_loss improved from 1.37818 to 1.24052, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/004-1.2405.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 1.3592 - acc: 0.5733 - val_loss: 1.2405 - val_acc: 0.6259\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2490 - acc: 0.6125\n",
      "Epoch 00005: val_loss improved from 1.24052 to 1.13757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/005-1.1376.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 1.2490 - acc: 0.6125 - val_loss: 1.1376 - val_acc: 0.6569\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1626 - acc: 0.6411\n",
      "Epoch 00006: val_loss improved from 1.13757 to 1.10317, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/006-1.1032.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.1627 - acc: 0.6411 - val_loss: 1.1032 - val_acc: 0.6697\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0978 - acc: 0.6617\n",
      "Epoch 00007: val_loss improved from 1.10317 to 1.04501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/007-1.0450.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.0977 - acc: 0.6617 - val_loss: 1.0450 - val_acc: 0.6758\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0409 - acc: 0.6816\n",
      "Epoch 00008: val_loss improved from 1.04501 to 0.98958, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/008-0.9896.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.0408 - acc: 0.6816 - val_loss: 0.9896 - val_acc: 0.7079\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9894 - acc: 0.6973\n",
      "Epoch 00009: val_loss improved from 0.98958 to 0.96246, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/009-0.9625.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.9893 - acc: 0.6974 - val_loss: 0.9625 - val_acc: 0.7144\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9385 - acc: 0.7134\n",
      "Epoch 00010: val_loss improved from 0.96246 to 0.93045, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/010-0.9304.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9385 - acc: 0.7134 - val_loss: 0.9304 - val_acc: 0.7223\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7260\n",
      "Epoch 00011: val_loss improved from 0.93045 to 0.91160, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/011-0.9116.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.9007 - acc: 0.7260 - val_loss: 0.9116 - val_acc: 0.7328\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8676 - acc: 0.7373\n",
      "Epoch 00012: val_loss improved from 0.91160 to 0.88407, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/012-0.8841.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8678 - acc: 0.7373 - val_loss: 0.8841 - val_acc: 0.7452\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8328 - acc: 0.7458\n",
      "Epoch 00013: val_loss improved from 0.88407 to 0.87902, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/013-0.8790.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8328 - acc: 0.7458 - val_loss: 0.8790 - val_acc: 0.7384\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7977 - acc: 0.7589\n",
      "Epoch 00014: val_loss improved from 0.87902 to 0.84231, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/014-0.8423.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.7976 - acc: 0.7589 - val_loss: 0.8423 - val_acc: 0.7603\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7706 - acc: 0.7652\n",
      "Epoch 00015: val_loss did not improve from 0.84231\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.7705 - acc: 0.7652 - val_loss: 0.8423 - val_acc: 0.7582\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7457 - acc: 0.7755\n",
      "Epoch 00016: val_loss improved from 0.84231 to 0.81530, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/016-0.8153.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.7457 - acc: 0.7755 - val_loss: 0.8153 - val_acc: 0.7603\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7131 - acc: 0.7843\n",
      "Epoch 00017: val_loss did not improve from 0.81530\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7131 - acc: 0.7843 - val_loss: 0.8199 - val_acc: 0.7622\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.7905\n",
      "Epoch 00018: val_loss improved from 0.81530 to 0.80492, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/018-0.8049.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.6924 - acc: 0.7905 - val_loss: 0.8049 - val_acc: 0.7664\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.7986\n",
      "Epoch 00019: val_loss improved from 0.80492 to 0.77358, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/019-0.7736.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6667 - acc: 0.7986 - val_loss: 0.7736 - val_acc: 0.7754\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6544 - acc: 0.7990\n",
      "Epoch 00020: val_loss did not improve from 0.77358\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6544 - acc: 0.7991 - val_loss: 0.7929 - val_acc: 0.7696\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8116\n",
      "Epoch 00021: val_loss did not improve from 0.77358\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6257 - acc: 0.8116 - val_loss: 0.7868 - val_acc: 0.7745\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8154\n",
      "Epoch 00022: val_loss improved from 0.77358 to 0.77020, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/022-0.7702.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.6054 - acc: 0.8154 - val_loss: 0.7702 - val_acc: 0.7829\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.8210\n",
      "Epoch 00023: val_loss improved from 0.77020 to 0.76545, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/023-0.7654.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.5872 - acc: 0.8210 - val_loss: 0.7654 - val_acc: 0.7743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8259\n",
      "Epoch 00024: val_loss did not improve from 0.76545\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.5717 - acc: 0.8259 - val_loss: 0.7710 - val_acc: 0.7808\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8311\n",
      "Epoch 00025: val_loss did not improve from 0.76545\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5529 - acc: 0.8311 - val_loss: 0.7798 - val_acc: 0.7801\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8376\n",
      "Epoch 00026: val_loss improved from 0.76545 to 0.75161, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/026-0.7516.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5340 - acc: 0.8376 - val_loss: 0.7516 - val_acc: 0.7871\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8406\n",
      "Epoch 00027: val_loss improved from 0.75161 to 0.74909, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/027-0.7491.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.5226 - acc: 0.8405 - val_loss: 0.7491 - val_acc: 0.7869\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8468\n",
      "Epoch 00028: val_loss did not improve from 0.74909\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5049 - acc: 0.8468 - val_loss: 0.7636 - val_acc: 0.7841\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8502\n",
      "Epoch 00029: val_loss did not improve from 0.74909\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.4874 - acc: 0.8502 - val_loss: 0.7538 - val_acc: 0.7866\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8537\n",
      "Epoch 00030: val_loss did not improve from 0.74909\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4750 - acc: 0.8537 - val_loss: 0.7574 - val_acc: 0.7852\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8549\n",
      "Epoch 00031: val_loss improved from 0.74909 to 0.74365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/031-0.7436.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4685 - acc: 0.8549 - val_loss: 0.7436 - val_acc: 0.7848\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8614\n",
      "Epoch 00032: val_loss did not improve from 0.74365\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.4484 - acc: 0.8613 - val_loss: 0.7552 - val_acc: 0.7857\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8618\n",
      "Epoch 00033: val_loss improved from 0.74365 to 0.73226, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/033-0.7323.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.4418 - acc: 0.8618 - val_loss: 0.7323 - val_acc: 0.7941\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8660\n",
      "Epoch 00034: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.4272 - acc: 0.8660 - val_loss: 0.7393 - val_acc: 0.7920\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8692\n",
      "Epoch 00035: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.4156 - acc: 0.8692 - val_loss: 0.7591 - val_acc: 0.7897\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8726\n",
      "Epoch 00036: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.4051 - acc: 0.8726 - val_loss: 0.7470 - val_acc: 0.7932\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8778\n",
      "Epoch 00037: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3916 - acc: 0.8778 - val_loss: 0.7648 - val_acc: 0.7932\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8793\n",
      "Epoch 00038: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3876 - acc: 0.8793 - val_loss: 0.7945 - val_acc: 0.7827\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8804\n",
      "Epoch 00039: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3770 - acc: 0.8804 - val_loss: 0.7830 - val_acc: 0.7925\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8826\n",
      "Epoch 00040: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3665 - acc: 0.8827 - val_loss: 0.7700 - val_acc: 0.7876\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8865\n",
      "Epoch 00041: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.3563 - acc: 0.8865 - val_loss: 0.7685 - val_acc: 0.7971\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8881\n",
      "Epoch 00042: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3521 - acc: 0.8880 - val_loss: 0.8038 - val_acc: 0.7845\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8870\n",
      "Epoch 00043: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3490 - acc: 0.8871 - val_loss: 0.7598 - val_acc: 0.7957\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8900\n",
      "Epoch 00044: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.3378 - acc: 0.8900 - val_loss: 0.7484 - val_acc: 0.7978\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8937\n",
      "Epoch 00045: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3295 - acc: 0.8937 - val_loss: 0.7558 - val_acc: 0.7971\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8970\n",
      "Epoch 00046: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3250 - acc: 0.8971 - val_loss: 0.7799 - val_acc: 0.8027\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8969\n",
      "Epoch 00047: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3209 - acc: 0.8969 - val_loss: 0.7692 - val_acc: 0.7971\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9024\n",
      "Epoch 00048: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3088 - acc: 0.9021 - val_loss: 0.8130 - val_acc: 0.7913\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9040\n",
      "Epoch 00049: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3033 - acc: 0.9040 - val_loss: 0.8143 - val_acc: 0.7885\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9063\n",
      "Epoch 00050: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2924 - acc: 0.9063 - val_loss: 0.7689 - val_acc: 0.8071\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9038\n",
      "Epoch 00051: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.2939 - acc: 0.9038 - val_loss: 0.7858 - val_acc: 0.7999\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9065\n",
      "Epoch 00052: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2878 - acc: 0.9065 - val_loss: 0.7757 - val_acc: 0.8036\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9099\n",
      "Epoch 00053: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2776 - acc: 0.9099 - val_loss: 0.7807 - val_acc: 0.7987\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9124\n",
      "Epoch 00054: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.2746 - acc: 0.9125 - val_loss: 0.7716 - val_acc: 0.8001\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9104\n",
      "Epoch 00055: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2727 - acc: 0.9104 - val_loss: 0.7641 - val_acc: 0.8064\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9119\n",
      "Epoch 00056: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2667 - acc: 0.9119 - val_loss: 0.7872 - val_acc: 0.8008\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9137\n",
      "Epoch 00057: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2619 - acc: 0.9137 - val_loss: 0.7884 - val_acc: 0.8055\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9157\n",
      "Epoch 00058: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2586 - acc: 0.9157 - val_loss: 0.7962 - val_acc: 0.8057\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9151\n",
      "Epoch 00059: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2567 - acc: 0.9151 - val_loss: 0.8044 - val_acc: 0.8004\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9188\n",
      "Epoch 00060: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2469 - acc: 0.9187 - val_loss: 0.8220 - val_acc: 0.8064\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9174\n",
      "Epoch 00061: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2529 - acc: 0.9175 - val_loss: 0.7813 - val_acc: 0.8034\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9196\n",
      "Epoch 00062: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2460 - acc: 0.9196 - val_loss: 0.7930 - val_acc: 0.8046\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9232\n",
      "Epoch 00063: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2386 - acc: 0.9232 - val_loss: 0.7926 - val_acc: 0.8113\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9235\n",
      "Epoch 00064: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2357 - acc: 0.9235 - val_loss: 0.7936 - val_acc: 0.8064\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9244\n",
      "Epoch 00065: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2302 - acc: 0.9244 - val_loss: 0.8038 - val_acc: 0.8111\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9248\n",
      "Epoch 00066: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2311 - acc: 0.9248 - val_loss: 0.8092 - val_acc: 0.8081\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9245\n",
      "Epoch 00067: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2283 - acc: 0.9245 - val_loss: 0.8162 - val_acc: 0.8088\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9262\n",
      "Epoch 00068: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2222 - acc: 0.9262 - val_loss: 0.8229 - val_acc: 0.8057\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9257\n",
      "Epoch 00069: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2250 - acc: 0.9257 - val_loss: 0.8085 - val_acc: 0.8048\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9295\n",
      "Epoch 00070: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2161 - acc: 0.9295 - val_loss: 0.8023 - val_acc: 0.8111\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9276\n",
      "Epoch 00071: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2170 - acc: 0.9276 - val_loss: 0.7913 - val_acc: 0.8178\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9301\n",
      "Epoch 00072: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2100 - acc: 0.9301 - val_loss: 0.7902 - val_acc: 0.8137\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9325\n",
      "Epoch 00073: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2067 - acc: 0.9325 - val_loss: 0.8585 - val_acc: 0.8057\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9288\n",
      "Epoch 00074: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2159 - acc: 0.9288 - val_loss: 0.8308 - val_acc: 0.8067\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9315\n",
      "Epoch 00075: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2086 - acc: 0.9314 - val_loss: 0.8464 - val_acc: 0.8027\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9344\n",
      "Epoch 00076: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1992 - acc: 0.9344 - val_loss: 0.8215 - val_acc: 0.8120\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9336\n",
      "Epoch 00077: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1988 - acc: 0.9335 - val_loss: 0.8138 - val_acc: 0.8130\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9351\n",
      "Epoch 00078: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1937 - acc: 0.9351 - val_loss: 0.8231 - val_acc: 0.8169\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9355\n",
      "Epoch 00079: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1956 - acc: 0.9355 - val_loss: 0.7980 - val_acc: 0.8125\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9390\n",
      "Epoch 00080: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1888 - acc: 0.9391 - val_loss: 0.8393 - val_acc: 0.8106\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9369\n",
      "Epoch 00081: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1933 - acc: 0.9369 - val_loss: 0.8170 - val_acc: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9369\n",
      "Epoch 00082: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1933 - acc: 0.9369 - val_loss: 0.8442 - val_acc: 0.8134\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9357\n",
      "Epoch 00083: val_loss did not improve from 0.73226\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1942 - acc: 0.9357 - val_loss: 0.8311 - val_acc: 0.8146\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 368us/sample - loss: 0.8164 - acc: 0.7599\n",
      "Loss: 0.816376467607722 Accuracy: 0.7599169\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5045 - acc: 0.1743\n",
      "Epoch 00001: val_loss improved from inf to 1.89096, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/001-1.8910.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 2.5044 - acc: 0.1743 - val_loss: 1.8910 - val_acc: 0.3927\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7480 - acc: 0.4296\n",
      "Epoch 00002: val_loss improved from 1.89096 to 1.45340, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/002-1.4534.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.7480 - acc: 0.4296 - val_loss: 1.4534 - val_acc: 0.5397\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5059 - acc: 0.5110\n",
      "Epoch 00003: val_loss improved from 1.45340 to 1.32935, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/003-1.3294.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.5059 - acc: 0.5110 - val_loss: 1.3294 - val_acc: 0.5947\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3922 - acc: 0.5483\n",
      "Epoch 00004: val_loss improved from 1.32935 to 1.24113, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/004-1.2411.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 1.3921 - acc: 0.5483 - val_loss: 1.2411 - val_acc: 0.6205\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2983 - acc: 0.5818\n",
      "Epoch 00005: val_loss improved from 1.24113 to 1.16198, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/005-1.1620.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 1.2983 - acc: 0.5818 - val_loss: 1.1620 - val_acc: 0.6317\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2328 - acc: 0.6058\n",
      "Epoch 00006: val_loss improved from 1.16198 to 1.10149, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/006-1.1015.hdf5\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 1.2328 - acc: 0.6057 - val_loss: 1.1015 - val_acc: 0.6662\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1663 - acc: 0.6288\n",
      "Epoch 00007: val_loss improved from 1.10149 to 1.03869, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/007-1.0387.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 1.1664 - acc: 0.6288 - val_loss: 1.0387 - val_acc: 0.6860\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1096 - acc: 0.6504\n",
      "Epoch 00008: val_loss improved from 1.03869 to 0.98509, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/008-0.9851.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.1095 - acc: 0.6504 - val_loss: 0.9851 - val_acc: 0.7067\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0598 - acc: 0.6690\n",
      "Epoch 00009: val_loss improved from 0.98509 to 0.95105, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/009-0.9510.hdf5\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 1.0598 - acc: 0.6690 - val_loss: 0.9510 - val_acc: 0.7172\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.6879\n",
      "Epoch 00010: val_loss improved from 0.95105 to 0.88915, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/010-0.8892.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 1.0069 - acc: 0.6880 - val_loss: 0.8892 - val_acc: 0.7324\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9531 - acc: 0.7066\n",
      "Epoch 00011: val_loss improved from 0.88915 to 0.87757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/011-0.8776.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.9530 - acc: 0.7066 - val_loss: 0.8776 - val_acc: 0.7400\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9120 - acc: 0.7199\n",
      "Epoch 00012: val_loss improved from 0.87757 to 0.80771, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/012-0.8077.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.9120 - acc: 0.7199 - val_loss: 0.8077 - val_acc: 0.7671\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8643 - acc: 0.7338\n",
      "Epoch 00013: val_loss did not improve from 0.80771\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8643 - acc: 0.7338 - val_loss: 0.8092 - val_acc: 0.7610\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8317 - acc: 0.7445\n",
      "Epoch 00014: val_loss improved from 0.80771 to 0.78436, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/014-0.7844.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8319 - acc: 0.7444 - val_loss: 0.7844 - val_acc: 0.7696\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.7570\n",
      "Epoch 00015: val_loss improved from 0.78436 to 0.71363, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/015-0.7136.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.7927 - acc: 0.7570 - val_loss: 0.7136 - val_acc: 0.7929\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7686\n",
      "Epoch 00016: val_loss improved from 0.71363 to 0.70723, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/016-0.7072.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.7598 - acc: 0.7686 - val_loss: 0.7072 - val_acc: 0.7880\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7793\n",
      "Epoch 00017: val_loss improved from 0.70723 to 0.66238, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/017-0.6624.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.7318 - acc: 0.7793 - val_loss: 0.6624 - val_acc: 0.8090\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7041 - acc: 0.7898\n",
      "Epoch 00018: val_loss did not improve from 0.66238\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.7042 - acc: 0.7898 - val_loss: 0.6751 - val_acc: 0.8036\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.7943\n",
      "Epoch 00019: val_loss improved from 0.66238 to 0.60405, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/019-0.6040.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.6799 - acc: 0.7943 - val_loss: 0.6040 - val_acc: 0.8274\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.8015\n",
      "Epoch 00020: val_loss did not improve from 0.60405\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.6508 - acc: 0.8015 - val_loss: 0.6119 - val_acc: 0.8244\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.8086\n",
      "Epoch 00021: val_loss improved from 0.60405 to 0.57429, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/021-0.5743.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.6368 - acc: 0.8087 - val_loss: 0.5743 - val_acc: 0.8346\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8153\n",
      "Epoch 00022: val_loss improved from 0.57429 to 0.56220, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/022-0.5622.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.6104 - acc: 0.8153 - val_loss: 0.5622 - val_acc: 0.8435\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8234\n",
      "Epoch 00023: val_loss improved from 0.56220 to 0.54808, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/023-0.5481.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5868 - acc: 0.8234 - val_loss: 0.5481 - val_acc: 0.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8270\n",
      "Epoch 00024: val_loss did not improve from 0.54808\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5742 - acc: 0.8270 - val_loss: 0.5546 - val_acc: 0.8456\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5617 - acc: 0.8291\n",
      "Epoch 00025: val_loss did not improve from 0.54808\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.5616 - acc: 0.8292 - val_loss: 0.5800 - val_acc: 0.8323\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8360\n",
      "Epoch 00026: val_loss improved from 0.54808 to 0.51662, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/026-0.5166.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.5456 - acc: 0.8359 - val_loss: 0.5166 - val_acc: 0.8551\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8395\n",
      "Epoch 00027: val_loss did not improve from 0.51662\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5310 - acc: 0.8395 - val_loss: 0.5220 - val_acc: 0.8570\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8404\n",
      "Epoch 00028: val_loss improved from 0.51662 to 0.51522, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/028-0.5152.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5163 - acc: 0.8404 - val_loss: 0.5152 - val_acc: 0.8530\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8482\n",
      "Epoch 00029: val_loss improved from 0.51522 to 0.48317, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/029-0.4832.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5039 - acc: 0.8482 - val_loss: 0.4832 - val_acc: 0.8658\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8512\n",
      "Epoch 00030: val_loss improved from 0.48317 to 0.47708, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/030-0.4771.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.4880 - acc: 0.8513 - val_loss: 0.4771 - val_acc: 0.8672\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8543\n",
      "Epoch 00031: val_loss improved from 0.47708 to 0.46768, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/031-0.4677.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.4786 - acc: 0.8543 - val_loss: 0.4677 - val_acc: 0.8719\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8593\n",
      "Epoch 00032: val_loss improved from 0.46768 to 0.45990, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/032-0.4599.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.4642 - acc: 0.8593 - val_loss: 0.4599 - val_acc: 0.8700\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8612\n",
      "Epoch 00033: val_loss did not improve from 0.45990\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4583 - acc: 0.8612 - val_loss: 0.5037 - val_acc: 0.8563\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8626\n",
      "Epoch 00034: val_loss did not improve from 0.45990\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4503 - acc: 0.8625 - val_loss: 0.4643 - val_acc: 0.8712\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8677\n",
      "Epoch 00035: val_loss improved from 0.45990 to 0.44986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/035-0.4499.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.4372 - acc: 0.8677 - val_loss: 0.4499 - val_acc: 0.8747\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.8700\n",
      "Epoch 00036: val_loss did not improve from 0.44986\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.4255 - acc: 0.8700 - val_loss: 0.4836 - val_acc: 0.8665\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8723\n",
      "Epoch 00037: val_loss improved from 0.44986 to 0.44853, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/037-0.4485.hdf5\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.4201 - acc: 0.8723 - val_loss: 0.4485 - val_acc: 0.8730\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8741\n",
      "Epoch 00038: val_loss did not improve from 0.44853\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.4149 - acc: 0.8741 - val_loss: 0.4531 - val_acc: 0.8740\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8782\n",
      "Epoch 00039: val_loss improved from 0.44853 to 0.43072, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/039-0.4307.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.4004 - acc: 0.8782 - val_loss: 0.4307 - val_acc: 0.8768\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8762\n",
      "Epoch 00040: val_loss did not improve from 0.43072\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.4011 - acc: 0.8762 - val_loss: 0.4495 - val_acc: 0.8779\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8804\n",
      "Epoch 00041: val_loss improved from 0.43072 to 0.42919, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/041-0.4292.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.3939 - acc: 0.8804 - val_loss: 0.4292 - val_acc: 0.8768\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8801\n",
      "Epoch 00042: val_loss did not improve from 0.42919\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3876 - acc: 0.8801 - val_loss: 0.4649 - val_acc: 0.8717\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8845\n",
      "Epoch 00043: val_loss improved from 0.42919 to 0.42397, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/043-0.4240.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3829 - acc: 0.8844 - val_loss: 0.4240 - val_acc: 0.8796\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8857\n",
      "Epoch 00044: val_loss improved from 0.42397 to 0.41456, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/044-0.4146.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.3728 - acc: 0.8857 - val_loss: 0.4146 - val_acc: 0.8856\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8875\n",
      "Epoch 00045: val_loss improved from 0.41456 to 0.41265, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/045-0.4127.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3637 - acc: 0.8875 - val_loss: 0.4127 - val_acc: 0.8842\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8885\n",
      "Epoch 00046: val_loss improved from 0.41265 to 0.41262, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/046-0.4126.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.3595 - acc: 0.8885 - val_loss: 0.4126 - val_acc: 0.8819\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8901\n",
      "Epoch 00047: val_loss did not improve from 0.41262\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3539 - acc: 0.8901 - val_loss: 0.4187 - val_acc: 0.8870\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8905\n",
      "Epoch 00048: val_loss improved from 0.41262 to 0.40615, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/048-0.4062.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.3546 - acc: 0.8906 - val_loss: 0.4062 - val_acc: 0.8870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8938\n",
      "Epoch 00049: val_loss did not improve from 0.40615\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3442 - acc: 0.8938 - val_loss: 0.4075 - val_acc: 0.8856\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8938\n",
      "Epoch 00050: val_loss did not improve from 0.40615\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3411 - acc: 0.8938 - val_loss: 0.4129 - val_acc: 0.8903\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8965\n",
      "Epoch 00051: val_loss did not improve from 0.40615\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3312 - acc: 0.8965 - val_loss: 0.4100 - val_acc: 0.8905\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8957\n",
      "Epoch 00052: val_loss improved from 0.40615 to 0.39897, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/052-0.3990.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3324 - acc: 0.8957 - val_loss: 0.3990 - val_acc: 0.8954\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9008\n",
      "Epoch 00053: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3198 - acc: 0.9007 - val_loss: 0.4091 - val_acc: 0.8915\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8995\n",
      "Epoch 00054: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3233 - acc: 0.8995 - val_loss: 0.4037 - val_acc: 0.8940\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9006\n",
      "Epoch 00055: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3197 - acc: 0.9006 - val_loss: 0.4022 - val_acc: 0.8952\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.9035\n",
      "Epoch 00056: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3095 - acc: 0.9035 - val_loss: 0.3993 - val_acc: 0.8896\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9031\n",
      "Epoch 00057: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3093 - acc: 0.9031 - val_loss: 0.4004 - val_acc: 0.8877\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9050\n",
      "Epoch 00058: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3069 - acc: 0.9050 - val_loss: 0.4145 - val_acc: 0.8908\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9036\n",
      "Epoch 00059: val_loss did not improve from 0.39897\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3020 - acc: 0.9036 - val_loss: 0.4163 - val_acc: 0.8854\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9076\n",
      "Epoch 00060: val_loss improved from 0.39897 to 0.39780, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/060-0.3978.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2932 - acc: 0.9076 - val_loss: 0.3978 - val_acc: 0.8935\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9085\n",
      "Epoch 00061: val_loss improved from 0.39780 to 0.39668, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/061-0.3967.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2931 - acc: 0.9085 - val_loss: 0.3967 - val_acc: 0.9001\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9077\n",
      "Epoch 00062: val_loss did not improve from 0.39668\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2903 - acc: 0.9077 - val_loss: 0.3982 - val_acc: 0.8931\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9108\n",
      "Epoch 00063: val_loss improved from 0.39668 to 0.38777, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/063-0.3878.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2815 - acc: 0.9108 - val_loss: 0.3878 - val_acc: 0.9024\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9102\n",
      "Epoch 00064: val_loss did not improve from 0.38777\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2836 - acc: 0.9102 - val_loss: 0.3888 - val_acc: 0.9008\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9121\n",
      "Epoch 00065: val_loss did not improve from 0.38777\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2770 - acc: 0.9121 - val_loss: 0.3883 - val_acc: 0.9022\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9139\n",
      "Epoch 00066: val_loss did not improve from 0.38777\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2702 - acc: 0.9139 - val_loss: 0.3904 - val_acc: 0.8975\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9153\n",
      "Epoch 00067: val_loss did not improve from 0.38777\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2666 - acc: 0.9153 - val_loss: 0.3971 - val_acc: 0.8959\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9149\n",
      "Epoch 00068: val_loss improved from 0.38777 to 0.38365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/068-0.3837.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2669 - acc: 0.9150 - val_loss: 0.3837 - val_acc: 0.9012\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9160\n",
      "Epoch 00069: val_loss improved from 0.38365 to 0.38056, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/069-0.3806.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2642 - acc: 0.9160 - val_loss: 0.3806 - val_acc: 0.9012\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9166\n",
      "Epoch 00070: val_loss did not improve from 0.38056\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2611 - acc: 0.9166 - val_loss: 0.3958 - val_acc: 0.9008\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9204\n",
      "Epoch 00071: val_loss did not improve from 0.38056\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2562 - acc: 0.9204 - val_loss: 0.3806 - val_acc: 0.9001\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9164\n",
      "Epoch 00072: val_loss improved from 0.38056 to 0.37771, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/072-0.3777.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2620 - acc: 0.9164 - val_loss: 0.3777 - val_acc: 0.9043\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9185\n",
      "Epoch 00073: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2567 - acc: 0.9185 - val_loss: 0.3976 - val_acc: 0.8970\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9193\n",
      "Epoch 00074: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2519 - acc: 0.9193 - val_loss: 0.3952 - val_acc: 0.8996\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9190\n",
      "Epoch 00075: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2530 - acc: 0.9190 - val_loss: 0.4032 - val_acc: 0.8952\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9217\n",
      "Epoch 00076: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2466 - acc: 0.9217 - val_loss: 0.3861 - val_acc: 0.9033\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9206\n",
      "Epoch 00077: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2463 - acc: 0.9206 - val_loss: 0.4044 - val_acc: 0.8991\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9224\n",
      "Epoch 00078: val_loss did not improve from 0.37771\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2426 - acc: 0.9224 - val_loss: 0.3848 - val_acc: 0.9036\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9219\n",
      "Epoch 00079: val_loss improved from 0.37771 to 0.37395, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/079-0.3740.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2406 - acc: 0.9219 - val_loss: 0.3740 - val_acc: 0.9089\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9235\n",
      "Epoch 00080: val_loss did not improve from 0.37395\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2368 - acc: 0.9235 - val_loss: 0.3801 - val_acc: 0.9010\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9247\n",
      "Epoch 00081: val_loss improved from 0.37395 to 0.36888, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/081-0.3689.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2337 - acc: 0.9247 - val_loss: 0.3689 - val_acc: 0.9052\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9247\n",
      "Epoch 00082: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2347 - acc: 0.9247 - val_loss: 0.3891 - val_acc: 0.8968\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9272\n",
      "Epoch 00083: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2299 - acc: 0.9272 - val_loss: 0.4069 - val_acc: 0.9022\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9276\n",
      "Epoch 00084: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2276 - acc: 0.9276 - val_loss: 0.3927 - val_acc: 0.9019\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9256\n",
      "Epoch 00085: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2278 - acc: 0.9256 - val_loss: 0.3977 - val_acc: 0.8994\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9263\n",
      "Epoch 00086: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2259 - acc: 0.9262 - val_loss: 0.3952 - val_acc: 0.9038\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9290\n",
      "Epoch 00087: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2227 - acc: 0.9290 - val_loss: 0.3763 - val_acc: 0.9085\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9289\n",
      "Epoch 00088: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2196 - acc: 0.9289 - val_loss: 0.3805 - val_acc: 0.9040\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9297\n",
      "Epoch 00089: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2150 - acc: 0.9297 - val_loss: 0.3998 - val_acc: 0.9026\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9305\n",
      "Epoch 00090: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2146 - acc: 0.9306 - val_loss: 0.3981 - val_acc: 0.9003\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9312\n",
      "Epoch 00091: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2118 - acc: 0.9312 - val_loss: 0.3835 - val_acc: 0.9059\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9317\n",
      "Epoch 00092: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2121 - acc: 0.9317 - val_loss: 0.3860 - val_acc: 0.9054\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9333\n",
      "Epoch 00093: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.2078 - acc: 0.9333 - val_loss: 0.3801 - val_acc: 0.9078\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9320\n",
      "Epoch 00094: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2106 - acc: 0.9320 - val_loss: 0.4053 - val_acc: 0.9059\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9321\n",
      "Epoch 00095: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2095 - acc: 0.9322 - val_loss: 0.3845 - val_acc: 0.9047\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9318\n",
      "Epoch 00096: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2092 - acc: 0.9318 - val_loss: 0.3778 - val_acc: 0.9082\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9330\n",
      "Epoch 00097: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2047 - acc: 0.9330 - val_loss: 0.3950 - val_acc: 0.9033\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9345\n",
      "Epoch 00098: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2008 - acc: 0.9345 - val_loss: 0.3811 - val_acc: 0.9073\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9374\n",
      "Epoch 00099: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1952 - acc: 0.9374 - val_loss: 0.3867 - val_acc: 0.9050\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9354\n",
      "Epoch 00100: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1983 - acc: 0.9354 - val_loss: 0.3975 - val_acc: 0.9038\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9370\n",
      "Epoch 00101: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1977 - acc: 0.9370 - val_loss: 0.3986 - val_acc: 0.9054\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9344\n",
      "Epoch 00102: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2002 - acc: 0.9344 - val_loss: 0.3851 - val_acc: 0.9101\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9360\n",
      "Epoch 00103: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1992 - acc: 0.9360 - val_loss: 0.3724 - val_acc: 0.9087\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9375\n",
      "Epoch 00104: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1905 - acc: 0.9375 - val_loss: 0.3874 - val_acc: 0.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9384\n",
      "Epoch 00105: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1881 - acc: 0.9384 - val_loss: 0.3751 - val_acc: 0.9089\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9376\n",
      "Epoch 00106: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1891 - acc: 0.9376 - val_loss: 0.3851 - val_acc: 0.9031\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9387\n",
      "Epoch 00107: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1882 - acc: 0.9387 - val_loss: 0.3823 - val_acc: 0.9106\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9385\n",
      "Epoch 00108: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1898 - acc: 0.9385 - val_loss: 0.3923 - val_acc: 0.9075\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9385\n",
      "Epoch 00109: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1829 - acc: 0.9385 - val_loss: 0.4094 - val_acc: 0.9073\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9395\n",
      "Epoch 00110: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1828 - acc: 0.9395 - val_loss: 0.3813 - val_acc: 0.9040\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9405\n",
      "Epoch 00111: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1823 - acc: 0.9404 - val_loss: 0.3845 - val_acc: 0.9113\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9404\n",
      "Epoch 00112: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1814 - acc: 0.9404 - val_loss: 0.3911 - val_acc: 0.9089\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9405\n",
      "Epoch 00113: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1839 - acc: 0.9405 - val_loss: 0.4020 - val_acc: 0.9026\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9434\n",
      "Epoch 00114: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1733 - acc: 0.9434 - val_loss: 0.3946 - val_acc: 0.9073\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9385\n",
      "Epoch 00115: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1832 - acc: 0.9385 - val_loss: 0.3880 - val_acc: 0.9092\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9415\n",
      "Epoch 00116: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1779 - acc: 0.9415 - val_loss: 0.3904 - val_acc: 0.9078\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9413\n",
      "Epoch 00117: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1780 - acc: 0.9413 - val_loss: 0.3927 - val_acc: 0.9099\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9440\n",
      "Epoch 00118: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1709 - acc: 0.9440 - val_loss: 0.3737 - val_acc: 0.9054\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9439\n",
      "Epoch 00119: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1718 - acc: 0.9439 - val_loss: 0.3750 - val_acc: 0.9173\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9441\n",
      "Epoch 00120: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1726 - acc: 0.9441 - val_loss: 0.3913 - val_acc: 0.9054\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9441\n",
      "Epoch 00121: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1727 - acc: 0.9441 - val_loss: 0.3893 - val_acc: 0.9075\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9435\n",
      "Epoch 00122: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1665 - acc: 0.9435 - val_loss: 0.3861 - val_acc: 0.9087\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9420\n",
      "Epoch 00123: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1732 - acc: 0.9421 - val_loss: 0.3823 - val_acc: 0.9092\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9448\n",
      "Epoch 00124: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1671 - acc: 0.9448 - val_loss: 0.3815 - val_acc: 0.9106\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9434\n",
      "Epoch 00125: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1688 - acc: 0.9434 - val_loss: 0.3847 - val_acc: 0.9061\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9438\n",
      "Epoch 00126: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1712 - acc: 0.9438 - val_loss: 0.3924 - val_acc: 0.9099\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9453\n",
      "Epoch 00127: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1645 - acc: 0.9453 - val_loss: 0.3843 - val_acc: 0.9115\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9455\n",
      "Epoch 00128: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1679 - acc: 0.9456 - val_loss: 0.4475 - val_acc: 0.9043\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9469\n",
      "Epoch 00129: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1624 - acc: 0.9469 - val_loss: 0.3816 - val_acc: 0.9143\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9449\n",
      "Epoch 00130: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1642 - acc: 0.9449 - val_loss: 0.3935 - val_acc: 0.9085\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9471\n",
      "Epoch 00131: val_loss did not improve from 0.36888\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1599 - acc: 0.9471 - val_loss: 0.3964 - val_acc: 0.9068\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 412us/sample - loss: 0.4395 - acc: 0.8808\n",
      "Loss: 0.43945830760838955 Accuracy: 0.8807892\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4629 - acc: 0.1942\n",
      "Epoch 00001: val_loss improved from inf to 1.78466, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/001-1.7847.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 2.4629 - acc: 0.1942 - val_loss: 1.7847 - val_acc: 0.4528\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7490 - acc: 0.4299\n",
      "Epoch 00002: val_loss improved from 1.78466 to 1.42730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/002-1.4273.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.7489 - acc: 0.4299 - val_loss: 1.4273 - val_acc: 0.5623\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5272 - acc: 0.5023\n",
      "Epoch 00003: val_loss improved from 1.42730 to 1.26730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/003-1.2673.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.5272 - acc: 0.5024 - val_loss: 1.2673 - val_acc: 0.6122\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3808 - acc: 0.5486\n",
      "Epoch 00004: val_loss improved from 1.26730 to 1.15460, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/004-1.1546.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.3812 - acc: 0.5485 - val_loss: 1.1546 - val_acc: 0.6471\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2747 - acc: 0.5873\n",
      "Epoch 00005: val_loss improved from 1.15460 to 1.04632, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/005-1.0463.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.2745 - acc: 0.5875 - val_loss: 1.0463 - val_acc: 0.6862\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1761 - acc: 0.6242\n",
      "Epoch 00006: val_loss improved from 1.04632 to 0.93781, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/006-0.9378.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1761 - acc: 0.6242 - val_loss: 0.9378 - val_acc: 0.7209\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0979 - acc: 0.6504\n",
      "Epoch 00007: val_loss improved from 0.93781 to 0.86755, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/007-0.8675.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0979 - acc: 0.6504 - val_loss: 0.8675 - val_acc: 0.7477\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0225 - acc: 0.6796\n",
      "Epoch 00008: val_loss improved from 0.86755 to 0.83501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/008-0.8350.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.0225 - acc: 0.6796 - val_loss: 0.8350 - val_acc: 0.7496\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9610 - acc: 0.6987\n",
      "Epoch 00009: val_loss improved from 0.83501 to 0.78359, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/009-0.7836.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.9607 - acc: 0.6988 - val_loss: 0.7836 - val_acc: 0.7680\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9061 - acc: 0.7164\n",
      "Epoch 00010: val_loss improved from 0.78359 to 0.70222, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/010-0.7022.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.9060 - acc: 0.7164 - val_loss: 0.7022 - val_acc: 0.7915\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8603 - acc: 0.7320\n",
      "Epoch 00011: val_loss improved from 0.70222 to 0.66894, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/011-0.6689.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.8604 - acc: 0.7319 - val_loss: 0.6689 - val_acc: 0.8097\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.7454\n",
      "Epoch 00012: val_loss improved from 0.66894 to 0.64832, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/012-0.6483.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.8276 - acc: 0.7454 - val_loss: 0.6483 - val_acc: 0.8148\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7565\n",
      "Epoch 00013: val_loss improved from 0.64832 to 0.62494, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/013-0.6249.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.7821 - acc: 0.7564 - val_loss: 0.6249 - val_acc: 0.8218\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7679\n",
      "Epoch 00014: val_loss improved from 0.62494 to 0.58719, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/014-0.5872.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.7500 - acc: 0.7679 - val_loss: 0.5872 - val_acc: 0.8321\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7189 - acc: 0.7783\n",
      "Epoch 00015: val_loss improved from 0.58719 to 0.56278, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/015-0.5628.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.7189 - acc: 0.7783 - val_loss: 0.5628 - val_acc: 0.8404\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.7864\n",
      "Epoch 00016: val_loss improved from 0.56278 to 0.55862, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/016-0.5586.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.6893 - acc: 0.7864 - val_loss: 0.5586 - val_acc: 0.8386\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6656 - acc: 0.7947\n",
      "Epoch 00017: val_loss improved from 0.55862 to 0.54960, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/017-0.5496.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.6656 - acc: 0.7947 - val_loss: 0.5496 - val_acc: 0.8351\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6398 - acc: 0.8023\n",
      "Epoch 00018: val_loss improved from 0.54960 to 0.49948, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/018-0.4995.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.6398 - acc: 0.8023 - val_loss: 0.4995 - val_acc: 0.8558\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.8104\n",
      "Epoch 00019: val_loss did not improve from 0.49948\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.6184 - acc: 0.8105 - val_loss: 0.5075 - val_acc: 0.8530\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.8157\n",
      "Epoch 00020: val_loss improved from 0.49948 to 0.46378, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/020-0.4638.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.5945 - acc: 0.8157 - val_loss: 0.4638 - val_acc: 0.8707\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.8236\n",
      "Epoch 00021: val_loss did not improve from 0.46378\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.5738 - acc: 0.8236 - val_loss: 0.4738 - val_acc: 0.8565\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.8264\n",
      "Epoch 00022: val_loss improved from 0.46378 to 0.41770, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/022-0.4177.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.5629 - acc: 0.8264 - val_loss: 0.4177 - val_acc: 0.8805\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8343\n",
      "Epoch 00023: val_loss did not improve from 0.41770\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.5358 - acc: 0.8343 - val_loss: 0.4231 - val_acc: 0.8807\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.8390\n",
      "Epoch 00024: val_loss improved from 0.41770 to 0.40476, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/024-0.4048.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5214 - acc: 0.8390 - val_loss: 0.4048 - val_acc: 0.8810\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8420\n",
      "Epoch 00025: val_loss improved from 0.40476 to 0.40122, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/025-0.4012.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.5053 - acc: 0.8421 - val_loss: 0.4012 - val_acc: 0.8866\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4963 - acc: 0.8467\n",
      "Epoch 00026: val_loss improved from 0.40122 to 0.37971, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/026-0.3797.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4962 - acc: 0.8467 - val_loss: 0.3797 - val_acc: 0.8952\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8513\n",
      "Epoch 00027: val_loss did not improve from 0.37971\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4828 - acc: 0.8513 - val_loss: 0.3873 - val_acc: 0.8882\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8559\n",
      "Epoch 00028: val_loss improved from 0.37971 to 0.35713, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/028-0.3571.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4670 - acc: 0.8559 - val_loss: 0.3571 - val_acc: 0.9010\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4591 - acc: 0.8579\n",
      "Epoch 00029: val_loss did not improve from 0.35713\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.4591 - acc: 0.8579 - val_loss: 0.3661 - val_acc: 0.8973\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8592\n",
      "Epoch 00030: val_loss improved from 0.35713 to 0.35084, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/030-0.3508.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4504 - acc: 0.8592 - val_loss: 0.3508 - val_acc: 0.8996\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8659\n",
      "Epoch 00031: val_loss improved from 0.35084 to 0.33283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/031-0.3328.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4355 - acc: 0.8659 - val_loss: 0.3328 - val_acc: 0.9075\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8657\n",
      "Epoch 00032: val_loss did not improve from 0.33283\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4319 - acc: 0.8657 - val_loss: 0.3365 - val_acc: 0.9050\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8708\n",
      "Epoch 00033: val_loss improved from 0.33283 to 0.33114, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/033-0.3311.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4166 - acc: 0.8708 - val_loss: 0.3311 - val_acc: 0.9059\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8713\n",
      "Epoch 00034: val_loss did not improve from 0.33114\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.4124 - acc: 0.8713 - val_loss: 0.3337 - val_acc: 0.9057\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8751\n",
      "Epoch 00035: val_loss improved from 0.33114 to 0.32564, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/035-0.3256.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4037 - acc: 0.8751 - val_loss: 0.3256 - val_acc: 0.9054\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8772\n",
      "Epoch 00036: val_loss improved from 0.32564 to 0.32375, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/036-0.3237.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3959 - acc: 0.8772 - val_loss: 0.3237 - val_acc: 0.9087\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8770\n",
      "Epoch 00037: val_loss improved from 0.32375 to 0.31501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/037-0.3150.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3924 - acc: 0.8770 - val_loss: 0.3150 - val_acc: 0.9101\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8819\n",
      "Epoch 00038: val_loss did not improve from 0.31501\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3778 - acc: 0.8820 - val_loss: 0.3629 - val_acc: 0.8961\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8815\n",
      "Epoch 00039: val_loss improved from 0.31501 to 0.30109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/039-0.3011.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3779 - acc: 0.8815 - val_loss: 0.3011 - val_acc: 0.9154\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8855\n",
      "Epoch 00040: val_loss did not improve from 0.30109\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.3690 - acc: 0.8855 - val_loss: 0.3119 - val_acc: 0.9143\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8874\n",
      "Epoch 00041: val_loss improved from 0.30109 to 0.27933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/041-0.2793.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3621 - acc: 0.8874 - val_loss: 0.2793 - val_acc: 0.9224\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8877\n",
      "Epoch 00042: val_loss did not improve from 0.27933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.3601 - acc: 0.8877 - val_loss: 0.2939 - val_acc: 0.9171\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8886\n",
      "Epoch 00043: val_loss did not improve from 0.27933\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3553 - acc: 0.8886 - val_loss: 0.2822 - val_acc: 0.9203\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8915\n",
      "Epoch 00044: val_loss did not improve from 0.27933\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3496 - acc: 0.8915 - val_loss: 0.2871 - val_acc: 0.9161\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8935\n",
      "Epoch 00045: val_loss improved from 0.27933 to 0.27524, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/045-0.2752.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3416 - acc: 0.8935 - val_loss: 0.2752 - val_acc: 0.9224\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.8947\n",
      "Epoch 00046: val_loss did not improve from 0.27524\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3359 - acc: 0.8947 - val_loss: 0.2762 - val_acc: 0.9229\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8936\n",
      "Epoch 00047: val_loss improved from 0.27524 to 0.27227, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/047-0.2723.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3347 - acc: 0.8936 - val_loss: 0.2723 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8985\n",
      "Epoch 00048: val_loss improved from 0.27227 to 0.26516, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/048-0.2652.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3290 - acc: 0.8985 - val_loss: 0.2652 - val_acc: 0.9248\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8988\n",
      "Epoch 00049: val_loss did not improve from 0.26516\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3251 - acc: 0.8988 - val_loss: 0.2763 - val_acc: 0.9201\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8984\n",
      "Epoch 00050: val_loss improved from 0.26516 to 0.26056, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/050-0.2606.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3262 - acc: 0.8984 - val_loss: 0.2606 - val_acc: 0.9287\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8982\n",
      "Epoch 00051: val_loss did not improve from 0.26056\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3247 - acc: 0.8982 - val_loss: 0.2651 - val_acc: 0.9252\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9020\n",
      "Epoch 00052: val_loss improved from 0.26056 to 0.25809, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/052-0.2581.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3131 - acc: 0.9020 - val_loss: 0.2581 - val_acc: 0.9276\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9020\n",
      "Epoch 00053: val_loss improved from 0.25809 to 0.25712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/053-0.2571.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3119 - acc: 0.9020 - val_loss: 0.2571 - val_acc: 0.9313\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9021\n",
      "Epoch 00054: val_loss did not improve from 0.25712\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3078 - acc: 0.9021 - val_loss: 0.2659 - val_acc: 0.9297\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9054\n",
      "Epoch 00055: val_loss improved from 0.25712 to 0.25282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/055-0.2528.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3020 - acc: 0.9054 - val_loss: 0.2528 - val_acc: 0.9308\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9051\n",
      "Epoch 00056: val_loss did not improve from 0.25282\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3020 - acc: 0.9051 - val_loss: 0.2681 - val_acc: 0.9252\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9082\n",
      "Epoch 00057: val_loss did not improve from 0.25282\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2914 - acc: 0.9082 - val_loss: 0.2534 - val_acc: 0.9320\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9089\n",
      "Epoch 00058: val_loss did not improve from 0.25282\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2903 - acc: 0.9088 - val_loss: 0.2559 - val_acc: 0.9317\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9082\n",
      "Epoch 00059: val_loss improved from 0.25282 to 0.24626, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/059-0.2463.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2906 - acc: 0.9082 - val_loss: 0.2463 - val_acc: 0.9329\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9110\n",
      "Epoch 00060: val_loss did not improve from 0.24626\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2813 - acc: 0.9110 - val_loss: 0.2555 - val_acc: 0.9294\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9118\n",
      "Epoch 00061: val_loss improved from 0.24626 to 0.24494, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/061-0.2449.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2832 - acc: 0.9118 - val_loss: 0.2449 - val_acc: 0.9324\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9115\n",
      "Epoch 00062: val_loss did not improve from 0.24494\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2797 - acc: 0.9115 - val_loss: 0.2459 - val_acc: 0.9343\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9139\n",
      "Epoch 00063: val_loss did not improve from 0.24494\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2740 - acc: 0.9139 - val_loss: 0.2625 - val_acc: 0.9299\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9120\n",
      "Epoch 00064: val_loss improved from 0.24494 to 0.24393, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/064-0.2439.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2758 - acc: 0.9119 - val_loss: 0.2439 - val_acc: 0.9338\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9156\n",
      "Epoch 00065: val_loss improved from 0.24393 to 0.23773, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/065-0.2377.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2688 - acc: 0.9156 - val_loss: 0.2377 - val_acc: 0.9359\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9146\n",
      "Epoch 00066: val_loss did not improve from 0.23773\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2661 - acc: 0.9146 - val_loss: 0.2551 - val_acc: 0.9308\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9182\n",
      "Epoch 00067: val_loss did not improve from 0.23773\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2555 - acc: 0.9182 - val_loss: 0.2454 - val_acc: 0.9317\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9151\n",
      "Epoch 00068: val_loss improved from 0.23773 to 0.23534, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/068-0.2353.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2645 - acc: 0.9151 - val_loss: 0.2353 - val_acc: 0.9390\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9180\n",
      "Epoch 00069: val_loss did not improve from 0.23534\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2594 - acc: 0.9180 - val_loss: 0.2424 - val_acc: 0.9324\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9170\n",
      "Epoch 00070: val_loss improved from 0.23534 to 0.23380, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/070-0.2338.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2564 - acc: 0.9170 - val_loss: 0.2338 - val_acc: 0.9352\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9201\n",
      "Epoch 00071: val_loss did not improve from 0.23380\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2517 - acc: 0.9201 - val_loss: 0.2339 - val_acc: 0.9385\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9195\n",
      "Epoch 00072: val_loss did not improve from 0.23380\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2520 - acc: 0.9195 - val_loss: 0.2578 - val_acc: 0.9283\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9199\n",
      "Epoch 00073: val_loss improved from 0.23380 to 0.22731, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/073-0.2273.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2491 - acc: 0.9200 - val_loss: 0.2273 - val_acc: 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9179\n",
      "Epoch 00074: val_loss did not improve from 0.22731\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2553 - acc: 0.9179 - val_loss: 0.2408 - val_acc: 0.9371\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9207\n",
      "Epoch 00075: val_loss did not improve from 0.22731\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2460 - acc: 0.9207 - val_loss: 0.2329 - val_acc: 0.9397\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9209\n",
      "Epoch 00076: val_loss did not improve from 0.22731\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2446 - acc: 0.9209 - val_loss: 0.2509 - val_acc: 0.9341\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9214\n",
      "Epoch 00077: val_loss improved from 0.22731 to 0.22410, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/077-0.2241.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2500 - acc: 0.9213 - val_loss: 0.2241 - val_acc: 0.9397\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9203\n",
      "Epoch 00078: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2466 - acc: 0.9203 - val_loss: 0.2351 - val_acc: 0.9392\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9249\n",
      "Epoch 00079: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2351 - acc: 0.9249 - val_loss: 0.2434 - val_acc: 0.9378\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9237\n",
      "Epoch 00080: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2351 - acc: 0.9237 - val_loss: 0.2352 - val_acc: 0.9408\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9257\n",
      "Epoch 00081: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.2351 - acc: 0.9257 - val_loss: 0.2311 - val_acc: 0.9415\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9250\n",
      "Epoch 00082: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2307 - acc: 0.9250 - val_loss: 0.2319 - val_acc: 0.9408\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9263\n",
      "Epoch 00083: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2313 - acc: 0.9263 - val_loss: 0.2327 - val_acc: 0.9397\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9271\n",
      "Epoch 00084: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2264 - acc: 0.9271 - val_loss: 0.2278 - val_acc: 0.9420\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9285\n",
      "Epoch 00085: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2249 - acc: 0.9285 - val_loss: 0.2448 - val_acc: 0.9329\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9277\n",
      "Epoch 00086: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2263 - acc: 0.9277 - val_loss: 0.2275 - val_acc: 0.9434\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9263\n",
      "Epoch 00087: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2268 - acc: 0.9263 - val_loss: 0.2279 - val_acc: 0.9411\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9287\n",
      "Epoch 00088: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2248 - acc: 0.9287 - val_loss: 0.2252 - val_acc: 0.9434\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9299\n",
      "Epoch 00089: val_loss did not improve from 0.22410\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2161 - acc: 0.9300 - val_loss: 0.2275 - val_acc: 0.9420\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9312\n",
      "Epoch 00090: val_loss improved from 0.22410 to 0.22266, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/090-0.2227.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2127 - acc: 0.9312 - val_loss: 0.2227 - val_acc: 0.9406\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9301\n",
      "Epoch 00091: val_loss did not improve from 0.22266\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2159 - acc: 0.9301 - val_loss: 0.2347 - val_acc: 0.9392\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9314\n",
      "Epoch 00092: val_loss did not improve from 0.22266\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2135 - acc: 0.9314 - val_loss: 0.2416 - val_acc: 0.9352\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9308\n",
      "Epoch 00093: val_loss did not improve from 0.22266\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2109 - acc: 0.9308 - val_loss: 0.2346 - val_acc: 0.9401\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9313\n",
      "Epoch 00094: val_loss did not improve from 0.22266\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2096 - acc: 0.9313 - val_loss: 0.2230 - val_acc: 0.9415\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9322\n",
      "Epoch 00095: val_loss did not improve from 0.22266\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2063 - acc: 0.9322 - val_loss: 0.2262 - val_acc: 0.9390\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9319\n",
      "Epoch 00096: val_loss improved from 0.22266 to 0.21653, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/096-0.2165.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2055 - acc: 0.9319 - val_loss: 0.2165 - val_acc: 0.9464\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9325\n",
      "Epoch 00097: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2094 - acc: 0.9326 - val_loss: 0.2222 - val_acc: 0.9441\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9334\n",
      "Epoch 00098: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2025 - acc: 0.9334 - val_loss: 0.2194 - val_acc: 0.9453\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9343\n",
      "Epoch 00099: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2023 - acc: 0.9343 - val_loss: 0.2336 - val_acc: 0.9436\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9342\n",
      "Epoch 00100: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2030 - acc: 0.9342 - val_loss: 0.2306 - val_acc: 0.9429\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9361\n",
      "Epoch 00101: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1965 - acc: 0.9361 - val_loss: 0.2314 - val_acc: 0.9401\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9359\n",
      "Epoch 00102: val_loss did not improve from 0.21653\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1965 - acc: 0.9359 - val_loss: 0.2185 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9346\n",
      "Epoch 00103: val_loss improved from 0.21653 to 0.21594, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/103-0.2159.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1986 - acc: 0.9346 - val_loss: 0.2159 - val_acc: 0.9469\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9383\n",
      "Epoch 00104: val_loss did not improve from 0.21594\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1944 - acc: 0.9384 - val_loss: 0.2347 - val_acc: 0.9408\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9363\n",
      "Epoch 00105: val_loss improved from 0.21594 to 0.21334, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/105-0.2133.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1962 - acc: 0.9363 - val_loss: 0.2133 - val_acc: 0.9453\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9361\n",
      "Epoch 00106: val_loss did not improve from 0.21334\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1961 - acc: 0.9361 - val_loss: 0.2246 - val_acc: 0.9415\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9372\n",
      "Epoch 00107: val_loss did not improve from 0.21334\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1912 - acc: 0.9372 - val_loss: 0.2190 - val_acc: 0.9464\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9362\n",
      "Epoch 00108: val_loss did not improve from 0.21334\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1917 - acc: 0.9362 - val_loss: 0.2267 - val_acc: 0.9434\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9390\n",
      "Epoch 00109: val_loss improved from 0.21334 to 0.21188, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/109-0.2119.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1853 - acc: 0.9391 - val_loss: 0.2119 - val_acc: 0.9467\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9398\n",
      "Epoch 00110: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1830 - acc: 0.9398 - val_loss: 0.2186 - val_acc: 0.9446\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9405\n",
      "Epoch 00111: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1802 - acc: 0.9405 - val_loss: 0.2157 - val_acc: 0.9460\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9366\n",
      "Epoch 00112: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1856 - acc: 0.9366 - val_loss: 0.2146 - val_acc: 0.9418\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9420\n",
      "Epoch 00113: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1770 - acc: 0.9420 - val_loss: 0.2147 - val_acc: 0.9464\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9397\n",
      "Epoch 00114: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1844 - acc: 0.9397 - val_loss: 0.2189 - val_acc: 0.9427\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9397\n",
      "Epoch 00115: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1803 - acc: 0.9397 - val_loss: 0.2141 - val_acc: 0.9488\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9429\n",
      "Epoch 00116: val_loss did not improve from 0.21188\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1745 - acc: 0.9429 - val_loss: 0.2142 - val_acc: 0.9457\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9407\n",
      "Epoch 00117: val_loss improved from 0.21188 to 0.20551, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/117-0.2055.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1792 - acc: 0.9407 - val_loss: 0.2055 - val_acc: 0.9476\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9436\n",
      "Epoch 00118: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1716 - acc: 0.9436 - val_loss: 0.2062 - val_acc: 0.9478\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9418\n",
      "Epoch 00119: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1782 - acc: 0.9419 - val_loss: 0.2101 - val_acc: 0.9404\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9435\n",
      "Epoch 00120: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1713 - acc: 0.9436 - val_loss: 0.2129 - val_acc: 0.9443\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9431\n",
      "Epoch 00121: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1738 - acc: 0.9431 - val_loss: 0.2187 - val_acc: 0.9453\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9446\n",
      "Epoch 00122: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1683 - acc: 0.9447 - val_loss: 0.2144 - val_acc: 0.9490\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9451\n",
      "Epoch 00123: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1686 - acc: 0.9451 - val_loss: 0.2106 - val_acc: 0.9462\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9451\n",
      "Epoch 00124: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1654 - acc: 0.9451 - val_loss: 0.2396 - val_acc: 0.9383\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9447\n",
      "Epoch 00125: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1681 - acc: 0.9447 - val_loss: 0.2188 - val_acc: 0.9432\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9447\n",
      "Epoch 00126: val_loss did not improve from 0.20551\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1665 - acc: 0.9447 - val_loss: 0.2161 - val_acc: 0.9464\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9448\n",
      "Epoch 00127: val_loss improved from 0.20551 to 0.19991, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/127-0.1999.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1671 - acc: 0.9448 - val_loss: 0.1999 - val_acc: 0.9483\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9455\n",
      "Epoch 00128: val_loss did not improve from 0.19991\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1626 - acc: 0.9456 - val_loss: 0.2130 - val_acc: 0.9448\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9468\n",
      "Epoch 00129: val_loss did not improve from 0.19991\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1635 - acc: 0.9468 - val_loss: 0.2006 - val_acc: 0.9478\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9439\n",
      "Epoch 00130: val_loss improved from 0.19991 to 0.19933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/130-0.1993.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1694 - acc: 0.9439 - val_loss: 0.1993 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9464\n",
      "Epoch 00131: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1617 - acc: 0.9464 - val_loss: 0.2092 - val_acc: 0.9497\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9456\n",
      "Epoch 00132: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1596 - acc: 0.9456 - val_loss: 0.2002 - val_acc: 0.9495\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9468\n",
      "Epoch 00133: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1571 - acc: 0.9469 - val_loss: 0.2214 - val_acc: 0.9464\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9476\n",
      "Epoch 00134: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1580 - acc: 0.9476 - val_loss: 0.2056 - val_acc: 0.9485\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9459\n",
      "Epoch 00135: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1617 - acc: 0.9459 - val_loss: 0.2212 - val_acc: 0.9469\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9471\n",
      "Epoch 00136: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1576 - acc: 0.9471 - val_loss: 0.2103 - val_acc: 0.9513\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9479\n",
      "Epoch 00137: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1590 - acc: 0.9479 - val_loss: 0.2025 - val_acc: 0.9467\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9478\n",
      "Epoch 00138: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1535 - acc: 0.9478 - val_loss: 0.2257 - val_acc: 0.9483\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9469\n",
      "Epoch 00139: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1573 - acc: 0.9469 - val_loss: 0.2247 - val_acc: 0.9460\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9487\n",
      "Epoch 00140: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1503 - acc: 0.9487 - val_loss: 0.2141 - val_acc: 0.9467\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9483\n",
      "Epoch 00141: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1538 - acc: 0.9483 - val_loss: 0.2099 - val_acc: 0.9509\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9488\n",
      "Epoch 00142: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1514 - acc: 0.9488 - val_loss: 0.2109 - val_acc: 0.9495\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9487\n",
      "Epoch 00143: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.1543 - acc: 0.9487 - val_loss: 0.2010 - val_acc: 0.9464\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9505\n",
      "Epoch 00144: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1505 - acc: 0.9506 - val_loss: 0.2115 - val_acc: 0.9481\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9498\n",
      "Epoch 00145: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1488 - acc: 0.9498 - val_loss: 0.2041 - val_acc: 0.9490\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9496\n",
      "Epoch 00146: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1470 - acc: 0.9497 - val_loss: 0.2046 - val_acc: 0.9506\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9518\n",
      "Epoch 00147: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1420 - acc: 0.9517 - val_loss: 0.2113 - val_acc: 0.9483\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9516\n",
      "Epoch 00148: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1453 - acc: 0.9516 - val_loss: 0.2128 - val_acc: 0.9502\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9520\n",
      "Epoch 00149: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1427 - acc: 0.9519 - val_loss: 0.2095 - val_acc: 0.9511\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9520\n",
      "Epoch 00150: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1449 - acc: 0.9520 - val_loss: 0.2005 - val_acc: 0.9499\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9514\n",
      "Epoch 00151: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1429 - acc: 0.9514 - val_loss: 0.2109 - val_acc: 0.9481\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9529\n",
      "Epoch 00152: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1396 - acc: 0.9528 - val_loss: 0.2114 - val_acc: 0.9518\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9506\n",
      "Epoch 00153: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1442 - acc: 0.9506 - val_loss: 0.2319 - val_acc: 0.9443\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9531\n",
      "Epoch 00154: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1396 - acc: 0.9531 - val_loss: 0.2097 - val_acc: 0.9488\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9523\n",
      "Epoch 00155: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1394 - acc: 0.9523 - val_loss: 0.2000 - val_acc: 0.9529\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9530\n",
      "Epoch 00156: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1374 - acc: 0.9530 - val_loss: 0.2116 - val_acc: 0.9483\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9544\n",
      "Epoch 00157: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1362 - acc: 0.9544 - val_loss: 0.2079 - val_acc: 0.9504\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9539\n",
      "Epoch 00158: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1375 - acc: 0.9539 - val_loss: 0.2116 - val_acc: 0.9518\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9540\n",
      "Epoch 00159: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1374 - acc: 0.9540 - val_loss: 0.2033 - val_acc: 0.9515\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9526\n",
      "Epoch 00160: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1367 - acc: 0.9526 - val_loss: 0.2155 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9545\n",
      "Epoch 00161: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1356 - acc: 0.9545 - val_loss: 0.2147 - val_acc: 0.9506\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9538\n",
      "Epoch 00162: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1349 - acc: 0.9538 - val_loss: 0.2133 - val_acc: 0.9502\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9549\n",
      "Epoch 00163: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1326 - acc: 0.9549 - val_loss: 0.2028 - val_acc: 0.9481\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9550\n",
      "Epoch 00164: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1336 - acc: 0.9550 - val_loss: 0.2071 - val_acc: 0.9509\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9551\n",
      "Epoch 00165: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1335 - acc: 0.9550 - val_loss: 0.2093 - val_acc: 0.9509\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9550\n",
      "Epoch 00166: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1349 - acc: 0.9550 - val_loss: 0.2072 - val_acc: 0.9467\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9546\n",
      "Epoch 00167: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1320 - acc: 0.9546 - val_loss: 0.2210 - val_acc: 0.9497\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9554\n",
      "Epoch 00168: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1315 - acc: 0.9554 - val_loss: 0.2158 - val_acc: 0.9469\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9548\n",
      "Epoch 00169: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1317 - acc: 0.9548 - val_loss: 0.2132 - val_acc: 0.9483\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9565\n",
      "Epoch 00170: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1275 - acc: 0.9565 - val_loss: 0.2184 - val_acc: 0.9504\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9558\n",
      "Epoch 00171: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1255 - acc: 0.9558 - val_loss: 0.2099 - val_acc: 0.9529\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9572\n",
      "Epoch 00172: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1251 - acc: 0.9572 - val_loss: 0.2098 - val_acc: 0.9532\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9547\n",
      "Epoch 00173: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1256 - acc: 0.9547 - val_loss: 0.2181 - val_acc: 0.9513\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9576\n",
      "Epoch 00174: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1258 - acc: 0.9575 - val_loss: 0.2166 - val_acc: 0.9483\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9559\n",
      "Epoch 00175: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1278 - acc: 0.9559 - val_loss: 0.2275 - val_acc: 0.9476\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9576\n",
      "Epoch 00176: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1259 - acc: 0.9576 - val_loss: 0.2029 - val_acc: 0.9527\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9580\n",
      "Epoch 00177: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1266 - acc: 0.9580 - val_loss: 0.2151 - val_acc: 0.9481\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9577\n",
      "Epoch 00178: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1244 - acc: 0.9576 - val_loss: 0.2449 - val_acc: 0.9476\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9583\n",
      "Epoch 00179: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1256 - acc: 0.9583 - val_loss: 0.2101 - val_acc: 0.9515\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9578\n",
      "Epoch 00180: val_loss did not improve from 0.19933\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1235 - acc: 0.9578 - val_loss: 0.2240 - val_acc: 0.9504\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 420us/sample - loss: 0.2462 - acc: 0.9277\n",
      "Loss: 0.24616374088101173 Accuracy: 0.92772585\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5675 - acc: 0.1552\n",
      "Epoch 00001: val_loss improved from inf to 2.03985, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/001-2.0399.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 2.5674 - acc: 0.1553 - val_loss: 2.0399 - val_acc: 0.3562\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8662 - acc: 0.3821\n",
      "Epoch 00002: val_loss improved from 2.03985 to 1.46425, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/002-1.4643.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.8662 - acc: 0.3821 - val_loss: 1.4643 - val_acc: 0.5525\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5585 - acc: 0.4779\n",
      "Epoch 00003: val_loss improved from 1.46425 to 1.21728, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/003-1.2173.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.5585 - acc: 0.4779 - val_loss: 1.2173 - val_acc: 0.6238\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3653 - acc: 0.5459\n",
      "Epoch 00004: val_loss improved from 1.21728 to 1.09701, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/004-1.0970.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.3655 - acc: 0.5459 - val_loss: 1.0970 - val_acc: 0.6662\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2382 - acc: 0.5876\n",
      "Epoch 00005: val_loss improved from 1.09701 to 0.94203, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/005-0.9420.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.2382 - acc: 0.5875 - val_loss: 0.9420 - val_acc: 0.7091\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1218 - acc: 0.6292\n",
      "Epoch 00006: val_loss improved from 0.94203 to 0.86787, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/006-0.8679.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.1218 - acc: 0.6292 - val_loss: 0.8679 - val_acc: 0.7379\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0289 - acc: 0.6621\n",
      "Epoch 00007: val_loss improved from 0.86787 to 0.80404, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/007-0.8040.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.0289 - acc: 0.6622 - val_loss: 0.8040 - val_acc: 0.7540\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.6847\n",
      "Epoch 00008: val_loss improved from 0.80404 to 0.71540, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/008-0.7154.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.9639 - acc: 0.6847 - val_loss: 0.7154 - val_acc: 0.7780\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8983 - acc: 0.7078\n",
      "Epoch 00009: val_loss improved from 0.71540 to 0.66654, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/009-0.6665.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.8981 - acc: 0.7077 - val_loss: 0.6665 - val_acc: 0.7959\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8411 - acc: 0.7271\n",
      "Epoch 00010: val_loss improved from 0.66654 to 0.62198, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/010-0.6220.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.8411 - acc: 0.7271 - val_loss: 0.6220 - val_acc: 0.8116\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7446\n",
      "Epoch 00011: val_loss improved from 0.62198 to 0.57624, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/011-0.5762.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.7929 - acc: 0.7446 - val_loss: 0.5762 - val_acc: 0.8295\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7526 - acc: 0.7602\n",
      "Epoch 00012: val_loss improved from 0.57624 to 0.54241, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/012-0.5424.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.7526 - acc: 0.7602 - val_loss: 0.5424 - val_acc: 0.8360\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7095 - acc: 0.7731\n",
      "Epoch 00013: val_loss improved from 0.54241 to 0.50856, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/013-0.5086.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.7095 - acc: 0.7731 - val_loss: 0.5086 - val_acc: 0.8495\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.7863\n",
      "Epoch 00014: val_loss improved from 0.50856 to 0.48495, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/014-0.4849.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.6681 - acc: 0.7865 - val_loss: 0.4849 - val_acc: 0.8577\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6373 - acc: 0.7975\n",
      "Epoch 00015: val_loss improved from 0.48495 to 0.46214, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/015-0.4621.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.6375 - acc: 0.7974 - val_loss: 0.4621 - val_acc: 0.8628\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.8056\n",
      "Epoch 00016: val_loss improved from 0.46214 to 0.44551, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/016-0.4455.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.6095 - acc: 0.8056 - val_loss: 0.4455 - val_acc: 0.8707\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8145\n",
      "Epoch 00017: val_loss improved from 0.44551 to 0.42167, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/017-0.4217.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.5804 - acc: 0.8146 - val_loss: 0.4217 - val_acc: 0.8714\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8258\n",
      "Epoch 00018: val_loss improved from 0.42167 to 0.38570, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/018-0.3857.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.5523 - acc: 0.8258 - val_loss: 0.3857 - val_acc: 0.8898\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8302\n",
      "Epoch 00019: val_loss improved from 0.38570 to 0.36058, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/019-0.3606.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5356 - acc: 0.8302 - val_loss: 0.3606 - val_acc: 0.8980\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8376\n",
      "Epoch 00020: val_loss improved from 0.36058 to 0.35158, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/020-0.3516.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.5074 - acc: 0.8377 - val_loss: 0.3516 - val_acc: 0.8989\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8442\n",
      "Epoch 00021: val_loss improved from 0.35158 to 0.33441, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/021-0.3344.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4902 - acc: 0.8441 - val_loss: 0.3344 - val_acc: 0.9080\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8484\n",
      "Epoch 00022: val_loss improved from 0.33441 to 0.32284, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/022-0.3228.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4745 - acc: 0.8484 - val_loss: 0.3228 - val_acc: 0.9047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8548\n",
      "Epoch 00023: val_loss improved from 0.32284 to 0.29849, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/023-0.2985.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4561 - acc: 0.8548 - val_loss: 0.2985 - val_acc: 0.9147\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4392 - acc: 0.8612\n",
      "Epoch 00024: val_loss improved from 0.29849 to 0.28787, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/024-0.2879.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4392 - acc: 0.8613 - val_loss: 0.2879 - val_acc: 0.9173\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8648\n",
      "Epoch 00025: val_loss improved from 0.28787 to 0.27381, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/025-0.2738.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4258 - acc: 0.8648 - val_loss: 0.2738 - val_acc: 0.9220\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8696\n",
      "Epoch 00026: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4151 - acc: 0.8696 - val_loss: 0.3007 - val_acc: 0.9119\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8744\n",
      "Epoch 00027: val_loss improved from 0.27381 to 0.26796, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/027-0.2680.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.4006 - acc: 0.8744 - val_loss: 0.2680 - val_acc: 0.9269\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8780\n",
      "Epoch 00028: val_loss improved from 0.26796 to 0.26074, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/028-0.2607.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.3841 - acc: 0.8780 - val_loss: 0.2607 - val_acc: 0.9269\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8811\n",
      "Epoch 00029: val_loss improved from 0.26074 to 0.25311, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/029-0.2531.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3773 - acc: 0.8811 - val_loss: 0.2531 - val_acc: 0.9292\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8846\n",
      "Epoch 00030: val_loss improved from 0.25311 to 0.24878, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/030-0.2488.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.3684 - acc: 0.8846 - val_loss: 0.2488 - val_acc: 0.9276\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8877\n",
      "Epoch 00031: val_loss improved from 0.24878 to 0.24841, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/031-0.2484.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3569 - acc: 0.8877 - val_loss: 0.2484 - val_acc: 0.9250\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8923\n",
      "Epoch 00032: val_loss improved from 0.24841 to 0.24117, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/032-0.2412.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.3428 - acc: 0.8923 - val_loss: 0.2412 - val_acc: 0.9311\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8927\n",
      "Epoch 00033: val_loss improved from 0.24117 to 0.23375, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/033-0.2337.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.3363 - acc: 0.8928 - val_loss: 0.2337 - val_acc: 0.9357\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8963\n",
      "Epoch 00034: val_loss improved from 0.23375 to 0.22668, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/034-0.2267.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3319 - acc: 0.8963 - val_loss: 0.2267 - val_acc: 0.9366\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8968\n",
      "Epoch 00035: val_loss improved from 0.22668 to 0.21932, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/035-0.2193.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.3255 - acc: 0.8969 - val_loss: 0.2193 - val_acc: 0.9357\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.8993\n",
      "Epoch 00036: val_loss did not improve from 0.21932\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3192 - acc: 0.8993 - val_loss: 0.2260 - val_acc: 0.9317\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9017\n",
      "Epoch 00037: val_loss improved from 0.21932 to 0.21300, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/037-0.2130.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3063 - acc: 0.9017 - val_loss: 0.2130 - val_acc: 0.9350\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9043\n",
      "Epoch 00038: val_loss improved from 0.21300 to 0.19986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/038-0.1999.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3020 - acc: 0.9043 - val_loss: 0.1999 - val_acc: 0.9415\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9073\n",
      "Epoch 00039: val_loss did not improve from 0.19986\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2930 - acc: 0.9073 - val_loss: 0.2044 - val_acc: 0.9392\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9086\n",
      "Epoch 00040: val_loss improved from 0.19986 to 0.19946, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/040-0.1995.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2874 - acc: 0.9086 - val_loss: 0.1995 - val_acc: 0.9392\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9095\n",
      "Epoch 00041: val_loss improved from 0.19946 to 0.19510, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/041-0.1951.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2860 - acc: 0.9095 - val_loss: 0.1951 - val_acc: 0.9434\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9123\n",
      "Epoch 00042: val_loss improved from 0.19510 to 0.18654, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/042-0.1865.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2781 - acc: 0.9124 - val_loss: 0.1865 - val_acc: 0.9446\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9120\n",
      "Epoch 00043: val_loss improved from 0.18654 to 0.18142, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/043-0.1814.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2706 - acc: 0.9121 - val_loss: 0.1814 - val_acc: 0.9448\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9146\n",
      "Epoch 00044: val_loss did not improve from 0.18142\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2682 - acc: 0.9146 - val_loss: 0.1882 - val_acc: 0.9460\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9171\n",
      "Epoch 00045: val_loss did not improve from 0.18142\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2642 - acc: 0.9171 - val_loss: 0.1890 - val_acc: 0.9427\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9171\n",
      "Epoch 00046: val_loss did not improve from 0.18142\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2580 - acc: 0.9171 - val_loss: 0.2112 - val_acc: 0.9385\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9184\n",
      "Epoch 00047: val_loss did not improve from 0.18142\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2524 - acc: 0.9184 - val_loss: 0.1830 - val_acc: 0.9436\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9207\n",
      "Epoch 00048: val_loss improved from 0.18142 to 0.17715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/048-0.1772.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2512 - acc: 0.9206 - val_loss: 0.1772 - val_acc: 0.9434\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9222\n",
      "Epoch 00049: val_loss did not improve from 0.17715\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2460 - acc: 0.9222 - val_loss: 0.1788 - val_acc: 0.9483\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9214\n",
      "Epoch 00050: val_loss improved from 0.17715 to 0.17446, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/050-0.1745.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2457 - acc: 0.9214 - val_loss: 0.1745 - val_acc: 0.9490\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9230\n",
      "Epoch 00051: val_loss improved from 0.17446 to 0.16720, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/051-0.1672.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2383 - acc: 0.9229 - val_loss: 0.1672 - val_acc: 0.9471\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9252\n",
      "Epoch 00052: val_loss did not improve from 0.16720\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2352 - acc: 0.9252 - val_loss: 0.1986 - val_acc: 0.9425\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9261\n",
      "Epoch 00053: val_loss improved from 0.16720 to 0.16625, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/053-0.1663.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.2316 - acc: 0.9261 - val_loss: 0.1663 - val_acc: 0.9497\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9258\n",
      "Epoch 00054: val_loss did not improve from 0.16625\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2318 - acc: 0.9258 - val_loss: 0.1771 - val_acc: 0.9509\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9287\n",
      "Epoch 00055: val_loss improved from 0.16625 to 0.16597, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/055-0.1660.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2252 - acc: 0.9287 - val_loss: 0.1660 - val_acc: 0.9509\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9304\n",
      "Epoch 00056: val_loss improved from 0.16597 to 0.16561, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/056-0.1656.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2209 - acc: 0.9304 - val_loss: 0.1656 - val_acc: 0.9536\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9320\n",
      "Epoch 00057: val_loss improved from 0.16561 to 0.16236, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/057-0.1624.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2156 - acc: 0.9320 - val_loss: 0.1624 - val_acc: 0.9506\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9316\n",
      "Epoch 00058: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2126 - acc: 0.9316 - val_loss: 0.1661 - val_acc: 0.9518\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9329\n",
      "Epoch 00059: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2101 - acc: 0.9329 - val_loss: 0.1692 - val_acc: 0.9502\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9333\n",
      "Epoch 00060: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2059 - acc: 0.9333 - val_loss: 0.1735 - val_acc: 0.9488\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9338\n",
      "Epoch 00061: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2076 - acc: 0.9338 - val_loss: 0.1645 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9327\n",
      "Epoch 00062: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2059 - acc: 0.9328 - val_loss: 0.1742 - val_acc: 0.9471\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9344\n",
      "Epoch 00063: val_loss did not improve from 0.16236\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2025 - acc: 0.9344 - val_loss: 0.1646 - val_acc: 0.9509\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9339\n",
      "Epoch 00064: val_loss improved from 0.16236 to 0.15711, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/064-0.1571.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1984 - acc: 0.9339 - val_loss: 0.1571 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9366\n",
      "Epoch 00065: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1949 - acc: 0.9366 - val_loss: 0.1604 - val_acc: 0.9560\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9341\n",
      "Epoch 00066: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1987 - acc: 0.9341 - val_loss: 0.1609 - val_acc: 0.9529\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9372\n",
      "Epoch 00067: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1926 - acc: 0.9372 - val_loss: 0.1621 - val_acc: 0.9529\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9379\n",
      "Epoch 00068: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1921 - acc: 0.9379 - val_loss: 0.1576 - val_acc: 0.9550\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9408\n",
      "Epoch 00069: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1828 - acc: 0.9408 - val_loss: 0.1811 - val_acc: 0.9436\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9381\n",
      "Epoch 00070: val_loss did not improve from 0.15711\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1908 - acc: 0.9381 - val_loss: 0.1574 - val_acc: 0.9488\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9420\n",
      "Epoch 00071: val_loss improved from 0.15711 to 0.15142, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/071-0.1514.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1807 - acc: 0.9419 - val_loss: 0.1514 - val_acc: 0.9550\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9412\n",
      "Epoch 00072: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1800 - acc: 0.9412 - val_loss: 0.1613 - val_acc: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9400\n",
      "Epoch 00073: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1815 - acc: 0.9400 - val_loss: 0.1563 - val_acc: 0.9515\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9415\n",
      "Epoch 00074: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1764 - acc: 0.9415 - val_loss: 0.1604 - val_acc: 0.9539\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9421\n",
      "Epoch 00075: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1758 - acc: 0.9422 - val_loss: 0.1671 - val_acc: 0.9488\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9435\n",
      "Epoch 00076: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1742 - acc: 0.9435 - val_loss: 0.1616 - val_acc: 0.9553\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9443\n",
      "Epoch 00077: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1705 - acc: 0.9443 - val_loss: 0.1676 - val_acc: 0.9488\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9443\n",
      "Epoch 00078: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1682 - acc: 0.9444 - val_loss: 0.1617 - val_acc: 0.9485\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9467\n",
      "Epoch 00079: val_loss did not improve from 0.15142\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1671 - acc: 0.9467 - val_loss: 0.1587 - val_acc: 0.9522\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9453\n",
      "Epoch 00080: val_loss improved from 0.15142 to 0.14905, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/080-0.1490.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1664 - acc: 0.9453 - val_loss: 0.1490 - val_acc: 0.9560\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9449\n",
      "Epoch 00081: val_loss did not improve from 0.14905\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1661 - acc: 0.9449 - val_loss: 0.1595 - val_acc: 0.9541\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9472\n",
      "Epoch 00082: val_loss improved from 0.14905 to 0.14853, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/082-0.1485.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1617 - acc: 0.9472 - val_loss: 0.1485 - val_acc: 0.9555\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9471\n",
      "Epoch 00083: val_loss did not improve from 0.14853\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1575 - acc: 0.9471 - val_loss: 0.1660 - val_acc: 0.9532\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9472\n",
      "Epoch 00084: val_loss did not improve from 0.14853\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1634 - acc: 0.9472 - val_loss: 0.1524 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9484\n",
      "Epoch 00085: val_loss improved from 0.14853 to 0.14778, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/085-0.1478.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1557 - acc: 0.9484 - val_loss: 0.1478 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9491\n",
      "Epoch 00086: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1537 - acc: 0.9491 - val_loss: 0.1530 - val_acc: 0.9548\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9489\n",
      "Epoch 00087: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1543 - acc: 0.9489 - val_loss: 0.1669 - val_acc: 0.9534\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9501\n",
      "Epoch 00088: val_loss improved from 0.14778 to 0.14590, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/088-0.1459.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1488 - acc: 0.9501 - val_loss: 0.1459 - val_acc: 0.9592\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9494\n",
      "Epoch 00089: val_loss improved from 0.14590 to 0.14292, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/089-0.1429.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1517 - acc: 0.9493 - val_loss: 0.1429 - val_acc: 0.9571\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9490\n",
      "Epoch 00090: val_loss did not improve from 0.14292\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1522 - acc: 0.9490 - val_loss: 0.1462 - val_acc: 0.9576\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9521\n",
      "Epoch 00091: val_loss did not improve from 0.14292\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1453 - acc: 0.9522 - val_loss: 0.1535 - val_acc: 0.9543\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9513\n",
      "Epoch 00092: val_loss improved from 0.14292 to 0.14189, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/092-0.1419.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1464 - acc: 0.9513 - val_loss: 0.1419 - val_acc: 0.9574\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9522\n",
      "Epoch 00093: val_loss did not improve from 0.14189\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1440 - acc: 0.9522 - val_loss: 0.1589 - val_acc: 0.9553\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9527\n",
      "Epoch 00094: val_loss did not improve from 0.14189\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1443 - acc: 0.9527 - val_loss: 0.1504 - val_acc: 0.9578\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9521\n",
      "Epoch 00095: val_loss improved from 0.14189 to 0.14111, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/095-0.1411.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1440 - acc: 0.9522 - val_loss: 0.1411 - val_acc: 0.9599\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9528\n",
      "Epoch 00096: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1412 - acc: 0.9528 - val_loss: 0.1611 - val_acc: 0.9560\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9547\n",
      "Epoch 00097: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1342 - acc: 0.9547 - val_loss: 0.1491 - val_acc: 0.9602\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9509\n",
      "Epoch 00098: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1442 - acc: 0.9509 - val_loss: 0.1432 - val_acc: 0.9557\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9542\n",
      "Epoch 00099: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1380 - acc: 0.9542 - val_loss: 0.1586 - val_acc: 0.9592\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9551\n",
      "Epoch 00100: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1360 - acc: 0.9551 - val_loss: 0.1603 - val_acc: 0.9576\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9556\n",
      "Epoch 00101: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1334 - acc: 0.9556 - val_loss: 0.1605 - val_acc: 0.9597\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9564\n",
      "Epoch 00102: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1300 - acc: 0.9564 - val_loss: 0.1486 - val_acc: 0.9592\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9548\n",
      "Epoch 00103: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1326 - acc: 0.9548 - val_loss: 0.1508 - val_acc: 0.9550\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9558\n",
      "Epoch 00104: val_loss did not improve from 0.14111\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1326 - acc: 0.9558 - val_loss: 0.1537 - val_acc: 0.9569\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9563\n",
      "Epoch 00105: val_loss improved from 0.14111 to 0.14010, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/105-0.1401.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1317 - acc: 0.9563 - val_loss: 0.1401 - val_acc: 0.9585\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9578\n",
      "Epoch 00106: val_loss improved from 0.14010 to 0.13890, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/106-0.1389.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1254 - acc: 0.9578 - val_loss: 0.1389 - val_acc: 0.9623\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9574\n",
      "Epoch 00107: val_loss did not improve from 0.13890\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1253 - acc: 0.9573 - val_loss: 0.1577 - val_acc: 0.9569\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9563\n",
      "Epoch 00108: val_loss did not improve from 0.13890\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1279 - acc: 0.9564 - val_loss: 0.1457 - val_acc: 0.9583\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9588\n",
      "Epoch 00109: val_loss improved from 0.13890 to 0.13629, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/109-0.1363.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1242 - acc: 0.9588 - val_loss: 0.1363 - val_acc: 0.9618\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9573\n",
      "Epoch 00110: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1273 - acc: 0.9573 - val_loss: 0.1601 - val_acc: 0.9567\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9588\n",
      "Epoch 00111: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1232 - acc: 0.9588 - val_loss: 0.1461 - val_acc: 0.9595\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9582\n",
      "Epoch 00112: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1229 - acc: 0.9583 - val_loss: 0.1458 - val_acc: 0.9611\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9582\n",
      "Epoch 00113: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1226 - acc: 0.9582 - val_loss: 0.1602 - val_acc: 0.9546\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9588\n",
      "Epoch 00114: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1207 - acc: 0.9588 - val_loss: 0.1406 - val_acc: 0.9604\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9599\n",
      "Epoch 00115: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1196 - acc: 0.9600 - val_loss: 0.1437 - val_acc: 0.9604\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9589\n",
      "Epoch 00116: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1190 - acc: 0.9589 - val_loss: 0.1479 - val_acc: 0.9606\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9613\n",
      "Epoch 00117: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1157 - acc: 0.9613 - val_loss: 0.1418 - val_acc: 0.9620\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9611\n",
      "Epoch 00118: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1164 - acc: 0.9611 - val_loss: 0.1519 - val_acc: 0.9604\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9607\n",
      "Epoch 00119: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1162 - acc: 0.9607 - val_loss: 0.1563 - val_acc: 0.9557\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9615\n",
      "Epoch 00120: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1141 - acc: 0.9615 - val_loss: 0.1485 - val_acc: 0.9574\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9619\n",
      "Epoch 00121: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1091 - acc: 0.9619 - val_loss: 0.1653 - val_acc: 0.9583\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9625\n",
      "Epoch 00122: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1120 - acc: 0.9625 - val_loss: 0.1488 - val_acc: 0.9581\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9620\n",
      "Epoch 00123: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1119 - acc: 0.9620 - val_loss: 0.1526 - val_acc: 0.9597\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9627\n",
      "Epoch 00124: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1078 - acc: 0.9627 - val_loss: 0.1449 - val_acc: 0.9613\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9640\n",
      "Epoch 00125: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1051 - acc: 0.9640 - val_loss: 0.1587 - val_acc: 0.9581\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9634\n",
      "Epoch 00126: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1086 - acc: 0.9634 - val_loss: 0.1481 - val_acc: 0.9609\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9634\n",
      "Epoch 00127: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1071 - acc: 0.9634 - val_loss: 0.1484 - val_acc: 0.9604\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9638\n",
      "Epoch 00128: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1053 - acc: 0.9638 - val_loss: 0.1550 - val_acc: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9646\n",
      "Epoch 00129: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1057 - acc: 0.9646 - val_loss: 0.1619 - val_acc: 0.9588\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9646\n",
      "Epoch 00130: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1047 - acc: 0.9645 - val_loss: 0.1792 - val_acc: 0.9571\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9635\n",
      "Epoch 00131: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1052 - acc: 0.9635 - val_loss: 0.1635 - val_acc: 0.9571\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9652\n",
      "Epoch 00132: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1035 - acc: 0.9652 - val_loss: 0.1554 - val_acc: 0.9616\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9653\n",
      "Epoch 00133: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1016 - acc: 0.9653 - val_loss: 0.1807 - val_acc: 0.9569\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9628\n",
      "Epoch 00134: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1085 - acc: 0.9628 - val_loss: 0.1550 - val_acc: 0.9567\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9657\n",
      "Epoch 00135: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1025 - acc: 0.9657 - val_loss: 0.1527 - val_acc: 0.9590\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9662\n",
      "Epoch 00136: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0996 - acc: 0.9662 - val_loss: 0.1681 - val_acc: 0.9581\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9657\n",
      "Epoch 00137: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1001 - acc: 0.9657 - val_loss: 0.1425 - val_acc: 0.9623\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9655\n",
      "Epoch 00138: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0989 - acc: 0.9655 - val_loss: 0.1530 - val_acc: 0.9620\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9655\n",
      "Epoch 00139: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0993 - acc: 0.9655 - val_loss: 0.1610 - val_acc: 0.9588\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9662\n",
      "Epoch 00140: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0991 - acc: 0.9662 - val_loss: 0.1474 - val_acc: 0.9611\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9679\n",
      "Epoch 00141: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0965 - acc: 0.9679 - val_loss: 0.1574 - val_acc: 0.9583\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9667\n",
      "Epoch 00142: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.0970 - acc: 0.9666 - val_loss: 0.1497 - val_acc: 0.9588\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9662\n",
      "Epoch 00143: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0983 - acc: 0.9662 - val_loss: 0.1698 - val_acc: 0.9611\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9676\n",
      "Epoch 00144: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.0921 - acc: 0.9676 - val_loss: 0.1472 - val_acc: 0.9627\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9684\n",
      "Epoch 00145: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0929 - acc: 0.9684 - val_loss: 0.1543 - val_acc: 0.9620\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9684\n",
      "Epoch 00146: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.0909 - acc: 0.9684 - val_loss: 0.1503 - val_acc: 0.9597\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9676\n",
      "Epoch 00147: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0914 - acc: 0.9676 - val_loss: 0.1620 - val_acc: 0.9613\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9676\n",
      "Epoch 00148: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0951 - acc: 0.9676 - val_loss: 0.1569 - val_acc: 0.9599\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9686\n",
      "Epoch 00149: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0909 - acc: 0.9686 - val_loss: 0.1772 - val_acc: 0.9618\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9686\n",
      "Epoch 00150: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0912 - acc: 0.9686 - val_loss: 0.1478 - val_acc: 0.9623\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9696\n",
      "Epoch 00151: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.0894 - acc: 0.9696 - val_loss: 0.1531 - val_acc: 0.9627\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9690\n",
      "Epoch 00152: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.0898 - acc: 0.9690 - val_loss: 0.1562 - val_acc: 0.9613\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9699\n",
      "Epoch 00153: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0860 - acc: 0.9698 - val_loss: 0.1544 - val_acc: 0.9632\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9674\n",
      "Epoch 00154: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0954 - acc: 0.9674 - val_loss: 0.1489 - val_acc: 0.9639\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9712\n",
      "Epoch 00155: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0860 - acc: 0.9713 - val_loss: 0.1523 - val_acc: 0.9623\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9709\n",
      "Epoch 00156: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0838 - acc: 0.9709 - val_loss: 0.1564 - val_acc: 0.9571\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9707\n",
      "Epoch 00157: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0853 - acc: 0.9707 - val_loss: 0.1487 - val_acc: 0.9590\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9698\n",
      "Epoch 00158: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0891 - acc: 0.9698 - val_loss: 0.1589 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9713\n",
      "Epoch 00159: val_loss did not improve from 0.13629\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0843 - acc: 0.9713 - val_loss: 0.1613 - val_acc: 0.9604\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 418us/sample - loss: 0.1955 - acc: 0.9464\n",
      "Loss: 0.19550527749527033 Accuracy: 0.94641745\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2946 - acc: 0.2532\n",
      "Epoch 00001: val_loss improved from inf to 1.59416, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/001-1.5942.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 2.2946 - acc: 0.2532 - val_loss: 1.5942 - val_acc: 0.4992\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5315 - acc: 0.4974\n",
      "Epoch 00002: val_loss improved from 1.59416 to 1.10816, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/002-1.1082.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.5318 - acc: 0.4974 - val_loss: 1.1082 - val_acc: 0.6594\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1958 - acc: 0.6066\n",
      "Epoch 00003: val_loss improved from 1.10816 to 0.86562, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/003-0.8656.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1958 - acc: 0.6066 - val_loss: 0.8656 - val_acc: 0.7275\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0117 - acc: 0.6680\n",
      "Epoch 00004: val_loss improved from 0.86562 to 0.73524, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/004-0.7352.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0114 - acc: 0.6682 - val_loss: 0.7352 - val_acc: 0.7747\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8928 - acc: 0.7085\n",
      "Epoch 00005: val_loss improved from 0.73524 to 0.63060, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/005-0.6306.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.8927 - acc: 0.7086 - val_loss: 0.6306 - val_acc: 0.8069\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7900 - acc: 0.7425\n",
      "Epoch 00006: val_loss improved from 0.63060 to 0.56439, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/006-0.5644.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.7902 - acc: 0.7424 - val_loss: 0.5644 - val_acc: 0.8276\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7160 - acc: 0.7665\n",
      "Epoch 00007: val_loss improved from 0.56439 to 0.50924, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/007-0.5092.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.7158 - acc: 0.7666 - val_loss: 0.5092 - val_acc: 0.8446\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.7921\n",
      "Epoch 00008: val_loss improved from 0.50924 to 0.45632, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/008-0.4563.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.6440 - acc: 0.7921 - val_loss: 0.4563 - val_acc: 0.8558\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.8086\n",
      "Epoch 00009: val_loss improved from 0.45632 to 0.39658, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/009-0.3966.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.5930 - acc: 0.8086 - val_loss: 0.3966 - val_acc: 0.8793\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8217\n",
      "Epoch 00010: val_loss improved from 0.39658 to 0.36740, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/010-0.3674.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.5474 - acc: 0.8217 - val_loss: 0.3674 - val_acc: 0.8898\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8388\n",
      "Epoch 00011: val_loss improved from 0.36740 to 0.35440, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/011-0.3544.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.5081 - acc: 0.8388 - val_loss: 0.3544 - val_acc: 0.8889\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8498\n",
      "Epoch 00012: val_loss improved from 0.35440 to 0.31600, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/012-0.3160.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.4705 - acc: 0.8499 - val_loss: 0.3160 - val_acc: 0.9031\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.8606\n",
      "Epoch 00013: val_loss improved from 0.31600 to 0.29053, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/013-0.2905.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.4399 - acc: 0.8606 - val_loss: 0.2905 - val_acc: 0.9131\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8650\n",
      "Epoch 00014: val_loss improved from 0.29053 to 0.28755, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/014-0.2876.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4184 - acc: 0.8650 - val_loss: 0.2876 - val_acc: 0.9166\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8748\n",
      "Epoch 00015: val_loss improved from 0.28755 to 0.26042, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/015-0.2604.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3924 - acc: 0.8748 - val_loss: 0.2604 - val_acc: 0.9248\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8817\n",
      "Epoch 00016: val_loss improved from 0.26042 to 0.25424, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/016-0.2542.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3726 - acc: 0.8817 - val_loss: 0.2542 - val_acc: 0.9215\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8872\n",
      "Epoch 00017: val_loss improved from 0.25424 to 0.23954, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/017-0.2395.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.3536 - acc: 0.8871 - val_loss: 0.2395 - val_acc: 0.9290\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8917\n",
      "Epoch 00018: val_loss improved from 0.23954 to 0.22679, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/018-0.2268.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3373 - acc: 0.8918 - val_loss: 0.2268 - val_acc: 0.9324\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8970\n",
      "Epoch 00019: val_loss did not improve from 0.22679\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3259 - acc: 0.8970 - val_loss: 0.2336 - val_acc: 0.9311\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9005\n",
      "Epoch 00020: val_loss did not improve from 0.22679\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3120 - acc: 0.9006 - val_loss: 0.2602 - val_acc: 0.9164\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9037\n",
      "Epoch 00021: val_loss improved from 0.22679 to 0.20075, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/021-0.2008.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.3007 - acc: 0.9037 - val_loss: 0.2008 - val_acc: 0.9385\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9094\n",
      "Epoch 00022: val_loss did not improve from 0.20075\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2844 - acc: 0.9094 - val_loss: 0.2160 - val_acc: 0.9331\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9110\n",
      "Epoch 00023: val_loss improved from 0.20075 to 0.19899, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/023-0.1990.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2790 - acc: 0.9110 - val_loss: 0.1990 - val_acc: 0.9408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9139\n",
      "Epoch 00024: val_loss did not improve from 0.19899\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2699 - acc: 0.9139 - val_loss: 0.2180 - val_acc: 0.9313\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9177\n",
      "Epoch 00025: val_loss improved from 0.19899 to 0.18995, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/025-0.1900.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2607 - acc: 0.9177 - val_loss: 0.1900 - val_acc: 0.9411\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9202\n",
      "Epoch 00026: val_loss improved from 0.18995 to 0.18977, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/026-0.1898.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2516 - acc: 0.9202 - val_loss: 0.1898 - val_acc: 0.9425\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9210\n",
      "Epoch 00027: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2435 - acc: 0.9210 - val_loss: 0.1909 - val_acc: 0.9434\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9254\n",
      "Epoch 00028: val_loss improved from 0.18977 to 0.18321, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/028-0.1832.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2334 - acc: 0.9254 - val_loss: 0.1832 - val_acc: 0.9434\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9272\n",
      "Epoch 00029: val_loss improved from 0.18321 to 0.17564, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/029-0.1756.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2272 - acc: 0.9272 - val_loss: 0.1756 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9292\n",
      "Epoch 00030: val_loss did not improve from 0.17564\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2179 - acc: 0.9292 - val_loss: 0.1774 - val_acc: 0.9476\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9282\n",
      "Epoch 00031: val_loss improved from 0.17564 to 0.17299, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/031-0.1730.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2227 - acc: 0.9282 - val_loss: 0.1730 - val_acc: 0.9453\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9319\n",
      "Epoch 00032: val_loss improved from 0.17299 to 0.16353, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/032-0.1635.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2099 - acc: 0.9319 - val_loss: 0.1635 - val_acc: 0.9499\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9345\n",
      "Epoch 00033: val_loss did not improve from 0.16353\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2049 - acc: 0.9345 - val_loss: 0.1726 - val_acc: 0.9464\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9351\n",
      "Epoch 00034: val_loss did not improve from 0.16353\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2018 - acc: 0.9350 - val_loss: 0.1705 - val_acc: 0.9474\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9364\n",
      "Epoch 00035: val_loss did not improve from 0.16353\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1959 - acc: 0.9364 - val_loss: 0.1652 - val_acc: 0.9483\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9392\n",
      "Epoch 00036: val_loss improved from 0.16353 to 0.16159, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/036-0.1616.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1896 - acc: 0.9392 - val_loss: 0.1616 - val_acc: 0.9515\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9390\n",
      "Epoch 00037: val_loss did not improve from 0.16159\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1870 - acc: 0.9390 - val_loss: 0.1621 - val_acc: 0.9476\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9408\n",
      "Epoch 00038: val_loss improved from 0.16159 to 0.15654, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/038-0.1565.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1799 - acc: 0.9408 - val_loss: 0.1565 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9426\n",
      "Epoch 00039: val_loss did not improve from 0.15654\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1754 - acc: 0.9426 - val_loss: 0.1645 - val_acc: 0.9502\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9434\n",
      "Epoch 00040: val_loss did not improve from 0.15654\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1712 - acc: 0.9434 - val_loss: 0.1627 - val_acc: 0.9520\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9447\n",
      "Epoch 00041: val_loss improved from 0.15654 to 0.15267, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/041-0.1527.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1679 - acc: 0.9447 - val_loss: 0.1527 - val_acc: 0.9541\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9467\n",
      "Epoch 00042: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1634 - acc: 0.9467 - val_loss: 0.1538 - val_acc: 0.9546\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9477\n",
      "Epoch 00043: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1604 - acc: 0.9477 - val_loss: 0.1566 - val_acc: 0.9539\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9480\n",
      "Epoch 00044: val_loss improved from 0.15267 to 0.15231, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/044-0.1523.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1580 - acc: 0.9480 - val_loss: 0.1523 - val_acc: 0.9562\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9507\n",
      "Epoch 00045: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1517 - acc: 0.9507 - val_loss: 0.1571 - val_acc: 0.9525\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9512\n",
      "Epoch 00046: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1491 - acc: 0.9511 - val_loss: 0.1600 - val_acc: 0.9506\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9524\n",
      "Epoch 00047: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1472 - acc: 0.9524 - val_loss: 0.1572 - val_acc: 0.9525\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9519\n",
      "Epoch 00048: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1453 - acc: 0.9519 - val_loss: 0.1625 - val_acc: 0.9527\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9533\n",
      "Epoch 00049: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1398 - acc: 0.9533 - val_loss: 0.1795 - val_acc: 0.9509\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9542\n",
      "Epoch 00050: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1376 - acc: 0.9542 - val_loss: 0.1864 - val_acc: 0.9488\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9527\n",
      "Epoch 00051: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1392 - acc: 0.9528 - val_loss: 0.1613 - val_acc: 0.9555\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9560\n",
      "Epoch 00052: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1328 - acc: 0.9560 - val_loss: 0.1629 - val_acc: 0.9525\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9561\n",
      "Epoch 00053: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1314 - acc: 0.9561 - val_loss: 0.1563 - val_acc: 0.9534\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9577\n",
      "Epoch 00054: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1274 - acc: 0.9576 - val_loss: 0.1602 - val_acc: 0.9536\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9583\n",
      "Epoch 00055: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1250 - acc: 0.9583 - val_loss: 0.1568 - val_acc: 0.9525\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9579\n",
      "Epoch 00056: val_loss did not improve from 0.15231\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1268 - acc: 0.9579 - val_loss: 0.1538 - val_acc: 0.9525\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9598\n",
      "Epoch 00057: val_loss improved from 0.15231 to 0.15219, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/057-0.1522.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1211 - acc: 0.9598 - val_loss: 0.1522 - val_acc: 0.9543\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9607\n",
      "Epoch 00058: val_loss did not improve from 0.15219\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1169 - acc: 0.9607 - val_loss: 0.1715 - val_acc: 0.9513\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9597\n",
      "Epoch 00059: val_loss improved from 0.15219 to 0.15171, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/059-0.1517.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1168 - acc: 0.9597 - val_loss: 0.1517 - val_acc: 0.9555\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9621\n",
      "Epoch 00060: val_loss did not improve from 0.15171\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1132 - acc: 0.9621 - val_loss: 0.1660 - val_acc: 0.9525\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9613\n",
      "Epoch 00061: val_loss did not improve from 0.15171\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1129 - acc: 0.9613 - val_loss: 0.1534 - val_acc: 0.9567\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9633\n",
      "Epoch 00062: val_loss improved from 0.15171 to 0.14631, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/062-0.1463.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1098 - acc: 0.9633 - val_loss: 0.1463 - val_acc: 0.9569\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9625\n",
      "Epoch 00063: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1108 - acc: 0.9625 - val_loss: 0.1466 - val_acc: 0.9574\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9649\n",
      "Epoch 00064: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1045 - acc: 0.9650 - val_loss: 0.1500 - val_acc: 0.9578\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9632\n",
      "Epoch 00065: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1056 - acc: 0.9632 - val_loss: 0.1572 - val_acc: 0.9560\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9644\n",
      "Epoch 00066: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1039 - acc: 0.9644 - val_loss: 0.1534 - val_acc: 0.9564\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9651\n",
      "Epoch 00067: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1016 - acc: 0.9651 - val_loss: 0.1491 - val_acc: 0.9560\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9652\n",
      "Epoch 00068: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1023 - acc: 0.9652 - val_loss: 0.1697 - val_acc: 0.9513\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9658\n",
      "Epoch 00069: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1000 - acc: 0.9658 - val_loss: 0.1629 - val_acc: 0.9555\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9664\n",
      "Epoch 00070: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0969 - acc: 0.9664 - val_loss: 0.1512 - val_acc: 0.9583\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9676\n",
      "Epoch 00071: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0948 - acc: 0.9676 - val_loss: 0.1488 - val_acc: 0.9581\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9687\n",
      "Epoch 00072: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0930 - acc: 0.9687 - val_loss: 0.1591 - val_acc: 0.9588\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9678\n",
      "Epoch 00073: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0926 - acc: 0.9678 - val_loss: 0.1651 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9676\n",
      "Epoch 00074: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0964 - acc: 0.9676 - val_loss: 0.1500 - val_acc: 0.9567\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9704\n",
      "Epoch 00075: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0865 - acc: 0.9704 - val_loss: 0.1506 - val_acc: 0.9574\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9699\n",
      "Epoch 00076: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0868 - acc: 0.9699 - val_loss: 0.1640 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9706\n",
      "Epoch 00077: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0858 - acc: 0.9706 - val_loss: 0.1554 - val_acc: 0.9578\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9714\n",
      "Epoch 00078: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0844 - acc: 0.9714 - val_loss: 0.1555 - val_acc: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9711\n",
      "Epoch 00079: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0823 - acc: 0.9711 - val_loss: 0.1516 - val_acc: 0.9588\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9719\n",
      "Epoch 00080: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0830 - acc: 0.9719 - val_loss: 0.1667 - val_acc: 0.9578\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9705\n",
      "Epoch 00081: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0844 - acc: 0.9705 - val_loss: 0.1610 - val_acc: 0.9611\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9736\n",
      "Epoch 00082: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0762 - acc: 0.9736 - val_loss: 0.1625 - val_acc: 0.9557\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9714\n",
      "Epoch 00083: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0828 - acc: 0.9714 - val_loss: 0.1566 - val_acc: 0.9595\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9737\n",
      "Epoch 00084: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0759 - acc: 0.9737 - val_loss: 0.1553 - val_acc: 0.9606\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9738\n",
      "Epoch 00085: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0753 - acc: 0.9738 - val_loss: 0.1500 - val_acc: 0.9627\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9739\n",
      "Epoch 00086: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0776 - acc: 0.9739 - val_loss: 0.1795 - val_acc: 0.9557\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9739\n",
      "Epoch 00087: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0758 - acc: 0.9739 - val_loss: 0.1566 - val_acc: 0.9564\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9759\n",
      "Epoch 00088: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0720 - acc: 0.9759 - val_loss: 0.1496 - val_acc: 0.9637\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9754\n",
      "Epoch 00089: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0706 - acc: 0.9754 - val_loss: 0.1766 - val_acc: 0.9590\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9758\n",
      "Epoch 00090: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0697 - acc: 0.9758 - val_loss: 0.1471 - val_acc: 0.9618\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9758\n",
      "Epoch 00091: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0715 - acc: 0.9758 - val_loss: 0.1706 - val_acc: 0.9578\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9744\n",
      "Epoch 00092: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0731 - acc: 0.9744 - val_loss: 0.1518 - val_acc: 0.9623\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9776\n",
      "Epoch 00093: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0664 - acc: 0.9776 - val_loss: 0.1589 - val_acc: 0.9606\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9754\n",
      "Epoch 00094: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0698 - acc: 0.9754 - val_loss: 0.1636 - val_acc: 0.9583\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9767\n",
      "Epoch 00095: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0663 - acc: 0.9767 - val_loss: 0.1572 - val_acc: 0.9609\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9768\n",
      "Epoch 00096: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0647 - acc: 0.9768 - val_loss: 0.1704 - val_acc: 0.9574\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9784\n",
      "Epoch 00097: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0630 - acc: 0.9784 - val_loss: 0.1692 - val_acc: 0.9602\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9778\n",
      "Epoch 00098: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0645 - acc: 0.9778 - val_loss: 0.1681 - val_acc: 0.9595\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9777\n",
      "Epoch 00099: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0666 - acc: 0.9777 - val_loss: 0.1686 - val_acc: 0.9611\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9779\n",
      "Epoch 00100: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0624 - acc: 0.9779 - val_loss: 0.1643 - val_acc: 0.9606\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9788\n",
      "Epoch 00101: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0604 - acc: 0.9788 - val_loss: 0.1844 - val_acc: 0.9576\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9786\n",
      "Epoch 00102: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0635 - acc: 0.9786 - val_loss: 0.1561 - val_acc: 0.9604\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9794\n",
      "Epoch 00103: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0610 - acc: 0.9794 - val_loss: 0.1588 - val_acc: 0.9620\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9794\n",
      "Epoch 00104: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0598 - acc: 0.9794 - val_loss: 0.1704 - val_acc: 0.9604\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9783\n",
      "Epoch 00105: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0604 - acc: 0.9783 - val_loss: 0.1720 - val_acc: 0.9585\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9806\n",
      "Epoch 00106: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0565 - acc: 0.9806 - val_loss: 0.1744 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9795\n",
      "Epoch 00107: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0578 - acc: 0.9795 - val_loss: 0.1668 - val_acc: 0.9618\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9800\n",
      "Epoch 00108: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0572 - acc: 0.9799 - val_loss: 0.1758 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9801\n",
      "Epoch 00109: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0589 - acc: 0.9801 - val_loss: 0.1823 - val_acc: 0.9599\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9811\n",
      "Epoch 00110: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0540 - acc: 0.9811 - val_loss: 0.1616 - val_acc: 0.9616\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9812\n",
      "Epoch 00111: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0549 - acc: 0.9812 - val_loss: 0.1743 - val_acc: 0.9611\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9820\n",
      "Epoch 00112: val_loss did not improve from 0.14631\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0520 - acc: 0.9820 - val_loss: 0.1731 - val_acc: 0.9618\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 433us/sample - loss: 0.2108 - acc: 0.9406\n",
      "Loss: 0.21079661664964006 Accuracy: 0.9406023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 403us/sample - loss: 1.4879 - acc: 0.5381\n",
      "Loss: 1.4878783402041855 Accuracy: 0.5381101\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 411us/sample - loss: 1.1527 - acc: 0.6671\n",
      "Loss: 1.152688188320132 Accuracy: 0.667082\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 447us/sample - loss: 0.8164 - acc: 0.7599\n",
      "Loss: 0.816376467607722 Accuracy: 0.7599169\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 445us/sample - loss: 0.4395 - acc: 0.8808\n",
      "Loss: 0.43945830760838955 Accuracy: 0.8807892\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 461us/sample - loss: 0.2462 - acc: 0.9277\n",
      "Loss: 0.24616374088101173 Accuracy: 0.92772585\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 482us/sample - loss: 0.1955 - acc: 0.9464\n",
      "Loss: 0.19550527749527033 Accuracy: 0.94641745\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 496us/sample - loss: 0.2108 - acc: 0.9406\n",
      "Loss: 0.21079661664964006 Accuracy: 0.9406023\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 465us/sample - loss: 2.0887 - acc: 0.5826\n",
      "Loss: 2.0887432544278455 Accuracy: 0.5825545\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 478us/sample - loss: 1.3643 - acc: 0.6993\n",
      "Loss: 1.3643320033359627 Accuracy: 0.6992731\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 507us/sample - loss: 0.9605 - acc: 0.7728\n",
      "Loss: 0.960503781931175 Accuracy: 0.77279335\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 517us/sample - loss: 0.4874 - acc: 0.8818\n",
      "Loss: 0.48739094354777074 Accuracy: 0.8818276\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 543us/sample - loss: 0.2476 - acc: 0.9352\n",
      "Loss: 0.24764055660766232 Accuracy: 0.9352025\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 524us/sample - loss: 0.2085 - acc: 0.9462\n",
      "Loss: 0.20854686871248365 Accuracy: 0.9462098\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 565us/sample - loss: 0.2415 - acc: 0.9450\n",
      "Loss: 0.24145616913383086 Accuracy: 0.94496363\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
