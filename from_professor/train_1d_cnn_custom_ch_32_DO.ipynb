{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3735 - acc: 0.2478\n",
      "Epoch 00001: val_loss improved from inf to 2.01200, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/001-2.0120.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 2.3734 - acc: 0.2479 - val_loss: 2.0120 - val_acc: 0.3753\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8500 - acc: 0.4309\n",
      "Epoch 00002: val_loss improved from 2.01200 to 1.76180, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/002-1.7618.hdf5\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 1.8501 - acc: 0.4309 - val_loss: 1.7618 - val_acc: 0.4454\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5935 - acc: 0.5117\n",
      "Epoch 00003: val_loss improved from 1.76180 to 1.60145, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/003-1.6015.hdf5\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 1.5935 - acc: 0.5116 - val_loss: 1.6015 - val_acc: 0.4983\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4261 - acc: 0.5612\n",
      "Epoch 00004: val_loss improved from 1.60145 to 1.54723, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/004-1.5472.hdf5\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 1.4260 - acc: 0.5613 - val_loss: 1.5472 - val_acc: 0.5206\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3040 - acc: 0.5964\n",
      "Epoch 00005: val_loss improved from 1.54723 to 1.50679, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/005-1.5068.hdf5\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 1.3039 - acc: 0.5964 - val_loss: 1.5068 - val_acc: 0.5309\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2069 - acc: 0.6277\n",
      "Epoch 00006: val_loss improved from 1.50679 to 1.48629, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/006-1.4863.hdf5\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 1.2069 - acc: 0.6277 - val_loss: 1.4863 - val_acc: 0.5455\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6506\n",
      "Epoch 00007: val_loss improved from 1.48629 to 1.45991, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/007-1.4599.hdf5\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 1.1302 - acc: 0.6506 - val_loss: 1.4599 - val_acc: 0.5502\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 0.6721\n",
      "Epoch 00008: val_loss improved from 1.45991 to 1.44930, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_3_conv_checkpoint/008-1.4493.hdf5\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 1.0613 - acc: 0.6721 - val_loss: 1.4493 - val_acc: 0.5486\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.6894\n",
      "Epoch 00009: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 1.0003 - acc: 0.6894 - val_loss: 1.4682 - val_acc: 0.5455\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9448 - acc: 0.7067\n",
      "Epoch 00010: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.9447 - acc: 0.7068 - val_loss: 1.4778 - val_acc: 0.5479\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.7205\n",
      "Epoch 00011: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.8988 - acc: 0.7204 - val_loss: 1.4619 - val_acc: 0.5544\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7338\n",
      "Epoch 00012: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.8565 - acc: 0.7338 - val_loss: 1.4527 - val_acc: 0.5618\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7429\n",
      "Epoch 00013: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.8149 - acc: 0.7429 - val_loss: 1.4758 - val_acc: 0.5618\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7579\n",
      "Epoch 00014: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.7758 - acc: 0.7579 - val_loss: 1.4745 - val_acc: 0.5681\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7389 - acc: 0.7663\n",
      "Epoch 00015: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.7389 - acc: 0.7663 - val_loss: 1.4991 - val_acc: 0.5597\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7797\n",
      "Epoch 00016: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.7061 - acc: 0.7797 - val_loss: 1.5088 - val_acc: 0.5586\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.7880\n",
      "Epoch 00017: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.6758 - acc: 0.7880 - val_loss: 1.5293 - val_acc: 0.5660\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.7954\n",
      "Epoch 00018: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.6466 - acc: 0.7954 - val_loss: 1.5194 - val_acc: 0.5693\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6118 - acc: 0.8087\n",
      "Epoch 00019: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.6119 - acc: 0.8087 - val_loss: 1.5687 - val_acc: 0.5625\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8128\n",
      "Epoch 00020: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 611us/sample - loss: 0.5903 - acc: 0.8128 - val_loss: 1.5518 - val_acc: 0.5702\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8205\n",
      "Epoch 00021: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.5671 - acc: 0.8204 - val_loss: 1.5785 - val_acc: 0.5723\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8249\n",
      "Epoch 00022: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.5484 - acc: 0.8249 - val_loss: 1.5840 - val_acc: 0.5674\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8323\n",
      "Epoch 00023: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.5268 - acc: 0.8323 - val_loss: 1.6325 - val_acc: 0.5663\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8362\n",
      "Epoch 00024: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.5088 - acc: 0.8362 - val_loss: 1.6466 - val_acc: 0.5658\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8425\n",
      "Epoch 00025: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4898 - acc: 0.8425 - val_loss: 1.6352 - val_acc: 0.5763\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8483\n",
      "Epoch 00026: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4705 - acc: 0.8483 - val_loss: 1.6478 - val_acc: 0.5802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8572\n",
      "Epoch 00027: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 0.4493 - acc: 0.8572 - val_loss: 1.6700 - val_acc: 0.5756\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8604\n",
      "Epoch 00028: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.4373 - acc: 0.8604 - val_loss: 1.6849 - val_acc: 0.5765\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8669\n",
      "Epoch 00029: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.4199 - acc: 0.8669 - val_loss: 1.7049 - val_acc: 0.5751\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8705\n",
      "Epoch 00030: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.4009 - acc: 0.8705 - val_loss: 1.7889 - val_acc: 0.5681\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8740\n",
      "Epoch 00031: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3912 - acc: 0.8741 - val_loss: 1.7474 - val_acc: 0.5840\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8790\n",
      "Epoch 00032: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 619us/sample - loss: 0.3788 - acc: 0.8790 - val_loss: 1.7647 - val_acc: 0.5800\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8835\n",
      "Epoch 00033: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.3639 - acc: 0.8835 - val_loss: 1.7561 - val_acc: 0.5842\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8865\n",
      "Epoch 00034: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3536 - acc: 0.8865 - val_loss: 1.7978 - val_acc: 0.5779\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8892\n",
      "Epoch 00035: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.3471 - acc: 0.8893 - val_loss: 1.8275 - val_acc: 0.5823\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8928\n",
      "Epoch 00036: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 0.3367 - acc: 0.8928 - val_loss: 1.8428 - val_acc: 0.5823\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8939\n",
      "Epoch 00037: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.3271 - acc: 0.8940 - val_loss: 1.8238 - val_acc: 0.5924\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9017\n",
      "Epoch 00038: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 616us/sample - loss: 0.3069 - acc: 0.9017 - val_loss: 1.8186 - val_acc: 0.5886\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9025\n",
      "Epoch 00039: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 613us/sample - loss: 0.3042 - acc: 0.9025 - val_loss: 1.8626 - val_acc: 0.5802\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9070\n",
      "Epoch 00040: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2900 - acc: 0.9070 - val_loss: 1.9247 - val_acc: 0.5786\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9082\n",
      "Epoch 00041: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.2937 - acc: 0.9082 - val_loss: 1.8953 - val_acc: 0.5840\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9104\n",
      "Epoch 00042: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 612us/sample - loss: 0.2804 - acc: 0.9104 - val_loss: 1.9258 - val_acc: 0.5777\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9133\n",
      "Epoch 00043: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 607us/sample - loss: 0.2722 - acc: 0.9133 - val_loss: 1.9462 - val_acc: 0.5807\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9148\n",
      "Epoch 00044: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2695 - acc: 0.9148 - val_loss: 1.9455 - val_acc: 0.5856\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9146\n",
      "Epoch 00045: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2671 - acc: 0.9146 - val_loss: 1.9179 - val_acc: 0.5868\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9183\n",
      "Epoch 00046: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.2581 - acc: 0.9182 - val_loss: 1.9540 - val_acc: 0.5896\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9196\n",
      "Epoch 00047: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2534 - acc: 0.9197 - val_loss: 1.9579 - val_acc: 0.5912\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9228\n",
      "Epoch 00048: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 607us/sample - loss: 0.2451 - acc: 0.9228 - val_loss: 2.0164 - val_acc: 0.5854\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9242\n",
      "Epoch 00049: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 609us/sample - loss: 0.2374 - acc: 0.9242 - val_loss: 2.0253 - val_acc: 0.5870\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9264\n",
      "Epoch 00050: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.2336 - acc: 0.9264 - val_loss: 2.0225 - val_acc: 0.5889\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9253\n",
      "Epoch 00051: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2355 - acc: 0.9253 - val_loss: 2.0127 - val_acc: 0.5928\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9291\n",
      "Epoch 00052: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2285 - acc: 0.9291 - val_loss: 2.0311 - val_acc: 0.5935\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9322\n",
      "Epoch 00053: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2186 - acc: 0.9322 - val_loss: 2.0257 - val_acc: 0.5931\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9323\n",
      "Epoch 00054: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2173 - acc: 0.9323 - val_loss: 2.0371 - val_acc: 0.5830\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9312\n",
      "Epoch 00055: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2176 - acc: 0.9312 - val_loss: 2.0086 - val_acc: 0.5996\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9343\n",
      "Epoch 00056: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 614us/sample - loss: 0.2067 - acc: 0.9343 - val_loss: 2.0403 - val_acc: 0.5952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9355\n",
      "Epoch 00057: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 0.2072 - acc: 0.9355 - val_loss: 2.0605 - val_acc: 0.5947\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9381\n",
      "Epoch 00058: val_loss did not improve from 1.44930\n",
      "36805/36805 [==============================] - 23s 617us/sample - loss: 0.2007 - acc: 0.9381 - val_loss: 2.0634 - val_acc: 0.5961\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 323us/sample - loss: 1.5276 - acc: 0.5169\n",
      "Loss: 1.5275790819126498 Accuracy: 0.5169263\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3194 - acc: 0.2541\n",
      "Epoch 00001: val_loss improved from inf to 1.82589, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/001-1.8259.hdf5\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 2.3193 - acc: 0.2541 - val_loss: 1.8259 - val_acc: 0.4228\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7313 - acc: 0.4477\n",
      "Epoch 00002: val_loss improved from 1.82589 to 1.61547, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/002-1.6155.hdf5\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 1.7312 - acc: 0.4477 - val_loss: 1.6155 - val_acc: 0.4885\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5481 - acc: 0.5094\n",
      "Epoch 00003: val_loss improved from 1.61547 to 1.46045, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/003-1.4604.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.5481 - acc: 0.5094 - val_loss: 1.4604 - val_acc: 0.5434\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4206 - acc: 0.5555\n",
      "Epoch 00004: val_loss improved from 1.46045 to 1.37365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/004-1.3737.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.4205 - acc: 0.5555 - val_loss: 1.3737 - val_acc: 0.5884\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.5879\n",
      "Epoch 00005: val_loss improved from 1.37365 to 1.30498, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/005-1.3050.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.3265 - acc: 0.5878 - val_loss: 1.3050 - val_acc: 0.6056\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2574 - acc: 0.6093\n",
      "Epoch 00006: val_loss improved from 1.30498 to 1.25890, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/006-1.2589.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.2573 - acc: 0.6094 - val_loss: 1.2589 - val_acc: 0.6215\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1923 - acc: 0.6346\n",
      "Epoch 00007: val_loss improved from 1.25890 to 1.23930, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/007-1.2393.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 1.1923 - acc: 0.6346 - val_loss: 1.2393 - val_acc: 0.6184\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1423 - acc: 0.6492\n",
      "Epoch 00008: val_loss improved from 1.23930 to 1.19953, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/008-1.1995.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.1423 - acc: 0.6491 - val_loss: 1.1995 - val_acc: 0.6341\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6632\n",
      "Epoch 00009: val_loss improved from 1.19953 to 1.18560, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/009-1.1856.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.0949 - acc: 0.6632 - val_loss: 1.1856 - val_acc: 0.6252\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.6754\n",
      "Epoch 00010: val_loss improved from 1.18560 to 1.15145, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/010-1.1515.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.0568 - acc: 0.6753 - val_loss: 1.1515 - val_acc: 0.6406\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0131 - acc: 0.6885\n",
      "Epoch 00011: val_loss did not improve from 1.15145\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 1.0131 - acc: 0.6885 - val_loss: 1.1635 - val_acc: 0.6401\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9762 - acc: 0.7025\n",
      "Epoch 00012: val_loss improved from 1.15145 to 1.11850, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/012-1.1185.hdf5\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.9763 - acc: 0.7025 - val_loss: 1.1185 - val_acc: 0.6564\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9351 - acc: 0.7148\n",
      "Epoch 00013: val_loss improved from 1.11850 to 1.11414, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/013-1.1141.hdf5\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 0.9352 - acc: 0.7148 - val_loss: 1.1141 - val_acc: 0.6557\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7231\n",
      "Epoch 00014: val_loss improved from 1.11414 to 1.10492, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/014-1.1049.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.9046 - acc: 0.7231 - val_loss: 1.1049 - val_acc: 0.6536\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8694 - acc: 0.7332\n",
      "Epoch 00015: val_loss did not improve from 1.10492\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.8694 - acc: 0.7332 - val_loss: 1.1103 - val_acc: 0.6599\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7408\n",
      "Epoch 00016: val_loss improved from 1.10492 to 1.06307, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/016-1.0631.hdf5\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.8406 - acc: 0.7408 - val_loss: 1.0631 - val_acc: 0.6720\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8084 - acc: 0.7523\n",
      "Epoch 00017: val_loss did not improve from 1.06307\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.8084 - acc: 0.7522 - val_loss: 1.0775 - val_acc: 0.6737\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7807 - acc: 0.7609\n",
      "Epoch 00018: val_loss did not improve from 1.06307\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.7807 - acc: 0.7609 - val_loss: 1.0790 - val_acc: 0.6748\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7712\n",
      "Epoch 00019: val_loss improved from 1.06307 to 1.05903, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/019-1.0590.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.7461 - acc: 0.7712 - val_loss: 1.0590 - val_acc: 0.6862\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7226 - acc: 0.7757\n",
      "Epoch 00020: val_loss improved from 1.05903 to 1.04501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/020-1.0450.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.7226 - acc: 0.7757 - val_loss: 1.0450 - val_acc: 0.6897\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7831\n",
      "Epoch 00021: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.6940 - acc: 0.7830 - val_loss: 1.0715 - val_acc: 0.6827\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7914\n",
      "Epoch 00022: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.6733 - acc: 0.7914 - val_loss: 1.0683 - val_acc: 0.6799\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.7985\n",
      "Epoch 00023: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.6482 - acc: 0.7985 - val_loss: 1.0499 - val_acc: 0.6895\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6238 - acc: 0.8049\n",
      "Epoch 00024: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.6238 - acc: 0.8049 - val_loss: 1.0702 - val_acc: 0.6918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8090\n",
      "Epoch 00025: val_loss did not improve from 1.04501\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.6104 - acc: 0.8090 - val_loss: 1.0478 - val_acc: 0.6942\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5846 - acc: 0.8160\n",
      "Epoch 00026: val_loss improved from 1.04501 to 1.03054, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/026-1.0305.hdf5\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.5845 - acc: 0.8160 - val_loss: 1.0305 - val_acc: 0.7028\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.8222\n",
      "Epoch 00027: val_loss improved from 1.03054 to 1.02962, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_4_conv_checkpoint/027-1.0296.hdf5\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5649 - acc: 0.8223 - val_loss: 1.0296 - val_acc: 0.7009\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8264\n",
      "Epoch 00028: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.5526 - acc: 0.8264 - val_loss: 1.0669 - val_acc: 0.6925\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8305\n",
      "Epoch 00029: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5372 - acc: 0.8305 - val_loss: 1.0426 - val_acc: 0.7014\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.8323\n",
      "Epoch 00030: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.5252 - acc: 0.8323 - val_loss: 1.0487 - val_acc: 0.7056\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8422\n",
      "Epoch 00031: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5036 - acc: 0.8422 - val_loss: 1.0437 - val_acc: 0.7060\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8461\n",
      "Epoch 00032: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4848 - acc: 0.8461 - val_loss: 1.0659 - val_acc: 0.7070\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8474\n",
      "Epoch 00033: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.4770 - acc: 0.8474 - val_loss: 1.0696 - val_acc: 0.7079\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8507\n",
      "Epoch 00034: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.4630 - acc: 0.8508 - val_loss: 1.0721 - val_acc: 0.7074\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8562\n",
      "Epoch 00035: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.4500 - acc: 0.8562 - val_loss: 1.0715 - val_acc: 0.7112\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8574\n",
      "Epoch 00036: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4406 - acc: 0.8574 - val_loss: 1.0688 - val_acc: 0.7191\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8593\n",
      "Epoch 00037: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.4347 - acc: 0.8593 - val_loss: 1.0967 - val_acc: 0.7051\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8630\n",
      "Epoch 00038: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.4221 - acc: 0.8630 - val_loss: 1.1282 - val_acc: 0.7056\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8674\n",
      "Epoch 00039: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.4114 - acc: 0.8674 - val_loss: 1.0924 - val_acc: 0.7049\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8679\n",
      "Epoch 00040: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.4033 - acc: 0.8680 - val_loss: 1.0979 - val_acc: 0.7093\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8732\n",
      "Epoch 00041: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.3894 - acc: 0.8732 - val_loss: 1.0964 - val_acc: 0.7072\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8730\n",
      "Epoch 00042: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3855 - acc: 0.8730 - val_loss: 1.0912 - val_acc: 0.7142\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8752\n",
      "Epoch 00043: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3762 - acc: 0.8752 - val_loss: 1.1153 - val_acc: 0.7056\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8797\n",
      "Epoch 00044: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.3696 - acc: 0.8797 - val_loss: 1.1092 - val_acc: 0.7133\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8811\n",
      "Epoch 00045: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3631 - acc: 0.8812 - val_loss: 1.1130 - val_acc: 0.7102\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8858\n",
      "Epoch 00046: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3468 - acc: 0.8858 - val_loss: 1.0952 - val_acc: 0.7216\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8882\n",
      "Epoch 00047: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.3437 - acc: 0.8882 - val_loss: 1.1370 - val_acc: 0.7147\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8891\n",
      "Epoch 00048: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 630us/sample - loss: 0.3390 - acc: 0.8891 - val_loss: 1.1277 - val_acc: 0.7184\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8897\n",
      "Epoch 00049: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.3383 - acc: 0.8897 - val_loss: 1.1329 - val_acc: 0.7202\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8926\n",
      "Epoch 00050: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.3265 - acc: 0.8926 - val_loss: 1.1125 - val_acc: 0.7244\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.8935\n",
      "Epoch 00051: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.3185 - acc: 0.8935 - val_loss: 1.1299 - val_acc: 0.7163\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8944\n",
      "Epoch 00052: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 0.3179 - acc: 0.8943 - val_loss: 1.1238 - val_acc: 0.7237\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8982\n",
      "Epoch 00053: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.3127 - acc: 0.8982 - val_loss: 1.1180 - val_acc: 0.7242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9007\n",
      "Epoch 00054: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.3059 - acc: 0.9007 - val_loss: 1.1377 - val_acc: 0.7212\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9010\n",
      "Epoch 00055: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2990 - acc: 0.9010 - val_loss: 1.1552 - val_acc: 0.7195\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9022\n",
      "Epoch 00056: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2953 - acc: 0.9022 - val_loss: 1.1468 - val_acc: 0.7212\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9037\n",
      "Epoch 00057: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.2912 - acc: 0.9037 - val_loss: 1.1276 - val_acc: 0.7310\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9045\n",
      "Epoch 00058: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.2886 - acc: 0.9045 - val_loss: 1.1601 - val_acc: 0.7195\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9078\n",
      "Epoch 00059: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2829 - acc: 0.9078 - val_loss: 1.2020 - val_acc: 0.7205\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9059\n",
      "Epoch 00060: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.2799 - acc: 0.9059 - val_loss: 1.1723 - val_acc: 0.7258\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9090\n",
      "Epoch 00061: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2709 - acc: 0.9090 - val_loss: 1.1544 - val_acc: 0.7312\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9111\n",
      "Epoch 00062: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2726 - acc: 0.9110 - val_loss: 1.1645 - val_acc: 0.7256\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9100\n",
      "Epoch 00063: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 0.2676 - acc: 0.9100 - val_loss: 1.1643 - val_acc: 0.7300\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9138\n",
      "Epoch 00064: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 0.2603 - acc: 0.9138 - val_loss: 1.1658 - val_acc: 0.7244\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9151\n",
      "Epoch 00065: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2591 - acc: 0.9151 - val_loss: 1.1761 - val_acc: 0.7284\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9160\n",
      "Epoch 00066: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2547 - acc: 0.9159 - val_loss: 1.1859 - val_acc: 0.7265\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9139\n",
      "Epoch 00067: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2558 - acc: 0.9139 - val_loss: 1.1527 - val_acc: 0.7279\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9183\n",
      "Epoch 00068: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2474 - acc: 0.9184 - val_loss: 1.1637 - val_acc: 0.7321\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9206\n",
      "Epoch 00069: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2424 - acc: 0.9206 - val_loss: 1.2327 - val_acc: 0.7275\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9214\n",
      "Epoch 00070: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2413 - acc: 0.9214 - val_loss: 1.2002 - val_acc: 0.7310\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9223\n",
      "Epoch 00071: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2376 - acc: 0.9223 - val_loss: 1.1923 - val_acc: 0.7338\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9214\n",
      "Epoch 00072: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2345 - acc: 0.9214 - val_loss: 1.2038 - val_acc: 0.7331\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9217\n",
      "Epoch 00073: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.2321 - acc: 0.9217 - val_loss: 1.2106 - val_acc: 0.7291\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9224\n",
      "Epoch 00074: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2340 - acc: 0.9224 - val_loss: 1.1947 - val_acc: 0.7300\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9258\n",
      "Epoch 00075: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.2260 - acc: 0.9258 - val_loss: 1.2039 - val_acc: 0.7317\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9259\n",
      "Epoch 00076: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.2229 - acc: 0.9259 - val_loss: 1.2297 - val_acc: 0.7307\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9249\n",
      "Epoch 00077: val_loss did not improve from 1.02962\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.2243 - acc: 0.9248 - val_loss: 1.2062 - val_acc: 0.7393\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 335us/sample - loss: 1.1507 - acc: 0.6656\n",
      "Loss: 1.150674240363845 Accuracy: 0.66562825\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3854 - acc: 0.2131\n",
      "Epoch 00001: val_loss improved from inf to 1.80858, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/001-1.8086.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 2.3853 - acc: 0.2131 - val_loss: 1.8086 - val_acc: 0.4172\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7117 - acc: 0.4407\n",
      "Epoch 00002: val_loss improved from 1.80858 to 1.50537, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/002-1.5054.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 1.7117 - acc: 0.4407 - val_loss: 1.5054 - val_acc: 0.5227\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5012 - acc: 0.5214\n",
      "Epoch 00003: val_loss improved from 1.50537 to 1.38713, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/003-1.3871.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.5011 - acc: 0.5215 - val_loss: 1.3871 - val_acc: 0.5586\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3636 - acc: 0.5700\n",
      "Epoch 00004: val_loss improved from 1.38713 to 1.25903, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/004-1.2590.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.3636 - acc: 0.5699 - val_loss: 1.2590 - val_acc: 0.6191\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2606 - acc: 0.6055\n",
      "Epoch 00005: val_loss improved from 1.25903 to 1.18096, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/005-1.1810.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.2605 - acc: 0.6055 - val_loss: 1.1810 - val_acc: 0.6408\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1682 - acc: 0.6372\n",
      "Epoch 00006: val_loss improved from 1.18096 to 1.10013, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/006-1.1001.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 1.1683 - acc: 0.6372 - val_loss: 1.1001 - val_acc: 0.6755\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6616\n",
      "Epoch 00007: val_loss improved from 1.10013 to 1.06895, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/007-1.0689.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.0985 - acc: 0.6616 - val_loss: 1.0689 - val_acc: 0.6713\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0426 - acc: 0.6772\n",
      "Epoch 00008: val_loss improved from 1.06895 to 1.00095, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/008-1.0009.hdf5\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 1.0425 - acc: 0.6772 - val_loss: 1.0009 - val_acc: 0.7000\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9903 - acc: 0.6971\n",
      "Epoch 00009: val_loss did not improve from 1.00095\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.9907 - acc: 0.6971 - val_loss: 1.0388 - val_acc: 0.6879\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9519 - acc: 0.7093\n",
      "Epoch 00010: val_loss improved from 1.00095 to 0.94815, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/010-0.9482.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.9519 - acc: 0.7094 - val_loss: 0.9482 - val_acc: 0.7140\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9038 - acc: 0.7242\n",
      "Epoch 00011: val_loss improved from 0.94815 to 0.93063, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/011-0.9306.hdf5\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.9037 - acc: 0.7242 - val_loss: 0.9306 - val_acc: 0.7165\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7340\n",
      "Epoch 00012: val_loss improved from 0.93063 to 0.92876, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/012-0.9288.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.8720 - acc: 0.7341 - val_loss: 0.9288 - val_acc: 0.7205\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.7481\n",
      "Epoch 00013: val_loss improved from 0.92876 to 0.88534, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/013-0.8853.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.8311 - acc: 0.7481 - val_loss: 0.8853 - val_acc: 0.7382\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7979 - acc: 0.7567\n",
      "Epoch 00014: val_loss improved from 0.88534 to 0.85750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/014-0.8575.hdf5\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.7981 - acc: 0.7566 - val_loss: 0.8575 - val_acc: 0.7410\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7665\n",
      "Epoch 00015: val_loss did not improve from 0.85750\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.7705 - acc: 0.7665 - val_loss: 0.8702 - val_acc: 0.7421\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7711\n",
      "Epoch 00016: val_loss improved from 0.85750 to 0.81078, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/016-0.8108.hdf5\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.7492 - acc: 0.7711 - val_loss: 0.8108 - val_acc: 0.7589\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.7823\n",
      "Epoch 00017: val_loss improved from 0.81078 to 0.80756, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/017-0.8076.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.7180 - acc: 0.7822 - val_loss: 0.8076 - val_acc: 0.7636\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7886\n",
      "Epoch 00018: val_loss did not improve from 0.80756\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.6932 - acc: 0.7886 - val_loss: 0.8129 - val_acc: 0.7580\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.7962\n",
      "Epoch 00019: val_loss did not improve from 0.80756\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6688 - acc: 0.7962 - val_loss: 0.8249 - val_acc: 0.7582\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.8040\n",
      "Epoch 00020: val_loss improved from 0.80756 to 0.80272, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/020-0.8027.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.6476 - acc: 0.8040 - val_loss: 0.8027 - val_acc: 0.7629\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8080\n",
      "Epoch 00021: val_loss improved from 0.80272 to 0.78100, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/021-0.7810.hdf5\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.6301 - acc: 0.8080 - val_loss: 0.7810 - val_acc: 0.7678\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.8164\n",
      "Epoch 00022: val_loss improved from 0.78100 to 0.77646, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/022-0.7765.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.6053 - acc: 0.8164 - val_loss: 0.7765 - val_acc: 0.7757\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.8224\n",
      "Epoch 00023: val_loss did not improve from 0.77646\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5834 - acc: 0.8224 - val_loss: 0.7907 - val_acc: 0.7785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8286\n",
      "Epoch 00024: val_loss improved from 0.77646 to 0.76112, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/024-0.7611.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.5637 - acc: 0.8286 - val_loss: 0.7611 - val_acc: 0.7764\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.8321\n",
      "Epoch 00025: val_loss did not improve from 0.76112\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5508 - acc: 0.8321 - val_loss: 0.7778 - val_acc: 0.7799\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8364\n",
      "Epoch 00026: val_loss improved from 0.76112 to 0.74828, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/026-0.7483.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5345 - acc: 0.8364 - val_loss: 0.7483 - val_acc: 0.7852\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8404\n",
      "Epoch 00027: val_loss did not improve from 0.74828\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5146 - acc: 0.8404 - val_loss: 0.7486 - val_acc: 0.7892\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8435\n",
      "Epoch 00028: val_loss improved from 0.74828 to 0.73654, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/028-0.7365.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5035 - acc: 0.8435 - val_loss: 0.7365 - val_acc: 0.7964\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8495\n",
      "Epoch 00029: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.4850 - acc: 0.8494 - val_loss: 0.7521 - val_acc: 0.7887\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8541\n",
      "Epoch 00030: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.4734 - acc: 0.8541 - val_loss: 0.7445 - val_acc: 0.7885\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8580\n",
      "Epoch 00031: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.4581 - acc: 0.8580 - val_loss: 0.7430 - val_acc: 0.7855\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8605\n",
      "Epoch 00032: val_loss did not improve from 0.73654\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.4503 - acc: 0.8605 - val_loss: 0.7490 - val_acc: 0.7876\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8636\n",
      "Epoch 00033: val_loss improved from 0.73654 to 0.72818, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_5_conv_checkpoint/033-0.7282.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4353 - acc: 0.8636 - val_loss: 0.7282 - val_acc: 0.7927\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8669\n",
      "Epoch 00034: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.4206 - acc: 0.8671 - val_loss: 0.7594 - val_acc: 0.7897\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8693\n",
      "Epoch 00035: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4115 - acc: 0.8693 - val_loss: 0.7426 - val_acc: 0.7936\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8726\n",
      "Epoch 00036: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4047 - acc: 0.8725 - val_loss: 0.7423 - val_acc: 0.7925\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8746\n",
      "Epoch 00037: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3989 - acc: 0.8746 - val_loss: 0.7388 - val_acc: 0.7906\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8770\n",
      "Epoch 00038: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3855 - acc: 0.8771 - val_loss: 0.7394 - val_acc: 0.7950\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8836\n",
      "Epoch 00039: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3714 - acc: 0.8836 - val_loss: 0.7319 - val_acc: 0.7959\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8832\n",
      "Epoch 00040: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3670 - acc: 0.8832 - val_loss: 0.7420 - val_acc: 0.7966\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8839\n",
      "Epoch 00041: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3621 - acc: 0.8839 - val_loss: 0.7668 - val_acc: 0.7941\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8895\n",
      "Epoch 00042: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3462 - acc: 0.8894 - val_loss: 0.7482 - val_acc: 0.7969\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8924\n",
      "Epoch 00043: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3366 - acc: 0.8925 - val_loss: 0.7549 - val_acc: 0.7925\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8923\n",
      "Epoch 00044: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3371 - acc: 0.8923 - val_loss: 0.7408 - val_acc: 0.7962\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8934\n",
      "Epoch 00045: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3285 - acc: 0.8934 - val_loss: 0.7572 - val_acc: 0.7887\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8988\n",
      "Epoch 00046: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3177 - acc: 0.8988 - val_loss: 0.7480 - val_acc: 0.7922\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8982\n",
      "Epoch 00047: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3162 - acc: 0.8982 - val_loss: 0.7587 - val_acc: 0.7906\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9002\n",
      "Epoch 00048: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3062 - acc: 0.9002 - val_loss: 0.7541 - val_acc: 0.7943\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9044\n",
      "Epoch 00049: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3016 - acc: 0.9044 - val_loss: 0.7453 - val_acc: 0.7992\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9040\n",
      "Epoch 00050: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2978 - acc: 0.9040 - val_loss: 0.7473 - val_acc: 0.8011\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9071\n",
      "Epoch 00051: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2889 - acc: 0.9071 - val_loss: 0.8072 - val_acc: 0.7927\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9056\n",
      "Epoch 00052: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.2884 - acc: 0.9056 - val_loss: 0.7608 - val_acc: 0.7934\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9086\n",
      "Epoch 00053: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2819 - acc: 0.9086 - val_loss: 0.7828 - val_acc: 0.7955\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9114\n",
      "Epoch 00054: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2748 - acc: 0.9114 - val_loss: 0.7594 - val_acc: 0.8039\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9118\n",
      "Epoch 00055: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2715 - acc: 0.9119 - val_loss: 0.7829 - val_acc: 0.7964\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9135\n",
      "Epoch 00056: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2651 - acc: 0.9135 - val_loss: 0.7605 - val_acc: 0.8015\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9155\n",
      "Epoch 00057: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2621 - acc: 0.9155 - val_loss: 0.7667 - val_acc: 0.8069\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9171\n",
      "Epoch 00058: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2597 - acc: 0.9170 - val_loss: 0.7667 - val_acc: 0.7962\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9165\n",
      "Epoch 00059: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2533 - acc: 0.9165 - val_loss: 0.7645 - val_acc: 0.8099\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9186\n",
      "Epoch 00060: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2499 - acc: 0.9185 - val_loss: 0.7663 - val_acc: 0.8006\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9211\n",
      "Epoch 00061: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2445 - acc: 0.9212 - val_loss: 0.7953 - val_acc: 0.8036\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9216\n",
      "Epoch 00062: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2430 - acc: 0.9216 - val_loss: 0.8217 - val_acc: 0.7997\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9220\n",
      "Epoch 00063: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2424 - acc: 0.9220 - val_loss: 0.7776 - val_acc: 0.8006\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9218\n",
      "Epoch 00064: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2377 - acc: 0.9218 - val_loss: 0.7773 - val_acc: 0.8088\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9248\n",
      "Epoch 00065: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2299 - acc: 0.9248 - val_loss: 0.7982 - val_acc: 0.8085\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9248\n",
      "Epoch 00066: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2307 - acc: 0.9248 - val_loss: 0.7929 - val_acc: 0.8053\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9268\n",
      "Epoch 00067: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2250 - acc: 0.9268 - val_loss: 0.7924 - val_acc: 0.8078\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9279\n",
      "Epoch 00068: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2229 - acc: 0.9279 - val_loss: 0.8041 - val_acc: 0.8113\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9273\n",
      "Epoch 00069: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2212 - acc: 0.9273 - val_loss: 0.8143 - val_acc: 0.8069\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9286\n",
      "Epoch 00070: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2211 - acc: 0.9287 - val_loss: 0.8196 - val_acc: 0.8074\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9295\n",
      "Epoch 00071: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2145 - acc: 0.9295 - val_loss: 0.8046 - val_acc: 0.8057\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9320\n",
      "Epoch 00072: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2129 - acc: 0.9320 - val_loss: 0.8038 - val_acc: 0.8048\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9311\n",
      "Epoch 00073: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2065 - acc: 0.9311 - val_loss: 0.8512 - val_acc: 0.8055\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9306\n",
      "Epoch 00074: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2089 - acc: 0.9306 - val_loss: 0.8092 - val_acc: 0.8123\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9321\n",
      "Epoch 00075: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2083 - acc: 0.9322 - val_loss: 0.8253 - val_acc: 0.8071\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9326\n",
      "Epoch 00076: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2016 - acc: 0.9326 - val_loss: 0.8203 - val_acc: 0.8071\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9329\n",
      "Epoch 00077: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2045 - acc: 0.9329 - val_loss: 0.8280 - val_acc: 0.8134\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9335\n",
      "Epoch 00078: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2008 - acc: 0.9335 - val_loss: 0.7871 - val_acc: 0.8118\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9347\n",
      "Epoch 00079: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.1982 - acc: 0.9347 - val_loss: 0.8503 - val_acc: 0.8088\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9351\n",
      "Epoch 00080: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.1949 - acc: 0.9351 - val_loss: 0.8262 - val_acc: 0.8130\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9376\n",
      "Epoch 00081: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.1929 - acc: 0.9376 - val_loss: 0.7923 - val_acc: 0.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9364\n",
      "Epoch 00082: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.1941 - acc: 0.9363 - val_loss: 0.8059 - val_acc: 0.8106\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9362\n",
      "Epoch 00083: val_loss did not improve from 0.72818\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.1890 - acc: 0.9362 - val_loss: 0.8167 - val_acc: 0.8164\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 329us/sample - loss: 0.8196 - acc: 0.7605\n",
      "Loss: 0.8195533891457016 Accuracy: 0.76054\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4982 - acc: 0.1763\n",
      "Epoch 00001: val_loss improved from inf to 1.92088, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/001-1.9209.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.4982 - acc: 0.1763 - val_loss: 1.9209 - val_acc: 0.3760\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7969 - acc: 0.4131\n",
      "Epoch 00002: val_loss improved from 1.92088 to 1.55590, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/002-1.5559.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 1.7968 - acc: 0.4131 - val_loss: 1.5559 - val_acc: 0.5066\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5721 - acc: 0.4904\n",
      "Epoch 00003: val_loss improved from 1.55590 to 1.37877, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/003-1.3788.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 1.5721 - acc: 0.4904 - val_loss: 1.3788 - val_acc: 0.5765\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4449 - acc: 0.5346\n",
      "Epoch 00004: val_loss improved from 1.37877 to 1.33987, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/004-1.3399.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.4449 - acc: 0.5347 - val_loss: 1.3399 - val_acc: 0.5884\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3538 - acc: 0.5642\n",
      "Epoch 00005: val_loss improved from 1.33987 to 1.21457, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/005-1.2146.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.3538 - acc: 0.5642 - val_loss: 1.2146 - val_acc: 0.6282\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2725 - acc: 0.5951\n",
      "Epoch 00006: val_loss improved from 1.21457 to 1.17436, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/006-1.1744.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 1.2724 - acc: 0.5952 - val_loss: 1.1744 - val_acc: 0.6518\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2047 - acc: 0.6218\n",
      "Epoch 00007: val_loss improved from 1.17436 to 1.08483, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/007-1.0848.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.2047 - acc: 0.6218 - val_loss: 1.0848 - val_acc: 0.6765\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1362 - acc: 0.6424\n",
      "Epoch 00008: val_loss improved from 1.08483 to 1.00837, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/008-1.0084.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 1.1362 - acc: 0.6424 - val_loss: 1.0084 - val_acc: 0.6916\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0793 - acc: 0.6645\n",
      "Epoch 00009: val_loss improved from 1.00837 to 0.94569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/009-0.9457.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 1.0795 - acc: 0.6644 - val_loss: 0.9457 - val_acc: 0.7172\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0263 - acc: 0.6803\n",
      "Epoch 00010: val_loss improved from 0.94569 to 0.90257, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/010-0.9026.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 1.0262 - acc: 0.6803 - val_loss: 0.9026 - val_acc: 0.7307\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9851 - acc: 0.6926\n",
      "Epoch 00011: val_loss improved from 0.90257 to 0.86548, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/011-0.8655.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9851 - acc: 0.6926 - val_loss: 0.8655 - val_acc: 0.7396\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9433 - acc: 0.7071\n",
      "Epoch 00012: val_loss improved from 0.86548 to 0.82906, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/012-0.8291.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.9432 - acc: 0.7072 - val_loss: 0.8291 - val_acc: 0.7510\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7207\n",
      "Epoch 00013: val_loss improved from 0.82906 to 0.79616, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/013-0.7962.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9046 - acc: 0.7207 - val_loss: 0.7962 - val_acc: 0.7710\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7314\n",
      "Epoch 00014: val_loss improved from 0.79616 to 0.76465, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/014-0.7647.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.8661 - acc: 0.7314 - val_loss: 0.7647 - val_acc: 0.7738\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7488\n",
      "Epoch 00015: val_loss did not improve from 0.76465\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.8236 - acc: 0.7488 - val_loss: 0.7698 - val_acc: 0.7724\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7574\n",
      "Epoch 00016: val_loss improved from 0.76465 to 0.72178, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/016-0.7218.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.7936 - acc: 0.7575 - val_loss: 0.7218 - val_acc: 0.7908\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.7655\n",
      "Epoch 00017: val_loss improved from 0.72178 to 0.69566, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/017-0.6957.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.7649 - acc: 0.7655 - val_loss: 0.6957 - val_acc: 0.8018\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7750\n",
      "Epoch 00018: val_loss improved from 0.69566 to 0.66264, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/018-0.6626.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.7398 - acc: 0.7749 - val_loss: 0.6626 - val_acc: 0.8062\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7154 - acc: 0.7824\n",
      "Epoch 00019: val_loss improved from 0.66264 to 0.66075, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/019-0.6607.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.7154 - acc: 0.7825 - val_loss: 0.6607 - val_acc: 0.8097\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7902\n",
      "Epoch 00020: val_loss improved from 0.66075 to 0.64051, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/020-0.6405.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6931 - acc: 0.7902 - val_loss: 0.6405 - val_acc: 0.8143\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6669 - acc: 0.7987\n",
      "Epoch 00021: val_loss did not improve from 0.64051\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6669 - acc: 0.7986 - val_loss: 0.6441 - val_acc: 0.8167\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.8055\n",
      "Epoch 00022: val_loss improved from 0.64051 to 0.59966, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/022-0.5997.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6462 - acc: 0.8055 - val_loss: 0.5997 - val_acc: 0.8255\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.8096\n",
      "Epoch 00023: val_loss did not improve from 0.59966\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6276 - acc: 0.8096 - val_loss: 0.6061 - val_acc: 0.8253\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8131\n",
      "Epoch 00024: val_loss improved from 0.59966 to 0.57283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/024-0.5728.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6090 - acc: 0.8132 - val_loss: 0.5728 - val_acc: 0.8390\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8223\n",
      "Epoch 00025: val_loss did not improve from 0.57283\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.5935 - acc: 0.8223 - val_loss: 0.5785 - val_acc: 0.8332\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.8223\n",
      "Epoch 00026: val_loss improved from 0.57283 to 0.55860, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/026-0.5586.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.5823 - acc: 0.8223 - val_loss: 0.5586 - val_acc: 0.8404\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.8281\n",
      "Epoch 00027: val_loss improved from 0.55860 to 0.55577, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/027-0.5558.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5640 - acc: 0.8281 - val_loss: 0.5558 - val_acc: 0.8418\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8332\n",
      "Epoch 00028: val_loss improved from 0.55577 to 0.53802, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/028-0.5380.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.5494 - acc: 0.8332 - val_loss: 0.5380 - val_acc: 0.8474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8364\n",
      "Epoch 00029: val_loss did not improve from 0.53802\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5383 - acc: 0.8364 - val_loss: 0.5398 - val_acc: 0.8439\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8407\n",
      "Epoch 00030: val_loss improved from 0.53802 to 0.50454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/030-0.5045.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.5219 - acc: 0.8407 - val_loss: 0.5045 - val_acc: 0.8588\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8437\n",
      "Epoch 00031: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.5113 - acc: 0.8437 - val_loss: 0.5583 - val_acc: 0.8386\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8475\n",
      "Epoch 00032: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5007 - acc: 0.8475 - val_loss: 0.5179 - val_acc: 0.8519\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8518\n",
      "Epoch 00033: val_loss did not improve from 0.50454\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4875 - acc: 0.8518 - val_loss: 0.5166 - val_acc: 0.8535\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8553\n",
      "Epoch 00034: val_loss improved from 0.50454 to 0.49462, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/034-0.4946.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4775 - acc: 0.8553 - val_loss: 0.4946 - val_acc: 0.8602\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.8550\n",
      "Epoch 00035: val_loss did not improve from 0.49462\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4722 - acc: 0.8550 - val_loss: 0.4988 - val_acc: 0.8558\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8598\n",
      "Epoch 00036: val_loss improved from 0.49462 to 0.48582, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/036-0.4858.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4620 - acc: 0.8598 - val_loss: 0.4858 - val_acc: 0.8677\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8626\n",
      "Epoch 00037: val_loss improved from 0.48582 to 0.47951, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/037-0.4795.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4531 - acc: 0.8626 - val_loss: 0.4795 - val_acc: 0.8686\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8652\n",
      "Epoch 00038: val_loss improved from 0.47951 to 0.47656, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/038-0.4766.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4397 - acc: 0.8652 - val_loss: 0.4766 - val_acc: 0.8686\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8683\n",
      "Epoch 00039: val_loss improved from 0.47656 to 0.45949, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/039-0.4595.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4334 - acc: 0.8683 - val_loss: 0.4595 - val_acc: 0.8768\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8696\n",
      "Epoch 00040: val_loss improved from 0.45949 to 0.45348, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/040-0.4535.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4293 - acc: 0.8696 - val_loss: 0.4535 - val_acc: 0.8698\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8710\n",
      "Epoch 00041: val_loss improved from 0.45348 to 0.44869, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/041-0.4487.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.4217 - acc: 0.8710 - val_loss: 0.4487 - val_acc: 0.8754\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8730\n",
      "Epoch 00042: val_loss improved from 0.44869 to 0.44419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/042-0.4442.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4123 - acc: 0.8730 - val_loss: 0.4442 - val_acc: 0.8779\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8758\n",
      "Epoch 00043: val_loss improved from 0.44419 to 0.43576, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/043-0.4358.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4040 - acc: 0.8758 - val_loss: 0.4358 - val_acc: 0.8826\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8774\n",
      "Epoch 00044: val_loss improved from 0.43576 to 0.43255, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/044-0.4326.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3969 - acc: 0.8774 - val_loss: 0.4326 - val_acc: 0.8812\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8796\n",
      "Epoch 00045: val_loss did not improve from 0.43255\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.3907 - acc: 0.8797 - val_loss: 0.4336 - val_acc: 0.8866\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8855\n",
      "Epoch 00046: val_loss improved from 0.43255 to 0.42838, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/046-0.4284.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3786 - acc: 0.8855 - val_loss: 0.4284 - val_acc: 0.8863\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8844\n",
      "Epoch 00047: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.3770 - acc: 0.8844 - val_loss: 0.4377 - val_acc: 0.8805\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8851\n",
      "Epoch 00048: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.3760 - acc: 0.8851 - val_loss: 0.4316 - val_acc: 0.8842\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8845\n",
      "Epoch 00049: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3747 - acc: 0.8845 - val_loss: 0.4445 - val_acc: 0.8761\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8913\n",
      "Epoch 00050: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3577 - acc: 0.8913 - val_loss: 0.4672 - val_acc: 0.8812\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8881\n",
      "Epoch 00051: val_loss improved from 0.42838 to 0.41929, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/051-0.4193.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.3575 - acc: 0.8881 - val_loss: 0.4193 - val_acc: 0.8915\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8920\n",
      "Epoch 00052: val_loss improved from 0.41929 to 0.41435, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/052-0.4143.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3484 - acc: 0.8919 - val_loss: 0.4143 - val_acc: 0.8894\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8936\n",
      "Epoch 00053: val_loss did not improve from 0.41435\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3478 - acc: 0.8935 - val_loss: 0.4181 - val_acc: 0.8884\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8933\n",
      "Epoch 00054: val_loss improved from 0.41435 to 0.41050, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/054-0.4105.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3415 - acc: 0.8933 - val_loss: 0.4105 - val_acc: 0.8859\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8956\n",
      "Epoch 00055: val_loss did not improve from 0.41050\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3340 - acc: 0.8956 - val_loss: 0.4116 - val_acc: 0.8912\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8943\n",
      "Epoch 00056: val_loss improved from 0.41050 to 0.40054, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/056-0.4005.hdf5\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.3366 - acc: 0.8942 - val_loss: 0.4005 - val_acc: 0.8882\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.8997\n",
      "Epoch 00057: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.3210 - acc: 0.8997 - val_loss: 0.4258 - val_acc: 0.8901\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9011\n",
      "Epoch 00058: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.3196 - acc: 0.9011 - val_loss: 0.4510 - val_acc: 0.8761\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9003\n",
      "Epoch 00059: val_loss did not improve from 0.40054\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.3208 - acc: 0.9003 - val_loss: 0.4011 - val_acc: 0.8898\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9026\n",
      "Epoch 00060: val_loss improved from 0.40054 to 0.39376, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/060-0.3938.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3148 - acc: 0.9026 - val_loss: 0.3938 - val_acc: 0.8968\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9032\n",
      "Epoch 00061: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3106 - acc: 0.9032 - val_loss: 0.3989 - val_acc: 0.8966\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9043\n",
      "Epoch 00062: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.3063 - acc: 0.9043 - val_loss: 0.4545 - val_acc: 0.8812\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9055\n",
      "Epoch 00063: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3018 - acc: 0.9055 - val_loss: 0.3943 - val_acc: 0.8963\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9048\n",
      "Epoch 00064: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2994 - acc: 0.9047 - val_loss: 0.3965 - val_acc: 0.8940\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9084\n",
      "Epoch 00065: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2934 - acc: 0.9084 - val_loss: 0.4059 - val_acc: 0.8921\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9079\n",
      "Epoch 00066: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2915 - acc: 0.9079 - val_loss: 0.4108 - val_acc: 0.8928\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9105\n",
      "Epoch 00067: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.2873 - acc: 0.9106 - val_loss: 0.3983 - val_acc: 0.8973\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9118\n",
      "Epoch 00068: val_loss did not improve from 0.39376\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.2825 - acc: 0.9118 - val_loss: 0.3966 - val_acc: 0.8968\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9141\n",
      "Epoch 00069: val_loss improved from 0.39376 to 0.38444, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/069-0.3844.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2759 - acc: 0.9141 - val_loss: 0.3844 - val_acc: 0.9026\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9129\n",
      "Epoch 00070: val_loss did not improve from 0.38444\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2745 - acc: 0.9129 - val_loss: 0.3890 - val_acc: 0.9022\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9139\n",
      "Epoch 00071: val_loss improved from 0.38444 to 0.37927, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/071-0.3793.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2691 - acc: 0.9139 - val_loss: 0.3793 - val_acc: 0.9031\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9125\n",
      "Epoch 00072: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2763 - acc: 0.9124 - val_loss: 0.3864 - val_acc: 0.9045\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9158\n",
      "Epoch 00073: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2649 - acc: 0.9158 - val_loss: 0.3866 - val_acc: 0.9012\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9165\n",
      "Epoch 00074: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2597 - acc: 0.9165 - val_loss: 0.3824 - val_acc: 0.9036\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9161\n",
      "Epoch 00075: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2643 - acc: 0.9161 - val_loss: 0.3922 - val_acc: 0.8994\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9187\n",
      "Epoch 00076: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2567 - acc: 0.9187 - val_loss: 0.4135 - val_acc: 0.8947\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9177\n",
      "Epoch 00077: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2572 - acc: 0.9177 - val_loss: 0.3899 - val_acc: 0.9015\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9217\n",
      "Epoch 00078: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2436 - acc: 0.9217 - val_loss: 0.3860 - val_acc: 0.9019\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9233\n",
      "Epoch 00079: val_loss did not improve from 0.37927\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2447 - acc: 0.9234 - val_loss: 0.3882 - val_acc: 0.9052\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9209\n",
      "Epoch 00080: val_loss improved from 0.37927 to 0.37398, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/080-0.3740.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2458 - acc: 0.9209 - val_loss: 0.3740 - val_acc: 0.9068\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9230\n",
      "Epoch 00081: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2408 - acc: 0.9230 - val_loss: 0.3764 - val_acc: 0.9089\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9260\n",
      "Epoch 00082: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.2362 - acc: 0.9259 - val_loss: 0.3826 - val_acc: 0.9008\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9252\n",
      "Epoch 00083: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2345 - acc: 0.9253 - val_loss: 0.4144 - val_acc: 0.8980\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9252\n",
      "Epoch 00084: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2337 - acc: 0.9252 - val_loss: 0.3756 - val_acc: 0.9052\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9248\n",
      "Epoch 00085: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2327 - acc: 0.9247 - val_loss: 0.3845 - val_acc: 0.9059\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9260\n",
      "Epoch 00086: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2319 - acc: 0.9260 - val_loss: 0.3752 - val_acc: 0.9064\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9263\n",
      "Epoch 00087: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2270 - acc: 0.9263 - val_loss: 0.3889 - val_acc: 0.9078\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00088: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.3780 - val_acc: 0.9073\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9279\n",
      "Epoch 00089: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2243 - acc: 0.9279 - val_loss: 0.3914 - val_acc: 0.9073\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9291\n",
      "Epoch 00090: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.2185 - acc: 0.9291 - val_loss: 0.3916 - val_acc: 0.9054\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9272\n",
      "Epoch 00091: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2217 - acc: 0.9272 - val_loss: 0.3837 - val_acc: 0.9054\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9291\n",
      "Epoch 00092: val_loss did not improve from 0.37398\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2167 - acc: 0.9291 - val_loss: 0.4016 - val_acc: 0.8949\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9310\n",
      "Epoch 00093: val_loss improved from 0.37398 to 0.36884, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/093-0.3688.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2137 - acc: 0.9310 - val_loss: 0.3688 - val_acc: 0.9103\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9311\n",
      "Epoch 00094: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2130 - acc: 0.9311 - val_loss: 0.3812 - val_acc: 0.9110\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9321\n",
      "Epoch 00095: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2124 - acc: 0.9322 - val_loss: 0.3725 - val_acc: 0.9094\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9314\n",
      "Epoch 00096: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2088 - acc: 0.9314 - val_loss: 0.3792 - val_acc: 0.9066\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9306\n",
      "Epoch 00097: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2104 - acc: 0.9306 - val_loss: 0.3828 - val_acc: 0.9101\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9343\n",
      "Epoch 00098: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2037 - acc: 0.9343 - val_loss: 0.3864 - val_acc: 0.9071\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9338\n",
      "Epoch 00099: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2050 - acc: 0.9338 - val_loss: 0.3705 - val_acc: 0.9099\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9346\n",
      "Epoch 00100: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2011 - acc: 0.9346 - val_loss: 0.3848 - val_acc: 0.9047\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9318\n",
      "Epoch 00101: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2109 - acc: 0.9318 - val_loss: 0.3694 - val_acc: 0.9110\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9348\n",
      "Epoch 00102: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1950 - acc: 0.9348 - val_loss: 0.3724 - val_acc: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9357\n",
      "Epoch 00103: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1932 - acc: 0.9357 - val_loss: 0.3702 - val_acc: 0.9106\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9361\n",
      "Epoch 00104: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1938 - acc: 0.9361 - val_loss: 0.3793 - val_acc: 0.9045\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9367\n",
      "Epoch 00105: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1945 - acc: 0.9367 - val_loss: 0.3843 - val_acc: 0.9099\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9361\n",
      "Epoch 00106: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1951 - acc: 0.9361 - val_loss: 0.3823 - val_acc: 0.9078\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9378\n",
      "Epoch 00107: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1902 - acc: 0.9378 - val_loss: 0.3907 - val_acc: 0.9073\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9379\n",
      "Epoch 00108: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1868 - acc: 0.9379 - val_loss: 0.3877 - val_acc: 0.9080\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9377\n",
      "Epoch 00109: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1860 - acc: 0.9376 - val_loss: 0.3907 - val_acc: 0.9092\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9381\n",
      "Epoch 00110: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1893 - acc: 0.9381 - val_loss: 0.3703 - val_acc: 0.9124\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9405\n",
      "Epoch 00111: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1835 - acc: 0.9405 - val_loss: 0.3780 - val_acc: 0.9096\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9393\n",
      "Epoch 00112: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1837 - acc: 0.9393 - val_loss: 0.3782 - val_acc: 0.9145\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9419\n",
      "Epoch 00113: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1800 - acc: 0.9419 - val_loss: 0.3852 - val_acc: 0.9092\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9400\n",
      "Epoch 00114: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1826 - acc: 0.9400 - val_loss: 0.3818 - val_acc: 0.9103\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9415\n",
      "Epoch 00115: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1780 - acc: 0.9415 - val_loss: 0.3898 - val_acc: 0.9110\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9404\n",
      "Epoch 00116: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1804 - acc: 0.9404 - val_loss: 0.3832 - val_acc: 0.9145\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9421\n",
      "Epoch 00117: val_loss did not improve from 0.36884\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1753 - acc: 0.9420 - val_loss: 0.3842 - val_acc: 0.9138\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9423\n",
      "Epoch 00118: val_loss improved from 0.36884 to 0.36817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/118-0.3682.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1752 - acc: 0.9423 - val_loss: 0.3682 - val_acc: 0.9145\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9438\n",
      "Epoch 00119: val_loss did not improve from 0.36817\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1721 - acc: 0.9438 - val_loss: 0.3894 - val_acc: 0.9087\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9425\n",
      "Epoch 00120: val_loss did not improve from 0.36817\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1773 - acc: 0.9425 - val_loss: 0.3703 - val_acc: 0.9180\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9439\n",
      "Epoch 00121: val_loss improved from 0.36817 to 0.36709, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_6_conv_checkpoint/121-0.3671.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1720 - acc: 0.9439 - val_loss: 0.3671 - val_acc: 0.9119\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9437\n",
      "Epoch 00122: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1665 - acc: 0.9437 - val_loss: 0.3883 - val_acc: 0.9096\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9428\n",
      "Epoch 00123: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1688 - acc: 0.9428 - val_loss: 0.3850 - val_acc: 0.9115\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9458\n",
      "Epoch 00124: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1628 - acc: 0.9458 - val_loss: 0.3902 - val_acc: 0.9122\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9449\n",
      "Epoch 00125: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1705 - acc: 0.9448 - val_loss: 0.3801 - val_acc: 0.9110\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9434\n",
      "Epoch 00126: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1676 - acc: 0.9434 - val_loss: 0.4010 - val_acc: 0.9101\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9486\n",
      "Epoch 00127: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3774 - val_acc: 0.9164\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9459\n",
      "Epoch 00128: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1613 - acc: 0.9459 - val_loss: 0.3732 - val_acc: 0.9143\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9482\n",
      "Epoch 00129: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1609 - acc: 0.9482 - val_loss: 0.3757 - val_acc: 0.9161\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9464\n",
      "Epoch 00130: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1629 - acc: 0.9464 - val_loss: 0.3781 - val_acc: 0.9101\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9479\n",
      "Epoch 00131: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1554 - acc: 0.9479 - val_loss: 0.3805 - val_acc: 0.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9482\n",
      "Epoch 00132: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1572 - acc: 0.9482 - val_loss: 0.3851 - val_acc: 0.9150\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9476\n",
      "Epoch 00133: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.1578 - acc: 0.9476 - val_loss: 0.3912 - val_acc: 0.9106\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9457\n",
      "Epoch 00134: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1598 - acc: 0.9457 - val_loss: 0.3834 - val_acc: 0.9140\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9494\n",
      "Epoch 00135: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1540 - acc: 0.9494 - val_loss: 0.3759 - val_acc: 0.9124\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9485\n",
      "Epoch 00136: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1555 - acc: 0.9485 - val_loss: 0.3793 - val_acc: 0.9173\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9496\n",
      "Epoch 00137: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1513 - acc: 0.9496 - val_loss: 0.3677 - val_acc: 0.9150\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9504\n",
      "Epoch 00138: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1480 - acc: 0.9504 - val_loss: 0.3893 - val_acc: 0.9159\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9485\n",
      "Epoch 00139: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1528 - acc: 0.9485 - val_loss: 0.3799 - val_acc: 0.9113\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9508\n",
      "Epoch 00140: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1504 - acc: 0.9508 - val_loss: 0.3707 - val_acc: 0.9152\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9511\n",
      "Epoch 00141: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1442 - acc: 0.9511 - val_loss: 0.3838 - val_acc: 0.9129\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9545\n",
      "Epoch 00142: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1431 - acc: 0.9545 - val_loss: 0.3782 - val_acc: 0.9192\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9522\n",
      "Epoch 00143: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1485 - acc: 0.9522 - val_loss: 0.3870 - val_acc: 0.9147\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9523\n",
      "Epoch 00144: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1452 - acc: 0.9523 - val_loss: 0.3786 - val_acc: 0.9147\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9500\n",
      "Epoch 00145: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1476 - acc: 0.9500 - val_loss: 0.3833 - val_acc: 0.9140\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9536\n",
      "Epoch 00146: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1395 - acc: 0.9536 - val_loss: 0.3756 - val_acc: 0.9159\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9511\n",
      "Epoch 00147: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1423 - acc: 0.9511 - val_loss: 0.3833 - val_acc: 0.9154\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9531\n",
      "Epoch 00148: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1447 - acc: 0.9531 - val_loss: 0.3776 - val_acc: 0.9173\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9527\n",
      "Epoch 00149: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1439 - acc: 0.9527 - val_loss: 0.3875 - val_acc: 0.9150\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9543\n",
      "Epoch 00150: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1400 - acc: 0.9543 - val_loss: 0.3912 - val_acc: 0.9126\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9534\n",
      "Epoch 00151: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1408 - acc: 0.9534 - val_loss: 0.3881 - val_acc: 0.9136\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9528\n",
      "Epoch 00152: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1398 - acc: 0.9528 - val_loss: 0.3823 - val_acc: 0.9171\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9557\n",
      "Epoch 00153: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1332 - acc: 0.9557 - val_loss: 0.3863 - val_acc: 0.9154\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9546\n",
      "Epoch 00154: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1357 - acc: 0.9547 - val_loss: 0.3914 - val_acc: 0.9152\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9554\n",
      "Epoch 00155: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1363 - acc: 0.9554 - val_loss: 0.3822 - val_acc: 0.9173\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9523\n",
      "Epoch 00156: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1381 - acc: 0.9523 - val_loss: 0.3860 - val_acc: 0.9129\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9540\n",
      "Epoch 00157: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1327 - acc: 0.9539 - val_loss: 0.4208 - val_acc: 0.9099\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9548\n",
      "Epoch 00158: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1349 - acc: 0.9548 - val_loss: 0.4135 - val_acc: 0.9143\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9554\n",
      "Epoch 00159: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1336 - acc: 0.9554 - val_loss: 0.3696 - val_acc: 0.9182\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9573\n",
      "Epoch 00160: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1282 - acc: 0.9573 - val_loss: 0.3739 - val_acc: 0.9220\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9565\n",
      "Epoch 00161: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1284 - acc: 0.9565 - val_loss: 0.4090 - val_acc: 0.9157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9541\n",
      "Epoch 00162: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1360 - acc: 0.9541 - val_loss: 0.3691 - val_acc: 0.9171\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9562\n",
      "Epoch 00163: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1344 - acc: 0.9562 - val_loss: 0.3825 - val_acc: 0.9196\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9557\n",
      "Epoch 00164: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1318 - acc: 0.9557 - val_loss: 0.3897 - val_acc: 0.9173\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9590\n",
      "Epoch 00165: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1246 - acc: 0.9590 - val_loss: 0.3822 - val_acc: 0.9180\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9581\n",
      "Epoch 00166: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1272 - acc: 0.9581 - val_loss: 0.3957 - val_acc: 0.9152\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9570\n",
      "Epoch 00167: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1276 - acc: 0.9570 - val_loss: 0.3862 - val_acc: 0.9131\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9567\n",
      "Epoch 00168: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.1251 - acc: 0.9567 - val_loss: 0.3831 - val_acc: 0.9154\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9575\n",
      "Epoch 00169: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1244 - acc: 0.9575 - val_loss: 0.3865 - val_acc: 0.9199\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9581\n",
      "Epoch 00170: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1247 - acc: 0.9581 - val_loss: 0.3876 - val_acc: 0.9147\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9566\n",
      "Epoch 00171: val_loss did not improve from 0.36709\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1268 - acc: 0.9566 - val_loss: 0.3805 - val_acc: 0.9203\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 348us/sample - loss: 0.4329 - acc: 0.8893\n",
      "Loss: 0.4329061113166413 Accuracy: 0.8893043\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5627 - acc: 0.1510\n",
      "Epoch 00001: val_loss improved from inf to 2.01433, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/001-2.0143.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 2.5627 - acc: 0.1510 - val_loss: 2.0143 - val_acc: 0.3364\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8914 - acc: 0.3705\n",
      "Epoch 00002: val_loss improved from 2.01433 to 1.52395, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/002-1.5239.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 1.8914 - acc: 0.3705 - val_loss: 1.5239 - val_acc: 0.5178\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5867 - acc: 0.4679\n",
      "Epoch 00003: val_loss improved from 1.52395 to 1.30286, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/003-1.3029.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 1.5867 - acc: 0.4679 - val_loss: 1.3029 - val_acc: 0.5907\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4251 - acc: 0.5273\n",
      "Epoch 00004: val_loss improved from 1.30286 to 1.18139, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/004-1.1814.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.4251 - acc: 0.5272 - val_loss: 1.1814 - val_acc: 0.6375\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3094 - acc: 0.5677\n",
      "Epoch 00005: val_loss improved from 1.18139 to 1.08469, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/005-1.0847.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.3096 - acc: 0.5676 - val_loss: 1.0847 - val_acc: 0.6713\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2136 - acc: 0.6044\n",
      "Epoch 00006: val_loss improved from 1.08469 to 1.03232, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/006-1.0323.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 1.2131 - acc: 0.6046 - val_loss: 1.0323 - val_acc: 0.6969\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1271 - acc: 0.6386\n",
      "Epoch 00007: val_loss improved from 1.03232 to 0.94146, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/007-0.9415.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 1.1270 - acc: 0.6386 - val_loss: 0.9415 - val_acc: 0.7226\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0500 - acc: 0.6671\n",
      "Epoch 00008: val_loss improved from 0.94146 to 0.86306, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/008-0.8631.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 1.0500 - acc: 0.6671 - val_loss: 0.8631 - val_acc: 0.7386\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.6922\n",
      "Epoch 00009: val_loss improved from 0.86306 to 0.79546, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/009-0.7955.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9832 - acc: 0.6922 - val_loss: 0.7955 - val_acc: 0.7706\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7127\n",
      "Epoch 00010: val_loss improved from 0.79546 to 0.74712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/010-0.7471.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.9160 - acc: 0.7127 - val_loss: 0.7471 - val_acc: 0.7852\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8617 - acc: 0.7359\n",
      "Epoch 00011: val_loss improved from 0.74712 to 0.68827, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/011-0.6883.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.8617 - acc: 0.7359 - val_loss: 0.6883 - val_acc: 0.8022\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8063 - acc: 0.7517\n",
      "Epoch 00012: val_loss improved from 0.68827 to 0.64109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/012-0.6411.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.8063 - acc: 0.7517 - val_loss: 0.6411 - val_acc: 0.8188\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.7675\n",
      "Epoch 00013: val_loss improved from 0.64109 to 0.60197, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/013-0.6020.hdf5\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.7615 - acc: 0.7675 - val_loss: 0.6020 - val_acc: 0.8283\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7184 - acc: 0.7815\n",
      "Epoch 00014: val_loss improved from 0.60197 to 0.56504, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/014-0.5650.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.7185 - acc: 0.7816 - val_loss: 0.5650 - val_acc: 0.8430\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7887\n",
      "Epoch 00015: val_loss improved from 0.56504 to 0.55159, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/015-0.5516.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6827 - acc: 0.7887 - val_loss: 0.5516 - val_acc: 0.8451\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.8006\n",
      "Epoch 00016: val_loss improved from 0.55159 to 0.53894, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/016-0.5389.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.6518 - acc: 0.8005 - val_loss: 0.5389 - val_acc: 0.8479\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8120\n",
      "Epoch 00017: val_loss improved from 0.53894 to 0.49715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/017-0.4971.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.6171 - acc: 0.8120 - val_loss: 0.4971 - val_acc: 0.8595\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5944 - acc: 0.8180\n",
      "Epoch 00018: val_loss improved from 0.49715 to 0.46630, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/018-0.4663.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5944 - acc: 0.8180 - val_loss: 0.4663 - val_acc: 0.8707\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8271\n",
      "Epoch 00019: val_loss did not improve from 0.46630\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.5682 - acc: 0.8271 - val_loss: 0.4863 - val_acc: 0.8528\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.8322\n",
      "Epoch 00020: val_loss improved from 0.46630 to 0.42246, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/020-0.4225.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5468 - acc: 0.8322 - val_loss: 0.4225 - val_acc: 0.8835\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8390\n",
      "Epoch 00021: val_loss improved from 0.42246 to 0.40320, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/021-0.4032.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5279 - acc: 0.8390 - val_loss: 0.4032 - val_acc: 0.8870\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8466\n",
      "Epoch 00022: val_loss did not improve from 0.40320\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.5027 - acc: 0.8467 - val_loss: 0.4291 - val_acc: 0.8786\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8499\n",
      "Epoch 00023: val_loss improved from 0.40320 to 0.38296, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/023-0.3830.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4919 - acc: 0.8498 - val_loss: 0.3830 - val_acc: 0.8942\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8528\n",
      "Epoch 00024: val_loss improved from 0.38296 to 0.36440, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/024-0.3644.hdf5\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.4788 - acc: 0.8528 - val_loss: 0.3644 - val_acc: 0.8984\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8579\n",
      "Epoch 00025: val_loss did not improve from 0.36440\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4585 - acc: 0.8579 - val_loss: 0.3737 - val_acc: 0.8952\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8634\n",
      "Epoch 00026: val_loss improved from 0.36440 to 0.34750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/026-0.3475.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4468 - acc: 0.8634 - val_loss: 0.3475 - val_acc: 0.9036\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8679\n",
      "Epoch 00027: val_loss did not improve from 0.34750\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.4283 - acc: 0.8679 - val_loss: 0.3527 - val_acc: 0.9040\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8716\n",
      "Epoch 00028: val_loss improved from 0.34750 to 0.34621, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/028-0.3462.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.4220 - acc: 0.8716 - val_loss: 0.3462 - val_acc: 0.9008\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8732\n",
      "Epoch 00029: val_loss improved from 0.34621 to 0.30986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/029-0.3099.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.4113 - acc: 0.8732 - val_loss: 0.3099 - val_acc: 0.9108\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8766\n",
      "Epoch 00030: val_loss did not improve from 0.30986\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.4008 - acc: 0.8766 - val_loss: 0.3108 - val_acc: 0.9124\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8811\n",
      "Epoch 00031: val_loss did not improve from 0.30986\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3879 - acc: 0.8811 - val_loss: 0.3312 - val_acc: 0.9061\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8818\n",
      "Epoch 00032: val_loss improved from 0.30986 to 0.30608, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/032-0.3061.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3816 - acc: 0.8818 - val_loss: 0.3061 - val_acc: 0.9138\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8873\n",
      "Epoch 00033: val_loss improved from 0.30608 to 0.28891, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/033-0.2889.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3650 - acc: 0.8873 - val_loss: 0.2889 - val_acc: 0.9213\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8855\n",
      "Epoch 00034: val_loss improved from 0.28891 to 0.28709, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/034-0.2871.hdf5\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.3652 - acc: 0.8855 - val_loss: 0.2871 - val_acc: 0.9159\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8887\n",
      "Epoch 00035: val_loss improved from 0.28709 to 0.27911, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/035-0.2791.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3571 - acc: 0.8887 - val_loss: 0.2791 - val_acc: 0.9276\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8933\n",
      "Epoch 00036: val_loss improved from 0.27911 to 0.26817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/036-0.2682.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3446 - acc: 0.8933 - val_loss: 0.2682 - val_acc: 0.9259\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8924\n",
      "Epoch 00037: val_loss did not improve from 0.26817\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3416 - acc: 0.8924 - val_loss: 0.2922 - val_acc: 0.9133\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8942\n",
      "Epoch 00038: val_loss did not improve from 0.26817\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3404 - acc: 0.8942 - val_loss: 0.2689 - val_acc: 0.9271\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8973\n",
      "Epoch 00039: val_loss improved from 0.26817 to 0.25545, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/039-0.2555.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3327 - acc: 0.8973 - val_loss: 0.2555 - val_acc: 0.9304\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8998\n",
      "Epoch 00040: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3264 - acc: 0.8998 - val_loss: 0.2773 - val_acc: 0.9213\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9004\n",
      "Epoch 00041: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3151 - acc: 0.9003 - val_loss: 0.2633 - val_acc: 0.9262\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9007\n",
      "Epoch 00042: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3150 - acc: 0.9007 - val_loss: 0.2714 - val_acc: 0.9238\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9031\n",
      "Epoch 00043: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3101 - acc: 0.9031 - val_loss: 0.2589 - val_acc: 0.9280\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9050\n",
      "Epoch 00044: val_loss did not improve from 0.25545\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.3035 - acc: 0.9050 - val_loss: 0.2605 - val_acc: 0.9252\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9052\n",
      "Epoch 00045: val_loss improved from 0.25545 to 0.24513, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/045-0.2451.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2997 - acc: 0.9052 - val_loss: 0.2451 - val_acc: 0.9327\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9069\n",
      "Epoch 00046: val_loss did not improve from 0.24513\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2915 - acc: 0.9068 - val_loss: 0.2478 - val_acc: 0.9320\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9097\n",
      "Epoch 00047: val_loss did not improve from 0.24513\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.2889 - acc: 0.9097 - val_loss: 0.2487 - val_acc: 0.9308\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9108\n",
      "Epoch 00048: val_loss improved from 0.24513 to 0.23944, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/048-0.2394.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2837 - acc: 0.9108 - val_loss: 0.2394 - val_acc: 0.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9106\n",
      "Epoch 00049: val_loss did not improve from 0.23944\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2801 - acc: 0.9106 - val_loss: 0.2418 - val_acc: 0.9306\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9120\n",
      "Epoch 00050: val_loss improved from 0.23944 to 0.23385, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/050-0.2338.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2790 - acc: 0.9120 - val_loss: 0.2338 - val_acc: 0.9324\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9146\n",
      "Epoch 00051: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2745 - acc: 0.9147 - val_loss: 0.2506 - val_acc: 0.9299\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9139\n",
      "Epoch 00052: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2713 - acc: 0.9139 - val_loss: 0.2390 - val_acc: 0.9304\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9158\n",
      "Epoch 00053: val_loss did not improve from 0.23385\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2661 - acc: 0.9158 - val_loss: 0.2343 - val_acc: 0.9334\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9171\n",
      "Epoch 00054: val_loss improved from 0.23385 to 0.23209, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/054-0.2321.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2619 - acc: 0.9171 - val_loss: 0.2321 - val_acc: 0.9329\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9170\n",
      "Epoch 00055: val_loss improved from 0.23209 to 0.23034, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/055-0.2303.hdf5\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.2606 - acc: 0.9170 - val_loss: 0.2303 - val_acc: 0.9355\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9179\n",
      "Epoch 00056: val_loss did not improve from 0.23034\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2582 - acc: 0.9179 - val_loss: 0.2352 - val_acc: 0.9338\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9185\n",
      "Epoch 00057: val_loss did not improve from 0.23034\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2543 - acc: 0.9185 - val_loss: 0.2367 - val_acc: 0.9313\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9191\n",
      "Epoch 00058: val_loss improved from 0.23034 to 0.22321, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/058-0.2232.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2514 - acc: 0.9192 - val_loss: 0.2232 - val_acc: 0.9362\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9213\n",
      "Epoch 00059: val_loss did not improve from 0.22321\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2490 - acc: 0.9213 - val_loss: 0.2372 - val_acc: 0.9331\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9209\n",
      "Epoch 00060: val_loss improved from 0.22321 to 0.22080, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/060-0.2208.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2453 - acc: 0.9208 - val_loss: 0.2208 - val_acc: 0.9394\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.2455 - acc: 0.9220 - val_loss: 0.2264 - val_acc: 0.9355\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9228\n",
      "Epoch 00062: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2412 - acc: 0.9229 - val_loss: 0.2248 - val_acc: 0.9376\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9249\n",
      "Epoch 00063: val_loss did not improve from 0.22080\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2341 - acc: 0.9250 - val_loss: 0.2217 - val_acc: 0.9390\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9265\n",
      "Epoch 00064: val_loss improved from 0.22080 to 0.21875, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/064-0.2188.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2303 - acc: 0.9265 - val_loss: 0.2188 - val_acc: 0.9357\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9234\n",
      "Epoch 00065: val_loss did not improve from 0.21875\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2349 - acc: 0.9234 - val_loss: 0.2201 - val_acc: 0.9387\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9269\n",
      "Epoch 00066: val_loss improved from 0.21875 to 0.21768, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/066-0.2177.hdf5\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2253 - acc: 0.9269 - val_loss: 0.2177 - val_acc: 0.9413\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9286\n",
      "Epoch 00067: val_loss improved from 0.21768 to 0.20772, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/067-0.2077.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2238 - acc: 0.9287 - val_loss: 0.2077 - val_acc: 0.9411\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9270\n",
      "Epoch 00068: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2234 - acc: 0.9269 - val_loss: 0.2122 - val_acc: 0.9427\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9286\n",
      "Epoch 00069: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2209 - acc: 0.9286 - val_loss: 0.2138 - val_acc: 0.9385\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9282\n",
      "Epoch 00070: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2191 - acc: 0.9281 - val_loss: 0.2383 - val_acc: 0.9322\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9284\n",
      "Epoch 00071: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2214 - acc: 0.9284 - val_loss: 0.2198 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9320\n",
      "Epoch 00072: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.2141 - acc: 0.9320 - val_loss: 0.2322 - val_acc: 0.9376\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9314\n",
      "Epoch 00073: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2129 - acc: 0.9314 - val_loss: 0.2127 - val_acc: 0.9385\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9308\n",
      "Epoch 00074: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2101 - acc: 0.9309 - val_loss: 0.2234 - val_acc: 0.9362\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9312\n",
      "Epoch 00075: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2087 - acc: 0.9313 - val_loss: 0.2111 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9325\n",
      "Epoch 00076: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2065 - acc: 0.9325 - val_loss: 0.2315 - val_acc: 0.9383\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9346\n",
      "Epoch 00077: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2022 - acc: 0.9346 - val_loss: 0.2162 - val_acc: 0.9394\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9349\n",
      "Epoch 00078: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1998 - acc: 0.9350 - val_loss: 0.2113 - val_acc: 0.9420\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9350\n",
      "Epoch 00079: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2016 - acc: 0.9350 - val_loss: 0.2167 - val_acc: 0.9397\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9346\n",
      "Epoch 00080: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2025 - acc: 0.9346 - val_loss: 0.2119 - val_acc: 0.9399\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9356\n",
      "Epoch 00081: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1976 - acc: 0.9356 - val_loss: 0.2107 - val_acc: 0.9434\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9360\n",
      "Epoch 00082: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1941 - acc: 0.9360 - val_loss: 0.2206 - val_acc: 0.9392\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9354\n",
      "Epoch 00083: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1976 - acc: 0.9354 - val_loss: 0.2175 - val_acc: 0.9387\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9371\n",
      "Epoch 00084: val_loss did not improve from 0.20772\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1946 - acc: 0.9371 - val_loss: 0.2093 - val_acc: 0.9422\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9355\n",
      "Epoch 00085: val_loss improved from 0.20772 to 0.19980, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/085-0.1998.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1950 - acc: 0.9355 - val_loss: 0.1998 - val_acc: 0.9474\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9380\n",
      "Epoch 00086: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1872 - acc: 0.9380 - val_loss: 0.2007 - val_acc: 0.9434\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9374\n",
      "Epoch 00087: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1920 - acc: 0.9375 - val_loss: 0.2002 - val_acc: 0.9460\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9388\n",
      "Epoch 00088: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1839 - acc: 0.9388 - val_loss: 0.2107 - val_acc: 0.9418\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9389\n",
      "Epoch 00089: val_loss improved from 0.19980 to 0.19565, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/089-0.1957.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1870 - acc: 0.9388 - val_loss: 0.1957 - val_acc: 0.9453\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9391\n",
      "Epoch 00090: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1837 - acc: 0.9391 - val_loss: 0.2035 - val_acc: 0.9441\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9408\n",
      "Epoch 00091: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1807 - acc: 0.9408 - val_loss: 0.1984 - val_acc: 0.9443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9395\n",
      "Epoch 00092: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1833 - acc: 0.9395 - val_loss: 0.2052 - val_acc: 0.9462\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9403\n",
      "Epoch 00093: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1841 - acc: 0.9403 - val_loss: 0.2197 - val_acc: 0.9404\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9416\n",
      "Epoch 00094: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1742 - acc: 0.9416 - val_loss: 0.2035 - val_acc: 0.9429\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9420\n",
      "Epoch 00095: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1740 - acc: 0.9420 - val_loss: 0.2092 - val_acc: 0.9425\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9431\n",
      "Epoch 00096: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1704 - acc: 0.9431 - val_loss: 0.2097 - val_acc: 0.9439\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9431\n",
      "Epoch 00097: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1727 - acc: 0.9432 - val_loss: 0.2025 - val_acc: 0.9429\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9415\n",
      "Epoch 00098: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1753 - acc: 0.9415 - val_loss: 0.2012 - val_acc: 0.9446\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9436\n",
      "Epoch 00099: val_loss improved from 0.19565 to 0.19551, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/099-0.1955.hdf5\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1711 - acc: 0.9436 - val_loss: 0.1955 - val_acc: 0.9467\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9424\n",
      "Epoch 00100: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1703 - acc: 0.9424 - val_loss: 0.2062 - val_acc: 0.9413\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9440\n",
      "Epoch 00101: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1696 - acc: 0.9440 - val_loss: 0.1987 - val_acc: 0.9446\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9432\n",
      "Epoch 00102: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1694 - acc: 0.9432 - val_loss: 0.2009 - val_acc: 0.9474\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9438\n",
      "Epoch 00103: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1732 - acc: 0.9438 - val_loss: 0.2215 - val_acc: 0.9406\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9443\n",
      "Epoch 00104: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1688 - acc: 0.9443 - val_loss: 0.2039 - val_acc: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9459\n",
      "Epoch 00105: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1630 - acc: 0.9459 - val_loss: 0.2151 - val_acc: 0.9439\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9471\n",
      "Epoch 00106: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1614 - acc: 0.9471 - val_loss: 0.2079 - val_acc: 0.9427\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9468\n",
      "Epoch 00107: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1584 - acc: 0.9468 - val_loss: 0.2131 - val_acc: 0.9404\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9450\n",
      "Epoch 00108: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1617 - acc: 0.9450 - val_loss: 0.2157 - val_acc: 0.9422\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9468\n",
      "Epoch 00109: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1601 - acc: 0.9469 - val_loss: 0.2030 - val_acc: 0.9464\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9466\n",
      "Epoch 00110: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1569 - acc: 0.9466 - val_loss: 0.2011 - val_acc: 0.9432\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9480\n",
      "Epoch 00111: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1569 - acc: 0.9481 - val_loss: 0.2107 - val_acc: 0.9408\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9451\n",
      "Epoch 00112: val_loss did not improve from 0.19551\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1596 - acc: 0.9451 - val_loss: 0.2029 - val_acc: 0.9460\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9458\n",
      "Epoch 00113: val_loss improved from 0.19551 to 0.19370, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/113-0.1937.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1601 - acc: 0.9458 - val_loss: 0.1937 - val_acc: 0.9464\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9496\n",
      "Epoch 00114: val_loss did not improve from 0.19370\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1522 - acc: 0.9496 - val_loss: 0.1983 - val_acc: 0.9453\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9483\n",
      "Epoch 00115: val_loss did not improve from 0.19370\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1536 - acc: 0.9483 - val_loss: 0.1970 - val_acc: 0.9450\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9493\n",
      "Epoch 00116: val_loss improved from 0.19370 to 0.19354, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/116-0.1935.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1531 - acc: 0.9494 - val_loss: 0.1935 - val_acc: 0.9476\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9490\n",
      "Epoch 00117: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1536 - acc: 0.9489 - val_loss: 0.2059 - val_acc: 0.9432\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9488\n",
      "Epoch 00118: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1526 - acc: 0.9488 - val_loss: 0.2040 - val_acc: 0.9492\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9520\n",
      "Epoch 00119: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1447 - acc: 0.9520 - val_loss: 0.2139 - val_acc: 0.9415\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9509\n",
      "Epoch 00120: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1477 - acc: 0.9509 - val_loss: 0.2111 - val_acc: 0.9485\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9510\n",
      "Epoch 00121: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1455 - acc: 0.9510 - val_loss: 0.2132 - val_acc: 0.9446\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9514\n",
      "Epoch 00122: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1449 - acc: 0.9514 - val_loss: 0.2065 - val_acc: 0.9453\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9505\n",
      "Epoch 00123: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1475 - acc: 0.9505 - val_loss: 0.2156 - val_acc: 0.9427\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9505\n",
      "Epoch 00124: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1482 - acc: 0.9505 - val_loss: 0.2158 - val_acc: 0.9413\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9523\n",
      "Epoch 00125: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1428 - acc: 0.9523 - val_loss: 0.2002 - val_acc: 0.9455\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9508\n",
      "Epoch 00126: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1456 - acc: 0.9508 - val_loss: 0.2037 - val_acc: 0.9446\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9514\n",
      "Epoch 00127: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1410 - acc: 0.9514 - val_loss: 0.1969 - val_acc: 0.9485\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9527\n",
      "Epoch 00128: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1413 - acc: 0.9527 - val_loss: 0.2015 - val_acc: 0.9467\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9520\n",
      "Epoch 00129: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1396 - acc: 0.9520 - val_loss: 0.2161 - val_acc: 0.9455\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9538\n",
      "Epoch 00130: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1388 - acc: 0.9538 - val_loss: 0.2039 - val_acc: 0.9464\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9545\n",
      "Epoch 00131: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1371 - acc: 0.9545 - val_loss: 0.2129 - val_acc: 0.9434\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9537\n",
      "Epoch 00132: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1358 - acc: 0.9537 - val_loss: 0.2085 - val_acc: 0.9462\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9544\n",
      "Epoch 00133: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1336 - acc: 0.9544 - val_loss: 0.2040 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9554\n",
      "Epoch 00134: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1312 - acc: 0.9554 - val_loss: 0.2001 - val_acc: 0.9471\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9548\n",
      "Epoch 00135: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1348 - acc: 0.9548 - val_loss: 0.2001 - val_acc: 0.9476\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9530\n",
      "Epoch 00136: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1356 - acc: 0.9530 - val_loss: 0.2340 - val_acc: 0.9418\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9555\n",
      "Epoch 00137: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1326 - acc: 0.9555 - val_loss: 0.2072 - val_acc: 0.9427\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9564\n",
      "Epoch 00138: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1291 - acc: 0.9564 - val_loss: 0.2153 - val_acc: 0.9464\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9561\n",
      "Epoch 00139: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1291 - acc: 0.9561 - val_loss: 0.2087 - val_acc: 0.9455\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9557\n",
      "Epoch 00140: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1330 - acc: 0.9556 - val_loss: 0.2057 - val_acc: 0.9481\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9563\n",
      "Epoch 00141: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1287 - acc: 0.9563 - val_loss: 0.2101 - val_acc: 0.9469\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9564\n",
      "Epoch 00142: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1274 - acc: 0.9564 - val_loss: 0.2133 - val_acc: 0.9439\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9567\n",
      "Epoch 00143: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1266 - acc: 0.9567 - val_loss: 0.2070 - val_acc: 0.9474\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9572\n",
      "Epoch 00144: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1264 - acc: 0.9572 - val_loss: 0.2086 - val_acc: 0.9499\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9569\n",
      "Epoch 00145: val_loss improved from 0.19354 to 0.19160, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/145-0.1916.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1289 - acc: 0.9569 - val_loss: 0.1916 - val_acc: 0.9469\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9579\n",
      "Epoch 00146: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1244 - acc: 0.9579 - val_loss: 0.2080 - val_acc: 0.9455\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9565\n",
      "Epoch 00147: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1242 - acc: 0.9565 - val_loss: 0.2065 - val_acc: 0.9455\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9577\n",
      "Epoch 00148: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1240 - acc: 0.9577 - val_loss: 0.2085 - val_acc: 0.9441\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9586\n",
      "Epoch 00149: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1206 - acc: 0.9585 - val_loss: 0.2072 - val_acc: 0.9481\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9575\n",
      "Epoch 00150: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1241 - acc: 0.9575 - val_loss: 0.2207 - val_acc: 0.9478\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9578\n",
      "Epoch 00151: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1222 - acc: 0.9578 - val_loss: 0.2093 - val_acc: 0.9464\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9586\n",
      "Epoch 00152: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1192 - acc: 0.9585 - val_loss: 0.2090 - val_acc: 0.9450\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9578\n",
      "Epoch 00153: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1208 - acc: 0.9578 - val_loss: 0.1988 - val_acc: 0.9497\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9595\n",
      "Epoch 00154: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1198 - acc: 0.9595 - val_loss: 0.2036 - val_acc: 0.9474\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9591\n",
      "Epoch 00155: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1190 - acc: 0.9591 - val_loss: 0.2021 - val_acc: 0.9492\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9590\n",
      "Epoch 00156: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1202 - acc: 0.9590 - val_loss: 0.2211 - val_acc: 0.9462\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9595\n",
      "Epoch 00157: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1175 - acc: 0.9595 - val_loss: 0.1986 - val_acc: 0.9518\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9590\n",
      "Epoch 00158: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1171 - acc: 0.9590 - val_loss: 0.1965 - val_acc: 0.9499\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9589\n",
      "Epoch 00159: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1172 - acc: 0.9589 - val_loss: 0.1919 - val_acc: 0.9478\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9612\n",
      "Epoch 00160: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1153 - acc: 0.9612 - val_loss: 0.2241 - val_acc: 0.9448\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9607\n",
      "Epoch 00161: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1163 - acc: 0.9607 - val_loss: 0.2306 - val_acc: 0.9420\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9605\n",
      "Epoch 00162: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1157 - acc: 0.9605 - val_loss: 0.2083 - val_acc: 0.9448\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9606\n",
      "Epoch 00163: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1137 - acc: 0.9606 - val_loss: 0.1964 - val_acc: 0.9504\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9616\n",
      "Epoch 00164: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1121 - acc: 0.9616 - val_loss: 0.1958 - val_acc: 0.9513\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9610\n",
      "Epoch 00165: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1145 - acc: 0.9610 - val_loss: 0.2110 - val_acc: 0.9457\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9617\n",
      "Epoch 00166: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1102 - acc: 0.9617 - val_loss: 0.2388 - val_acc: 0.9476\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9619\n",
      "Epoch 00167: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1114 - acc: 0.9619 - val_loss: 0.1971 - val_acc: 0.9499\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9628\n",
      "Epoch 00168: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1097 - acc: 0.9628 - val_loss: 0.2216 - val_acc: 0.9415\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9625\n",
      "Epoch 00169: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1079 - acc: 0.9625 - val_loss: 0.2236 - val_acc: 0.9439\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9639\n",
      "Epoch 00170: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1052 - acc: 0.9639 - val_loss: 0.2106 - val_acc: 0.9460\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9634\n",
      "Epoch 00171: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1049 - acc: 0.9634 - val_loss: 0.2105 - val_acc: 0.9471\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9629\n",
      "Epoch 00172: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1065 - acc: 0.9629 - val_loss: 0.2162 - val_acc: 0.9490\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9633\n",
      "Epoch 00173: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1077 - acc: 0.9633 - val_loss: 0.2090 - val_acc: 0.9513\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9635\n",
      "Epoch 00174: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1090 - acc: 0.9635 - val_loss: 0.2306 - val_acc: 0.9490\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9628\n",
      "Epoch 00175: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1070 - acc: 0.9628 - val_loss: 0.2135 - val_acc: 0.9476\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9635\n",
      "Epoch 00176: val_loss improved from 0.19160 to 0.19119, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_7_conv_checkpoint/176-0.1912.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1039 - acc: 0.9635 - val_loss: 0.1912 - val_acc: 0.9490\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9632\n",
      "Epoch 00177: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1078 - acc: 0.9632 - val_loss: 0.2006 - val_acc: 0.9474\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9632\n",
      "Epoch 00178: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1062 - acc: 0.9632 - val_loss: 0.2161 - val_acc: 0.9441\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9660\n",
      "Epoch 00179: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0995 - acc: 0.9660 - val_loss: 0.2192 - val_acc: 0.9448\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9635\n",
      "Epoch 00180: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1056 - acc: 0.9635 - val_loss: 0.2047 - val_acc: 0.9483\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9646\n",
      "Epoch 00181: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1015 - acc: 0.9646 - val_loss: 0.2119 - val_acc: 0.9481\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9650\n",
      "Epoch 00182: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1032 - acc: 0.9650 - val_loss: 0.2060 - val_acc: 0.9504\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9644\n",
      "Epoch 00183: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1025 - acc: 0.9644 - val_loss: 0.2131 - val_acc: 0.9488\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9648\n",
      "Epoch 00184: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1028 - acc: 0.9648 - val_loss: 0.2085 - val_acc: 0.9509\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9653\n",
      "Epoch 00185: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1013 - acc: 0.9653 - val_loss: 0.2259 - val_acc: 0.9499\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9668\n",
      "Epoch 00186: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0985 - acc: 0.9668 - val_loss: 0.2174 - val_acc: 0.9488\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9640\n",
      "Epoch 00187: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1020 - acc: 0.9641 - val_loss: 0.2263 - val_acc: 0.9490\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9661\n",
      "Epoch 00188: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0979 - acc: 0.9661 - val_loss: 0.2011 - val_acc: 0.9529\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9652\n",
      "Epoch 00189: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1005 - acc: 0.9652 - val_loss: 0.1973 - val_acc: 0.9509\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9659\n",
      "Epoch 00190: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0984 - acc: 0.9659 - val_loss: 0.2019 - val_acc: 0.9509\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9670\n",
      "Epoch 00191: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0963 - acc: 0.9670 - val_loss: 0.2207 - val_acc: 0.9483\n",
      "Epoch 192/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9658\n",
      "Epoch 00192: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0981 - acc: 0.9658 - val_loss: 0.2159 - val_acc: 0.9481\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9655\n",
      "Epoch 00193: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0988 - acc: 0.9655 - val_loss: 0.2236 - val_acc: 0.9469\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9668\n",
      "Epoch 00194: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0972 - acc: 0.9668 - val_loss: 0.2179 - val_acc: 0.9478\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9662\n",
      "Epoch 00195: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0955 - acc: 0.9662 - val_loss: 0.2219 - val_acc: 0.9457\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9670\n",
      "Epoch 00196: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0956 - acc: 0.9670 - val_loss: 0.2033 - val_acc: 0.9513\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9671\n",
      "Epoch 00197: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.0952 - acc: 0.9671 - val_loss: 0.2193 - val_acc: 0.9483\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9668\n",
      "Epoch 00198: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.0957 - acc: 0.9667 - val_loss: 0.2257 - val_acc: 0.9467\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9670\n",
      "Epoch 00199: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.0981 - acc: 0.9670 - val_loss: 0.2125 - val_acc: 0.9483\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9671\n",
      "Epoch 00200: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.0929 - acc: 0.9671 - val_loss: 0.2113 - val_acc: 0.9471\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9679\n",
      "Epoch 00201: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0916 - acc: 0.9679 - val_loss: 0.2242 - val_acc: 0.9467\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9668\n",
      "Epoch 00202: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0952 - acc: 0.9668 - val_loss: 0.2188 - val_acc: 0.9509\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9697\n",
      "Epoch 00203: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0905 - acc: 0.9697 - val_loss: 0.2132 - val_acc: 0.9495\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9681\n",
      "Epoch 00204: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0946 - acc: 0.9681 - val_loss: 0.2167 - val_acc: 0.9506\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9674\n",
      "Epoch 00205: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0944 - acc: 0.9674 - val_loss: 0.2236 - val_acc: 0.9488\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9699\n",
      "Epoch 00206: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0866 - acc: 0.9699 - val_loss: 0.2122 - val_acc: 0.9502\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9682\n",
      "Epoch 00207: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0913 - acc: 0.9682 - val_loss: 0.2175 - val_acc: 0.9520\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9691\n",
      "Epoch 00208: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0904 - acc: 0.9691 - val_loss: 0.2135 - val_acc: 0.9511\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9692\n",
      "Epoch 00209: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0883 - acc: 0.9692 - val_loss: 0.2141 - val_acc: 0.9495\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9690\n",
      "Epoch 00210: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0885 - acc: 0.9689 - val_loss: 0.2174 - val_acc: 0.9497\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9693\n",
      "Epoch 00211: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0874 - acc: 0.9692 - val_loss: 0.2180 - val_acc: 0.9492\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9702\n",
      "Epoch 00212: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0862 - acc: 0.9702 - val_loss: 0.2175 - val_acc: 0.9476\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9692\n",
      "Epoch 00213: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.0878 - acc: 0.9692 - val_loss: 0.2140 - val_acc: 0.9471\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9693\n",
      "Epoch 00214: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0880 - acc: 0.9693 - val_loss: 0.2141 - val_acc: 0.9520\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9694\n",
      "Epoch 00215: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0892 - acc: 0.9694 - val_loss: 0.2124 - val_acc: 0.9509\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9708\n",
      "Epoch 00216: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0847 - acc: 0.9708 - val_loss: 0.2103 - val_acc: 0.9483\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9695\n",
      "Epoch 00217: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0878 - acc: 0.9695 - val_loss: 0.2335 - val_acc: 0.9478\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9708\n",
      "Epoch 00218: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.0885 - acc: 0.9708 - val_loss: 0.2161 - val_acc: 0.9481\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9701\n",
      "Epoch 00219: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.0872 - acc: 0.9701 - val_loss: 0.2157 - val_acc: 0.9485\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9703\n",
      "Epoch 00220: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.0862 - acc: 0.9703 - val_loss: 0.2375 - val_acc: 0.9490\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9690\n",
      "Epoch 00221: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.0880 - acc: 0.9690 - val_loss: 0.2284 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9710\n",
      "Epoch 00222: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.0860 - acc: 0.9710 - val_loss: 0.2038 - val_acc: 0.9513\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9723\n",
      "Epoch 00223: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.0850 - acc: 0.9723 - val_loss: 0.2124 - val_acc: 0.9506\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9715\n",
      "Epoch 00224: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.0817 - acc: 0.9715 - val_loss: 0.2395 - val_acc: 0.9457\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9718\n",
      "Epoch 00225: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.0810 - acc: 0.9718 - val_loss: 0.2175 - val_acc: 0.9513\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9717\n",
      "Epoch 00226: val_loss did not improve from 0.19119\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.0844 - acc: 0.9717 - val_loss: 0.2304 - val_acc: 0.9478\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 372us/sample - loss: 0.2330 - acc: 0.9373\n",
      "Loss: 0.2330334084025301 Accuracy: 0.93727934\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5511 - acc: 0.1581\n",
      "Epoch 00001: val_loss improved from inf to 1.98775, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/001-1.9877.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 2.5510 - acc: 0.1581 - val_loss: 1.9877 - val_acc: 0.3659\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8731 - acc: 0.3815\n",
      "Epoch 00002: val_loss improved from 1.98775 to 1.40025, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/002-1.4002.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.8730 - acc: 0.3816 - val_loss: 1.4002 - val_acc: 0.5761\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4865 - acc: 0.5078\n",
      "Epoch 00003: val_loss improved from 1.40025 to 1.14365, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/003-1.1437.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 1.4864 - acc: 0.5078 - val_loss: 1.1437 - val_acc: 0.6378\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2878 - acc: 0.5734\n",
      "Epoch 00004: val_loss improved from 1.14365 to 0.95705, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/004-0.9571.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 1.2877 - acc: 0.5734 - val_loss: 0.9571 - val_acc: 0.7060\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1456 - acc: 0.6211\n",
      "Epoch 00005: val_loss improved from 0.95705 to 0.85468, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/005-0.8547.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.1455 - acc: 0.6211 - val_loss: 0.8547 - val_acc: 0.7307\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0564 - acc: 0.6527\n",
      "Epoch 00006: val_loss improved from 0.85468 to 0.79047, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/006-0.7905.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.0565 - acc: 0.6527 - val_loss: 0.7905 - val_acc: 0.7517\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.6804\n",
      "Epoch 00007: val_loss improved from 0.79047 to 0.71186, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/007-0.7119.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.9770 - acc: 0.6804 - val_loss: 0.7119 - val_acc: 0.7880\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9119 - acc: 0.7030\n",
      "Epoch 00008: val_loss improved from 0.71186 to 0.65937, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/008-0.6594.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9118 - acc: 0.7031 - val_loss: 0.6594 - val_acc: 0.7983\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8469 - acc: 0.7257\n",
      "Epoch 00009: val_loss improved from 0.65937 to 0.61510, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/009-0.6151.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.8469 - acc: 0.7256 - val_loss: 0.6151 - val_acc: 0.8062\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7486\n",
      "Epoch 00010: val_loss improved from 0.61510 to 0.56573, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/010-0.5657.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.7870 - acc: 0.7486 - val_loss: 0.5657 - val_acc: 0.8288\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7620\n",
      "Epoch 00011: val_loss improved from 0.56573 to 0.52099, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/011-0.5210.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7443 - acc: 0.7620 - val_loss: 0.5210 - val_acc: 0.8470\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7009 - acc: 0.7754\n",
      "Epoch 00012: val_loss improved from 0.52099 to 0.48750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/012-0.4875.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7009 - acc: 0.7753 - val_loss: 0.4875 - val_acc: 0.8591\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.7876\n",
      "Epoch 00013: val_loss improved from 0.48750 to 0.46318, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/013-0.4632.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.6659 - acc: 0.7876 - val_loss: 0.4632 - val_acc: 0.8675\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8014\n",
      "Epoch 00014: val_loss improved from 0.46318 to 0.45067, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/014-0.4507.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.6255 - acc: 0.8013 - val_loss: 0.4507 - val_acc: 0.8570\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8077\n",
      "Epoch 00015: val_loss improved from 0.45067 to 0.39757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/015-0.3976.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.6028 - acc: 0.8077 - val_loss: 0.3976 - val_acc: 0.8833\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8213\n",
      "Epoch 00016: val_loss improved from 0.39757 to 0.37791, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/016-0.3779.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.5616 - acc: 0.8213 - val_loss: 0.3779 - val_acc: 0.8873\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8280\n",
      "Epoch 00017: val_loss did not improve from 0.37791\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5450 - acc: 0.8279 - val_loss: 0.3926 - val_acc: 0.8826\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.8333\n",
      "Epoch 00018: val_loss improved from 0.37791 to 0.36620, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/018-0.3662.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.5230 - acc: 0.8333 - val_loss: 0.3662 - val_acc: 0.8868\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8416\n",
      "Epoch 00019: val_loss improved from 0.36620 to 0.33511, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/019-0.3351.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5001 - acc: 0.8417 - val_loss: 0.3351 - val_acc: 0.8994\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8468\n",
      "Epoch 00020: val_loss improved from 0.33511 to 0.30998, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/020-0.3100.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4795 - acc: 0.8468 - val_loss: 0.3100 - val_acc: 0.9031\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8531\n",
      "Epoch 00021: val_loss did not improve from 0.30998\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4611 - acc: 0.8531 - val_loss: 0.3100 - val_acc: 0.9108\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8605\n",
      "Epoch 00022: val_loss did not improve from 0.30998\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4421 - acc: 0.8605 - val_loss: 0.3113 - val_acc: 0.9047\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8609\n",
      "Epoch 00023: val_loss improved from 0.30998 to 0.27188, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/023-0.2719.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4362 - acc: 0.8609 - val_loss: 0.2719 - val_acc: 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8682\n",
      "Epoch 00024: val_loss improved from 0.27188 to 0.27084, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/024-0.2708.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4164 - acc: 0.8682 - val_loss: 0.2708 - val_acc: 0.9222\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8715\n",
      "Epoch 00025: val_loss did not improve from 0.27084\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.4022 - acc: 0.8715 - val_loss: 0.2839 - val_acc: 0.9145\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8759\n",
      "Epoch 00026: val_loss improved from 0.27084 to 0.25191, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/026-0.2519.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3899 - acc: 0.8759 - val_loss: 0.2519 - val_acc: 0.9299\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8788\n",
      "Epoch 00027: val_loss improved from 0.25191 to 0.25031, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/027-0.2503.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3783 - acc: 0.8788 - val_loss: 0.2503 - val_acc: 0.9285\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8813\n",
      "Epoch 00028: val_loss improved from 0.25031 to 0.23283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/028-0.2328.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3729 - acc: 0.8813 - val_loss: 0.2328 - val_acc: 0.9341\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8848\n",
      "Epoch 00029: val_loss did not improve from 0.23283\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3640 - acc: 0.8848 - val_loss: 0.2347 - val_acc: 0.9324\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8864\n",
      "Epoch 00030: val_loss improved from 0.23283 to 0.21914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/030-0.2191.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.3534 - acc: 0.8864 - val_loss: 0.2191 - val_acc: 0.9373\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8897\n",
      "Epoch 00031: val_loss did not improve from 0.21914\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3478 - acc: 0.8897 - val_loss: 0.2553 - val_acc: 0.9241\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8919\n",
      "Epoch 00032: val_loss improved from 0.21914 to 0.21756, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/032-0.2176.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3367 - acc: 0.8919 - val_loss: 0.2176 - val_acc: 0.9369\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8952\n",
      "Epoch 00033: val_loss did not improve from 0.21756\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.3299 - acc: 0.8952 - val_loss: 0.2198 - val_acc: 0.9317\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8983\n",
      "Epoch 00034: val_loss did not improve from 0.21756\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.3195 - acc: 0.8983 - val_loss: 0.2180 - val_acc: 0.9322\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9000\n",
      "Epoch 00035: val_loss improved from 0.21756 to 0.21137, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/035-0.2114.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3129 - acc: 0.9000 - val_loss: 0.2114 - val_acc: 0.9394\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9027\n",
      "Epoch 00036: val_loss improved from 0.21137 to 0.20871, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/036-0.2087.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3049 - acc: 0.9027 - val_loss: 0.2087 - val_acc: 0.9394\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9047\n",
      "Epoch 00037: val_loss improved from 0.20871 to 0.19285, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/037-0.1929.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2999 - acc: 0.9047 - val_loss: 0.1929 - val_acc: 0.9462\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9064\n",
      "Epoch 00038: val_loss improved from 0.19285 to 0.19003, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/038-0.1900.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2960 - acc: 0.9064 - val_loss: 0.1900 - val_acc: 0.9453\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9061\n",
      "Epoch 00039: val_loss improved from 0.19003 to 0.18984, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/039-0.1898.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2932 - acc: 0.9061 - val_loss: 0.1898 - val_acc: 0.9448\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9106\n",
      "Epoch 00040: val_loss improved from 0.18984 to 0.18550, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/040-0.1855.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2811 - acc: 0.9106 - val_loss: 0.1855 - val_acc: 0.9434\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9120\n",
      "Epoch 00041: val_loss did not improve from 0.18550\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2761 - acc: 0.9120 - val_loss: 0.1897 - val_acc: 0.9450\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9105\n",
      "Epoch 00042: val_loss did not improve from 0.18550\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2759 - acc: 0.9105 - val_loss: 0.1878 - val_acc: 0.9441\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9154\n",
      "Epoch 00043: val_loss improved from 0.18550 to 0.17579, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/043-0.1758.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2643 - acc: 0.9154 - val_loss: 0.1758 - val_acc: 0.9488\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9168\n",
      "Epoch 00044: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2605 - acc: 0.9168 - val_loss: 0.1840 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9171\n",
      "Epoch 00045: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2585 - acc: 0.9171 - val_loss: 0.1811 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9196\n",
      "Epoch 00046: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2544 - acc: 0.9196 - val_loss: 0.1810 - val_acc: 0.9478\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9183\n",
      "Epoch 00047: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2493 - acc: 0.9183 - val_loss: 0.1761 - val_acc: 0.9481\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9186\n",
      "Epoch 00048: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2482 - acc: 0.9186 - val_loss: 0.1866 - val_acc: 0.9434\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9202\n",
      "Epoch 00049: val_loss improved from 0.17579 to 0.17262, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/049-0.1726.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2485 - acc: 0.9202 - val_loss: 0.1726 - val_acc: 0.9509\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9210\n",
      "Epoch 00050: val_loss improved from 0.17262 to 0.16472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/050-0.1647.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2413 - acc: 0.9210 - val_loss: 0.1647 - val_acc: 0.9539\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9255\n",
      "Epoch 00051: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2333 - acc: 0.9255 - val_loss: 0.1859 - val_acc: 0.9455\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9264\n",
      "Epoch 00052: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2276 - acc: 0.9264 - val_loss: 0.1706 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9258\n",
      "Epoch 00053: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2253 - acc: 0.9258 - val_loss: 0.1902 - val_acc: 0.9446\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9271\n",
      "Epoch 00054: val_loss improved from 0.16472 to 0.16094, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/054-0.1609.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2268 - acc: 0.9271 - val_loss: 0.1609 - val_acc: 0.9522\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9287\n",
      "Epoch 00055: val_loss improved from 0.16094 to 0.15707, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/055-0.1571.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2183 - acc: 0.9287 - val_loss: 0.1571 - val_acc: 0.9527\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9274\n",
      "Epoch 00056: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2206 - acc: 0.9274 - val_loss: 0.1581 - val_acc: 0.9536\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9296\n",
      "Epoch 00057: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2156 - acc: 0.9295 - val_loss: 0.1646 - val_acc: 0.9520\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9287\n",
      "Epoch 00058: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2197 - acc: 0.9288 - val_loss: 0.1863 - val_acc: 0.9455\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9327\n",
      "Epoch 00059: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2067 - acc: 0.9328 - val_loss: 0.1592 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9305\n",
      "Epoch 00060: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2108 - acc: 0.9305 - val_loss: 0.1675 - val_acc: 0.9511\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9308\n",
      "Epoch 00061: val_loss did not improve from 0.15707\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2092 - acc: 0.9309 - val_loss: 0.1622 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9326\n",
      "Epoch 00062: val_loss improved from 0.15707 to 0.15235, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/062-0.1523.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2032 - acc: 0.9326 - val_loss: 0.1523 - val_acc: 0.9585\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9352\n",
      "Epoch 00063: val_loss did not improve from 0.15235\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1982 - acc: 0.9352 - val_loss: 0.1541 - val_acc: 0.9555\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9341\n",
      "Epoch 00064: val_loss improved from 0.15235 to 0.15001, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/064-0.1500.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1983 - acc: 0.9341 - val_loss: 0.1500 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9366\n",
      "Epoch 00065: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1941 - acc: 0.9366 - val_loss: 0.1532 - val_acc: 0.9569\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9367\n",
      "Epoch 00066: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1904 - acc: 0.9366 - val_loss: 0.1591 - val_acc: 0.9518\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9376\n",
      "Epoch 00067: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1913 - acc: 0.9376 - val_loss: 0.1575 - val_acc: 0.9534\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9367\n",
      "Epoch 00068: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1911 - acc: 0.9367 - val_loss: 0.1573 - val_acc: 0.9532\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9385\n",
      "Epoch 00069: val_loss did not improve from 0.15001\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1844 - acc: 0.9385 - val_loss: 0.1545 - val_acc: 0.9536\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9391\n",
      "Epoch 00070: val_loss improved from 0.15001 to 0.14389, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/070-0.1439.hdf5\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1862 - acc: 0.9391 - val_loss: 0.1439 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9393\n",
      "Epoch 00071: val_loss did not improve from 0.14389\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1815 - acc: 0.9393 - val_loss: 0.1569 - val_acc: 0.9543\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9417\n",
      "Epoch 00072: val_loss improved from 0.14389 to 0.14257, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/072-0.1426.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1781 - acc: 0.9416 - val_loss: 0.1426 - val_acc: 0.9590\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9399\n",
      "Epoch 00073: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1800 - acc: 0.9399 - val_loss: 0.1442 - val_acc: 0.9574\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9413\n",
      "Epoch 00074: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1771 - acc: 0.9413 - val_loss: 0.1470 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9438\n",
      "Epoch 00075: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1704 - acc: 0.9437 - val_loss: 0.1524 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9423\n",
      "Epoch 00076: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1735 - acc: 0.9423 - val_loss: 0.1500 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9435\n",
      "Epoch 00077: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1712 - acc: 0.9435 - val_loss: 0.1472 - val_acc: 0.9585\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9464\n",
      "Epoch 00078: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1665 - acc: 0.9464 - val_loss: 0.1431 - val_acc: 0.9564\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9461\n",
      "Epoch 00079: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1646 - acc: 0.9461 - val_loss: 0.1519 - val_acc: 0.9576\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9455\n",
      "Epoch 00080: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1650 - acc: 0.9455 - val_loss: 0.1452 - val_acc: 0.9574\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9464\n",
      "Epoch 00081: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1632 - acc: 0.9463 - val_loss: 0.1455 - val_acc: 0.9550\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9466\n",
      "Epoch 00082: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1584 - acc: 0.9465 - val_loss: 0.1646 - val_acc: 0.9513\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9463\n",
      "Epoch 00083: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1609 - acc: 0.9463 - val_loss: 0.1492 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9471\n",
      "Epoch 00084: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1554 - acc: 0.9471 - val_loss: 0.1631 - val_acc: 0.9522\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9469\n",
      "Epoch 00085: val_loss improved from 0.14257 to 0.14189, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/085-0.1419.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1594 - acc: 0.9469 - val_loss: 0.1419 - val_acc: 0.9597\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9484\n",
      "Epoch 00086: val_loss improved from 0.14189 to 0.13715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/086-0.1371.hdf5\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1517 - acc: 0.9484 - val_loss: 0.1371 - val_acc: 0.9604\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9495\n",
      "Epoch 00087: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1505 - acc: 0.9495 - val_loss: 0.1415 - val_acc: 0.9583\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9508\n",
      "Epoch 00088: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1471 - acc: 0.9508 - val_loss: 0.1487 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9508\n",
      "Epoch 00089: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1473 - acc: 0.9508 - val_loss: 0.1435 - val_acc: 0.9590\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9502\n",
      "Epoch 00090: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1498 - acc: 0.9503 - val_loss: 0.1416 - val_acc: 0.9616\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9509\n",
      "Epoch 00091: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1419 - acc: 0.9509 - val_loss: 0.1481 - val_acc: 0.9592\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9508\n",
      "Epoch 00092: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1468 - acc: 0.9508 - val_loss: 0.1376 - val_acc: 0.9597\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9526\n",
      "Epoch 00093: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1435 - acc: 0.9526 - val_loss: 0.1398 - val_acc: 0.9588\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9515\n",
      "Epoch 00094: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1457 - acc: 0.9515 - val_loss: 0.1485 - val_acc: 0.9597\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9532\n",
      "Epoch 00095: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1397 - acc: 0.9531 - val_loss: 0.1462 - val_acc: 0.9581\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9531\n",
      "Epoch 00096: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1391 - acc: 0.9530 - val_loss: 0.1447 - val_acc: 0.9578\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9524\n",
      "Epoch 00097: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1374 - acc: 0.9525 - val_loss: 0.1502 - val_acc: 0.9583\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9556\n",
      "Epoch 00098: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1329 - acc: 0.9556 - val_loss: 0.1407 - val_acc: 0.9606\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9534\n",
      "Epoch 00099: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1385 - acc: 0.9534 - val_loss: 0.1447 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9521\n",
      "Epoch 00100: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1390 - acc: 0.9521 - val_loss: 0.1400 - val_acc: 0.9581\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9559\n",
      "Epoch 00101: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1308 - acc: 0.9559 - val_loss: 0.1475 - val_acc: 0.9592\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9553\n",
      "Epoch 00102: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1311 - acc: 0.9553 - val_loss: 0.1439 - val_acc: 0.9613\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9554\n",
      "Epoch 00103: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1319 - acc: 0.9554 - val_loss: 0.1462 - val_acc: 0.9578\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9571\n",
      "Epoch 00104: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1285 - acc: 0.9571 - val_loss: 0.1429 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9539\n",
      "Epoch 00105: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1357 - acc: 0.9539 - val_loss: 0.1398 - val_acc: 0.9609\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9577\n",
      "Epoch 00106: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1242 - acc: 0.9577 - val_loss: 0.1444 - val_acc: 0.9592\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9586\n",
      "Epoch 00107: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1218 - acc: 0.9586 - val_loss: 0.1393 - val_acc: 0.9590\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9584\n",
      "Epoch 00108: val_loss improved from 0.13715 to 0.13528, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/108-0.1353.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1253 - acc: 0.9584 - val_loss: 0.1353 - val_acc: 0.9639\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9586\n",
      "Epoch 00109: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1210 - acc: 0.9586 - val_loss: 0.1584 - val_acc: 0.9546\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9574\n",
      "Epoch 00110: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1228 - acc: 0.9574 - val_loss: 0.1426 - val_acc: 0.9595\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9590\n",
      "Epoch 00111: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1201 - acc: 0.9590 - val_loss: 0.1438 - val_acc: 0.9602\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9586\n",
      "Epoch 00112: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1216 - acc: 0.9586 - val_loss: 0.1519 - val_acc: 0.9616\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9581\n",
      "Epoch 00113: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1231 - acc: 0.9581 - val_loss: 0.1488 - val_acc: 0.9583\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9592\n",
      "Epoch 00114: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1227 - acc: 0.9592 - val_loss: 0.1402 - val_acc: 0.9627\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9587\n",
      "Epoch 00115: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1181 - acc: 0.9587 - val_loss: 0.1375 - val_acc: 0.9616\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9609\n",
      "Epoch 00116: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1137 - acc: 0.9609 - val_loss: 0.1374 - val_acc: 0.9606\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9611\n",
      "Epoch 00117: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1143 - acc: 0.9611 - val_loss: 0.1377 - val_acc: 0.9602\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9589\n",
      "Epoch 00118: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1186 - acc: 0.9589 - val_loss: 0.1396 - val_acc: 0.9618\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9619\n",
      "Epoch 00119: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1111 - acc: 0.9619 - val_loss: 0.1433 - val_acc: 0.9620\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9620\n",
      "Epoch 00120: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1126 - acc: 0.9620 - val_loss: 0.1496 - val_acc: 0.9599\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9601\n",
      "Epoch 00121: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1147 - acc: 0.9601 - val_loss: 0.1404 - val_acc: 0.9625\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9617\n",
      "Epoch 00122: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1149 - acc: 0.9617 - val_loss: 0.1374 - val_acc: 0.9630\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9624\n",
      "Epoch 00123: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1107 - acc: 0.9624 - val_loss: 0.1478 - val_acc: 0.9597\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9621\n",
      "Epoch 00124: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1104 - acc: 0.9621 - val_loss: 0.1521 - val_acc: 0.9606\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9623\n",
      "Epoch 00125: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1066 - acc: 0.9623 - val_loss: 0.1406 - val_acc: 0.9627\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9632\n",
      "Epoch 00126: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1057 - acc: 0.9632 - val_loss: 0.1477 - val_acc: 0.9613\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9644\n",
      "Epoch 00127: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1051 - acc: 0.9644 - val_loss: 0.1424 - val_acc: 0.9616\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9634\n",
      "Epoch 00128: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1056 - acc: 0.9634 - val_loss: 0.1442 - val_acc: 0.9602\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9648\n",
      "Epoch 00129: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.1041 - acc: 0.9648 - val_loss: 0.1395 - val_acc: 0.9604\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9637\n",
      "Epoch 00130: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1041 - acc: 0.9637 - val_loss: 0.1442 - val_acc: 0.9630\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9640\n",
      "Epoch 00131: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1014 - acc: 0.9640 - val_loss: 0.1426 - val_acc: 0.9599\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9657\n",
      "Epoch 00132: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1021 - acc: 0.9656 - val_loss: 0.1388 - val_acc: 0.9597\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9649\n",
      "Epoch 00133: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1011 - acc: 0.9649 - val_loss: 0.1431 - val_acc: 0.9639\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9643\n",
      "Epoch 00134: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1005 - acc: 0.9644 - val_loss: 0.1514 - val_acc: 0.9585\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9649\n",
      "Epoch 00135: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1018 - acc: 0.9649 - val_loss: 0.1544 - val_acc: 0.9595\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9661\n",
      "Epoch 00136: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0973 - acc: 0.9661 - val_loss: 0.1449 - val_acc: 0.9644\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9654\n",
      "Epoch 00137: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1000 - acc: 0.9654 - val_loss: 0.1446 - val_acc: 0.9602\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9655\n",
      "Epoch 00138: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1000 - acc: 0.9655 - val_loss: 0.1465 - val_acc: 0.9627\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9670\n",
      "Epoch 00139: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0978 - acc: 0.9670 - val_loss: 0.1509 - val_acc: 0.9595\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9668\n",
      "Epoch 00140: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0939 - acc: 0.9669 - val_loss: 0.1498 - val_acc: 0.9592\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9671\n",
      "Epoch 00141: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0957 - acc: 0.9672 - val_loss: 0.1453 - val_acc: 0.9611\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9678\n",
      "Epoch 00142: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0906 - acc: 0.9678 - val_loss: 0.1500 - val_acc: 0.9620\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9679\n",
      "Epoch 00143: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0930 - acc: 0.9679 - val_loss: 0.1534 - val_acc: 0.9611\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9674\n",
      "Epoch 00144: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0953 - acc: 0.9674 - val_loss: 0.1419 - val_acc: 0.9632\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9688\n",
      "Epoch 00145: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0875 - acc: 0.9688 - val_loss: 0.1470 - val_acc: 0.9623\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9682\n",
      "Epoch 00146: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0902 - acc: 0.9682 - val_loss: 0.1514 - val_acc: 0.9604\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9672\n",
      "Epoch 00147: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0925 - acc: 0.9672 - val_loss: 0.1608 - val_acc: 0.9616\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9686\n",
      "Epoch 00148: val_loss did not improve from 0.13528\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0905 - acc: 0.9686 - val_loss: 0.1582 - val_acc: 0.9588\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9683\n",
      "Epoch 00149: val_loss improved from 0.13528 to 0.13419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_8_conv_checkpoint/149-0.1342.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0909 - acc: 0.9683 - val_loss: 0.1342 - val_acc: 0.9637\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9688\n",
      "Epoch 00150: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0904 - acc: 0.9688 - val_loss: 0.1595 - val_acc: 0.9623\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9691\n",
      "Epoch 00151: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0893 - acc: 0.9691 - val_loss: 0.1575 - val_acc: 0.9620\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9696\n",
      "Epoch 00152: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0872 - acc: 0.9696 - val_loss: 0.1429 - val_acc: 0.9602\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9692\n",
      "Epoch 00153: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0889 - acc: 0.9692 - val_loss: 0.1512 - val_acc: 0.9613\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9711\n",
      "Epoch 00154: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0857 - acc: 0.9711 - val_loss: 0.1436 - val_acc: 0.9620\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9697\n",
      "Epoch 00155: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0864 - acc: 0.9697 - val_loss: 0.1469 - val_acc: 0.9609\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9702\n",
      "Epoch 00156: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0851 - acc: 0.9702 - val_loss: 0.1504 - val_acc: 0.9611\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9710\n",
      "Epoch 00157: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0850 - acc: 0.9710 - val_loss: 0.1440 - val_acc: 0.9623\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9709\n",
      "Epoch 00158: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0824 - acc: 0.9709 - val_loss: 0.1467 - val_acc: 0.9616\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9710\n",
      "Epoch 00159: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0828 - acc: 0.9710 - val_loss: 0.1431 - val_acc: 0.9646\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9701\n",
      "Epoch 00160: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0868 - acc: 0.9700 - val_loss: 0.1410 - val_acc: 0.9639\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9708\n",
      "Epoch 00161: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0855 - acc: 0.9708 - val_loss: 0.1442 - val_acc: 0.9644\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9730\n",
      "Epoch 00162: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0788 - acc: 0.9730 - val_loss: 0.1488 - val_acc: 0.9613\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9714\n",
      "Epoch 00163: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0850 - acc: 0.9714 - val_loss: 0.1729 - val_acc: 0.9571\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9699\n",
      "Epoch 00164: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0890 - acc: 0.9699 - val_loss: 0.1449 - val_acc: 0.9632\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9713\n",
      "Epoch 00165: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0841 - acc: 0.9713 - val_loss: 0.1510 - val_acc: 0.9611\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9715\n",
      "Epoch 00166: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0808 - acc: 0.9715 - val_loss: 0.1595 - val_acc: 0.9592\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9744\n",
      "Epoch 00167: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0765 - acc: 0.9744 - val_loss: 0.1527 - val_acc: 0.9613\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9715\n",
      "Epoch 00168: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0795 - acc: 0.9715 - val_loss: 0.1421 - val_acc: 0.9632\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9729\n",
      "Epoch 00169: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0763 - acc: 0.9729 - val_loss: 0.1505 - val_acc: 0.9639\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9733\n",
      "Epoch 00170: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0793 - acc: 0.9733 - val_loss: 0.1492 - val_acc: 0.9641\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9728\n",
      "Epoch 00171: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0771 - acc: 0.9728 - val_loss: 0.1521 - val_acc: 0.9646\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9733\n",
      "Epoch 00172: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.0757 - acc: 0.9733 - val_loss: 0.1511 - val_acc: 0.9623\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9723\n",
      "Epoch 00173: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.0816 - acc: 0.9723 - val_loss: 0.1488 - val_acc: 0.9646\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9744\n",
      "Epoch 00174: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0723 - acc: 0.9744 - val_loss: 0.1600 - val_acc: 0.9618\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9724\n",
      "Epoch 00175: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.0775 - acc: 0.9724 - val_loss: 0.1576 - val_acc: 0.9644\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9740\n",
      "Epoch 00176: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0740 - acc: 0.9740 - val_loss: 0.1500 - val_acc: 0.9606\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9745\n",
      "Epoch 00177: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0754 - acc: 0.9745 - val_loss: 0.1426 - val_acc: 0.9613\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9744\n",
      "Epoch 00178: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.0745 - acc: 0.9744 - val_loss: 0.1513 - val_acc: 0.9620\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9735\n",
      "Epoch 00179: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0756 - acc: 0.9735 - val_loss: 0.1543 - val_acc: 0.9618\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9734\n",
      "Epoch 00180: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.0762 - acc: 0.9734 - val_loss: 0.1567 - val_acc: 0.9604\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9734\n",
      "Epoch 00181: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.0750 - acc: 0.9734 - val_loss: 0.1550 - val_acc: 0.9630\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9759\n",
      "Epoch 00182: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0698 - acc: 0.9759 - val_loss: 0.1568 - val_acc: 0.9604\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9757\n",
      "Epoch 00183: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.0709 - acc: 0.9757 - val_loss: 0.1579 - val_acc: 0.9609\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9747\n",
      "Epoch 00184: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0732 - acc: 0.9747 - val_loss: 0.1502 - val_acc: 0.9630\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9746\n",
      "Epoch 00185: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.0735 - acc: 0.9746 - val_loss: 0.1465 - val_acc: 0.9637\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9767\n",
      "Epoch 00186: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.0701 - acc: 0.9767 - val_loss: 0.1577 - val_acc: 0.9602\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9750\n",
      "Epoch 00187: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0707 - acc: 0.9750 - val_loss: 0.1537 - val_acc: 0.9630\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9765\n",
      "Epoch 00188: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0699 - acc: 0.9766 - val_loss: 0.1499 - val_acc: 0.9618\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9763\n",
      "Epoch 00189: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.0700 - acc: 0.9763 - val_loss: 0.1532 - val_acc: 0.9620\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9769\n",
      "Epoch 00190: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.0682 - acc: 0.9769 - val_loss: 0.1519 - val_acc: 0.9616\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9750\n",
      "Epoch 00191: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.0717 - acc: 0.9750 - val_loss: 0.1587 - val_acc: 0.9639\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9770\n",
      "Epoch 00192: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.0658 - acc: 0.9770 - val_loss: 0.1565 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9763\n",
      "Epoch 00193: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.0675 - acc: 0.9763 - val_loss: 0.1624 - val_acc: 0.9595\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9760\n",
      "Epoch 00194: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.0679 - acc: 0.9760 - val_loss: 0.1613 - val_acc: 0.9613\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9767\n",
      "Epoch 00195: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.0670 - acc: 0.9767 - val_loss: 0.1657 - val_acc: 0.9609\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9773\n",
      "Epoch 00196: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.0660 - acc: 0.9773 - val_loss: 0.1541 - val_acc: 0.9651\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9771\n",
      "Epoch 00197: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.0668 - acc: 0.9771 - val_loss: 0.1590 - val_acc: 0.9639\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9779\n",
      "Epoch 00198: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.0646 - acc: 0.9779 - val_loss: 0.1505 - val_acc: 0.9623\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9776\n",
      "Epoch 00199: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.0661 - acc: 0.9776 - val_loss: 0.1477 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 395us/sample - loss: 0.1897 - acc: 0.9477\n",
      "Loss: 0.18966707489603157 Accuracy: 0.94766355\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4064 - acc: 0.2193\n",
      "Epoch 00001: val_loss improved from inf to 1.60643, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/001-1.6064.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 2.4062 - acc: 0.2193 - val_loss: 1.6064 - val_acc: 0.5188\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4865 - acc: 0.5120\n",
      "Epoch 00002: val_loss improved from 1.60643 to 1.06528, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/002-1.0653.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 1.4864 - acc: 0.5121 - val_loss: 1.0653 - val_acc: 0.6576\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1512 - acc: 0.6211\n",
      "Epoch 00003: val_loss improved from 1.06528 to 0.83861, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/003-0.8386.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.1511 - acc: 0.6211 - val_loss: 0.8386 - val_acc: 0.7382\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9823 - acc: 0.6788\n",
      "Epoch 00004: val_loss improved from 0.83861 to 0.71781, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/004-0.7178.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.9823 - acc: 0.6788 - val_loss: 0.7178 - val_acc: 0.7803\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8711 - acc: 0.7161\n",
      "Epoch 00005: val_loss improved from 0.71781 to 0.63255, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/005-0.6326.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.8710 - acc: 0.7162 - val_loss: 0.6326 - val_acc: 0.8050\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7484\n",
      "Epoch 00006: val_loss improved from 0.63255 to 0.57700, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/006-0.5770.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.7784 - acc: 0.7484 - val_loss: 0.5770 - val_acc: 0.8230\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7114 - acc: 0.7685\n",
      "Epoch 00007: val_loss improved from 0.57700 to 0.50882, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/007-0.5088.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.7112 - acc: 0.7687 - val_loss: 0.5088 - val_acc: 0.8446\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.7840\n",
      "Epoch 00008: val_loss improved from 0.50882 to 0.45914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/008-0.4591.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.6598 - acc: 0.7841 - val_loss: 0.4591 - val_acc: 0.8570\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8041\n",
      "Epoch 00009: val_loss improved from 0.45914 to 0.43817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/009-0.4382.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.6066 - acc: 0.8041 - val_loss: 0.4382 - val_acc: 0.8637\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8168\n",
      "Epoch 00010: val_loss improved from 0.43817 to 0.39193, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/010-0.3919.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.5678 - acc: 0.8168 - val_loss: 0.3919 - val_acc: 0.8756\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8280\n",
      "Epoch 00011: val_loss improved from 0.39193 to 0.39095, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/011-0.3909.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5309 - acc: 0.8280 - val_loss: 0.3909 - val_acc: 0.8803\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.8373\n",
      "Epoch 00012: val_loss improved from 0.39095 to 0.35714, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/012-0.3571.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.5023 - acc: 0.8373 - val_loss: 0.3571 - val_acc: 0.8891\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8481\n",
      "Epoch 00013: val_loss improved from 0.35714 to 0.35282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/013-0.3528.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4719 - acc: 0.8481 - val_loss: 0.3528 - val_acc: 0.8863\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8539\n",
      "Epoch 00014: val_loss improved from 0.35282 to 0.32157, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/014-0.3216.hdf5\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.4521 - acc: 0.8539 - val_loss: 0.3216 - val_acc: 0.9015\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8612\n",
      "Epoch 00015: val_loss improved from 0.32157 to 0.28539, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/015-0.2854.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.4310 - acc: 0.8612 - val_loss: 0.2854 - val_acc: 0.9115\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8714\n",
      "Epoch 00016: val_loss did not improve from 0.28539\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.4002 - acc: 0.8714 - val_loss: 0.2857 - val_acc: 0.9101\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8749\n",
      "Epoch 00017: val_loss improved from 0.28539 to 0.27066, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/017-0.2707.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3875 - acc: 0.8750 - val_loss: 0.2707 - val_acc: 0.9171\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8825\n",
      "Epoch 00018: val_loss improved from 0.27066 to 0.25140, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/018-0.2514.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.3625 - acc: 0.8825 - val_loss: 0.2514 - val_acc: 0.9241\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8858\n",
      "Epoch 00019: val_loss improved from 0.25140 to 0.24630, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/019-0.2463.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.3527 - acc: 0.8858 - val_loss: 0.2463 - val_acc: 0.9187\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8921\n",
      "Epoch 00020: val_loss improved from 0.24630 to 0.24149, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/020-0.2415.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3379 - acc: 0.8922 - val_loss: 0.2415 - val_acc: 0.9215\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8966\n",
      "Epoch 00021: val_loss improved from 0.24149 to 0.22456, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/021-0.2246.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.3212 - acc: 0.8966 - val_loss: 0.2246 - val_acc: 0.9269\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8987\n",
      "Epoch 00022: val_loss improved from 0.22456 to 0.21376, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/022-0.2138.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3145 - acc: 0.8987 - val_loss: 0.2138 - val_acc: 0.9322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9018\n",
      "Epoch 00023: val_loss did not improve from 0.21376\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3008 - acc: 0.9018 - val_loss: 0.2251 - val_acc: 0.9278\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9063\n",
      "Epoch 00024: val_loss improved from 0.21376 to 0.20566, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/024-0.2057.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2908 - acc: 0.9063 - val_loss: 0.2057 - val_acc: 0.9350\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9091\n",
      "Epoch 00025: val_loss did not improve from 0.20566\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2828 - acc: 0.9091 - val_loss: 0.2057 - val_acc: 0.9357\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9127\n",
      "Epoch 00026: val_loss improved from 0.20566 to 0.20390, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/026-0.2039.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.2695 - acc: 0.9128 - val_loss: 0.2039 - val_acc: 0.9373\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9171\n",
      "Epoch 00027: val_loss improved from 0.20390 to 0.19999, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/027-0.2000.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.2608 - acc: 0.9172 - val_loss: 0.2000 - val_acc: 0.9383\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9163\n",
      "Epoch 00028: val_loss improved from 0.19999 to 0.18762, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/028-0.1876.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.2536 - acc: 0.9163 - val_loss: 0.1876 - val_acc: 0.9422\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9188\n",
      "Epoch 00029: val_loss improved from 0.18762 to 0.18733, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/029-0.1873.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.2483 - acc: 0.9188 - val_loss: 0.1873 - val_acc: 0.9415\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9236\n",
      "Epoch 00030: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.2342 - acc: 0.9236 - val_loss: 0.1918 - val_acc: 0.9427\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9232\n",
      "Epoch 00031: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2351 - acc: 0.9231 - val_loss: 0.1888 - val_acc: 0.9422\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00032: val_loss improved from 0.18733 to 0.17006, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/032-0.1701.hdf5\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.1701 - val_acc: 0.9446\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9291\n",
      "Epoch 00033: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2184 - acc: 0.9291 - val_loss: 0.1940 - val_acc: 0.9383\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9301\n",
      "Epoch 00034: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2135 - acc: 0.9301 - val_loss: 0.1913 - val_acc: 0.9406\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9315\n",
      "Epoch 00035: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2075 - acc: 0.9315 - val_loss: 0.1745 - val_acc: 0.9495\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9334\n",
      "Epoch 00036: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.2020 - acc: 0.9334 - val_loss: 0.1793 - val_acc: 0.9474\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9343\n",
      "Epoch 00037: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1978 - acc: 0.9344 - val_loss: 0.1706 - val_acc: 0.9492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9362\n",
      "Epoch 00038: val_loss improved from 0.17006 to 0.16171, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/038-0.1617.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1924 - acc: 0.9362 - val_loss: 0.1617 - val_acc: 0.9513\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9392\n",
      "Epoch 00039: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1857 - acc: 0.9392 - val_loss: 0.1704 - val_acc: 0.9460\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9390\n",
      "Epoch 00040: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1849 - acc: 0.9391 - val_loss: 0.1745 - val_acc: 0.9462\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9420\n",
      "Epoch 00041: val_loss did not improve from 0.16171\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1769 - acc: 0.9420 - val_loss: 0.1803 - val_acc: 0.9476\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9420\n",
      "Epoch 00042: val_loss improved from 0.16171 to 0.16092, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/042-0.1609.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1753 - acc: 0.9420 - val_loss: 0.1609 - val_acc: 0.9525\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9439\n",
      "Epoch 00043: val_loss improved from 0.16092 to 0.15897, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/043-0.1590.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1714 - acc: 0.9439 - val_loss: 0.1590 - val_acc: 0.9506\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9449\n",
      "Epoch 00044: val_loss did not improve from 0.15897\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1657 - acc: 0.9449 - val_loss: 0.1657 - val_acc: 0.9485\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9445\n",
      "Epoch 00045: val_loss improved from 0.15897 to 0.15125, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/045-0.1512.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1677 - acc: 0.9445 - val_loss: 0.1512 - val_acc: 0.9539\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9482\n",
      "Epoch 00046: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1589 - acc: 0.9481 - val_loss: 0.1608 - val_acc: 0.9502\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9472\n",
      "Epoch 00047: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1579 - acc: 0.9472 - val_loss: 0.1540 - val_acc: 0.9541\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9486\n",
      "Epoch 00048: val_loss did not improve from 0.15125\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1515 - acc: 0.9486 - val_loss: 0.1581 - val_acc: 0.9527\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9490\n",
      "Epoch 00049: val_loss improved from 0.15125 to 0.15035, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/049-0.1503.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1516 - acc: 0.9490 - val_loss: 0.1503 - val_acc: 0.9529\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9515\n",
      "Epoch 00050: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1458 - acc: 0.9515 - val_loss: 0.1581 - val_acc: 0.9502\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9500\n",
      "Epoch 00051: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1504 - acc: 0.9500 - val_loss: 0.1617 - val_acc: 0.9522\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9530\n",
      "Epoch 00052: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1408 - acc: 0.9530 - val_loss: 0.1586 - val_acc: 0.9527\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9530\n",
      "Epoch 00053: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1393 - acc: 0.9530 - val_loss: 0.1540 - val_acc: 0.9525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9542\n",
      "Epoch 00054: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1375 - acc: 0.9542 - val_loss: 0.1663 - val_acc: 0.9511\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9559\n",
      "Epoch 00055: val_loss did not improve from 0.15035\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1298 - acc: 0.9559 - val_loss: 0.1575 - val_acc: 0.9534\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9555\n",
      "Epoch 00056: val_loss improved from 0.15035 to 0.15018, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/056-0.1502.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1314 - acc: 0.9555 - val_loss: 0.1502 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9572\n",
      "Epoch 00057: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1297 - acc: 0.9572 - val_loss: 0.1539 - val_acc: 0.9557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9568\n",
      "Epoch 00058: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1270 - acc: 0.9568 - val_loss: 0.1596 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9598\n",
      "Epoch 00059: val_loss did not improve from 0.15018\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1201 - acc: 0.9597 - val_loss: 0.1650 - val_acc: 0.9520\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9590\n",
      "Epoch 00060: val_loss improved from 0.15018 to 0.14282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_9_conv_checkpoint/060-0.1428.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1223 - acc: 0.9590 - val_loss: 0.1428 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9589\n",
      "Epoch 00061: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1210 - acc: 0.9589 - val_loss: 0.1481 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9608\n",
      "Epoch 00062: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1163 - acc: 0.9608 - val_loss: 0.1599 - val_acc: 0.9543\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9607\n",
      "Epoch 00063: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1138 - acc: 0.9607 - val_loss: 0.1477 - val_acc: 0.9571\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9617\n",
      "Epoch 00064: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1103 - acc: 0.9617 - val_loss: 0.1561 - val_acc: 0.9557\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9606\n",
      "Epoch 00065: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1139 - acc: 0.9606 - val_loss: 0.1477 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9626\n",
      "Epoch 00066: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1090 - acc: 0.9625 - val_loss: 0.1518 - val_acc: 0.9564\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9629\n",
      "Epoch 00067: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1075 - acc: 0.9629 - val_loss: 0.1513 - val_acc: 0.9581\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9648\n",
      "Epoch 00068: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1031 - acc: 0.9648 - val_loss: 0.1496 - val_acc: 0.9571\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9645\n",
      "Epoch 00069: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1032 - acc: 0.9645 - val_loss: 0.1627 - val_acc: 0.9534\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9648\n",
      "Epoch 00070: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0997 - acc: 0.9648 - val_loss: 0.1560 - val_acc: 0.9539\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9640\n",
      "Epoch 00071: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1018 - acc: 0.9640 - val_loss: 0.1477 - val_acc: 0.9606\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9661\n",
      "Epoch 00072: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0965 - acc: 0.9660 - val_loss: 0.1504 - val_acc: 0.9576\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9677\n",
      "Epoch 00073: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0953 - acc: 0.9676 - val_loss: 0.1445 - val_acc: 0.9602\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9671\n",
      "Epoch 00074: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0959 - acc: 0.9671 - val_loss: 0.1663 - val_acc: 0.9562\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9677\n",
      "Epoch 00075: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0940 - acc: 0.9677 - val_loss: 0.1522 - val_acc: 0.9592\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9687\n",
      "Epoch 00076: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0909 - acc: 0.9687 - val_loss: 0.1516 - val_acc: 0.9560\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9669\n",
      "Epoch 00077: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0927 - acc: 0.9669 - val_loss: 0.1543 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9698\n",
      "Epoch 00078: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0894 - acc: 0.9698 - val_loss: 0.1501 - val_acc: 0.9583\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9700\n",
      "Epoch 00079: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.0873 - acc: 0.9700 - val_loss: 0.1755 - val_acc: 0.9527\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9709\n",
      "Epoch 00080: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0850 - acc: 0.9709 - val_loss: 0.1482 - val_acc: 0.9571\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9706\n",
      "Epoch 00081: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0851 - acc: 0.9706 - val_loss: 0.1541 - val_acc: 0.9590\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9718\n",
      "Epoch 00082: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.0817 - acc: 0.9718 - val_loss: 0.1538 - val_acc: 0.9590\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9707\n",
      "Epoch 00083: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0864 - acc: 0.9707 - val_loss: 0.1777 - val_acc: 0.9564\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9727\n",
      "Epoch 00084: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0798 - acc: 0.9727 - val_loss: 0.1555 - val_acc: 0.9592\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9722\n",
      "Epoch 00085: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.0790 - acc: 0.9722 - val_loss: 0.1447 - val_acc: 0.9620\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9719\n",
      "Epoch 00086: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0802 - acc: 0.9719 - val_loss: 0.1616 - val_acc: 0.9567\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9737\n",
      "Epoch 00087: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0769 - acc: 0.9737 - val_loss: 0.1842 - val_acc: 0.9550\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9731\n",
      "Epoch 00088: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0791 - acc: 0.9731 - val_loss: 0.1520 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9745\n",
      "Epoch 00089: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0744 - acc: 0.9745 - val_loss: 0.1534 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9743\n",
      "Epoch 00090: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0743 - acc: 0.9743 - val_loss: 0.1589 - val_acc: 0.9567\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9740\n",
      "Epoch 00091: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0739 - acc: 0.9741 - val_loss: 0.1604 - val_acc: 0.9569\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9753\n",
      "Epoch 00092: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0725 - acc: 0.9753 - val_loss: 0.1630 - val_acc: 0.9578\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9754\n",
      "Epoch 00093: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0715 - acc: 0.9754 - val_loss: 0.1617 - val_acc: 0.9590\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9754\n",
      "Epoch 00094: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0711 - acc: 0.9754 - val_loss: 0.1632 - val_acc: 0.9597\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9770\n",
      "Epoch 00095: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0667 - acc: 0.9770 - val_loss: 0.1644 - val_acc: 0.9595\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9754\n",
      "Epoch 00096: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.0704 - acc: 0.9754 - val_loss: 0.1764 - val_acc: 0.9576\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9762\n",
      "Epoch 00097: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0680 - acc: 0.9763 - val_loss: 0.1629 - val_acc: 0.9599\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9764\n",
      "Epoch 00098: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.0666 - acc: 0.9764 - val_loss: 0.1617 - val_acc: 0.9597\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9768\n",
      "Epoch 00099: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0672 - acc: 0.9768 - val_loss: 0.1497 - val_acc: 0.9590\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9776\n",
      "Epoch 00100: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.0654 - acc: 0.9776 - val_loss: 0.1595 - val_acc: 0.9585\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9775\n",
      "Epoch 00101: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0660 - acc: 0.9775 - val_loss: 0.1735 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9771\n",
      "Epoch 00102: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0654 - acc: 0.9771 - val_loss: 0.1681 - val_acc: 0.9609\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9774\n",
      "Epoch 00103: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.0651 - acc: 0.9774 - val_loss: 0.1546 - val_acc: 0.9571\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9767\n",
      "Epoch 00104: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0658 - acc: 0.9767 - val_loss: 0.1566 - val_acc: 0.9599\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9784\n",
      "Epoch 00105: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.0616 - acc: 0.9784 - val_loss: 0.1759 - val_acc: 0.9590\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9789\n",
      "Epoch 00106: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.0623 - acc: 0.9789 - val_loss: 0.1633 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9776\n",
      "Epoch 00107: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.0616 - acc: 0.9776 - val_loss: 0.1713 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9803\n",
      "Epoch 00108: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0579 - acc: 0.9803 - val_loss: 0.1764 - val_acc: 0.9597\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9808\n",
      "Epoch 00109: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.0550 - acc: 0.9808 - val_loss: 0.1728 - val_acc: 0.9599\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9809\n",
      "Epoch 00110: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0543 - acc: 0.9809 - val_loss: 0.1642 - val_acc: 0.9627\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 415us/sample - loss: 0.2135 - acc: 0.9379\n",
      "Loss: 0.21350653049761384 Accuracy: 0.9379024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 386us/sample - loss: 1.5276 - acc: 0.5169\n",
      "Loss: 1.5275790819126498 Accuracy: 0.5169263\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 413us/sample - loss: 1.1507 - acc: 0.6656\n",
      "Loss: 1.150674240363845 Accuracy: 0.66562825\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 422us/sample - loss: 0.8196 - acc: 0.7605\n",
      "Loss: 0.8195533891457016 Accuracy: 0.76054\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 433us/sample - loss: 0.4329 - acc: 0.8893\n",
      "Loss: 0.4329061113166413 Accuracy: 0.8893043\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 449us/sample - loss: 0.2330 - acc: 0.9373\n",
      "Loss: 0.2330334084025301 Accuracy: 0.93727934\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 452us/sample - loss: 0.1897 - acc: 0.9477\n",
      "Loss: 0.18966707489603157 Accuracy: 0.94766355\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 471us/sample - loss: 0.2135 - acc: 0.9379\n",
      "Loss: 0.21350653049761384 Accuracy: 0.9379024\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
