{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      kernel_initializer='he_uniform', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      kernel_initializer='he_uniform')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same', kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same', kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4586 - acc: 0.3209\n",
      "Epoch 00001: val_loss improved from inf to 1.63576, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv_checkpoint/001-1.6358.hdf5\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 2.4586 - acc: 0.3208 - val_loss: 1.6358 - val_acc: 0.5134\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6454 - acc: 0.5098\n",
      "Epoch 00002: val_loss improved from 1.63576 to 1.57037, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv_checkpoint/002-1.5704.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 1.6456 - acc: 0.5098 - val_loss: 1.5704 - val_acc: 0.5213\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3123 - acc: 0.6035\n",
      "Epoch 00003: val_loss improved from 1.57037 to 1.52048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv_checkpoint/003-1.5205.hdf5\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.3126 - acc: 0.6034 - val_loss: 1.5205 - val_acc: 0.5542\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6632\n",
      "Epoch 00004: val_loss did not improve from 1.52048\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.0920 - acc: 0.6631 - val_loss: 1.5458 - val_acc: 0.5630\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8784 - acc: 0.7225\n",
      "Epoch 00005: val_loss improved from 1.52048 to 1.36965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv_checkpoint/005-1.3696.hdf5\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.8785 - acc: 0.7225 - val_loss: 1.3696 - val_acc: 0.6059\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7186 - acc: 0.7740\n",
      "Epoch 00006: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.7191 - acc: 0.7739 - val_loss: 1.5052 - val_acc: 0.5809\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8021\n",
      "Epoch 00007: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.6174 - acc: 0.8020 - val_loss: 1.4520 - val_acc: 0.6049\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.8339\n",
      "Epoch 00008: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.5178 - acc: 0.8339 - val_loss: 1.5576 - val_acc: 0.6052\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8550\n",
      "Epoch 00009: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.4460 - acc: 0.8550 - val_loss: 1.5889 - val_acc: 0.6082\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8763\n",
      "Epoch 00010: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3861 - acc: 0.8762 - val_loss: 1.5846 - val_acc: 0.6154\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8910\n",
      "Epoch 00011: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.3378 - acc: 0.8910 - val_loss: 1.8049 - val_acc: 0.5926\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.8988\n",
      "Epoch 00012: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.3101 - acc: 0.8988 - val_loss: 1.6376 - val_acc: 0.6229\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9135\n",
      "Epoch 00013: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.2699 - acc: 0.9134 - val_loss: 1.6352 - val_acc: 0.6366\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9177\n",
      "Epoch 00014: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.2526 - acc: 0.9177 - val_loss: 1.5784 - val_acc: 0.6492\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9229\n",
      "Epoch 00015: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.2358 - acc: 0.9229 - val_loss: 1.9292 - val_acc: 0.6084\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9309\n",
      "Epoch 00016: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.2184 - acc: 0.9309 - val_loss: 1.7403 - val_acc: 0.6275\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9316\n",
      "Epoch 00017: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 0.2115 - acc: 0.9315 - val_loss: 1.8939 - val_acc: 0.6117\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9410\n",
      "Epoch 00018: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 0.1842 - acc: 0.9410 - val_loss: 1.6740 - val_acc: 0.6483\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9432\n",
      "Epoch 00019: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.1831 - acc: 0.9432 - val_loss: 1.7885 - val_acc: 0.6504\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9493\n",
      "Epoch 00020: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 0.1607 - acc: 0.9494 - val_loss: 1.7259 - val_acc: 0.6469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9499\n",
      "Epoch 00021: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.1577 - acc: 0.9499 - val_loss: 1.6968 - val_acc: 0.6487\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9524\n",
      "Epoch 00022: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.1488 - acc: 0.9523 - val_loss: 1.8169 - val_acc: 0.6487\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9502\n",
      "Epoch 00023: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.1627 - acc: 0.9502 - val_loss: 1.8071 - val_acc: 0.6473\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9527\n",
      "Epoch 00024: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1515 - acc: 0.9526 - val_loss: 1.8020 - val_acc: 0.6543\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9543\n",
      "Epoch 00025: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1463 - acc: 0.9543 - val_loss: 1.8236 - val_acc: 0.6504\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9570\n",
      "Epoch 00026: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1332 - acc: 0.9570 - val_loss: 1.8832 - val_acc: 0.6422\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9638\n",
      "Epoch 00027: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1177 - acc: 0.9638 - val_loss: 1.8945 - val_acc: 0.6587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9616\n",
      "Epoch 00028: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1249 - acc: 0.9615 - val_loss: 1.9967 - val_acc: 0.6452\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9608\n",
      "Epoch 00029: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1277 - acc: 0.9608 - val_loss: 1.7833 - val_acc: 0.6513\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9671\n",
      "Epoch 00030: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1063 - acc: 0.9672 - val_loss: 1.8625 - val_acc: 0.6529\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9667\n",
      "Epoch 00031: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1073 - acc: 0.9667 - val_loss: 1.9389 - val_acc: 0.6462\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9666\n",
      "Epoch 00032: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1078 - acc: 0.9666 - val_loss: 1.9738 - val_acc: 0.6415\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9699\n",
      "Epoch 00033: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0979 - acc: 0.9699 - val_loss: 2.0200 - val_acc: 0.6534\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9689\n",
      "Epoch 00034: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1032 - acc: 0.9688 - val_loss: 1.9752 - val_acc: 0.6352\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9692\n",
      "Epoch 00035: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1004 - acc: 0.9692 - val_loss: 2.1463 - val_acc: 0.6264\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9702\n",
      "Epoch 00036: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0963 - acc: 0.9702 - val_loss: 2.0883 - val_acc: 0.6324\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9710\n",
      "Epoch 00037: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0953 - acc: 0.9710 - val_loss: 1.9047 - val_acc: 0.6632\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9696\n",
      "Epoch 00038: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0982 - acc: 0.9695 - val_loss: 1.8636 - val_acc: 0.6632\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9679\n",
      "Epoch 00039: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.1061 - acc: 0.9679 - val_loss: 1.9509 - val_acc: 0.6632\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9752\n",
      "Epoch 00040: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0834 - acc: 0.9751 - val_loss: 1.9968 - val_acc: 0.6546\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9736\n",
      "Epoch 00041: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0882 - acc: 0.9736 - val_loss: 1.9775 - val_acc: 0.6615\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9758\n",
      "Epoch 00042: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0795 - acc: 0.9758 - val_loss: 2.0037 - val_acc: 0.6578\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9760\n",
      "Epoch 00043: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0824 - acc: 0.9760 - val_loss: 2.0622 - val_acc: 0.6485\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9741\n",
      "Epoch 00044: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0849 - acc: 0.9741 - val_loss: 2.0096 - val_acc: 0.6604\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9771\n",
      "Epoch 00045: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0779 - acc: 0.9771 - val_loss: 2.0649 - val_acc: 0.6564\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9747\n",
      "Epoch 00046: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0858 - acc: 0.9747 - val_loss: 1.9827 - val_acc: 0.6657\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9782\n",
      "Epoch 00047: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0757 - acc: 0.9781 - val_loss: 2.0381 - val_acc: 0.6506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9764\n",
      "Epoch 00048: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0789 - acc: 0.9764 - val_loss: 2.0119 - val_acc: 0.6585\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9766\n",
      "Epoch 00049: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 0.0778 - acc: 0.9766 - val_loss: 2.0342 - val_acc: 0.6678\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9769\n",
      "Epoch 00050: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0760 - acc: 0.9769 - val_loss: 2.0380 - val_acc: 0.6611\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9784\n",
      "Epoch 00051: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0775 - acc: 0.9784 - val_loss: 2.0039 - val_acc: 0.6674\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9802\n",
      "Epoch 00052: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0664 - acc: 0.9802 - val_loss: 1.9789 - val_acc: 0.6758\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9772\n",
      "Epoch 00053: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 0.0762 - acc: 0.9772 - val_loss: 2.0250 - val_acc: 0.6562\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9810\n",
      "Epoch 00054: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0657 - acc: 0.9809 - val_loss: 2.0487 - val_acc: 0.6643\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9786\n",
      "Epoch 00055: val_loss did not improve from 1.36965\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.0749 - acc: 0.9785 - val_loss: 2.1310 - val_acc: 0.6541\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.4715 - acc: 0.5755\n",
      "Loss: 1.4714836027268186 Accuracy: 0.5754933\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3482 - acc: 0.3180\n",
      "Epoch 00001: val_loss improved from inf to 1.52980, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/001-1.5298.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 2.3480 - acc: 0.3181 - val_loss: 1.5298 - val_acc: 0.5285\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5818 - acc: 0.5154\n",
      "Epoch 00002: val_loss improved from 1.52980 to 1.24527, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/002-1.2453.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 1.5818 - acc: 0.5154 - val_loss: 1.2453 - val_acc: 0.6122\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2966 - acc: 0.5970\n",
      "Epoch 00003: val_loss improved from 1.24527 to 1.15425, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/003-1.1542.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 1.2966 - acc: 0.5969 - val_loss: 1.1542 - val_acc: 0.6429\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1052 - acc: 0.6585\n",
      "Epoch 00004: val_loss improved from 1.15425 to 1.06033, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/004-1.0603.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 1.1053 - acc: 0.6585 - val_loss: 1.0603 - val_acc: 0.6751\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9745 - acc: 0.6976\n",
      "Epoch 00005: val_loss improved from 1.06033 to 0.93999, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/005-0.9400.hdf5\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.9746 - acc: 0.6975 - val_loss: 0.9400 - val_acc: 0.7137\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8676 - acc: 0.7308\n",
      "Epoch 00006: val_loss improved from 0.93999 to 0.92090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/006-0.9209.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.8677 - acc: 0.7308 - val_loss: 0.9209 - val_acc: 0.7144\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7819 - acc: 0.7568\n",
      "Epoch 00007: val_loss improved from 0.92090 to 0.89606, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/007-0.8961.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.7819 - acc: 0.7567 - val_loss: 0.8961 - val_acc: 0.7305\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.7755\n",
      "Epoch 00008: val_loss improved from 0.89606 to 0.88348, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/008-0.8835.hdf5\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.7154 - acc: 0.7755 - val_loss: 0.8835 - val_acc: 0.7414\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7921\n",
      "Epoch 00009: val_loss improved from 0.88348 to 0.85722, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv_checkpoint/009-0.8572.hdf5\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.6623 - acc: 0.7921 - val_loss: 0.8572 - val_acc: 0.7419\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5976 - acc: 0.8128\n",
      "Epoch 00010: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.5976 - acc: 0.8128 - val_loss: 0.9375 - val_acc: 0.7163\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8277\n",
      "Epoch 00011: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.5445 - acc: 0.8276 - val_loss: 0.8697 - val_acc: 0.7519\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8384\n",
      "Epoch 00012: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.5058 - acc: 0.8384 - val_loss: 0.8578 - val_acc: 0.7612\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8494\n",
      "Epoch 00013: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.4681 - acc: 0.8494 - val_loss: 0.8757 - val_acc: 0.7524\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8618\n",
      "Epoch 00014: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.4296 - acc: 0.8617 - val_loss: 0.8777 - val_acc: 0.7503\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8701\n",
      "Epoch 00015: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.3988 - acc: 0.8700 - val_loss: 0.9287 - val_acc: 0.7489\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8744\n",
      "Epoch 00016: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.3865 - acc: 0.8744 - val_loss: 0.9562 - val_acc: 0.7358\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8842\n",
      "Epoch 00017: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.3580 - acc: 0.8842 - val_loss: 0.9398 - val_acc: 0.7547\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8924\n",
      "Epoch 00018: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.3332 - acc: 0.8924 - val_loss: 0.9353 - val_acc: 0.7498\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8967\n",
      "Epoch 00019: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.3146 - acc: 0.8966 - val_loss: 0.9949 - val_acc: 0.7298\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8987\n",
      "Epoch 00020: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.3089 - acc: 0.8986 - val_loss: 0.9660 - val_acc: 0.7456\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9105\n",
      "Epoch 00021: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.2770 - acc: 0.9105 - val_loss: 0.9802 - val_acc: 0.7501\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9108\n",
      "Epoch 00022: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.2724 - acc: 0.9107 - val_loss: 1.0197 - val_acc: 0.7442\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9154\n",
      "Epoch 00023: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.2605 - acc: 0.9154 - val_loss: 0.9507 - val_acc: 0.7622\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9157\n",
      "Epoch 00024: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.2561 - acc: 0.9157 - val_loss: 0.9541 - val_acc: 0.7636\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9198\n",
      "Epoch 00025: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.2448 - acc: 0.9198 - val_loss: 0.9351 - val_acc: 0.7654\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9252\n",
      "Epoch 00026: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.2259 - acc: 0.9252 - val_loss: 0.9900 - val_acc: 0.7587\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9252\n",
      "Epoch 00027: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.2274 - acc: 0.9252 - val_loss: 0.9381 - val_acc: 0.7692\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9287\n",
      "Epoch 00028: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.2175 - acc: 0.9286 - val_loss: 1.0056 - val_acc: 0.7508\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9295\n",
      "Epoch 00029: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.2165 - acc: 0.9295 - val_loss: 1.0179 - val_acc: 0.7563\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9343\n",
      "Epoch 00030: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.2034 - acc: 0.9342 - val_loss: 1.0183 - val_acc: 0.7519\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9373\n",
      "Epoch 00031: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1975 - acc: 0.9372 - val_loss: 0.9355 - val_acc: 0.7750\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9394\n",
      "Epoch 00032: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1855 - acc: 0.9394 - val_loss: 1.0244 - val_acc: 0.7647\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9384\n",
      "Epoch 00033: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1875 - acc: 0.9383 - val_loss: 0.9983 - val_acc: 0.7687\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9407\n",
      "Epoch 00034: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1835 - acc: 0.9406 - val_loss: 1.0306 - val_acc: 0.7584\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9419\n",
      "Epoch 00035: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1797 - acc: 0.9418 - val_loss: 0.9797 - val_acc: 0.7703\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9437\n",
      "Epoch 00036: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1763 - acc: 0.9437 - val_loss: 0.9853 - val_acc: 0.7741\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9460\n",
      "Epoch 00037: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1644 - acc: 0.9460 - val_loss: 0.9804 - val_acc: 0.7738\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9488\n",
      "Epoch 00038: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1591 - acc: 0.9487 - val_loss: 1.0199 - val_acc: 0.7796\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9479\n",
      "Epoch 00039: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1595 - acc: 0.9479 - val_loss: 1.0218 - val_acc: 0.7664\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9514\n",
      "Epoch 00040: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1510 - acc: 0.9514 - val_loss: 1.0039 - val_acc: 0.7741\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9520\n",
      "Epoch 00041: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1500 - acc: 0.9519 - val_loss: 1.0356 - val_acc: 0.7673\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9532\n",
      "Epoch 00042: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1497 - acc: 0.9531 - val_loss: 1.0664 - val_acc: 0.7603\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9518\n",
      "Epoch 00043: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1483 - acc: 0.9518 - val_loss: 1.0136 - val_acc: 0.7759\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9553\n",
      "Epoch 00044: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1383 - acc: 0.9552 - val_loss: 1.1043 - val_acc: 0.7540\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9536\n",
      "Epoch 00045: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1422 - acc: 0.9535 - val_loss: 1.0073 - val_acc: 0.7764\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9550\n",
      "Epoch 00046: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1382 - acc: 0.9550 - val_loss: 1.0965 - val_acc: 0.7657\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9601\n",
      "Epoch 00047: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1247 - acc: 0.9600 - val_loss: 1.0112 - val_acc: 0.7848\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9583\n",
      "Epoch 00048: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1275 - acc: 0.9583 - val_loss: 1.0390 - val_acc: 0.7738\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9588\n",
      "Epoch 00049: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1262 - acc: 0.9588 - val_loss: 1.0874 - val_acc: 0.7722\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9601\n",
      "Epoch 00050: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1215 - acc: 0.9601 - val_loss: 1.0413 - val_acc: 0.7857\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9602\n",
      "Epoch 00051: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1252 - acc: 0.9601 - val_loss: 1.0759 - val_acc: 0.7717\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9588\n",
      "Epoch 00052: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1281 - acc: 0.9588 - val_loss: 1.0692 - val_acc: 0.7757\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9596\n",
      "Epoch 00053: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1278 - acc: 0.9597 - val_loss: 1.0249 - val_acc: 0.7801\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9626\n",
      "Epoch 00054: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1133 - acc: 0.9626 - val_loss: 1.0432 - val_acc: 0.7789\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9632\n",
      "Epoch 00055: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1169 - acc: 0.9632 - val_loss: 1.0766 - val_acc: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9643\n",
      "Epoch 00056: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 0.1132 - acc: 0.9643 - val_loss: 1.0518 - val_acc: 0.7803\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9633\n",
      "Epoch 00057: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 141s 4ms/sample - loss: 0.1157 - acc: 0.9632 - val_loss: 1.0759 - val_acc: 0.7778\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9649\n",
      "Epoch 00058: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1096 - acc: 0.9649 - val_loss: 1.0879 - val_acc: 0.7785\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9665\n",
      "Epoch 00059: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 0.1048 - acc: 0.9664 - val_loss: 1.0814 - val_acc: 0.7689\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9891 - acc: 0.7080\n",
      "Loss: 0.9890744084757312 Accuracy: 0.70799583\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4499 - acc: 0.2992\n",
      "Epoch 00001: val_loss improved from inf to 1.40025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/001-1.4003.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 2.4499 - acc: 0.2992 - val_loss: 1.4003 - val_acc: 0.5761\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5073 - acc: 0.5402\n",
      "Epoch 00002: val_loss improved from 1.40025 to 1.04423, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/002-1.0442.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 1.5073 - acc: 0.5402 - val_loss: 1.0442 - val_acc: 0.6886\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1985 - acc: 0.6327\n",
      "Epoch 00003: val_loss improved from 1.04423 to 0.93261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/003-0.9326.hdf5\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 1.1984 - acc: 0.6327 - val_loss: 0.9326 - val_acc: 0.7181\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0333 - acc: 0.6849\n",
      "Epoch 00004: val_loss improved from 0.93261 to 0.82019, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/004-0.8202.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 1.0333 - acc: 0.6849 - val_loss: 0.8202 - val_acc: 0.7559\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9196 - acc: 0.7224\n",
      "Epoch 00005: val_loss improved from 0.82019 to 0.77541, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/005-0.7754.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.9195 - acc: 0.7224 - val_loss: 0.7754 - val_acc: 0.7785\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8355 - acc: 0.7488\n",
      "Epoch 00006: val_loss improved from 0.77541 to 0.74514, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/006-0.7451.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.8355 - acc: 0.7488 - val_loss: 0.7451 - val_acc: 0.7827\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7739 - acc: 0.7683\n",
      "Epoch 00007: val_loss improved from 0.74514 to 0.67135, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/007-0.6713.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.7740 - acc: 0.7683 - val_loss: 0.6713 - val_acc: 0.8032\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7190 - acc: 0.7824\n",
      "Epoch 00008: val_loss did not improve from 0.67135\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.7194 - acc: 0.7824 - val_loss: 0.7613 - val_acc: 0.7869\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.7931\n",
      "Epoch 00009: val_loss did not improve from 0.67135\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.6830 - acc: 0.7930 - val_loss: 0.7030 - val_acc: 0.8006\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.8095\n",
      "Epoch 00010: val_loss did not improve from 0.67135\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.6326 - acc: 0.8095 - val_loss: 0.6802 - val_acc: 0.8078\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8239\n",
      "Epoch 00011: val_loss improved from 0.67135 to 0.65801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/011-0.6580.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.5872 - acc: 0.8238 - val_loss: 0.6580 - val_acc: 0.8232\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.8302\n",
      "Epoch 00012: val_loss did not improve from 0.65801\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.5616 - acc: 0.8302 - val_loss: 0.6999 - val_acc: 0.8008\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8360\n",
      "Epoch 00013: val_loss did not improve from 0.65801\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.5368 - acc: 0.8359 - val_loss: 0.6755 - val_acc: 0.8139\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8460\n",
      "Epoch 00014: val_loss did not improve from 0.65801\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.5023 - acc: 0.8460 - val_loss: 0.6740 - val_acc: 0.8090\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8529\n",
      "Epoch 00015: val_loss did not improve from 0.65801\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.4788 - acc: 0.8529 - val_loss: 0.7010 - val_acc: 0.8148\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8620\n",
      "Epoch 00016: val_loss did not improve from 0.65801\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.4475 - acc: 0.8620 - val_loss: 0.6624 - val_acc: 0.8197\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8710\n",
      "Epoch 00017: val_loss improved from 0.65801 to 0.64385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/017-0.6439.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.4202 - acc: 0.8710 - val_loss: 0.6439 - val_acc: 0.8223\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8738\n",
      "Epoch 00018: val_loss did not improve from 0.64385\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.4065 - acc: 0.8738 - val_loss: 0.6619 - val_acc: 0.8232\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8832\n",
      "Epoch 00019: val_loss improved from 0.64385 to 0.63480, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/019-0.6348.hdf5\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.3805 - acc: 0.8832 - val_loss: 0.6348 - val_acc: 0.8255\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8880\n",
      "Epoch 00020: val_loss improved from 0.63480 to 0.62596, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv_checkpoint/020-0.6260.hdf5\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.3638 - acc: 0.8880 - val_loss: 0.6260 - val_acc: 0.8369\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8933\n",
      "Epoch 00021: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.3487 - acc: 0.8934 - val_loss: 0.6584 - val_acc: 0.8293\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8993\n",
      "Epoch 00022: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.3252 - acc: 0.8993 - val_loss: 0.7399 - val_acc: 0.8006\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8999\n",
      "Epoch 00023: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.3147 - acc: 0.8999 - val_loss: 0.6387 - val_acc: 0.8355\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9045\n",
      "Epoch 00024: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.3008 - acc: 0.9044 - val_loss: 0.6674 - val_acc: 0.8297\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9076\n",
      "Epoch 00025: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2951 - acc: 0.9076 - val_loss: 0.6546 - val_acc: 0.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9138\n",
      "Epoch 00026: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2760 - acc: 0.9138 - val_loss: 0.6680 - val_acc: 0.8281\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9157\n",
      "Epoch 00027: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2649 - acc: 0.9157 - val_loss: 0.6793 - val_acc: 0.8307\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9177\n",
      "Epoch 00028: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2551 - acc: 0.9177 - val_loss: 0.6459 - val_acc: 0.8348\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9234\n",
      "Epoch 00029: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2419 - acc: 0.9234 - val_loss: 0.6865 - val_acc: 0.8279\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9264\n",
      "Epoch 00030: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2293 - acc: 0.9264 - val_loss: 0.6795 - val_acc: 0.8332\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9253\n",
      "Epoch 00031: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2363 - acc: 0.9252 - val_loss: 0.7496 - val_acc: 0.8185\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9294\n",
      "Epoch 00032: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2235 - acc: 0.9294 - val_loss: 0.7395 - val_acc: 0.8225\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9326\n",
      "Epoch 00033: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.2154 - acc: 0.9326 - val_loss: 0.6776 - val_acc: 0.8297\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9365\n",
      "Epoch 00034: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2003 - acc: 0.9365 - val_loss: 0.7107 - val_acc: 0.8262\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9338\n",
      "Epoch 00035: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.2037 - acc: 0.9338 - val_loss: 0.6950 - val_acc: 0.8367\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9399\n",
      "Epoch 00036: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1898 - acc: 0.9399 - val_loss: 0.6798 - val_acc: 0.8390\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9416\n",
      "Epoch 00037: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1851 - acc: 0.9416 - val_loss: 0.6986 - val_acc: 0.8404\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9412\n",
      "Epoch 00038: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1823 - acc: 0.9412 - val_loss: 0.7240 - val_acc: 0.8358\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9442\n",
      "Epoch 00039: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1734 - acc: 0.9442 - val_loss: 0.6967 - val_acc: 0.8395\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9447\n",
      "Epoch 00040: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1714 - acc: 0.9447 - val_loss: 0.6692 - val_acc: 0.8470\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9485\n",
      "Epoch 00041: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1604 - acc: 0.9485 - val_loss: 0.7072 - val_acc: 0.8323\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9497\n",
      "Epoch 00042: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1599 - acc: 0.9497 - val_loss: 0.6996 - val_acc: 0.8435\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9496\n",
      "Epoch 00043: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1560 - acc: 0.9496 - val_loss: 0.6903 - val_acc: 0.8439\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9507\n",
      "Epoch 00044: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1568 - acc: 0.9506 - val_loss: 0.7273 - val_acc: 0.8376\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9471\n",
      "Epoch 00045: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1625 - acc: 0.9471 - val_loss: 0.6852 - val_acc: 0.8451\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9527\n",
      "Epoch 00046: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1459 - acc: 0.9527 - val_loss: 0.7162 - val_acc: 0.8386\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9565\n",
      "Epoch 00047: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1362 - acc: 0.9566 - val_loss: 0.6846 - val_acc: 0.8449\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9565\n",
      "Epoch 00048: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1333 - acc: 0.9565 - val_loss: 0.6823 - val_acc: 0.8474\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9585\n",
      "Epoch 00049: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1318 - acc: 0.9585 - val_loss: 0.7169 - val_acc: 0.8376\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9582\n",
      "Epoch 00050: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1317 - acc: 0.9582 - val_loss: 0.7344 - val_acc: 0.8386\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9559\n",
      "Epoch 00051: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1329 - acc: 0.9558 - val_loss: 0.7623 - val_acc: 0.8325\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9592\n",
      "Epoch 00052: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1278 - acc: 0.9592 - val_loss: 0.7032 - val_acc: 0.8498\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9624\n",
      "Epoch 00053: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1179 - acc: 0.9623 - val_loss: 0.7456 - val_acc: 0.8388\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9549\n",
      "Epoch 00054: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1405 - acc: 0.9549 - val_loss: 0.6948 - val_acc: 0.8486\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9635\n",
      "Epoch 00055: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1163 - acc: 0.9635 - val_loss: 0.7110 - val_acc: 0.8474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9613\n",
      "Epoch 00056: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1187 - acc: 0.9612 - val_loss: 0.7081 - val_acc: 0.8467\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9630\n",
      "Epoch 00057: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1179 - acc: 0.9630 - val_loss: 0.6983 - val_acc: 0.8411\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9637\n",
      "Epoch 00058: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1137 - acc: 0.9637 - val_loss: 0.7152 - val_acc: 0.8498\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9631\n",
      "Epoch 00059: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1148 - acc: 0.9631 - val_loss: 0.7954 - val_acc: 0.8321\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9664\n",
      "Epoch 00060: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1057 - acc: 0.9663 - val_loss: 0.7623 - val_acc: 0.8332\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9656\n",
      "Epoch 00061: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1102 - acc: 0.9655 - val_loss: 0.8013 - val_acc: 0.8293\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9653\n",
      "Epoch 00062: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.1120 - acc: 0.9653 - val_loss: 0.7279 - val_acc: 0.8472\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9683\n",
      "Epoch 00063: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1019 - acc: 0.9683 - val_loss: 0.7853 - val_acc: 0.8388\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9674\n",
      "Epoch 00064: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1035 - acc: 0.9673 - val_loss: 0.7876 - val_acc: 0.8325\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9663\n",
      "Epoch 00065: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.1022 - acc: 0.9663 - val_loss: 0.7438 - val_acc: 0.8411\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9714\n",
      "Epoch 00066: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 0.0913 - acc: 0.9713 - val_loss: 0.7415 - val_acc: 0.8435\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9681\n",
      "Epoch 00067: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1035 - acc: 0.9680 - val_loss: 0.7475 - val_acc: 0.8351\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9702\n",
      "Epoch 00068: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.0952 - acc: 0.9702 - val_loss: 0.7798 - val_acc: 0.8365\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9707\n",
      "Epoch 00069: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.0977 - acc: 0.9707 - val_loss: 0.7608 - val_acc: 0.8358\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9701\n",
      "Epoch 00070: val_loss did not improve from 0.62596\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 0.0964 - acc: 0.9700 - val_loss: 0.7051 - val_acc: 0.8532\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XecVOW9+PHPM7OzvXdYytI7LNLWiwVrLAlWRBM1mhtNbrwmxoSfxCSGtKuxJMao8WKLGq8NY9RoRIkiakAEBKUpHXZp23uZmfP9/fFsbyyww+zufN+v13nNzsyZc74zO/N8n3LOc4yIoJRSSgG4gh2AUkqp3kOTglJKqSaaFJRSSjXRpKCUUqqJJgWllFJNNCkopZRqoklBKaVUE00KSimlmmhSUEop1SQs2AEcrdTUVMnOzg52GEop1aesXbu2UETSjrRen0sK2dnZrFmzJthhKKVUn2KM2dOd9QLWfWSMGWyMec8Ys9kYs8kY84MO1pljjCkzxqxvWO4IVDxKKaWOLJAtBR/wIxFZZ4yJA9YaY94Rkc1t1vtARL4awDiUUkp1U8BaCiJyQETWNfxdAWwBsgK1P6WUUsfvhIwpGGOyganAxx08fbIxZgOwH/ixiGw62u17vV7y8vKora09rjhDWWRkJIMGDcLj8QQ7FKVUEAU8KRhjYoGXgVtEpLzN0+uAoSJSaYy5APg7MKqDbdwI3AgwZMiQdvvIy8sjLi6O7OxsjDE9/Rb6PRGhqKiIvLw8hg0bFuxwlFJBFNDzFIwxHmxCeFZE/tb2eREpF5HKhr/fBDzGmNQO1lssItNFZHpaWvsjqmpra0lJSdGEcIyMMaSkpGhLSykV0KOPDPA4sEVEft/JOpkN62GMmdkQT9Ex7u9YQ1Xo56eUsgLZfTQbuAb43BizvuGx24EhACLyCHA58F/GGB9QA1wpAbo+qN9fg89XjMeTjsul/eZKKdWRQB599KGIGBGZLCI5DcubIvJIQ0JARB4UkQkiMkVEckXk34GKx3Fqqa8/gIi3x7ddWlrKww8/fEyvveCCCygtLe32+osWLeLee+89pn0ppdSRhMzcR8a4ARDx9/i2u0oKPp+vy9e++eabJCYm9nhMSil1LEIuKUDPJ4WFCxeyY8cOcnJyWLBgAcuXL+fUU09l7ty5jB8/HoCLL76YadOmMWHCBBYvXtz02uzsbAoLC9m9ezfjxo3jhhtuYMKECZx77rnU1NR0ud/169eTm5vL5MmTueSSSygpKQHggQceYPz48UyePJkrr7wSgPfff5+cnBxycnKYOnUqFRUVPf45KKX6vj4399GRbNt2C5WV6zt4xsHvr8LlisQeFNV9sbE5jBp1f6fP33XXXWzcuJH16+1+ly9fzrp169i4cWPTIZ5PPPEEycnJ1NTUMGPGDC677DJSUlLaxL6N5557jkcffZQrrriCl19+mauvvrrT/V577bX86U9/4vTTT+eOO+7gl7/8Jffffz933XUXu3btIiIioqlr6t577+Whhx5i9uzZVFZWEhkZeVSfgVIqNIRMSwFO7NE1M2fObHXM/wMPPMCUKVPIzc1l3759bNu2rd1rhg0bRk5ODgDTpk1j9+7dnW6/rKyM0tJSTj/9dAC++c1vsmLFCgAmT57MN77xDf76178SFmbz/uzZs7n11lt54IEHKC0tbXpcKaVa6nclQ2c1ehE/lZWfEh4+iIiIzIDHERMT0/T38uXLWbZsGStXriQ6Opo5c+Z0eE5ARERE099ut/uI3UedeeONN1ixYgWvv/46v/3tb/n8889ZuHAhF154IW+++SazZ89m6dKljB079pi2r5Tqv0KopdD4Vnt+TCEuLq7LPvqysjKSkpKIjo5m69atrFq16rj3mZCQQFJSEh988AEAzzzzDKeffjqO47Bv3z7OOOMMfve731FWVkZlZSU7duxg0qRJ3HbbbcyYMYOtW7cedwxKqf6n37UUOmNPznIH5OijlJQUZs+ezcSJEzn//PO58MILWz1/3nnn8cgjjzBu3DjGjBlDbm5uj+z3qaee4rvf/S7V1dUMHz6cJ598Er/fz9VXX01ZWRkiwve//30SExP5+c9/znvvvYfL5WLChAmcf/75PRKDUqp/MQE6Vyxgpk+fLm0vsrNlyxbGjRt3xNdWVn6G2x1HVJTO79OR7n6OSqm+xxizVkSmH2m9EOo+ajwstedbCkop1V+EXFIIRPeRUkr1FyGVFAI1pqCUUv1FSCUFbSkopVTXQi4pgBPsMJRSqtcKqaRgu4+6nqBOKaVCWUglBdtSEESC31qIjY09qseVUupECLGkYN9ub0gKSinVG4VYUgjM9NkLFy7koYcearrfeCGcyspKzjrrLE466SQmTZrEq6++2u1tiggLFixg4sSJTJo0iRdeeAGAAwcOcNppp5GTk8PEiRP54IMP8Pv9XHfddU3r/uEPf+jR96eUCh39b5qLW26B9R1NnQ1u8RHl1GBcMWCOIh/m5MD9nU+dPX/+fG655RZuuukmAF588UWWLl1KZGQkr7zyCvHx8RQWFpKbm8vcuXO7dT3kv/3tb6xfv54NGzZQWFjIjBkzOO200/i///s/vvKVr/DTn/4Uv99PdXU169evJz8/n40bNwIc1ZXclFKqpf6XFLpgmqbP7tmpPaZOncrhw4fZv38/BQUFJCUlMXjwYLxeL7fffjsrVqzA5XKRn5/PoUOHyMw88iytH374IVdddRVut5uMjAxOP/10PvnkE2bMmMG3vvUtvF4vF198MTk5OQwfPpydO3dy8803c+GFF3Luuef26PtTSoWO/pcUuqjRO/4qaqq3EBk5Eo+nZy+BOW/ePJYsWcLBgweZP38+AM8++ywFBQWsXbsWj8dDdnZ2h1NmH43TTjuNFStW8MYbb3Dddddx6623cu2117JhwwaWLl3KI488wosvvsgTTzzRE29LKRViQmpMAQJ3Sc758+fz/PPPs2TJEubNmwfYKbPT09PxeDy899577Nmzp9vbO/XUU3nhhRfw+/0UFBSwYsUKZs6cyZ49e8jIyOCGG27g29/+NuvWraOwsBDHcbjsssv4zW9+w7p163r8/SmlQkP/ayl0oXGgORBnNU+YMIGKigqysrIYMGAAAN/4xjf42te+xqRJk5g+ffpRXdTmkksuYeXKlUyZMgVjDHfffTeZmZk89dRT3HPPPXg8HmJjY3n66afJz8/n+uuvx3HsUVV33nlnj78/pVRoCKmps0UcKivXER6eRUTEgECF2Gfp1NlK9V86dXaHTMOi8x8ppVRHQiopNF99TU9eU0qpjoRUUgCdKVUppboSgknBpUlBKaU6EYJJQS/JqZRSnQm5pKBXX1NKqc6FXFIIxJhCaWkpDz/88DG99oILLtC5ipRSvUZIJoWe7j7qKin4fF1f1OfNN98kMbFnp9xQSqljFXJJIRDdRwsXLmTHjh3k5OSwYMECli9fzqmnnsrcuXMZP348ABdffDHTpk1jwoQJLF68uOm12dnZFBYWsnv3bsaNG8cNN9zAhAkTOPfcc6mpqWm3r9dff51Zs2YxdepUzj77bA4dOgRAZWUl119/PZMmTWLy5Mm8/PLLALz11lucdNJJTJkyhbPOOqtH37dSqv/pd9NcdDFzNgCOk4ZIAm63AEeewhqOOHM2d911Fxs3bmR9w46XL1/OunXr2LhxI8OGDQPgiSeeIDk5mZqaGmbMmMFll11GSkpKq+1s27aN5557jkcffZQrrriCl19+mauvvrrVOqeccgqrVq3CGMNjjz3G3XffzX333cevf/1rEhIS+PzzzwEoKSmhoKCAG264gRUrVjBs2DCKi4u79X6VUqErYEnBGDMYeBrIwM5VvVhE/thmHQP8EbgAqAauE5GAzuZmjMHO7NH9pHAsZs6c2ZQQAB544AFeeeUVAPbt28e2bdvaJYVhw4aRk5MDwLRp09i9e3e77ebl5TF//nwOHDhAfX190z6WLVvG888/37ReUlISr7/+OqeddlrTOsnJyT36HpVS/U8gWwo+4Eciss4YEwesNca8IyKbW6xzPjCqYZkF/Lnh9ph1VaMH8HrLqa3dTUzMRFyuyOPZVZdiYmKa/l6+fDnLli1j5cqVREdHM2fOnA6n0I6IiGj62+12d9h9dPPNN3Prrbcyd+5cli9fzqJFiwISv1IqNAVsTEFEDjTW+kWkAtgCZLVZ7SLgabFWAYnGmADPVBfWEF/PTXURFxdHRUVFp8+XlZWRlJREdHQ0W7duZdWqVce8r7KyMrKy7Mf41FNPNT1+zjnntLokaElJCbm5uaxYsYJdu3YBaPeRUuqITshAszEmG5gKfNzmqSxgX4v7ebRPHD0ci33LPTnYnJKSwuzZs5k4cSILFixo9/x5552Hz+dj3LhxLFy4kNzc3GPe16JFi5g3bx7Tpk0jNTW16fGf/exnlJSUMHHiRKZMmcJ7771HWloaixcv5tJLL2XKlClNF/9RSqnOBHzqbGNMLPA+8FsR+Vub5/4B3CUiHzbc/xdwm4isabPejcCNAEOGDJnW9mI1RzPls99fRXWArr7W1+nU2Ur1X71i6mxjjAd4GXi2bUJokA8MbnF/UMNjrYjIYhGZLiLT09LSjjOqwF19TSml+rqAJYWGI4seB7aIyO87We014Fpj5QJlInIgUDHZuAJ39TWllOrrAnn00WzgGuBzY0zjmQO3A0MAROQR4E3s4ajbsYekXh/AeABNCkop1ZWAJYWGcYIuTwQQO6BxU6Bi6IgdaNarrymlVEdCcJoLvdCOUkp1JiSTgk6frZRSHQvJpNAbWgqxsbFB3b9SSnUkZJMC9NwZzUop1V+EZFKAnr1O88KFC1tNMbFo0SLuvfdeKisrOeusszjppJOYNGkSr7766hG31dkU2x1Ngd3ZdNlKKXWs+t/U2W/dwvqDXcydDThOLSJ+3O6YLtdrlJOZw/3ndT7T3vz587nlllu46SZ7INWLL77I0qVLiYyM5JVXXiE+Pp7CwkJyc3OZO3cu9hSOjnU0xbbjOB1Ogd3RdNlKKXU8+l1S6L6em95j6tSpHD58mP3791NQUEBSUhKDBw/G6/Vy++23s2LFClwuF/n5+Rw6dIjMzMxOt9XRFNsFBQUdToHd0XTZSil1PPpdUuiqRt+ori6P+vpDxMVN67H9zps3jyVLlnDw4MGmieeeffZZCgoKWLt2LR6Ph+zs7A6nzG7U3Sm2lVIqUEJ0TMENSI9Onz1//nyef/55lixZwrx58wA7zXV6ejoej4f33nuPthP5tdXZFNudTYHd0XTZSil1PEIyKQRiqosJEyZQUVFBVlYWAwbYS0J84xvfYM2aNUyaNImnn36asWPHdrmNzqbY7mwK7I6my1ZKqeMR8Kmze9r06dNlzZpWM2sf9ZTPXm8RtbW7iI6eiNsduKuv9TU6dbZS/VevmDq799Lps5VSqiMhmRR0plSllOpYv0kKR9MN1pwU9KzmRn2tG1EpFRj9IilERkZSVFTU7YKt8TrN2n1kiQhFRUVERur4ilKhrl+cpzBo0CDy8vIoKCjo1voifurqCgkLcwgLOxzg6PqGyMhIBg0aFOwwlFJB1i+SgsfjaTrbtzscp44VKyYxbNhvGTr09gBGppRSfUu/6D46Wi5XBMZE4POVBzsUpZTqVUIyKQCEhcXj92tSUEqplkI2Kbjd8fh8ZcEOQymlepWQTQphYQnaUlBKqTZCOCnE65iCUkq1EbJJwe3WMQWllGorZJNCWFiCjikopVQbIZsU7ECzthSUUqqlkE0KjYek6pw/SinVLISTQgIiXhxHL3eplFKNQjYpuN3xADrYrJRSLYRsUggLs0lBxxWUUqpZyCaFxpaCHoGklFLNQjYphIUlANp9pJRSLYVsUmhuKWhSUEqpRiGbFBrHFLSloJRSzQKWFIwxTxhjDhtjNnby/BxjTJkxZn3DckegYulIY/eRjikopVSzQF557S/Ag8DTXazzgYh8NYAxdMrtjgO0paCUUi0FrKUgIiuA4kBt/3i5XOG4XJE6pqCUUi0Ee0zhZGPMBmPMP40xE070zj2eNOrrD5zo3SqlVK8VyO6jI1kHDBWRSmPMBcDfgVEdrWiMuRG4EWDIkCE9FkB09Hiqqjb12PaUUqqvC1pLQUTKRaSy4e83AY8xJrWTdReLyHQRmZ6WltZjMcTETKC6ejMi/h7bplJK9WVBSwrGmExjjGn4e2ZDLEUnMoaYmIk4Ti01NTtP5G6VUqrXClj3kTHmOWAOkGqMyQN+AXgAROQR4HLgv4wxPqAGuFJO8DzWMTETAaiq2kR0dIc9V0opFVIClhRE5KojPP8g9pDVoImOHgdAVdVG0tIuDmYoSinVKwT76KOgCguLJTJyGFVVHZ5fp5RSISekkwI0DjbrEUhKKQWhlBTWr4dbboHS0lYPx8RMpLp6K45TH6TAlFKq9widpLBvH/zxj7B1a6uHY2ImIuKjpmZbkAJTSqneI3SSwujR9vbLL1s9HB1tT6TWk9iUUiqUksKwYeB2w7bWLYLo6LGASweblVKKUEoK4eE2MbRpKbjdkURFjdKkoJRSdDMpGGN+YIyJN9bjxph1xphzAx1cjxs9ul1SAHsEknYfKaVU91sK3xKRcuBcIAm4BrgrYFEFyqhRNim0OXE6JmYiNTXb8ftrghSYUkr1Dt1NCqbh9gLgGRHZ1OKxvmP0aKiuhv37Wz0cEzMBcKiu3trx65RSKkR0NymsNca8jU0KS40xcYATuLACpJMjkFrOgaSUUqGsu0nhP4GFwAwRqcZObHd9wKIKlMak0OYIpKioURjj0cFmpVTI625SOBn4QkRKjTFXAz8D+t4V7wcNgsjIdi0Fl8tDdPQYne5CKRXyupsU/gxUG2OmAD8CdgBPByyqQHG5mgeb24iJmagtBaVUyOtuUvA1XOvgIuBBEXkIiAtcWAHUSVKIjp5Abe1ufL7KIASllFK9Q3eTQoUx5ifYQ1HfMMa4aLhgTp8zejTs2AE+X6uHGwebq6s3ByMqpZTqFbqbFOYDddjzFQ4Cg4B7AhZVII0ebRPC7t2tHm4+Akm7kJRSoatbSaEhETwLJBhjvgrUikjfG1OATg9LjYoahssVqUlBKRXSujvNxRXAamAecAXwsTHm8kAGFjCdHJZqjJvo6PF6roJSKqR19xrNP8Weo3AYwBiTBiwDlgQqsIBJTYXExE6PQCopeScIQSmlVO/Q3TEFV2NCaFB0FK/tXYzp9AikuLgZ1NcfoLq6/XNKKRUKuluwv2WMWWqMuc4Ycx3wBvBm4MIKsE5mS01NvQiAwsJXTnRESinVK3R3oHkBsBiY3LAsFpHbAhlYQI0eDXv3Qk3rWVEjIwcTFzedgoK/BSkwpZQKrm53AYnIyyJya8PSt6vSjYPN27e3eyo19VIqKlZTW5t3goNSSqng6zIpGGMqjDHlHSwVxpjyExVkj+vkCCSA1NRLACgs/PuJjEgppXqFLpOCiMSJSHwHS5yIxJ+oIHvcqFH2tsMjkMYSHT1OxxWUUiGpbx5BdLzi4iAzs8OkALYLqbT0fbzeohMcmFJKBVdoJgXo9AgkgLS0SwA/hYWvn9iYlFIqyDQpdCA29iQiIoZQWKhHISmlQktoJ4WCAigtbfeUMYbU1EsoLn4bn68iCMEppVRwhHZSgA6PQAJIS7sUkTqKi986gUEppVRwaVLopAspIWE2Hk+adiEppUJK6CaF4cPt5Tm3bu3waWPcpKZeRFHRGzhO3QkOTimlgiNgScEY84Qx5rAxpsMLFBjrAWPMdmPMZ8aYkwIVS4ciImDWLHjuuXZXYWuUmnopfn8FxcU6c6pSKjQEsqXwF+C8Lp4/HxjVsNwI/DmAsXRswQJ7ac4lHc8AnpR0FuHhmeTn/+kEB6aUUsERsKQgIiuA4i5WuQh4WqxVQKIxZkCg4uk4gotg7Fi46y4Qafe0yxXOoEG3UFLyNhUVn57Q0JRSKhiCOaaQBexrcT+v4bETx+WC226DDRvgrY6PMhow4Du43XHs29c3L0mtlFJHo08MNBtjbjTGrDHGrCkoKOjZjX/96zB4MNx5Z4dPezyJDBz4XQ4ffpGaml09u2+llOpluns5zkDIBwa3uD+o4bF2RGQx9noOTJ8+vX0/z/EID4cf/xh+8AP46COYPbvdKoMG/YC8vPvJy/s9o0bp+IJSfZUIVFVBWRmUN8zz7HZDWJhd3G57ccbGRQQqKuy6ja/x+8Hjset7PO2XsDAoKYEDB+DgQXtbU2OvAty4xMVBXZ2Npbra3tbXg+M0LyI2npbL7Nlw5pmB/YyCmRReA/7bGPM8MAsoE5EDQYnkP/8TfvUrO7bwevv5jiIissjIuJoDBx5n6NBfEB6eGoQglQoMv98WYkVFUFxsb8vbTIwvYgux6mpbwFVXg9drC06Xq3mJjoaYGIiNtbc+n500oLTU7qO01BaujUt5OSQnw8iRzUt0tD1SfPNm2LLFnl/qctntNS4ul42jcamrs/vy+5tvjbEFqctlbx3H7tPvP7Gfb3g4REXZ99rB0GWTtp8l2Jh9vubXLVzYh5OCMeY5YA6QaozJA34BeABE5BHs5TwvALYD1cD1gYrliGJibEvhjjvg889h0qR2qwwevICDB58kP/9Bhg1bdOJjVH2Ozwe1tbbQ8npb/+CNgcrK1oVjTU1zLdHvt69pWVgXF9vCuL7eLl5vx0dT+3x28XrtEhYGSUnNS2wsHD4MeXmQn29rssdSULpctrDqqqBrye22teSEhObb7GwoLITXXrMxtZSaCuPHw9y59vOqqmpeHMduIyrKLhERrWv7breNy+9vXlyu5n0nJkJ8vN1uy2TSWAA3LsbYWn18vH1tfLzddsvPt6MlMREGDLBLUpLdjuPYVkdpqb2NiLBFT2Mi9Xg6/+wa38uJYKS7/9FeYvr06bJmzZqe33BJCQwZYo9I+utfO1zl888voqzsI04+eQ9ud0zPx6CCqrbWFsBlZc2129LS5tpz463jNBcUcXH2x5yfb6/wuncv7NtnC/lOTn85ai6XrU0nJ9sCvbGbIjy8ubujkUjrbo2wMBtHSYldiottMkpPh6wsGDTI3mZk2O2npNjb+Pjm2mqjiAhbgDUWxG536/36/c1dIVVVdj9uty0UExNtwdcy1rbKy+0R4pWV9qDAtLSe+fyUZYxZKyLTj7ieJoUWfvxjuP9+214dNqzd02Vl/+bTT2czcuQDDBp0c2BiUN0mYmtl1dW2IDlwoLn2m5fXXMA3LtXVzYVpY6FZXm5rqgUFtiDrSkKCLTBdLlvTq6hovsx3UpI9XmHIEHublASRkbbwjIy0+2rZX+w4toBPSGheGgvaxtZEYw0/IaF9Aa3U0dKkcCz277fJ4IYb4MEHO1xl3bpTqK3dycyZWwkL67sXn+tN6uqaC+7SUltI79vXvubd2Jfd8razJrXHY2uajc3+hARby/X5Wne/xMba9dLSbHdFSkrrAcGEhObHOmreN3YjREUF9jNS6nh1NykEc6C59xk4EK65Bh5/3I4vpKe3W2XkyN+zbl0uu3b9VI9EaqOy0hbgLZfS0tb9uvX1ttA/fLh5qa7ueHtut+3eGDzY9s227LqIirLdEVFRzX2yAwY0d4ekpp6Y2nVjP7ZS/YV+ndtasACeeAIeeAB+85t2T8fHzyQr6yby8x8iI+Ma4uNnBiHIE8vrtT1q+/bBoUN2OXiw+XC7/fvtUtHm0hONg3QtD6nzeGyBnZ5uJ6pNS7M18ZaDj8nJthtmwAAtcJU60bT7qCOXXw7/+hfs2WP7H9rw+cpZvXocHk8a06atweXquyWXiO2D37/fdtGUl9vC/fBh2LgRPvvMHhpYX9/6dVFR9jLXWVm2gTVwoC3EBw9urt0PHGj775VSwafdR8fjttvg5Zdh8WI7+NxGWFg8o0b9iU2bLiMv736GDGm/Tm9UWGgL+rZLWVnH6w8caI/OPeccezt8uD1KJSPD9sV3dSSJUqpv0pZCZ84+21aRd+2yx+K1ISJs3HgxJSXLmDFjE1FR2YGPqRsaD+vbudOGvnOnvY7Qxo2226dRUpIt6CdMgIkTbXdN43HYcXHNx7Sr4Kisr6SgqoDsxGxMD2dfETniNv2OnypvFZX1lVTUVVDlrWJw/GDSYo79ONGthVtZu38t49PGMzF9Ih5365H7Ol8dmws2k1+Rj8flweP2EO4Ox+PyEOYKI8wVhtvlxm3cxEfEkxGbQVibVnq1t5qthVvZWriV7MRscgfl4jLHNrjkiMOO4h0MjBtITHjwDkGv8dbw1va3eGnzS3x19Ff5+qSvH9N2tKVwvBYutFXkZ56Bb3+73dPGGEaN+hOrV49n27abmDTpHz3+4+1KRYU97HLnTli/Hj791C47d7ZeLynJniV6wQW28G9MBAMGHHtN3+f4+PzQ5xyoPMCIpBEMSxpGuPvI/UQiwtbCrcRHxJMZm4nb5T7ia47E7/gpqS2hvK6czNhMoj3RXe6/J/9HjjhsL97OloItlNeVU+WtoqreFqS1vlrq/HXU++up89XhMi7SY9JJj0knIzaDlKgUanw1FNcUNy155XlsK97G9uLtHKw8CEBmbCbnjjiXr4z4CucMPwe/+Fmdv5qP8z5m9f7VbCvahl/8+B0/jjgIQlp0GkMThzI0YShDEobgNm62F29ne8l2thdvJ788n/SYdLITsxmaOJTshGy8jpe9ZXvZU7aHvWV7m/bf1sC4geRk5jAlYwrZidn4HB8+x4fX78URh4zYDIYkDGFw/GAGxQ9iW/E2lmxewpLNS9hUsKlpO5FhkUzNnMq0AdMoqytj/cH1bCncgs/p/skdLuMiMzaTrLgsEiMT2V68nd2luxGaK7qD4gcxb/w8rphwBbOyZjX9/0UER5x230ERYXX+ap7f+DwvbX6J/Ao7886QhCGMTR3LuNRxjEwe2fTZDk0cSrQnmu3F29lcsJnNBZvZWriV4pripqRaVV9FuDu86fVjU8cyKmUULuNq+n7U+esQkaakF+YK43DVYf629W/848t/UFlfSUpUCrmDcrv9+RwrbSl0RgSmT7eH1Gze3PpMnRb27bufHTt+2OPnLtTUOrz17zyq9g9z9h5YAAAgAElEQVRm717D7t12iGPfPjsG0HYaghEjYOpUyMmxJ/4MH26Prk1M7OotCnX+uqZCzOv32h+448Xr97b6UlfUV7Dx8EZW5a1i7YG1VHubDxlyGzfZidmMSR3Dt3K+xaXjLm1X+O4t28u3X/s27+x8p+k1A+MGMjhhMHHhcbiMC5dxNf0oGmuJ4a5wPG4PNb4aKuoqbM21voLS2lIKqwsprinGEadpPwNiBzA8aTjDkoZhMOyv2N+01PnrOG3oaXxlxFc4b+R5jEsd1xSnIw4lNSXsr9jPjpId7CjewY6SHewt20uUJ4qkyCS7RCVxuOowaw+s5dMDn1JR32Z0vYHH5SEiLIJwdzgR7gj84qewurBVrC0ZDJmxmYxMHsmo5FGMTB5JYmQiK/au4O0db1Nc03oWerdxMzljMhPSJxDuCm/67ESEQ1WHmgr4xtelRKUwKsVuNysui8NVh9lTtofdpbvZW7YXt3E3FXJDE4aSFZdFfEQ8seGxxEXEERUWxa7SXaw/uP6oC3CD4bShp3H5+Ms5ZcgpbC3cyur81Xyy/xPWHVhHYmQiUzKmkJOZQ05mDtmJ2fgdP/X+eryOl3p/PT7Hh9/xNyWhsroy8svzya/IJ688j9LaUkYkj2B86njGp41ndMpoPjv0GS9ufpG3tr9Fvb+e2PBYALtdvxdBiAqLIiU6hdToVFKiUthRsoPdpbsJd4dz3sjzuGDkBRRWF7KlcAtbCrewtXBrq+9+4/trTEQGQ3ZiNmkxacR4YogJjyHGE9PUgtlRsqPT70BHUqNTuWTsJcwbP4852XPata6Ohp6n0BNeegmuuAL+539sy6GDWqaIn40bL6Oo6HUmTXqNlJQLj2lXIvDJZ2X8+e13eGfPG+RH/RNiD8HhCbD6v0ndfzXDsmKbBnIHZPnwpq6jLm4Lk0elMDw9k8zYTNJj0qmoqyCvPI+88jz2le9jf8V+DlYebFoOVR2ivK6cyvrKo6qZhbvDOWnASczKmsWsrFkMSRjCzpKdfFn0JduKt7E6fzW7Sndx8qCTufucuzllyCmICI+ue5Qfv/1jBOHnp/2c+Ih49pXtI68ij31l+6j2VuOIg19sbbex5tlYIHj9XiLDIomLiLOFVHgcCZEJpEWnkRqdSmp0KnHhceRX5LOzZCe7Snexs8Q2mbLishgYN5CBcQMBWLZzGVsKtzQ9FxMeQ1F1ESW1Je1+rEmRSQxJGEKdv46SmhJKakuo99cTGRbJlIwpTBswjWkDpzEpfRJJUUmtCoGOWkGOOBRVF3Go6hCF1YXEeGJIjkomOSqZhMiETrs5/I6ftQfWsmznMiLDIpmVNYupA6Z22Spq1Pg/TozsvHbgd/wYY46qm6XOV0dBdUFTN4/HZQurA5UH2Fe2j71le9lXvo+MmAwuHnsxGbEZHW6np1tvHSmtLeW1L15jzf41hLnCbGXDHY7buKmor6CopojC6kKKqotIjkpm3vh5XDT2og4/M0ccCqoKmlpUe0r3UF5XzuiU0YxPG8+Y1DFd/l/qfHVsL97OjpIdAES4I4gIiyDCHYExBr/jxy82+UWFRTEja0a7LrJjpUmhJ/j9MH++HXT+r/+yh6l2cIyk31/Fp5+eRnX1Fwwf/0/e2bedKE8U5488n4TIhHbr1/pq+TjvYzYd3sqKz3ewZud29lbuwJuwGdw+XPWJjJDzODn7JNbUPs/mknUkRCRwfc71ZMZmsnzPcj7c+yGV9ZXdehsu4yItOo3MWJs4MmIzSIhIaCpgY8NjiQyLxOO2fbeNP/QYTwyx4bFNBd2QhCFEhLUfX2nkc3w8tf4p7lh+B/sr9nPRmIuo8laxbOcyzhp2Fo/NfYzsxOxuf/yBsrdsL0u3L+Xd3e/iiENqVCop0SmkRKWQEZvBiKQRjEgeQXJUcqvXiQg1vhrC3eE99kNVvci+ffakmTFjgh1JQGhS6CmOAz/5Cdx9N5x7Lrz4oh2RbUFEeHfH37nv3at591ANdY79TD0uD3Oy53Dx2IuZlD6J93Z+yOsb/8WG4o/wUmtf7IvAlA4jLWwEJ2VN4vpTL+DSGSc3FToiwsq8lTy4+kFe2vwSPsfH+LTxnD70dOZkz2FKxhRKa0s5VHXItgIqDxEfEc+g+EFNS0cDcoFU7a3m/lX3c9eHdyEI9517HzecdMMJHXNRqpVXXoGHH7bnHs2a1f75Z5+F737X9gZs2NDhNDdA83SxLWfni4uzzfdeTpNCT3v8ceS73+H92YNYdesVFEY6FFQXUFhdyBeFX7CjZAdx4TGckVrHpcOGM2zkQ7y88W2WbHyV/XVfNm/n4GTYdRYJxWdy5oQpXHVhFuef5yI29sghNPZJp8e0P9O6NyqpKcHn+I7riBWljtuBA3a61dJSW+h/73u2Szg+3hbqN98MTz4J//Ef9jC9CRNgxYr2vQIlJXDeebB6devHXS74/e/h+98/tqM3HMdWNquq4NJLA3bYnyaFHlTtrebZz57lgXfvZGO1vfpaNB5S4zNJjUljYNxALh93OZePv5xtmz/mwQffZe3aq/j88/H4/YaIrK2Mmf0FZ446mdOnpzN9uj3pSyvOql9reaWYYBGBSy6BpUvhww/hqafsvGYDBsBPf2r/3rrV/v2LX8CSJXDVVXaam1/+snk71dW2p+CTT+z4YkpK88UdXngB/v735jnTjuaMzc8+sy2UlSvt/fBw+NrX4Oqr7SGDPXj2Z3eTAiLSp5Zp06bJiVJZVyk/WfYTSborSViE5DySI08s/Z2UX3S+nW59+HCRV16R8jJHnnxSZM6c5pnYx479WG644SX517+8Uvvo0yKjRols3XrCYlcqaLxekcWLRbKyRGbPFqmqCl4szz1nf5D33NP82Mcfi0yZYh/PyBB5553Wr/nmN0VcLpEVK+z9+nqRCy4QMUZkyZL2+/D7RW6/3W7v9NNFCguPHFdFhciPfiTidoukpor85S8iq1eL/OAHIunpdluZmSLr1x/rO28HWCPdKGODXsgf7XKiksLa/WtlzJ/GiFlk5PIXL5cP9nwgjuM0r/D223Jw9KnyI+6RGHe1gMjIkSK/+pXIrl0i+/Y9IO+9h2z54GJxEhPtRz10qEh+/gmJv1eoq7Nf/t4oP19kyBCRwYNF5s4V+cUvRP7+d5GCgmBH1nf5/SIvvGArQCCSk2ML0rlzbaJoq6pKZOFCkbfeCkw8hw/bAnfmTBGfr/VzXq/ISy+JHDzY/nXl5SIjRtjvR1GRyNVX2/fzv//b9f7++leRiAhbWVy0yCaihx8Weeopkccft499+9siX/lKc8F/4412Hy3V14u88Yb9bqan91hlUpPCMfI7frnno3vE8yuPZN2XJe/ufLfdOocOifz4xyJRUY64jF+uNn+Vj+bcLo7fabXenj13S/7XEMdtxHn6KZHYWJFJk0RKSgL6HnoFr1dk1iyRCRM6LhCCyeu1NbroaJErrxQZO9YWXiCSnCzyxRfBjrDn1NeLVFYGdh95eSJ//KNNAiAycaLIq6+KOI7Igw82F34tK1W7dzevHxYm8uKLPR/X/PkiHo/Ixo1H/9qPP7ZxZWXZGH/zm+69buVKW/lrfQG35iUjQ2T6dJHLLxf597+73tYXX9ikMGiQ/byOkyaFY7C3dK+c/fTZwiLk0hculcKq1s1Ar1fkl7+0ZYnLJXLNNQ3lx1132Y/y8cdbb3DdOnGMkb2XI1u33iD+pf+0X9LTThOpqQnY++gV7rmn+Yfw2GPBjqa1n/3MxvXUU82PVVaKvPeerVmOHNm9LoBjUVsr8sgjIqecIvLzn4uUlgZmPyK2O2L0aJG0NJHlyztfr6Tk6BNHYaHI/ffb7qHG//OkSSLPPNO+Vv6Tn9jnf/1re7/xc05IsC2LU06xP6innz66GLryt7+13uex+J//sdv4wQ9aJ7Tu8HpFyspEDhwQ2b5dZOdO23I+WuvXiyQm2pbL/v1H//oWNCkchYq6Cvn5uz+XqN9ESdRvomTxmsWtu4rE/l9zc+0ndsUVbVp0fr/IGWeIxMSIfPmlfcxxRE45RZy0NNn16a3y3nvImjXTpebJe+1GLr20/Y+nv9i2TSQy0nYbzJxpm+G1tcGOylq61LYKrr++4+c/+sh2AZx2WvsfcWmpyIIF9rUvvWR/9N1VWSny+9+LDBwoTX2NIJKUJHLnnUculB1H5PXXRZ58UuTDD23XSGcFlddr+zHdblvLHD3a1nofeKD1a6qqRO64w/6vUlJs8vb7jxzH88/bQr0xEfzqVyJbtnT9mmuvtetffbWNa9y45hZZZaXImWfa/8vixc2v2bTJfjZnn22XSy8Vue46W0j/7nf2f3noUPN+9u8XeeghkbPOsvvIybEtpWPlOCKffnrkzyTQVq60ZcvEicdVWdGk0A0+v08eX/e4ZN6bKSxCrlxypewq2dVqHcexFcrYWFuxef75Tja2b5/9gU+fbr+I//d/9uN99FERETl06CX54INkef/9SCn5xaX2ufHjbT9ldwbiSkpErrrKNok/+aR7b3DDBpFLLrExbdjQvdccL8exCTI+3nYrvPOOfa8PPHBi9t+V/Hxba54woevP/Nlnbczf/KZ9P45jk8CAAbZGm5QkTd0eZ51lC/svvui4kP78c5HbbmsuRM84Q2TZMrvuunUiF14oTd0Kf/xjx8mzpMT+39t2RSQk2KT7rW/ZGN55R2TtWpGTT7bPX3WVSHGxTWZz59rHrr1WpLravp8hQ+xj8+fb2jrYms/atR1/LgcO2O8T2O/UunXd/+zr60XOPde+du7c9gm1utoO5oLt0mtMmmAHhU8+2f7fBg+2362Wn8OAAc3jFyAyZowd+O1P43f/+petrHznO8e8CU0KR+B3/DLnL3OERUjuY7ny773t+/dqauz3E0ROPVVkz54jbHTJEmlqbmZliUyb1qo1UFu7XzZsOF/eew/ZfedE8U+ZKE392LfdJrJ3b8fb3bTJDt6FhTX/IM4809aUOiqINm+2zZnGgiM93X6hHn746JvBR+vRR6XVoJzj2MOyMjJ6rm+7trbrpnhNje2yuPJKe/voo/ZHdfrptsa1efOR97FokX0fCxaIfPWr9u+pU21C9npFPvjA/s/Gj28unEaMELn5ZpF//EPk7rubj3Bxu21B+NFHHe/ro49ssgBbUD/2WPM4zEcf2T5qt1vkt7+1LdE33rBdNzfdZL8HjYOWLZPFs8+23off3/yekpPt7eTJIu+/b59vrP2kp9vEd/XVtpvtzjttQv/d72wyjIiwfx/LOFFVlcg//9l5zbu21n5vIyJEzjvPfl/z8jpet6hI5N13bTK89lr7+f361/a30l+tWHFcB25oUjiCV7e+KixC7v7w7nZdRSK24nLuubby8etfH0VPz3/+Z/OPs4NCwHEcycv7s7z/frSsWjlCat950TaLXS77w7/sMtvn2hjTyy/bZkpGhi2IyspsgTNggDR1Q/zHf9gfxXnn2Wa2y2Vf87Of2ZrioUP2ObA1veLiY//gupKfbwukOXNa//A/+sju+84727+mpqbrAt5xbHfU00/bQnDGDDsuk5xsD+Nr+7/btcvWYkEkO9sm0pYF5jPPdO+9OI7I179uXxMdLXLffZ0XhDt32m6LCy8UiYpq3tesWSJ/+lPrLo6uLFtmXwO2EvC979nvxLBhtguhK4cO2df/7/92XrkQsQPAOTl2ALij91NSYhNbfHxzzbtxOfnkrruJekp/7VYNMk0KXXAcR05+7GTJvj9bvP72P4yqKlu2GiPyxBNHufGKCluj/N73ulyttHSlrFiRIP/+9xCpqtpmC7P/9/+aa3ETJtj+08bCpW2NqbbWDmx/7Ws22FNOsQVmTo6t3bY9tNLvF7n3XltIDh5suyo6OhyvsND26950k8iaNd1/3z6fyEUX2f7pbdvaP3/hhbam2XjkVVWVrfnGx9vXzJlj+7eXLbNdMY8/bmurjUd/gK3lz5lja+iNA5xnndW8v3/+035+8fEir7xiH/N67Wf77ru2L/5o1NTYWvLRHPlRXW1bJY1jS0fLcWzBPbGhFfn1rwd2MPpIsVRX2+/Svn3B71tXx0WTQhdW7F4hLEIe/PjBds9VVdlyxhhbET0m3fzxlJevlQ8+SJGPPhoglZUNzd7qajuYeNJJ9t9zww09O0i7enXztl0u2xz6y19s18H55zfXrBtvL7nE9ot3Zd0627fd9iShlj791D5/++22O6dxwPWii0R++EPb1eZyta6Zpqfb/u4//1nks89a1yD9fvt4Y1K54gr7T5s8ueOk1Nf4/fZohkB396mQoUmhCxc8e4Gk3p0qVfWtBxurqpoPgujJo+O6Ulm5UT76KFM+/DBVystbDNw5TmBriBs32gI6O7u5EB4yxLZW1q2z+160qLkb4aqrbF/27t3NBVVZmcj3v28L8/R024/dVSE2b17zvnJzbXdYS2Vltrb/6KO2b7g7BWJeXvPg57XXBvfsWaV6MU0KndhwcIOwCPn1++2PX77xRlu+dbfbuadUVX0p//73YFmxIl6Kipae2J07jsiqVXbpqBAuLLRnnUZHNxfosbG2ZTBggE0Y3/te907I27HDDri+/HLP14D37dNatVJd6G5SCLkJ8a7+29W8+sWr7L1lL0lRzbMRbtgAJ51kJzr8wx96ItKjU1u7j88/v5Cqqs2MHv0IAwe2vwRoUJWX2w9p06bmxeWys03OnBns6JRSR6DXaO7ArpJdPL/xeW7JvaVVQhCBH/7Qzlh7xx3BiS0ycjBTp37Ipk1X8OWXN1Bbu5Nhw36DOcaLjve4+Hg49VS7KKX6rV5S4pwY9628D5dx8cPcH7Z6/LXX4L337Ey5AZrKvFvCwuKZNOl1Bgy4kb1772Tz5qvwekuCF5BSKuSETFI4XHWYxz99nGsmX0NWfFbT4/X18OMf22twfOc7QQywgcvlYfToRxg+/G4KCl5i1aqh7Nx5O/X1BcEOTSkVAkImKbyz4x28fi8LZi9o9fiDD8L27XDffR1efjkojDEMGbKA6dM/JTn5PPbuvYtVq7LZvv1H1NUdCHZ4Sql+LKQGmvPL81u1EgoKYNQoexW+N9/sqQh7XlXVZvbuvZNDh/4PYzxkZl7H4ME/Jjp6ZLBDU0r1Ed0daA6ZlgLQKiGAvfpeZaVtJfRmMTHjGTfuGWbN+pLMzOs4ePBJVq8ew6ZNV1JRsT7Y4Sml+pGAJgVjzHnGmC+MMduNMQs7eP46Y0yBMWZ9w3LCjsOsr7fX6r7uOhg37kTt9fhERY1gzJhHyM3dzeDBCygufpO1a09i27ab8fkqgx2eUqofCFhSMMa4gYeA84HxwFXGmPEdrPqCiOQ0LI8FKp62PvsMamvhK185UXvsORERAxgx4i5yc/eSlXUz+fkPsWbNJIqLlwU7NKVUHxfIlsJMYLuI7BSReuB54KIA7u+orFplb3NzgxvH8fB4Ehk16o/k5KzAmHA+++wcvvjiBrzeomCHppTqowKZFLKAfS3u5zU81tZlxpjPjDFLjDGDAxhPK6tWwcCBMGjQidpj4CQmnsL06esZPPj/ceDAE6xcOYQvv7yJ6urtwQ5NKdXHBHug+XUgW0QmA+8AT3W0kjHmRmPMGmPMmoKCnjlef9Uq20owpkc2F3RudxQjRvyOGTM+Iz19PgcOPMbq1aPZuPESyspWBjs8pVQfEcikkA+0rPkPanisiYgUiUhdw93HgGkdbUhEFovIdBGZnpaWdtyBFRTAjh19u+uoMzExExg79glyc/cwZMjtlJau4NNP/4PNm79BXV3+kTeglAppgUwKnwCjjDHDjDHhwJXAay1XMMYMaHF3LrAlgPE0+fhjeztr1onYW3BERGQyfPhvOPnkvQwd+nMKCl7m44/HsHfv3ThOfbDDU0r1UgFLCiLiA/4bWIot7F8UkU3GmF8ZY+Y2rPZ9Y8wmY8wG4PvAdYGKp6WPPwa3G6Z12C7pX9zuGIYN+xUzZ24mKeksdu68jU8+mURBwSv0tRMXlVKBF1JnNDc65xwoKoJ163ooqD6kqOifbN/+Q2pqviAubjrDhv2WpKRzMP1lcEUp1SE9o7kTfr9tKfTH8YTuSEk5nxkzNjJmzBPU1xfw2WdfYf36ORQWvobPVx7s8JRSQdZLpoA7cbZuhYqK0E0KAC5XGAMGXE9Gxtc5cOAx9uz5DRs3XgS4iY+fSWLimSQnn0NCwmnaglAqxIRcS6E/nLTWU1yuCLKybiI3dzdTprzLkCF2JpK9e+9i/fo5rF07jYKCv+vYg1IhJORaCqtW2QvpjBoV7Eh6D5crgqSkM0hKOgMAn6+CgoIl7NnzWzZtuoSYmClkZ99BaurFvedKcEqpgAi5X3h/O2ktEMLC4hgw4HpmztzK2LFP4zg1bNp0GStXDubLL2+iuHgZjuMNdphKqQAIqZZCebm93vy8ecGOpG9wucLIzLyGjIyvU1DwMocPv8DBg39h//6HCQtLIiXlQpKTLyQ5+Vw8nuRgh6uU6gEhlRQ++QREdDzhaBnjJj39CtLTr8Dvr6a4+G0KC1+hqOgfHDr0V8BFfHwuKSkXkJFxDZGRQ4IdslLqGIVUUmgcZJ45M7hx9GVudzRpaReTlnYxIn7Kyz+huPhNiov/ya5dP2PXrl+Qnj6PQYNuJT5+RrDDVUodpZBLCuPGQWJisCPpH4xxk5CQS0JCLsOG/Yra2r3k5/+J/fsXc/jw8yQknEJm5reIi5tBdPRYXK6Q+rop1SeFzK9UxJ60duGFwY6k/4qMHMKIEfcwdOgdHDz4BHl59/PFF98CwOWKIjY2h7i46SQmnkFi4hl4PJqdleptQiYp7NplZ0fV8YTACwuLY9CgH5CVdTPV1V9QUbGGioq1VFau5cCBx8nP/xPgIi5uOklJZxMbm0NExCAiIgYRHp6Jy+UJ9ltQKmSFTFLQk9ZOPGNcxMSMIyZmHJmZ1wDgOPWUl39MSckySkqWsXfv7wB/y1cRGTmc9PR5ZGRcQ0xMR1dwVUoFSshMiFdebhPDmWdCWMikwt7P56uktnYXdXV5TUtFxWqKi98B/MTGnkRGxjUkJ59LdPQY7KW/lVJHq7sT4oVMUlB9S339IQ4deo5Dh56hstJOZ+t2xxIXN524uJnExU0nNjaHqKgRepa1Ut2gSUH1G9XV2ygvX0l5+WoqKlZTWbkeEXtGtdsdS0zMZGJjp5KQcAqJiacTETHgCFtUKvRoUlD9luPUUVW1icrK9S2WT/H7KwGIjh5LYuIcEhJOJzHxVCIisoIcsVLB192koL3rqs9xuSKIizuJuLiTmh5zHB+VlZ9SWrqc0tLlHDr0LPv3PwJAZORwEhJOJSZmAnV1+dTW7qCmZie1tXuIjBxKYuJpDQnkNCIiBgbrbSnVK2hLQfVLjuOjqmoDpaUfUFa2grKyD/B6C3G7Y4mMHE5U1AgiIgZTU7ONsrIP8fsrAIiIGEJ09Fiio8c0LOOIj8/F7Y4O8jtS6vhoS0GFNJcrjLi4acTFTWPw4FsQEXy+UsLCEttdOKg5gbxPRcVaqqu/4MCBJ3CcqoZtRZKYOIfk5PNJTr6AqKhh+HyleL0l+HwliPiIjZ2K2x0ZjLeqVI/SloJSHRAR6uv3U1n5GSUlb1NU9CY1NV92ur7LFUlCwqkkJZ1NUtLZREeP1yShehUdaFaqh9XU7KCo6J94vYV4PEmEhdlFxE9Z2fuUlCyjqmpj0/pudzzh4el4PBl4PKmEhcXjdscTFhaH251AVNRIYmImEBU1Us/iVgGn3UdK9bCoqBEMGvTfHT6XlnYxAHV1BygtXU5t7S7q6w/h9R6mvv4QtbU78fsr8PnK8fvLEfE1vdYYD9HRY4iIGIrbHY3LFY3bHY3bHU9s7BTi4mY0nI+hV4ZSgadJQakeFBExgIyMq7pcR0Tw+6uoqfmSqqpNVFVtorp6E3V1+3GcGvz+ahynGp+vtOl8jLCwJOLiphMePrDhZD0XxrhxucIJC0vG40nB40khLCwZY1yI+BHxIeLD5ytvdca4z1dCQsKppKVdQlzcDD35T7Wi3UdK9VKO46WqahMVFZ80LV5vCeAg4gccHKcWn68MOPLv2ONJJSJiEC5XFBUVnyDiIzx8IKmpF5GUdBaxsTlERg7TJNFP6ZiCUiFCxN9wJFQRXm8xIBgT1rC4cbtjCQ/PajXw7fWWUFT0DwoL/05x8Vs4TjXQeIb4FGJixjeMmdhxELc7rqFrKwKXKxJjIhrGQVp2aQmO40WkHhEvjlOPyxVFRMQAwsMH4PGkdppwRByKit4kP/9P1NbuYeDA7zBgwLcJC4sL3AcXYjQpKKW6xe+vpapqI1VVGxrODt9AdfWX+P1lOE5tD+7JTUREFrGxk4mNzSE2Nofo6HEUF79Nfv6D1NbuIDw8i8jIbMrLPyIsLJGBA79HVtbNRERk4jhe/P5K/P5KRLwtEl8YxoQTFhavrZwuaFJQSh03x6lvGCAvw3FqcJw6HKcWx6lrGu9oyZhwXC4PxoRjjAfHqaKu7gD19Xaprd3TkHS20nLK9Pj42Qwa9H1SUy/B5fJQXr6affvuoaDgZRrHT0TqjxCtu2FsJQ2PJxWXy9Oq5WJMRMNU7pOIiZlIdPQ4ROqprz+M11tAff1hfL7ShvdZg+PUIuInOnossbFTiYmZ2KcPM9akoJTqtfz+moZB9o3Exk4iLm5ah+tVV2/j0KGnEfHhdsficsXgdsdiTBjQOJjux3Fq8XqL8HoL8XoL8HoLEPFjjAdjPLhcHvz+KqqqNuHzFXcrRpfLJoDG1pIxYURHj8PjSWmRHGsR8TXsI7whKUYQFTWK+PiZxMXNICZmEsa4qaraTFnZh5SXf0Rl5XrCwwcSHT2O6OixxMSMw+2Ob2gJVeD3V+I4tS22G4HLFU5k5HCio0ce02euSUEppdqwJyUeoqpqI9XVW3G7o/B40hvOJ0knLCwRl2NotOUAAAf3SURBVCsKlysCYwwiDrW1u6io+JTKyk+prFyP31/ZNLZi1wtrapE4Tj2OU90q+TSOwfj9ZQCEh2cSG3sS9fWHqK7e2nTmfHcMHnwbI0bcdUzvXc9TUEqpNowxRERkEhGRSXLy2d1Y30VU1AiiokaQnn55t/cjItTW7mqY7v0T/P4qEhL+g4SEUxqO8DIN6znU1eVTXb0Fx6nB7Y5tWOJwuSKbBuxtd1094eGBnxZek4JSSvUwYwxRUcOJihpORsaVXaznIjJyMJGRg09gdF3ToXqllFJNNCkopZRqEtCkYIw5zxjzhTFmuzFmYQfPRxhjXmh4/mNjTHYg41FKKdW1gCUFY4wbeAg4HxgPXGWMGd9mtf8ESkRkJPAH4HeBikcppdSRBbKlMBPYLiI7xZ518jxwUZt1LgKeavh7CXCW0akglVIqaAKZFLKAfS3u5zU81uE6YucSLgNSAhiTUkqpLvSJgWZjzI3GmDXGmDUFBQXBDkcppfqtQCaFfKDlwbeDGh7rcB1jz1tPAIrabkhEFovIdBGZnpaWFqBwlVJKBfLktU+AUcaYYdjC/0rg623WeQ34JrASuBx4V44w78batWsLjTF7jjGmVKDwGF8bLH0tZo03sDTewOrP8Q7tzkoBSwoi4jPG/DewFHADT4jIJmPMr4A1IvIa8DjwjDFmO1CMTRxH2u4xNxWMMWu6M/dHb9LXYtZ4A0vjDSyNN8DTXIjIm8CbbR67o8XftcC8QMaglFKq+/rEQLNSSqkTI9SSwuJgB3AM+lrMGm9gabyBFfLx9rnrKSillAqcUGspKKWU6kLIJIUjTc4XbMaYJ4wxh40xG1s8lmyMeccYs63hNimYMbZkjBlsjHnPGLPZGLPJGPODhsd7ZczGmEhjzGpjzIaGeH/Z8PiwhskYtzdMzhge7FhbMsa4jTGfGmP+0XC/18ZrjNltjPncGLPeGLOm4bFe+X1oZIxJNMYsMcZsNcZsMcac3FtjNsaMafhsG5dyY8wtPR1vSCSFbk7OF2x/Ac5r89hC4F8iMgr4V8P93sIH/EhExgO5wE0Nn2lvjbkOOFNEpgA5wHnGmFzsJIx/aJiUsQQ7SWNv8gNgS4v7vT3eM0Qkp8Vhkr31+9Doj8BbIjIWmIL9rHtlzCLyRcNnmwNMA6qBV+jpeEWk3y/AycDSFvd/Avwk2HF1EGc2sLHF/S+AAQ1/DwC+CHaMXcT+KnBOX4gZiAbWAbOwJ/6EdfQ9CfaCnQXgX8CZwD8A08vj3Q2ktnms134fsDMo7KJhbLUvxNwixnOBjwIRb0i0FOje5Hy9UYaIHGj4+yCQEcxgOtNwHYypwMf04pgbumLWA4eBd4AdQKnYyRih930v7gf+H+A03E+hd8crwNvGmLXGmBsbHuu13wdgGFAAPNnQRfeYMSaG3h1zoyuB5xr+/v/t3c+LVWUcx/H3JwoxJ5wCg0goLKgIxFy4SAvJnYS0MPphLqKlG3chZUF/QK6kXLQwGiIsLWjpFAMuyswmM4WKCJooZ9MvgyKmT4vnuac71wmnYZz7xHxeMNx7zzlz+Vw4d77nPGfO91nUvMulKPzvuRwGNPevYpJGgLeAvbZ/6V/XWmbbMy6n3msprd3vHHKkfyXpQWDa9sfDzvIfbLG9kTJMu0fS/f0rW9sfKDfvbgResn0P8BsDQy8NZqZeR9oBHBlctxh5l0tRmE9zvhZdkHQTQH2cHnKeWSRdQykIY7aP1sVNZwaw/RPwPmX4ZbQ2Y4S29ovNwA5J31DmInmAMv7dal5sf1cfpylj3Ztoe3+YAqZsf1hfv0kpEi1nhlJ0T9u+UF8vat7lUhS65ny1yj5KacbXul7DQOrjO0PMMkudDOkV4LztF/tWNZlZ0hpJo/X5Ssr1j/OU4rCzbtZMXtv7bK+1fStlf33P9i4azStplaTres8pY95naXR/ALD9A/CtpDvqom3AORrOXD3GP0NHsNh5h33BZAkvzGwHvqCMIz8z7Dxz5Hsd+B74k3IE8xRlDHkc+BI4Dtww7Jx9ebdQTlPPAJP1Z3urmYH1wCc171ngubp8HXAS+IpyOr5i2FnnyL4VeLflvDXXp/Xn8953rNX9oS/3BuBU3S/eBq5vOTOwijK9wOq+ZYuaN3c0R0REZ7kMH0VExDykKERERCdFISIiOikKERHRSVGIiIhOikLEEpK0tdfxNKJFKQoREdFJUYiYg6Qn6vwLk5IO1WZ6FyUdqPMxjEtaU7fdIOkDSWckHev1s5d0u6TjdQ6H05Juq28/0tfDf6zeHR7RhBSFiAGS7gIeATa7NNCbAXZR7iY9ZftuYAJ4vv7Kq8DTttcDn/UtHwMOuszhcC/ljnUoHWX3Uub2WEfpcxTRhKsvv0nEsrONMonJR/UgfiWlydhfwBt1m9eAo5JWA6O2J+ryw8CR2gfoZtvHAGz/DlDf76Ttqfp6kjKPxokr/7EiLi9FIeJSAg7b3jdrobR/YLuF9oj5o+/5DPkeRkMyfBRxqXFgp6QboZtn+BbK96XXofRx4ITtn4EfJd1Xl+8GJmz/CkxJeqi+xwpJ1y7pp4hYgByhRAywfU7Ss5RZxK6idK7dQ5mEZVNdN0257gClXfHL9Y/+18CTdflu4JCkF+p7PLyEHyNiQdIlNWKeJF20PTLsHBFXUoaPIiKikzOFiIjo5EwhIiI6KQoREdFJUYiIiE6KQkREdFIUIiKik6IQERGdvwHeH3V5ADRoVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.7134 - acc: 0.8027\n",
      "Loss: 0.7134456227501604 Accuracy: 0.8026999\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4860 - acc: 0.2734\n",
      "Epoch 00001: val_loss improved from inf to 1.44894, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/001-1.4489.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 2.4859 - acc: 0.2734 - val_loss: 1.4489 - val_acc: 0.5518\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5521 - acc: 0.5104\n",
      "Epoch 00002: val_loss improved from 1.44894 to 1.03016, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/002-1.0302.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 1.5520 - acc: 0.5104 - val_loss: 1.0302 - val_acc: 0.6902\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2011 - acc: 0.6232\n",
      "Epoch 00003: val_loss improved from 1.03016 to 0.84260, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/003-0.8426.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 1.2012 - acc: 0.6231 - val_loss: 0.8426 - val_acc: 0.7494\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0057 - acc: 0.6868\n",
      "Epoch 00004: val_loss improved from 0.84260 to 0.80757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/004-0.8076.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 1.0057 - acc: 0.6868 - val_loss: 0.8076 - val_acc: 0.7689\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8937 - acc: 0.7257\n",
      "Epoch 00005: val_loss improved from 0.80757 to 0.69928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/005-0.6993.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.8937 - acc: 0.7257 - val_loss: 0.6993 - val_acc: 0.7978\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7571\n",
      "Epoch 00006: val_loss improved from 0.69928 to 0.63428, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/006-0.6343.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.8087 - acc: 0.7571 - val_loss: 0.6343 - val_acc: 0.8185\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7357 - acc: 0.7793\n",
      "Epoch 00007: val_loss improved from 0.63428 to 0.58813, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/007-0.5881.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.7356 - acc: 0.7794 - val_loss: 0.5881 - val_acc: 0.8351\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6727 - acc: 0.7999\n",
      "Epoch 00008: val_loss improved from 0.58813 to 0.56890, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/008-0.5689.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.6727 - acc: 0.7999 - val_loss: 0.5689 - val_acc: 0.8409\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.8152\n",
      "Epoch 00009: val_loss did not improve from 0.56890\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.6296 - acc: 0.8151 - val_loss: 0.6144 - val_acc: 0.8218\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8288\n",
      "Epoch 00010: val_loss improved from 0.56890 to 0.56063, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/010-0.5606.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.5787 - acc: 0.8288 - val_loss: 0.5606 - val_acc: 0.8437\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8398\n",
      "Epoch 00011: val_loss improved from 0.56063 to 0.48199, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/011-0.4820.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.5376 - acc: 0.8398 - val_loss: 0.4820 - val_acc: 0.8640\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8511\n",
      "Epoch 00012: val_loss did not improve from 0.48199\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.5039 - acc: 0.8511 - val_loss: 0.5114 - val_acc: 0.8619\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4676 - acc: 0.8621\n",
      "Epoch 00013: val_loss improved from 0.48199 to 0.48180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/013-0.4818.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.4678 - acc: 0.8621 - val_loss: 0.4818 - val_acc: 0.8602\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8668\n",
      "Epoch 00014: val_loss improved from 0.48180 to 0.44538, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/014-0.4454.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.4430 - acc: 0.8668 - val_loss: 0.4454 - val_acc: 0.8761\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8760\n",
      "Epoch 00015: val_loss improved from 0.44538 to 0.42744, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/015-0.4274.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.4167 - acc: 0.8759 - val_loss: 0.4274 - val_acc: 0.8852\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8815\n",
      "Epoch 00016: val_loss improved from 0.42744 to 0.42208, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/016-0.4221.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.3961 - acc: 0.8815 - val_loss: 0.4221 - val_acc: 0.8854\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8877\n",
      "Epoch 00017: val_loss did not improve from 0.42208\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.3702 - acc: 0.8876 - val_loss: 0.4250 - val_acc: 0.8761\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8920\n",
      "Epoch 00018: val_loss improved from 0.42208 to 0.41185, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/018-0.4119.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.3554 - acc: 0.8919 - val_loss: 0.4119 - val_acc: 0.8870\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8976\n",
      "Epoch 00019: val_loss did not improve from 0.41185\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.3431 - acc: 0.8975 - val_loss: 0.4298 - val_acc: 0.8866\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.9004\n",
      "Epoch 00020: val_loss improved from 0.41185 to 0.40361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/020-0.4036.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.3278 - acc: 0.9003 - val_loss: 0.4036 - val_acc: 0.8959\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9056\n",
      "Epoch 00021: val_loss improved from 0.40361 to 0.40187, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/021-0.4019.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.3050 - acc: 0.9056 - val_loss: 0.4019 - val_acc: 0.8940\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9102\n",
      "Epoch 00022: val_loss improved from 0.40187 to 0.36330, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/022-0.3633.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.2895 - acc: 0.9101 - val_loss: 0.3633 - val_acc: 0.9015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9119\n",
      "Epoch 00023: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2824 - acc: 0.9119 - val_loss: 0.3977 - val_acc: 0.8966\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9192\n",
      "Epoch 00024: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2602 - acc: 0.9192 - val_loss: 0.3700 - val_acc: 0.9068\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9214\n",
      "Epoch 00025: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2529 - acc: 0.9214 - val_loss: 0.4057 - val_acc: 0.8963\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9210\n",
      "Epoch 00026: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2512 - acc: 0.9210 - val_loss: 0.3807 - val_acc: 0.9036\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9282\n",
      "Epoch 00027: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.2306 - acc: 0.9282 - val_loss: 0.3684 - val_acc: 0.9012\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9286\n",
      "Epoch 00028: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2284 - acc: 0.9286 - val_loss: 0.3809 - val_acc: 0.9031\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9312\n",
      "Epoch 00029: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2159 - acc: 0.9313 - val_loss: 0.3882 - val_acc: 0.9061\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9346\n",
      "Epoch 00030: val_loss did not improve from 0.36330\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.2080 - acc: 0.9346 - val_loss: 0.3794 - val_acc: 0.9073\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9374\n",
      "Epoch 00031: val_loss improved from 0.36330 to 0.35975, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/031-0.3597.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1980 - acc: 0.9374 - val_loss: 0.3597 - val_acc: 0.9066\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9389\n",
      "Epoch 00032: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1926 - acc: 0.9389 - val_loss: 0.3898 - val_acc: 0.9012\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9445\n",
      "Epoch 00033: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1787 - acc: 0.9445 - val_loss: 0.3720 - val_acc: 0.9096\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9443\n",
      "Epoch 00034: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1734 - acc: 0.9444 - val_loss: 0.3947 - val_acc: 0.9054\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9440\n",
      "Epoch 00035: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1762 - acc: 0.9439 - val_loss: 0.3774 - val_acc: 0.9085\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9450\n",
      "Epoch 00036: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1707 - acc: 0.9450 - val_loss: 0.4027 - val_acc: 0.9010\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9495\n",
      "Epoch 00037: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1611 - acc: 0.9494 - val_loss: 0.3684 - val_acc: 0.9064\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9497\n",
      "Epoch 00038: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1595 - acc: 0.9497 - val_loss: 0.3800 - val_acc: 0.9040\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9521\n",
      "Epoch 00039: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1515 - acc: 0.9520 - val_loss: 0.3934 - val_acc: 0.9096\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9488\n",
      "Epoch 00040: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1567 - acc: 0.9488 - val_loss: 0.3822 - val_acc: 0.9080\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9541\n",
      "Epoch 00041: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1395 - acc: 0.9540 - val_loss: 0.4387 - val_acc: 0.9010\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9535\n",
      "Epoch 00042: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1450 - acc: 0.9535 - val_loss: 0.3744 - val_acc: 0.9124\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9587\n",
      "Epoch 00043: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1325 - acc: 0.9586 - val_loss: 0.3771 - val_acc: 0.9080\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9559\n",
      "Epoch 00044: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1364 - acc: 0.9558 - val_loss: 0.4230 - val_acc: 0.9068\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9591\n",
      "Epoch 00045: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1287 - acc: 0.9591 - val_loss: 0.4210 - val_acc: 0.9096\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9597\n",
      "Epoch 00046: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1262 - acc: 0.9597 - val_loss: 0.4076 - val_acc: 0.9136\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9636\n",
      "Epoch 00047: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1152 - acc: 0.9636 - val_loss: 0.3959 - val_acc: 0.9106\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9615\n",
      "Epoch 00048: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1195 - acc: 0.9614 - val_loss: 0.4243 - val_acc: 0.9050\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9635\n",
      "Epoch 00049: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1134 - acc: 0.9635 - val_loss: 0.4261 - val_acc: 0.9026\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9651\n",
      "Epoch 00050: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1104 - acc: 0.9651 - val_loss: 0.3935 - val_acc: 0.9152\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9663\n",
      "Epoch 00051: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1037 - acc: 0.9663 - val_loss: 0.4084 - val_acc: 0.9080\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9634\n",
      "Epoch 00052: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1109 - acc: 0.9634 - val_loss: 0.3972 - val_acc: 0.9117\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9686\n",
      "Epoch 00053: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1000 - acc: 0.9685 - val_loss: 0.3805 - val_acc: 0.9166\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9660\n",
      "Epoch 00054: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1026 - acc: 0.9659 - val_loss: 0.4105 - val_acc: 0.9143\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9701\n",
      "Epoch 00055: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0942 - acc: 0.9701 - val_loss: 0.3658 - val_acc: 0.9180\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9705\n",
      "Epoch 00056: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0925 - acc: 0.9704 - val_loss: 0.3844 - val_acc: 0.9192\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9675\n",
      "Epoch 00057: val_loss did not improve from 0.35975\n",
      "36805/36805 [==============================] - 149s 4ms/sample - loss: 0.1015 - acc: 0.9675 - val_loss: 0.6000 - val_acc: 0.8784\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9682\n",
      "Epoch 00058: val_loss improved from 0.35975 to 0.35706, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv_checkpoint/058-0.3571.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0988 - acc: 0.9681 - val_loss: 0.3571 - val_acc: 0.9171\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9704\n",
      "Epoch 00059: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0905 - acc: 0.9703 - val_loss: 0.3966 - val_acc: 0.9203\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9696\n",
      "Epoch 00060: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0961 - acc: 0.9696 - val_loss: 0.4036 - val_acc: 0.9117\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9752\n",
      "Epoch 00061: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0800 - acc: 0.9752 - val_loss: 0.4371 - val_acc: 0.9175\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9735\n",
      "Epoch 00062: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0796 - acc: 0.9735 - val_loss: 0.3875 - val_acc: 0.9164\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9736\n",
      "Epoch 00063: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0819 - acc: 0.9736 - val_loss: 0.3766 - val_acc: 0.9122\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9735\n",
      "Epoch 00064: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0810 - acc: 0.9735 - val_loss: 0.4083 - val_acc: 0.9154\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9766\n",
      "Epoch 00065: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0745 - acc: 0.9766 - val_loss: 0.3887 - val_acc: 0.9234\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9741\n",
      "Epoch 00066: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0806 - acc: 0.9741 - val_loss: 0.3977 - val_acc: 0.9157\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9751\n",
      "Epoch 00067: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0781 - acc: 0.9751 - val_loss: 0.3832 - val_acc: 0.9187\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9739\n",
      "Epoch 00068: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0799 - acc: 0.9738 - val_loss: 0.3813 - val_acc: 0.9164\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9752\n",
      "Epoch 00069: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0786 - acc: 0.9751 - val_loss: 0.3584 - val_acc: 0.9203\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9755\n",
      "Epoch 00070: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0764 - acc: 0.9755 - val_loss: 0.3892 - val_acc: 0.9168\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9771\n",
      "Epoch 00071: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0715 - acc: 0.9770 - val_loss: 0.3962 - val_acc: 0.9180\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9762\n",
      "Epoch 00072: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0718 - acc: 0.9762 - val_loss: 0.4342 - val_acc: 0.9094\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9769\n",
      "Epoch 00073: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0712 - acc: 0.9769 - val_loss: 0.3784 - val_acc: 0.9206\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9780\n",
      "Epoch 00074: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0662 - acc: 0.9779 - val_loss: 0.4061 - val_acc: 0.9143\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9779\n",
      "Epoch 00075: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0701 - acc: 0.9779 - val_loss: 0.3900 - val_acc: 0.9201\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9800\n",
      "Epoch 00076: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0613 - acc: 0.9799 - val_loss: 0.3875 - val_acc: 0.9166\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9767\n",
      "Epoch 00077: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0732 - acc: 0.9767 - val_loss: 0.5106 - val_acc: 0.9029\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9795\n",
      "Epoch 00078: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0632 - acc: 0.9795 - val_loss: 0.4286 - val_acc: 0.9152\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9774\n",
      "Epoch 00079: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0698 - acc: 0.9774 - val_loss: 0.3898 - val_acc: 0.9168\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9820\n",
      "Epoch 00080: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0556 - acc: 0.9820 - val_loss: 0.4016 - val_acc: 0.9206\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9800\n",
      "Epoch 00081: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0637 - acc: 0.9800 - val_loss: 0.4344 - val_acc: 0.9106\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9795\n",
      "Epoch 00082: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0645 - acc: 0.9794 - val_loss: 0.4254 - val_acc: 0.9138\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9770\n",
      "Epoch 00083: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0723 - acc: 0.9770 - val_loss: 0.4842 - val_acc: 0.9047\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9794\n",
      "Epoch 00084: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0624 - acc: 0.9794 - val_loss: 0.4076 - val_acc: 0.9168\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9808\n",
      "Epoch 00085: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0594 - acc: 0.9808 - val_loss: 0.3974 - val_acc: 0.9220\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9842\n",
      "Epoch 00086: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0492 - acc: 0.9842 - val_loss: 0.4023 - val_acc: 0.9194\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9835\n",
      "Epoch 00087: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0523 - acc: 0.9835 - val_loss: 0.4576 - val_acc: 0.9171\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9809\n",
      "Epoch 00088: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0617 - acc: 0.9809 - val_loss: 0.4263 - val_acc: 0.9175\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9824\n",
      "Epoch 00089: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0547 - acc: 0.9824 - val_loss: 0.3956 - val_acc: 0.9194\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9812\n",
      "Epoch 00090: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0601 - acc: 0.9812 - val_loss: 0.4239 - val_acc: 0.9206\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9841\n",
      "Epoch 00091: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0490 - acc: 0.9841 - val_loss: 0.4233 - val_acc: 0.9182\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9824\n",
      "Epoch 00092: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0560 - acc: 0.9823 - val_loss: 0.4292 - val_acc: 0.9192\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9807\n",
      "Epoch 00093: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0581 - acc: 0.9807 - val_loss: 0.3878 - val_acc: 0.9262\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9852\n",
      "Epoch 00094: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0481 - acc: 0.9852 - val_loss: 0.4290 - val_acc: 0.9259\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9832\n",
      "Epoch 00095: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0508 - acc: 0.9832 - val_loss: 0.4152 - val_acc: 0.9210\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9856\n",
      "Epoch 00096: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0433 - acc: 0.9855 - val_loss: 0.5171 - val_acc: 0.9106\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9819\n",
      "Epoch 00097: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0574 - acc: 0.9819 - val_loss: 0.3971 - val_acc: 0.9222\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9841\n",
      "Epoch 00098: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0495 - acc: 0.9841 - val_loss: 0.4141 - val_acc: 0.9138\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9827\n",
      "Epoch 00099: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0520 - acc: 0.9827 - val_loss: 0.3925 - val_acc: 0.9248\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9842\n",
      "Epoch 00100: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0480 - acc: 0.9842 - val_loss: 0.4170 - val_acc: 0.9231\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9838\n",
      "Epoch 00101: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0516 - acc: 0.9838 - val_loss: 0.4443 - val_acc: 0.9129\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9845\n",
      "Epoch 00102: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0487 - acc: 0.9845 - val_loss: 0.4554 - val_acc: 0.9147\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
      "Epoch 00103: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0488 - acc: 0.9850 - val_loss: 0.4137 - val_acc: 0.9222\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9844\n",
      "Epoch 00104: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0480 - acc: 0.9843 - val_loss: 0.4517 - val_acc: 0.9206\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9846\n",
      "Epoch 00105: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0487 - acc: 0.9845 - val_loss: 0.4808 - val_acc: 0.9113\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9839\n",
      "Epoch 00106: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0477 - acc: 0.9839 - val_loss: 0.4176 - val_acc: 0.9236\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9874\n",
      "Epoch 00107: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0412 - acc: 0.9874 - val_loss: 0.3857 - val_acc: 0.9229\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9864\n",
      "Epoch 00108: val_loss did not improve from 0.35706\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0419 - acc: 0.9864 - val_loss: 0.4797 - val_acc: 0.9196\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4141 - acc: 0.8939\n",
      "Loss: 0.41413822502114445 Accuracy: 0.89387333\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7153 - acc: 0.2073\n",
      "Epoch 00001: val_loss improved from inf to 1.71362, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/001-1.7136.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 2.7153 - acc: 0.2073 - val_loss: 1.7136 - val_acc: 0.4871\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7875 - acc: 0.4265\n",
      "Epoch 00002: val_loss improved from 1.71362 to 1.20835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/002-1.2083.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 1.7874 - acc: 0.4265 - val_loss: 1.2083 - val_acc: 0.6275\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3669 - acc: 0.5638\n",
      "Epoch 00003: val_loss improved from 1.20835 to 0.88791, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/003-0.8879.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 1.3670 - acc: 0.5638 - val_loss: 0.8879 - val_acc: 0.7426\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0865 - acc: 0.6600\n",
      "Epoch 00004: val_loss improved from 0.88791 to 0.69174, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/004-0.6917.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 1.0864 - acc: 0.6600 - val_loss: 0.6917 - val_acc: 0.7999\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7350\n",
      "Epoch 00005: val_loss improved from 0.69174 to 0.60055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/005-0.6006.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.8745 - acc: 0.7349 - val_loss: 0.6006 - val_acc: 0.8323\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7735\n",
      "Epoch 00006: val_loss improved from 0.60055 to 0.54023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/006-0.5402.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.7403 - acc: 0.7734 - val_loss: 0.5402 - val_acc: 0.8495\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6343 - acc: 0.8106\n",
      "Epoch 00007: val_loss improved from 0.54023 to 0.41283, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/007-0.4128.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.6344 - acc: 0.8106 - val_loss: 0.4128 - val_acc: 0.8807\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8369\n",
      "Epoch 00008: val_loss improved from 0.41283 to 0.37898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/008-0.3790.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.5422 - acc: 0.8370 - val_loss: 0.3790 - val_acc: 0.8961\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8526\n",
      "Epoch 00009: val_loss improved from 0.37898 to 0.34960, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/009-0.3496.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.4853 - acc: 0.8525 - val_loss: 0.3496 - val_acc: 0.9012\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8657\n",
      "Epoch 00010: val_loss improved from 0.34960 to 0.34354, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/010-0.3435.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.4426 - acc: 0.8656 - val_loss: 0.3435 - val_acc: 0.9026\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8782\n",
      "Epoch 00011: val_loss did not improve from 0.34354\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.4048 - acc: 0.8782 - val_loss: 0.3620 - val_acc: 0.8940\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8889\n",
      "Epoch 00012: val_loss improved from 0.34354 to 0.27521, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/012-0.2752.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.3636 - acc: 0.8888 - val_loss: 0.2752 - val_acc: 0.9173\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8966\n",
      "Epoch 00013: val_loss improved from 0.27521 to 0.26531, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/013-0.2653.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.3405 - acc: 0.8966 - val_loss: 0.2653 - val_acc: 0.9222\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9041\n",
      "Epoch 00014: val_loss improved from 0.26531 to 0.25285, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/014-0.2528.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.3130 - acc: 0.9041 - val_loss: 0.2528 - val_acc: 0.9271\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9084\n",
      "Epoch 00015: val_loss did not improve from 0.25285\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2966 - acc: 0.9083 - val_loss: 0.2558 - val_acc: 0.9264\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9148\n",
      "Epoch 00016: val_loss did not improve from 0.25285\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2722 - acc: 0.9147 - val_loss: 0.2568 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9223\n",
      "Epoch 00017: val_loss improved from 0.25285 to 0.23935, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/017-0.2394.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2551 - acc: 0.9223 - val_loss: 0.2394 - val_acc: 0.9327\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9228\n",
      "Epoch 00018: val_loss did not improve from 0.23935\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2468 - acc: 0.9228 - val_loss: 0.2475 - val_acc: 0.9301\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9267\n",
      "Epoch 00019: val_loss improved from 0.23935 to 0.22017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/019-0.2202.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2350 - acc: 0.9267 - val_loss: 0.2202 - val_acc: 0.9401\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9305\n",
      "Epoch 00020: val_loss did not improve from 0.22017\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 0.2210 - acc: 0.9305 - val_loss: 0.2258 - val_acc: 0.9399\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9331\n",
      "Epoch 00021: val_loss did not improve from 0.22017\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2129 - acc: 0.9331 - val_loss: 0.2291 - val_acc: 0.9336\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9363\n",
      "Epoch 00022: val_loss improved from 0.22017 to 0.20298, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/022-0.2030.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2027 - acc: 0.9362 - val_loss: 0.2030 - val_acc: 0.9399\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9378\n",
      "Epoch 00023: val_loss did not improve from 0.20298\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1928 - acc: 0.9378 - val_loss: 0.2219 - val_acc: 0.9408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9434\n",
      "Epoch 00024: val_loss improved from 0.20298 to 0.19085, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/024-0.1909.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1789 - acc: 0.9433 - val_loss: 0.1909 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9447\n",
      "Epoch 00025: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1764 - acc: 0.9447 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9477\n",
      "Epoch 00026: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1656 - acc: 0.9477 - val_loss: 0.2479 - val_acc: 0.9306\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9497\n",
      "Epoch 00027: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1552 - acc: 0.9497 - val_loss: 0.2353 - val_acc: 0.9397\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9530\n",
      "Epoch 00028: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1497 - acc: 0.9530 - val_loss: 0.2222 - val_acc: 0.9453\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9544\n",
      "Epoch 00029: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1436 - acc: 0.9544 - val_loss: 0.2088 - val_acc: 0.9436\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9560\n",
      "Epoch 00030: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.1371 - acc: 0.9560 - val_loss: 0.2187 - val_acc: 0.9408\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9574\n",
      "Epoch 00031: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.1339 - acc: 0.9574 - val_loss: 0.1925 - val_acc: 0.9464\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9592\n",
      "Epoch 00032: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1294 - acc: 0.9591 - val_loss: 0.2090 - val_acc: 0.9411\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9582\n",
      "Epoch 00033: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1310 - acc: 0.9582 - val_loss: 0.2119 - val_acc: 0.9457\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9635\n",
      "Epoch 00034: val_loss did not improve from 0.19085\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1160 - acc: 0.9634 - val_loss: 0.1937 - val_acc: 0.9462\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9625\n",
      "Epoch 00035: val_loss improved from 0.19085 to 0.19002, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/035-0.1900.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1140 - acc: 0.9625 - val_loss: 0.1900 - val_acc: 0.9490\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9635\n",
      "Epoch 00036: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1150 - acc: 0.9635 - val_loss: 0.2137 - val_acc: 0.9446\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9653\n",
      "Epoch 00037: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1074 - acc: 0.9653 - val_loss: 0.1932 - val_acc: 0.9509\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9656\n",
      "Epoch 00038: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1044 - acc: 0.9655 - val_loss: 0.2568 - val_acc: 0.9331\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9652\n",
      "Epoch 00039: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 0.1045 - acc: 0.9651 - val_loss: 0.1997 - val_acc: 0.9504\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9668\n",
      "Epoch 00040: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.1041 - acc: 0.9668 - val_loss: 0.2169 - val_acc: 0.9476\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9724\n",
      "Epoch 00041: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0867 - acc: 0.9723 - val_loss: 0.2044 - val_acc: 0.9511\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9701\n",
      "Epoch 00042: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0930 - acc: 0.9701 - val_loss: 0.2127 - val_acc: 0.9478\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9721\n",
      "Epoch 00043: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0868 - acc: 0.9721 - val_loss: 0.2263 - val_acc: 0.9443\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9731\n",
      "Epoch 00044: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0854 - acc: 0.9731 - val_loss: 0.2239 - val_acc: 0.9490\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9739\n",
      "Epoch 00045: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0822 - acc: 0.9739 - val_loss: 0.2095 - val_acc: 0.9499\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9737\n",
      "Epoch 00046: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0818 - acc: 0.9737 - val_loss: 0.2213 - val_acc: 0.9427\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9752\n",
      "Epoch 00047: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0782 - acc: 0.9752 - val_loss: 0.2047 - val_acc: 0.9474\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9760\n",
      "Epoch 00048: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0738 - acc: 0.9759 - val_loss: 0.2305 - val_acc: 0.9434\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9740\n",
      "Epoch 00049: val_loss did not improve from 0.19002\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0796 - acc: 0.9741 - val_loss: 0.2099 - val_acc: 0.9469\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9775\n",
      "Epoch 00050: val_loss improved from 0.19002 to 0.18934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv_checkpoint/050-0.1893.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0678 - acc: 0.9775 - val_loss: 0.1893 - val_acc: 0.9532\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9774\n",
      "Epoch 00051: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0725 - acc: 0.9774 - val_loss: 0.2168 - val_acc: 0.9478\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9783\n",
      "Epoch 00052: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0704 - acc: 0.9783 - val_loss: 0.2066 - val_acc: 0.9485\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9790\n",
      "Epoch 00053: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0642 - acc: 0.9791 - val_loss: 0.2105 - val_acc: 0.9509\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9811\n",
      "Epoch 00054: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0607 - acc: 0.9811 - val_loss: 0.1987 - val_acc: 0.9492\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9808\n",
      "Epoch 00055: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0621 - acc: 0.9808 - val_loss: 0.1976 - val_acc: 0.9504\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9814\n",
      "Epoch 00056: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0572 - acc: 0.9813 - val_loss: 0.2597 - val_acc: 0.9394\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9792\n",
      "Epoch 00057: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0628 - acc: 0.9792 - val_loss: 0.1982 - val_acc: 0.9504\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9815\n",
      "Epoch 00058: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0556 - acc: 0.9815 - val_loss: 0.1981 - val_acc: 0.9509\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9820\n",
      "Epoch 00059: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0580 - acc: 0.9819 - val_loss: 0.2304 - val_acc: 0.9492\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9801\n",
      "Epoch 00060: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0612 - acc: 0.9800 - val_loss: 0.2093 - val_acc: 0.9515\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9805\n",
      "Epoch 00061: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0604 - acc: 0.9805 - val_loss: 0.2115 - val_acc: 0.9457\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9848\n",
      "Epoch 00062: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0486 - acc: 0.9847 - val_loss: 0.2242 - val_acc: 0.9495\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9827\n",
      "Epoch 00063: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0534 - acc: 0.9826 - val_loss: 0.2104 - val_acc: 0.9499\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9841\n",
      "Epoch 00064: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0483 - acc: 0.9841 - val_loss: 0.2110 - val_acc: 0.9511\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9842\n",
      "Epoch 00065: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0482 - acc: 0.9842 - val_loss: 0.2102 - val_acc: 0.9513\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9836\n",
      "Epoch 00066: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0513 - acc: 0.9836 - val_loss: 0.2163 - val_acc: 0.9504\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9860\n",
      "Epoch 00067: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0441 - acc: 0.9860 - val_loss: 0.2562 - val_acc: 0.9411\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9827\n",
      "Epoch 00068: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0532 - acc: 0.9827 - val_loss: 0.1984 - val_acc: 0.9515\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9837\n",
      "Epoch 00069: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0509 - acc: 0.9836 - val_loss: 0.2320 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9835\n",
      "Epoch 00070: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0526 - acc: 0.9835 - val_loss: 0.2021 - val_acc: 0.9539\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9870\n",
      "Epoch 00071: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0419 - acc: 0.9870 - val_loss: 0.2115 - val_acc: 0.9529\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9875\n",
      "Epoch 00072: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0391 - acc: 0.9875 - val_loss: 0.2389 - val_acc: 0.9481\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9892\n",
      "Epoch 00073: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0358 - acc: 0.9892 - val_loss: 0.2554 - val_acc: 0.9411\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9846\n",
      "Epoch 00074: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0473 - acc: 0.9845 - val_loss: 0.2186 - val_acc: 0.9481\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9834\n",
      "Epoch 00075: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0498 - acc: 0.9834 - val_loss: 0.1966 - val_acc: 0.9548\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9897\n",
      "Epoch 00076: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0335 - acc: 0.9896 - val_loss: 0.2051 - val_acc: 0.9539\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9852\n",
      "Epoch 00077: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0467 - acc: 0.9851 - val_loss: 0.2077 - val_acc: 0.9485\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9830\n",
      "Epoch 00078: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0520 - acc: 0.9830 - val_loss: 0.2044 - val_acc: 0.9525\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9891\n",
      "Epoch 00079: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0348 - acc: 0.9891 - val_loss: 0.2150 - val_acc: 0.9511\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9883\n",
      "Epoch 00080: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0372 - acc: 0.9882 - val_loss: 0.2453 - val_acc: 0.9499\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9867\n",
      "Epoch 00081: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0388 - acc: 0.9866 - val_loss: 0.2280 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9859\n",
      "Epoch 00082: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0428 - acc: 0.9858 - val_loss: 0.1952 - val_acc: 0.9548\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9899\n",
      "Epoch 00083: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0327 - acc: 0.9899 - val_loss: 0.2157 - val_acc: 0.9529\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9872\n",
      "Epoch 00084: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.1946 - val_acc: 0.9571\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9898\n",
      "Epoch 00085: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0318 - acc: 0.9898 - val_loss: 0.2486 - val_acc: 0.9448\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9865\n",
      "Epoch 00086: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0402 - acc: 0.9865 - val_loss: 0.2421 - val_acc: 0.9485\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00087: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0301 - acc: 0.9905 - val_loss: 0.2531 - val_acc: 0.9518\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9877\n",
      "Epoch 00088: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0362 - acc: 0.9876 - val_loss: 0.2141 - val_acc: 0.9513\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9879\n",
      "Epoch 00089: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0372 - acc: 0.9879 - val_loss: 0.2077 - val_acc: 0.9557\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9903\n",
      "Epoch 00090: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0320 - acc: 0.9902 - val_loss: 0.2190 - val_acc: 0.9532\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9894\n",
      "Epoch 00091: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0340 - acc: 0.9894 - val_loss: 0.2277 - val_acc: 0.9511\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9920\n",
      "Epoch 00092: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0261 - acc: 0.9920 - val_loss: 0.2435 - val_acc: 0.9481\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9880\n",
      "Epoch 00093: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0396 - acc: 0.9879 - val_loss: 0.2617 - val_acc: 0.9443\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9871\n",
      "Epoch 00094: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0400 - acc: 0.9870 - val_loss: 0.2089 - val_acc: 0.9548\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9909\n",
      "Epoch 00095: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0298 - acc: 0.9909 - val_loss: 0.2077 - val_acc: 0.9571\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9909\n",
      "Epoch 00096: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0299 - acc: 0.9908 - val_loss: 0.2096 - val_acc: 0.9583\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9883\n",
      "Epoch 00097: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0365 - acc: 0.9883 - val_loss: 0.2390 - val_acc: 0.9460\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9891\n",
      "Epoch 00098: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0353 - acc: 0.9891 - val_loss: 0.2096 - val_acc: 0.9571\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9906\n",
      "Epoch 00099: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0288 - acc: 0.9906 - val_loss: 0.2323 - val_acc: 0.9518\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9893\n",
      "Epoch 00100: val_loss did not improve from 0.18934\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0320 - acc: 0.9893 - val_loss: 0.1994 - val_acc: 0.9548\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2123 - acc: 0.9418\n",
      "Loss: 0.21225474257706853 Accuracy: 0.9418484\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8214 - acc: 0.1902\n",
      "Epoch 00001: val_loss improved from inf to 1.81076, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/001-1.8108.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 2.8213 - acc: 0.1903 - val_loss: 1.8108 - val_acc: 0.4715\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8319 - acc: 0.4310\n",
      "Epoch 00002: val_loss improved from 1.81076 to 1.11627, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/002-1.1163.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 1.8320 - acc: 0.4310 - val_loss: 1.1163 - val_acc: 0.6853\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2752 - acc: 0.5984\n",
      "Epoch 00003: val_loss improved from 1.11627 to 0.72351, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/003-0.7235.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 1.2755 - acc: 0.5983 - val_loss: 0.7235 - val_acc: 0.8022\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9325 - acc: 0.7108\n",
      "Epoch 00004: val_loss improved from 0.72351 to 0.51967, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/004-0.5197.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.9326 - acc: 0.7107 - val_loss: 0.5197 - val_acc: 0.8560\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7210 - acc: 0.7751\n",
      "Epoch 00005: val_loss improved from 0.51967 to 0.40757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/005-0.4076.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.7211 - acc: 0.7750 - val_loss: 0.4076 - val_acc: 0.8856\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8184\n",
      "Epoch 00006: val_loss improved from 0.40757 to 0.33163, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/006-0.3316.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.5939 - acc: 0.8184 - val_loss: 0.3316 - val_acc: 0.9043\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8440\n",
      "Epoch 00007: val_loss improved from 0.33163 to 0.28984, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/007-0.2898.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.5112 - acc: 0.8440 - val_loss: 0.2898 - val_acc: 0.9178\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8670\n",
      "Epoch 00008: val_loss improved from 0.28984 to 0.27729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/008-0.2773.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.4373 - acc: 0.8669 - val_loss: 0.2773 - val_acc: 0.9203\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8795\n",
      "Epoch 00009: val_loss improved from 0.27729 to 0.27716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/009-0.2772.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.3865 - acc: 0.8795 - val_loss: 0.2772 - val_acc: 0.9194\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8955\n",
      "Epoch 00010: val_loss improved from 0.27716 to 0.23232, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/010-0.2323.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.3464 - acc: 0.8954 - val_loss: 0.2323 - val_acc: 0.9311\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.9004\n",
      "Epoch 00011: val_loss did not improve from 0.23232\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.3226 - acc: 0.9004 - val_loss: 0.2555 - val_acc: 0.9262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9093\n",
      "Epoch 00012: val_loss improved from 0.23232 to 0.19511, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/012-0.1951.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2903 - acc: 0.9093 - val_loss: 0.1951 - val_acc: 0.9453\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9179\n",
      "Epoch 00013: val_loss did not improve from 0.19511\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2653 - acc: 0.9178 - val_loss: 0.1976 - val_acc: 0.9408\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9235\n",
      "Epoch 00014: val_loss did not improve from 0.19511\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.2500 - acc: 0.9235 - val_loss: 0.2100 - val_acc: 0.9357\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9292\n",
      "Epoch 00015: val_loss improved from 0.19511 to 0.17512, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/015-0.1751.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.2273 - acc: 0.9291 - val_loss: 0.1751 - val_acc: 0.9502\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9339\n",
      "Epoch 00016: val_loss improved from 0.17512 to 0.16208, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/016-0.1621.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.2118 - acc: 0.9338 - val_loss: 0.1621 - val_acc: 0.9527\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9371\n",
      "Epoch 00017: val_loss improved from 0.16208 to 0.15898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/017-0.1590.hdf5\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 0.2005 - acc: 0.9370 - val_loss: 0.1590 - val_acc: 0.9534\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9427\n",
      "Epoch 00018: val_loss did not improve from 0.15898\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.1850 - acc: 0.9427 - val_loss: 0.1655 - val_acc: 0.9502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9438\n",
      "Epoch 00019: val_loss improved from 0.15898 to 0.15083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/019-0.1508.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1794 - acc: 0.9438 - val_loss: 0.1508 - val_acc: 0.9513\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9490\n",
      "Epoch 00020: val_loss improved from 0.15083 to 0.14404, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/020-0.1440.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.1637 - acc: 0.9489 - val_loss: 0.1440 - val_acc: 0.9560\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9507\n",
      "Epoch 00021: val_loss improved from 0.14404 to 0.14387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/021-0.1439.hdf5\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 0.1578 - acc: 0.9506 - val_loss: 0.1439 - val_acc: 0.9576\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9528\n",
      "Epoch 00022: val_loss improved from 0.14387 to 0.14140, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/022-0.1414.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1493 - acc: 0.9528 - val_loss: 0.1414 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9541\n",
      "Epoch 00023: val_loss did not improve from 0.14140\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1419 - acc: 0.9541 - val_loss: 0.1508 - val_acc: 0.9588\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9553\n",
      "Epoch 00024: val_loss did not improve from 0.14140\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.1375 - acc: 0.9553 - val_loss: 0.1471 - val_acc: 0.9550\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9589\n",
      "Epoch 00025: val_loss did not improve from 0.14140\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.1295 - acc: 0.9589 - val_loss: 0.1589 - val_acc: 0.9553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9611\n",
      "Epoch 00026: val_loss improved from 0.14140 to 0.12811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/026-0.1281.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1254 - acc: 0.9611 - val_loss: 0.1281 - val_acc: 0.9616\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9624\n",
      "Epoch 00027: val_loss did not improve from 0.12811\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1184 - acc: 0.9624 - val_loss: 0.1323 - val_acc: 0.9609\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9653\n",
      "Epoch 00028: val_loss did not improve from 0.12811\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.1106 - acc: 0.9653 - val_loss: 0.1317 - val_acc: 0.9637\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9666\n",
      "Epoch 00029: val_loss did not improve from 0.12811\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.1058 - acc: 0.9666 - val_loss: 0.1363 - val_acc: 0.9588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9693\n",
      "Epoch 00030: val_loss did not improve from 0.12811\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0997 - acc: 0.9693 - val_loss: 0.1423 - val_acc: 0.9620\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9689\n",
      "Epoch 00031: val_loss did not improve from 0.12811\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0977 - acc: 0.9689 - val_loss: 0.1400 - val_acc: 0.9585\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9708\n",
      "Epoch 00032: val_loss improved from 0.12811 to 0.12793, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/032-0.1279.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0943 - acc: 0.9707 - val_loss: 0.1279 - val_acc: 0.9630\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9732\n",
      "Epoch 00033: val_loss improved from 0.12793 to 0.12304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/033-0.1230.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0876 - acc: 0.9731 - val_loss: 0.1230 - val_acc: 0.9653\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9726\n",
      "Epoch 00034: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0868 - acc: 0.9726 - val_loss: 0.1276 - val_acc: 0.9625\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9732\n",
      "Epoch 00035: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0868 - acc: 0.9732 - val_loss: 0.1405 - val_acc: 0.9599\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9763\n",
      "Epoch 00036: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0766 - acc: 0.9763 - val_loss: 0.1260 - val_acc: 0.9651\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9726\n",
      "Epoch 00037: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0873 - acc: 0.9726 - val_loss: 0.1526 - val_acc: 0.9571\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9738\n",
      "Epoch 00038: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0809 - acc: 0.9738 - val_loss: 0.1377 - val_acc: 0.9595\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9780\n",
      "Epoch 00039: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0722 - acc: 0.9779 - val_loss: 0.1314 - val_acc: 0.9648\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9780\n",
      "Epoch 00040: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0683 - acc: 0.9780 - val_loss: 0.1248 - val_acc: 0.9606\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9772\n",
      "Epoch 00041: val_loss did not improve from 0.12304\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0708 - acc: 0.9771 - val_loss: 0.1264 - val_acc: 0.9632\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9765\n",
      "Epoch 00042: val_loss improved from 0.12304 to 0.12256, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/042-0.1226.hdf5\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0734 - acc: 0.9765 - val_loss: 0.1226 - val_acc: 0.9672\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9814\n",
      "Epoch 00043: val_loss did not improve from 0.12256\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0590 - acc: 0.9814 - val_loss: 0.1262 - val_acc: 0.9669\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9808\n",
      "Epoch 00044: val_loss did not improve from 0.12256\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0610 - acc: 0.9808 - val_loss: 0.1308 - val_acc: 0.9620\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9791\n",
      "Epoch 00045: val_loss did not improve from 0.12256\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0675 - acc: 0.9791 - val_loss: 0.1284 - val_acc: 0.9634\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9801\n",
      "Epoch 00046: val_loss did not improve from 0.12256\n",
      "36805/36805 [==============================] - 154s 4ms/sample - loss: 0.0615 - acc: 0.9801 - val_loss: 0.1361 - val_acc: 0.9646\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9820\n",
      "Epoch 00047: val_loss improved from 0.12256 to 0.11642, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv_checkpoint/047-0.1164.hdf5\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0581 - acc: 0.9820 - val_loss: 0.1164 - val_acc: 0.9676\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9824\n",
      "Epoch 00048: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0542 - acc: 0.9824 - val_loss: 0.1406 - val_acc: 0.9606\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9810\n",
      "Epoch 00049: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0576 - acc: 0.9810 - val_loss: 0.1264 - val_acc: 0.9646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9854\n",
      "Epoch 00050: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0469 - acc: 0.9854 - val_loss: 0.1353 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9818\n",
      "Epoch 00051: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0584 - acc: 0.9818 - val_loss: 0.1250 - val_acc: 0.9639\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9853\n",
      "Epoch 00052: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0465 - acc: 0.9852 - val_loss: 0.1219 - val_acc: 0.9651\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9839\n",
      "Epoch 00053: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0516 - acc: 0.9839 - val_loss: 0.1266 - val_acc: 0.9632\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9875\n",
      "Epoch 00054: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0407 - acc: 0.9875 - val_loss: 0.1246 - val_acc: 0.9681\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9872\n",
      "Epoch 00055: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0417 - acc: 0.9872 - val_loss: 0.1603 - val_acc: 0.9588\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9834\n",
      "Epoch 00056: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0524 - acc: 0.9834 - val_loss: 0.1414 - val_acc: 0.9646\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9873\n",
      "Epoch 00057: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0415 - acc: 0.9873 - val_loss: 0.1703 - val_acc: 0.9578\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9849\n",
      "Epoch 00058: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0464 - acc: 0.9848 - val_loss: 0.1367 - val_acc: 0.9630\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9873\n",
      "Epoch 00059: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0398 - acc: 0.9872 - val_loss: 0.1433 - val_acc: 0.9646\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9880\n",
      "Epoch 00060: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0388 - acc: 0.9879 - val_loss: 0.1346 - val_acc: 0.9644\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9868\n",
      "Epoch 00061: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0405 - acc: 0.9868 - val_loss: 0.1363 - val_acc: 0.9637\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9889\n",
      "Epoch 00062: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0355 - acc: 0.9888 - val_loss: 0.1504 - val_acc: 0.9602\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9858\n",
      "Epoch 00063: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0461 - acc: 0.9857 - val_loss: 0.1297 - val_acc: 0.9676\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9892\n",
      "Epoch 00064: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0344 - acc: 0.9892 - val_loss: 0.1374 - val_acc: 0.9660\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9907\n",
      "Epoch 00065: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0301 - acc: 0.9907 - val_loss: 0.1429 - val_acc: 0.9611\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9900\n",
      "Epoch 00066: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.1587 - val_acc: 0.9613\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9895\n",
      "Epoch 00067: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0329 - acc: 0.9895 - val_loss: 0.1361 - val_acc: 0.9651\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9858\n",
      "Epoch 00068: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0440 - acc: 0.9858 - val_loss: 0.1281 - val_acc: 0.9688\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9904\n",
      "Epoch 00069: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0313 - acc: 0.9903 - val_loss: 0.1247 - val_acc: 0.9641\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9908\n",
      "Epoch 00070: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0298 - acc: 0.9908 - val_loss: 0.1623 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9862\n",
      "Epoch 00071: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0432 - acc: 0.9863 - val_loss: 0.1391 - val_acc: 0.9627\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9918\n",
      "Epoch 00072: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0269 - acc: 0.9918 - val_loss: 0.1646 - val_acc: 0.9632\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9895\n",
      "Epoch 00073: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0340 - acc: 0.9894 - val_loss: 0.1446 - val_acc: 0.9630\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9880\n",
      "Epoch 00074: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0381 - acc: 0.9880 - val_loss: 0.1468 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9932\n",
      "Epoch 00075: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.1250 - val_acc: 0.9674\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9913\n",
      "Epoch 00076: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0286 - acc: 0.9913 - val_loss: 0.1287 - val_acc: 0.9683\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 00077: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0364 - acc: 0.9881 - val_loss: 0.1582 - val_acc: 0.9613\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9889\n",
      "Epoch 00078: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0343 - acc: 0.9889 - val_loss: 0.1448 - val_acc: 0.9630\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9882\n",
      "Epoch 00079: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0361 - acc: 0.9882 - val_loss: 0.1356 - val_acc: 0.9655\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9929\n",
      "Epoch 00080: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0231 - acc: 0.9929 - val_loss: 0.1290 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9926\n",
      "Epoch 00081: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0249 - acc: 0.9926 - val_loss: 0.1352 - val_acc: 0.9672\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9931\n",
      "Epoch 00082: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0223 - acc: 0.9930 - val_loss: 0.1672 - val_acc: 0.9599\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9896\n",
      "Epoch 00083: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0360 - acc: 0.9895 - val_loss: 0.1475 - val_acc: 0.9625\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9905\n",
      "Epoch 00084: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0304 - acc: 0.9905 - val_loss: 0.1549 - val_acc: 0.9613\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9935\n",
      "Epoch 00085: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0212 - acc: 0.9935 - val_loss: 0.1526 - val_acc: 0.9634\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00086: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.1688 - val_acc: 0.9583\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9940\n",
      "Epoch 00087: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0199 - acc: 0.9940 - val_loss: 0.1643 - val_acc: 0.9604\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9920\n",
      "Epoch 00088: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0251 - acc: 0.9920 - val_loss: 0.1638 - val_acc: 0.9632\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9919\n",
      "Epoch 00089: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0256 - acc: 0.9919 - val_loss: 0.1449 - val_acc: 0.9658\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 00090: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0308 - acc: 0.9904 - val_loss: 0.1247 - val_acc: 0.9667\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9932\n",
      "Epoch 00091: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0224 - acc: 0.9932 - val_loss: 0.1369 - val_acc: 0.9662\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 00092: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0181 - acc: 0.9946 - val_loss: 0.1536 - val_acc: 0.9611\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9930\n",
      "Epoch 00093: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0238 - acc: 0.9930 - val_loss: 0.1327 - val_acc: 0.9667\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00094: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0196 - acc: 0.9940 - val_loss: 0.1356 - val_acc: 0.9662\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9926\n",
      "Epoch 00095: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0234 - acc: 0.9926 - val_loss: 0.1383 - val_acc: 0.9679\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9915\n",
      "Epoch 00096: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0280 - acc: 0.9915 - val_loss: 0.1415 - val_acc: 0.9658\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9926\n",
      "Epoch 00097: val_loss did not improve from 0.11642\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 0.0242 - acc: 0.9925 - val_loss: 0.1644 - val_acc: 0.9613\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1671 - acc: 0.9543\n",
      "Loss: 0.16711079795232675 Accuracy: 0.95430946\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5653 - acc: 0.2709\n",
      "Epoch 00001: val_loss improved from inf to 1.29242, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/001-1.2924.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 2.5651 - acc: 0.2709 - val_loss: 1.2924 - val_acc: 0.5945\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3424 - acc: 0.5838\n",
      "Epoch 00002: val_loss improved from 1.29242 to 0.70729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/002-0.7073.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.3423 - acc: 0.5838 - val_loss: 0.7073 - val_acc: 0.7871\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7372\n",
      "Epoch 00003: val_loss improved from 0.70729 to 0.45210, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/003-0.4521.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.8456 - acc: 0.7372 - val_loss: 0.4521 - val_acc: 0.8647\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.8119\n",
      "Epoch 00004: val_loss improved from 0.45210 to 0.34665, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/004-0.3466.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.6147 - acc: 0.8119 - val_loss: 0.3466 - val_acc: 0.8956\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4900 - acc: 0.8500\n",
      "Epoch 00005: val_loss improved from 0.34665 to 0.29484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/005-0.2948.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4904 - acc: 0.8500 - val_loss: 0.2948 - val_acc: 0.9122\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8750\n",
      "Epoch 00006: val_loss improved from 0.29484 to 0.27625, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/006-0.2762.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4053 - acc: 0.8749 - val_loss: 0.2762 - val_acc: 0.9157\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8913\n",
      "Epoch 00007: val_loss improved from 0.27625 to 0.21501, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/007-0.2150.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3519 - acc: 0.8913 - val_loss: 0.2150 - val_acc: 0.9327\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9057\n",
      "Epoch 00008: val_loss did not improve from 0.21501\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3042 - acc: 0.9057 - val_loss: 0.2322 - val_acc: 0.9294\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9176\n",
      "Epoch 00009: val_loss improved from 0.21501 to 0.19238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/009-0.1924.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2682 - acc: 0.9175 - val_loss: 0.1924 - val_acc: 0.9436\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9223\n",
      "Epoch 00010: val_loss did not improve from 0.19238\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2468 - acc: 0.9223 - val_loss: 0.1943 - val_acc: 0.9397\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9282\n",
      "Epoch 00011: val_loss improved from 0.19238 to 0.17164, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/011-0.1716.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2251 - acc: 0.9282 - val_loss: 0.1716 - val_acc: 0.9471\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9359\n",
      "Epoch 00012: val_loss improved from 0.17164 to 0.16801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/012-0.1680.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2036 - acc: 0.9359 - val_loss: 0.1680 - val_acc: 0.9497\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9418\n",
      "Epoch 00013: val_loss improved from 0.16801 to 0.16109, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/013-0.1611.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1845 - acc: 0.9417 - val_loss: 0.1611 - val_acc: 0.9497\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9465\n",
      "Epoch 00014: val_loss did not improve from 0.16109\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1681 - acc: 0.9464 - val_loss: 0.1655 - val_acc: 0.9518\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9482\n",
      "Epoch 00015: val_loss improved from 0.16109 to 0.14639, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/015-0.1464.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1621 - acc: 0.9481 - val_loss: 0.1464 - val_acc: 0.9553\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9533\n",
      "Epoch 00016: val_loss improved from 0.14639 to 0.14068, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/016-0.1407.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1454 - acc: 0.9533 - val_loss: 0.1407 - val_acc: 0.9590\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9569\n",
      "Epoch 00017: val_loss did not improve from 0.14068\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1369 - acc: 0.9569 - val_loss: 0.1438 - val_acc: 0.9578\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9590\n",
      "Epoch 00018: val_loss did not improve from 0.14068\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1287 - acc: 0.9589 - val_loss: 0.1481 - val_acc: 0.9560\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9614\n",
      "Epoch 00019: val_loss did not improve from 0.14068\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1223 - acc: 0.9614 - val_loss: 0.1600 - val_acc: 0.9502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9606\n",
      "Epoch 00020: val_loss improved from 0.14068 to 0.12933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/020-0.1293.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1243 - acc: 0.9606 - val_loss: 0.1293 - val_acc: 0.9597\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9646\n",
      "Epoch 00021: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1128 - acc: 0.9647 - val_loss: 0.1379 - val_acc: 0.9604\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9699\n",
      "Epoch 00022: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0933 - acc: 0.9698 - val_loss: 0.1490 - val_acc: 0.9578\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9712\n",
      "Epoch 00023: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0898 - acc: 0.9713 - val_loss: 0.1449 - val_acc: 0.9595\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9716\n",
      "Epoch 00024: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0895 - acc: 0.9716 - val_loss: 0.1461 - val_acc: 0.9564\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9751\n",
      "Epoch 00025: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0794 - acc: 0.9750 - val_loss: 0.1534 - val_acc: 0.9541\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9744\n",
      "Epoch 00026: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0812 - acc: 0.9744 - val_loss: 0.1390 - val_acc: 0.9609\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9776\n",
      "Epoch 00027: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0712 - acc: 0.9776 - val_loss: 0.1528 - val_acc: 0.9595\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9791\n",
      "Epoch 00028: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0658 - acc: 0.9791 - val_loss: 0.1521 - val_acc: 0.9592\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9792\n",
      "Epoch 00029: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0661 - acc: 0.9792 - val_loss: 0.1415 - val_acc: 0.9606\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9777\n",
      "Epoch 00030: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0692 - acc: 0.9777 - val_loss: 0.1555 - val_acc: 0.9567\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9821\n",
      "Epoch 00031: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0556 - acc: 0.9821 - val_loss: 0.1323 - val_acc: 0.9630\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9763\n",
      "Epoch 00032: val_loss did not improve from 0.12933\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0721 - acc: 0.9762 - val_loss: 0.1343 - val_acc: 0.9637\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9805\n",
      "Epoch 00033: val_loss improved from 0.12933 to 0.12915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/033-0.1291.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0622 - acc: 0.9805 - val_loss: 0.1291 - val_acc: 0.9660\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9859\n",
      "Epoch 00034: val_loss did not improve from 0.12915\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0467 - acc: 0.9858 - val_loss: 0.1414 - val_acc: 0.9606\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9854\n",
      "Epoch 00035: val_loss improved from 0.12915 to 0.12843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv_checkpoint/035-0.1284.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0486 - acc: 0.9854 - val_loss: 0.1284 - val_acc: 0.9665\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9865\n",
      "Epoch 00036: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 150s 4ms/sample - loss: 0.0449 - acc: 0.9865 - val_loss: 0.1372 - val_acc: 0.9637\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9850\n",
      "Epoch 00037: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0476 - acc: 0.9850 - val_loss: 0.1382 - val_acc: 0.9611\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9842\n",
      "Epoch 00038: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0515 - acc: 0.9842 - val_loss: 0.1349 - val_acc: 0.9646\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9852\n",
      "Epoch 00039: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0471 - acc: 0.9852 - val_loss: 0.1710 - val_acc: 0.9553\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9857\n",
      "Epoch 00040: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0441 - acc: 0.9857 - val_loss: 0.1372 - val_acc: 0.9618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9867\n",
      "Epoch 00041: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0423 - acc: 0.9867 - val_loss: 0.1287 - val_acc: 0.9646\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9879\n",
      "Epoch 00042: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0378 - acc: 0.9879 - val_loss: 0.1529 - val_acc: 0.9599\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9852\n",
      "Epoch 00043: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0478 - acc: 0.9851 - val_loss: 0.1373 - val_acc: 0.9651\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9885\n",
      "Epoch 00044: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0362 - acc: 0.9884 - val_loss: 0.1404 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9892\n",
      "Epoch 00045: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0342 - acc: 0.9891 - val_loss: 0.1430 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9883\n",
      "Epoch 00046: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0377 - acc: 0.9883 - val_loss: 0.1552 - val_acc: 0.9597\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9861\n",
      "Epoch 00047: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0451 - acc: 0.9861 - val_loss: 0.1293 - val_acc: 0.9655\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9896\n",
      "Epoch 00048: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.1382 - val_acc: 0.9655\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9918\n",
      "Epoch 00049: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0264 - acc: 0.9918 - val_loss: 0.1492 - val_acc: 0.9630\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9922\n",
      "Epoch 00050: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0249 - acc: 0.9922 - val_loss: 0.1554 - val_acc: 0.9623\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9914\n",
      "Epoch 00051: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0284 - acc: 0.9914 - val_loss: 0.1384 - val_acc: 0.9623\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9917\n",
      "Epoch 00052: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0274 - acc: 0.9917 - val_loss: 0.1359 - val_acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9926\n",
      "Epoch 00053: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0253 - acc: 0.9926 - val_loss: 0.1429 - val_acc: 0.9641\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0256 - acc: 0.9917 - val_loss: 0.1552 - val_acc: 0.9620\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9917\n",
      "Epoch 00055: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0265 - acc: 0.9917 - val_loss: 0.1701 - val_acc: 0.9616\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9921\n",
      "Epoch 00056: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0254 - acc: 0.9921 - val_loss: 0.1587 - val_acc: 0.9604\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 00057: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0232 - acc: 0.9929 - val_loss: 0.1497 - val_acc: 0.9630\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9884\n",
      "Epoch 00058: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0360 - acc: 0.9884 - val_loss: 0.1528 - val_acc: 0.9623\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0209 - acc: 0.9941 - val_loss: 0.1626 - val_acc: 0.9599\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 00060: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0308 - acc: 0.9902 - val_loss: 0.1449 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9932\n",
      "Epoch 00061: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0214 - acc: 0.9931 - val_loss: 0.1432 - val_acc: 0.9651\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9910\n",
      "Epoch 00062: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0279 - acc: 0.9910 - val_loss: 0.1505 - val_acc: 0.9674\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9945\n",
      "Epoch 00063: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0177 - acc: 0.9944 - val_loss: 0.1826 - val_acc: 0.9618\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9902\n",
      "Epoch 00064: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0313 - acc: 0.9902 - val_loss: 0.1428 - val_acc: 0.9662\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9915\n",
      "Epoch 00065: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0260 - acc: 0.9914 - val_loss: 0.1558 - val_acc: 0.9651\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9933\n",
      "Epoch 00066: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0223 - acc: 0.9933 - val_loss: 0.1424 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9953\n",
      "Epoch 00067: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0170 - acc: 0.9953 - val_loss: 0.1810 - val_acc: 0.9595\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9929\n",
      "Epoch 00068: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0228 - acc: 0.9929 - val_loss: 0.1572 - val_acc: 0.9653\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9934\n",
      "Epoch 00069: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0210 - acc: 0.9934 - val_loss: 0.1623 - val_acc: 0.9627\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9952\n",
      "Epoch 00070: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0169 - acc: 0.9952 - val_loss: 0.1599 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9930\n",
      "Epoch 00071: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0219 - acc: 0.9930 - val_loss: 0.1402 - val_acc: 0.9674\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9946\n",
      "Epoch 00072: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1618 - val_acc: 0.9630\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 00073: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0191 - acc: 0.9940 - val_loss: 0.1703 - val_acc: 0.9641\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9920\n",
      "Epoch 00074: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0255 - acc: 0.9920 - val_loss: 0.1504 - val_acc: 0.9658\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9960\n",
      "Epoch 00075: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0143 - acc: 0.9960 - val_loss: 0.1549 - val_acc: 0.9674\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9959\n",
      "Epoch 00076: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.1783 - val_acc: 0.9644\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9954\n",
      "Epoch 00077: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0154 - acc: 0.9954 - val_loss: 0.1935 - val_acc: 0.9597\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9954\n",
      "Epoch 00078: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0158 - acc: 0.9954 - val_loss: 0.1695 - val_acc: 0.9634\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0141 - acc: 0.9955 - val_loss: 0.1755 - val_acc: 0.9639\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 00080: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0218 - acc: 0.9927 - val_loss: 0.1777 - val_acc: 0.9581\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9934\n",
      "Epoch 00081: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0219 - acc: 0.9934 - val_loss: 0.1628 - val_acc: 0.9686\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00082: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.1673 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9950\n",
      "Epoch 00083: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0169 - acc: 0.9949 - val_loss: 0.1603 - val_acc: 0.9655\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9957\n",
      "Epoch 00084: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1519 - val_acc: 0.9646\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 00085: val_loss did not improve from 0.12843\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0129 - acc: 0.9962 - val_loss: 0.1690 - val_acc: 0.9634\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1814 - acc: 0.9506\n",
      "Loss: 0.18136075342215233 Accuracy: 0.9505711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN'\n",
    "\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1883216     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.4715 - acc: 0.5755\n",
      "Loss: 1.4714836027268186 Accuracy: 0.5754933\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_96_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_96_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_96_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           694992      lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.9891 - acc: 0.7080\n",
      "Loss: 0.9890744084757312 Accuracy: 0.70799583\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_104_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_104_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_104_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           567248      lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.7134 - acc: 0.8027\n",
      "Loss: 0.7134456227501604 Accuracy: 0.8026999\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_114_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_114_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_114_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           396496      lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.4141 - acc: 0.8939\n",
      "Loss: 0.41413822502114445 Accuracy: 0.89387333\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_126_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_126_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_126_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           405968      lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.2123 - acc: 0.9418\n",
      "Loss: 0.21225474257706853 Accuracy: 0.9418484\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_140_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_140_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_140_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           476880      lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.1671 - acc: 0.9543\n",
      "Loss: 0.16711079795232675 Accuracy: 0.95430946\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_156_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 16000, 1)     0           conv1d_156_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 16000, 1)     0           conv1d_156_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 16)           768208      lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Concatenate)          (None, 16)           0           sequential_15[1][0]              \n",
      "                                                                 sequential_15[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1814 - acc: 0.9506\n",
      "Loss: 0.18136075342215233 Accuracy: 0.9505711\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1883216     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 2.4227 - acc: 0.6147\n",
      "Loss: 2.4227111378686574 Accuracy: 0.61474556\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_96_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_96_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_96_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           694992      lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 1.1513 - acc: 0.7458\n",
      "Loss: 1.1513042141220156 Accuracy: 0.7457944\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_104_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_104_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_104_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           567248      lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.8363 - acc: 0.8150\n",
      "Loss: 0.8363048417298345 Accuracy: 0.81495327\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_114_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_114_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_114_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           396496      lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.5223 - acc: 0.8906\n",
      "Loss: 0.5223240340733949 Accuracy: 0.8905504\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_126_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_126_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_126_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           405968      lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2379 - acc: 0.9423\n",
      "Loss: 0.23786049854350766 Accuracy: 0.9422638\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_140_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_140_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_140_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           476880      lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.2252 - acc: 0.9502\n",
      "Loss: 0.225223043251508 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_156_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 16000, 1)     0           conv1d_156_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 16000, 1)     0           conv1d_156_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 16)           768208      lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Concatenate)          (None, 16)           0           sequential_15[1][0]              \n",
      "                                                                 sequential_15[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 0.2238 - acc: 0.9526\n",
      "Loss: 0.22383014419473182 Accuracy: 0.952648\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
