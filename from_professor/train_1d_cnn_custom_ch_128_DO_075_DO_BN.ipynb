{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_128_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=128, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=128*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 512)            655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_128_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8650 - acc: 0.3099\n",
      "Epoch 00001: val_loss improved from inf to 2.02650, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_3_conv_checkpoint/001-2.0265.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 2.8652 - acc: 0.3099 - val_loss: 2.0265 - val_acc: 0.3953\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0108 - acc: 0.4666\n",
      "Epoch 00002: val_loss did not improve from 2.02650\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 2.0109 - acc: 0.4666 - val_loss: 2.0640 - val_acc: 0.4547\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6875 - acc: 0.5402\n",
      "Epoch 00003: val_loss improved from 2.02650 to 1.48894, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_3_conv_checkpoint/003-1.4889.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.6877 - acc: 0.5403 - val_loss: 1.4889 - val_acc: 0.5777\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4442 - acc: 0.5951\n",
      "Epoch 00004: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.4444 - acc: 0.5951 - val_loss: 1.7297 - val_acc: 0.5402\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2973 - acc: 0.6298\n",
      "Epoch 00005: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.2974 - acc: 0.6298 - val_loss: 1.6914 - val_acc: 0.5712\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1824 - acc: 0.6639\n",
      "Epoch 00006: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.1823 - acc: 0.6639 - val_loss: 1.7074 - val_acc: 0.5551\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0329 - acc: 0.7019\n",
      "Epoch 00007: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.0335 - acc: 0.7019 - val_loss: 1.7026 - val_acc: 0.5632\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9698 - acc: 0.7199\n",
      "Epoch 00008: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.9700 - acc: 0.7198 - val_loss: 1.9254 - val_acc: 0.5481\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8952 - acc: 0.7396\n",
      "Epoch 00009: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.8955 - acc: 0.7396 - val_loss: 1.5401 - val_acc: 0.6210\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7549\n",
      "Epoch 00010: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.8271 - acc: 0.7549 - val_loss: 2.0652 - val_acc: 0.5565\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7755\n",
      "Epoch 00011: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.7577 - acc: 0.7754 - val_loss: 1.5905 - val_acc: 0.6166\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7898\n",
      "Epoch 00012: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6984 - acc: 0.7897 - val_loss: 1.6343 - val_acc: 0.6236\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.8020\n",
      "Epoch 00013: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6658 - acc: 0.8020 - val_loss: 1.7464 - val_acc: 0.6014\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6181 - acc: 0.8140\n",
      "Epoch 00014: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6181 - acc: 0.8140 - val_loss: 1.7105 - val_acc: 0.6282\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.8291\n",
      "Epoch 00015: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.5708 - acc: 0.8291 - val_loss: 1.8183 - val_acc: 0.6073\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8390\n",
      "Epoch 00016: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.5354 - acc: 0.8389 - val_loss: 1.5433 - val_acc: 0.6632\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8441\n",
      "Epoch 00017: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.5259 - acc: 0.8441 - val_loss: 2.6515 - val_acc: 0.5092\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.8567\n",
      "Epoch 00018: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.4806 - acc: 0.8567 - val_loss: 1.8877 - val_acc: 0.6196\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.8635\n",
      "Epoch 00019: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.4551 - acc: 0.8635 - val_loss: 2.0651 - val_acc: 0.6047\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8679\n",
      "Epoch 00020: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.4342 - acc: 0.8679 - val_loss: 1.8317 - val_acc: 0.6187\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8826\n",
      "Epoch 00021: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3903 - acc: 0.8826 - val_loss: 2.8747 - val_acc: 0.5276\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8847\n",
      "Epoch 00022: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3839 - acc: 0.8846 - val_loss: 1.6945 - val_acc: 0.6450\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8882\n",
      "Epoch 00023: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3707 - acc: 0.8882 - val_loss: 1.7147 - val_acc: 0.6494\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8925\n",
      "Epoch 00024: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3534 - acc: 0.8925 - val_loss: 1.8402 - val_acc: 0.6327\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9005\n",
      "Epoch 00025: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3310 - acc: 0.9005 - val_loss: 1.8177 - val_acc: 0.6557\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9020\n",
      "Epoch 00026: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3207 - acc: 0.9020 - val_loss: 1.7200 - val_acc: 0.6611\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9072\n",
      "Epoch 00027: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3050 - acc: 0.9072 - val_loss: 1.7680 - val_acc: 0.6725\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9069\n",
      "Epoch 00028: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3086 - acc: 0.9068 - val_loss: 2.2521 - val_acc: 0.5928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9124\n",
      "Epoch 00029: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2951 - acc: 0.9123 - val_loss: 1.6665 - val_acc: 0.6730\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9162\n",
      "Epoch 00030: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2749 - acc: 0.9162 - val_loss: 1.7173 - val_acc: 0.6744\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9231\n",
      "Epoch 00031: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2567 - acc: 0.9231 - val_loss: 2.0388 - val_acc: 0.6201\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9237\n",
      "Epoch 00032: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2537 - acc: 0.9236 - val_loss: 2.3133 - val_acc: 0.6052\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9237\n",
      "Epoch 00033: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2513 - acc: 0.9237 - val_loss: 1.7675 - val_acc: 0.6711\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9304\n",
      "Epoch 00034: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2370 - acc: 0.9304 - val_loss: 2.2385 - val_acc: 0.6196\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9331\n",
      "Epoch 00035: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2218 - acc: 0.9331 - val_loss: 1.9107 - val_acc: 0.6443\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9330\n",
      "Epoch 00036: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2223 - acc: 0.9330 - val_loss: 1.9281 - val_acc: 0.6560\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9372\n",
      "Epoch 00037: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2151 - acc: 0.9372 - val_loss: 1.9745 - val_acc: 0.6564\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9393\n",
      "Epoch 00038: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2041 - acc: 0.9393 - val_loss: 2.0836 - val_acc: 0.6406\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9370\n",
      "Epoch 00039: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2095 - acc: 0.9370 - val_loss: 2.0662 - val_acc: 0.6296\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9438\n",
      "Epoch 00040: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1888 - acc: 0.9438 - val_loss: 2.7627 - val_acc: 0.5742\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9461\n",
      "Epoch 00041: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1824 - acc: 0.9461 - val_loss: 1.9340 - val_acc: 0.6678\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9475\n",
      "Epoch 00042: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1770 - acc: 0.9475 - val_loss: 2.4185 - val_acc: 0.6122\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9470\n",
      "Epoch 00043: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1801 - acc: 0.9470 - val_loss: 1.8768 - val_acc: 0.6636\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9472\n",
      "Epoch 00044: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1738 - acc: 0.9472 - val_loss: 2.0413 - val_acc: 0.6504\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9527\n",
      "Epoch 00045: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1623 - acc: 0.9527 - val_loss: 2.0248 - val_acc: 0.6490\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9536\n",
      "Epoch 00046: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1586 - acc: 0.9536 - val_loss: 1.8237 - val_acc: 0.6781\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9545\n",
      "Epoch 00047: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1515 - acc: 0.9545 - val_loss: 1.8222 - val_acc: 0.6830\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9557\n",
      "Epoch 00048: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1491 - acc: 0.9557 - val_loss: 1.7454 - val_acc: 0.6816\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9559\n",
      "Epoch 00049: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1507 - acc: 0.9559 - val_loss: 1.8157 - val_acc: 0.6702\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9587\n",
      "Epoch 00050: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1443 - acc: 0.9587 - val_loss: 1.8074 - val_acc: 0.6776\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9600\n",
      "Epoch 00051: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1393 - acc: 0.9600 - val_loss: 1.8677 - val_acc: 0.6660\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9604\n",
      "Epoch 00052: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1394 - acc: 0.9604 - val_loss: 1.6925 - val_acc: 0.6900\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9639\n",
      "Epoch 00053: val_loss did not improve from 1.48894\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1279 - acc: 0.9639 - val_loss: 1.6776 - val_acc: 0.6990\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXlc1NX6xz9nYFhkXwVBBcQFQUBEo8wlTVMrl0ytrMxupl2rn1nerFtdW7xZ2ma7ZaVmWletXLMNotRKRVRUFFFE9n1fZ+b8/nj4MgPMBswwA5736/V9zfLdnhmY8znneZ7zHMY5h0AgEAgEACCztAECgUAgsB6EKAgEAoGgGSEKAoFAIGhGiIJAIBAImhGiIBAIBIJmhCgIBAKBoBkhCgKBQCBoxmyiwBhzYIz9zRg7yRg7wxh7Ucsx9oyxrxljFxljfzHGgsxlj0AgEAgMY86RQj2ACZzzKADRAKYwxuJaHfMPAKWc81AAbwF4zYz2CAQCgcAAtua6MKep0lVNL+VNW+vp0zMArGp6vgPAe4wxxvVMs/b29uZBQUGmNVYgEAh6OMePHy/inPsYOs5sogAAjDEbAMcBhAJ4n3P+V6tDAgBcBQDOuYIxVg7AC0BRq+s8DOBhAOjXrx+OHTtmTrMFAoGgx8EYu2LMcWYNNHPOlZzzaACBAEYxxiI6eJ0NnPNYznmsj49BoRMIBAJBB+mS7CPOeRmAeABTWu3KBtAXABhjtgDcABR3hU0CgUAgaIs5s498GGPuTc8dAUwCkNrqsN0AFjQ9vxPAr/riCQKBQCAwL+aMKfgD2NQUV5AB+IZzvpcx9hKAY5zz3QA2AtjCGLsIoATAXR25UWNjI7KyslBXV2cq2685HBwcEBgYCLlcbmlTBAKBBTFn9tEpAMO1vP+CxvM6AHM6e6+srCy4uLggKCgIjLHOXu6ag3OO4uJiZGVlITg42NLmCAQCC9IjZjTX1dXBy8tLCEIHYYzBy8tLjLQEAkHPEAUAQhA6ifj+BAIB0INEQXAN0tgIbNwIKJWWtkQg6DFcM6KgUJSjujoFKlW9ya9dVlaGDz74oEPnTps2DWVlZUYfv2rVKqxbt65D9+px/Pwz8NBDwKFDlrZEIOgxXDOiAAAqVR1UqkaTX1efKCgUCr3n7t+/H+7u7ia36ZqgoIAei4r0HycQCIzmmhEFxijVknPTi8LKlSuRnp6O6OhorFixAgkJCRgzZgymT5+OoUOHAgBmzpyJESNGIDw8HBs2bGg+NygoCEVFRcjIyEBYWBgWLVqE8PBwTJ48GbW1tXrvm5ycjLi4OERGRmLWrFkoLS0FAKxfvx5Dhw5FZGQk7rqLsnx/++03REdHIzo6GsOHD0dlZaXJv4cuRxKDps8tEAg6j1lrH1mCtLRlqKpK1rKHQ6msgkzm0CwQxuLsHI2BA9/WuX/NmjVISUlBcjLdNyEhAUlJSUhJSWlO8fzss8/g6emJ2tpajBw5ErNnz4aXl1cr29Owbds2fPLJJ5g7dy527tyJe++9V+d977//frz77rsYN24cXnjhBbz44ot4++23sWbNGly+fBn29vbNrql169bh/fffx+jRo1FVVQUHB4d2fQdWiSQKJSWWtUMg6EFcMyMFQMquUXXJ3UaNGtUi53/9+vWIiopCXFwcrl69irS0tDbnBAcHIzo6GgAwYsQIZGRk6Lx+eXk5ysrKMG7cOADAggULkJiYCACIjIzE/Pnz8eWXX8LWlnR/9OjRWL58OdavX4+ysrLm97s1QhQEApPTA1qGlujr0VdVnYStrRscHILMboeTk1Pz84SEBPz88884cuQIevXqhfHjx2udE2Bvb9/83MbGxqD7SBf79u1DYmIi9uzZg9WrV+P06dNYuXIlbr31Vuzfvx+jR4/GwYMHMWTIkA5d32oQ7iOBwORcQyMFiiuYI9Ds4uKi10dfXl4ODw8P9OrVC6mpqfjzzz87fU83Nzd4eHjg999/BwBs2bIF48aNg0qlwtWrV3HTTTfhtddeQ3l5OaqqqpCeno5hw4bh6aefxsiRI5Ga2roMVTdEjBQEApPT40YK+mBMbpZAs5eXF0aPHo2IiAhMnToVt956a4v9U6ZMwUcffYSwsDAMHjwYcXGtF6DrGJs2bcKSJUtQU1ODkJAQfP7551Aqlbj33ntRXl4Ozjkef/xxuLu74/nnn0d8fDxkMhnCw8MxdepUk9hgUYQodG8OHgTi4gA3N0tbItCAdbeipLGxsbz1Ijvnzp1DWFiYwXNrazOgVJbD2TnKXOZ1a4z9Hq0GHx8ShpgY4PhxS1sjaA+FhYCvL7BuHfDkk5a25pqAMXaccx5r6Lhryn0kk8nBuQLdTQgFWlCp1CMEMVLofkhJFFevWtQMQVuuKVGgdXw4ONc/oUzQDSgrI2GwtRWB5u5IZiY95uZa1g5BG64xUTDfBDZBFyPFE4KDgfJywMDMcYGVIUTBahGiIOieSKIwaBA9tqN+lMAKkNxGQhSsDiEKgu5Ja1EQLqTuhRgpWC3XlCjIZCQKKpVwNXR7JFEYOJAeRbC5eyGJQnU10BPqcPUgrilRkJaLtoaRgrOzc7ve77akpQFJSaa/buuRghCF7kVmJiDN+hejBavimhIFwHwT2AQ6+Ne/gPvvN/11i4oAR0cgMJBeC/dR96GuDsjPB2KbUuZzcixrj6AF15wo0FwF04rCypUr8f777ze/lhbCqaqqwsSJExETE4Nhw4bh+++/N/qanHOsWLECERERGDZsGL7++msAQG5uLsaOHYvo6GhERETg999/h1KpxAMPPNB87FtvvWXSz9cp8vOBK1dMf92iIsDbG/DwoNdipNB9yMqix+uuo0cxUrAqel6Zi2XLgGRtpbMJe1UtwFWAjZPOY9oQHQ28rbvQ3rx587Bs2TIsXboUAPDNN9/g4MGDcHBwwLfffgtXV1cUFRUhLi4O06dPN2o95F27diE5ORknT55EUVERRo4cibFjx+Krr77CLbfcgn//+99QKpWoqalBcnIysrOzkZKSAgDtWsnN7BQXA1VVQEUF4OpquusKUei+SPEEIQpWSc8TBQMwyKCCadf0HT58OAoKCpCTk4PCwkJ4eHigb9++aGxsxLPPPovExETIZDJkZ2cjPz8ffn5+Bq/5xx9/4O6774aNjQ169+6NcePG4ejRoxg5ciQefPBBNDY2YubMmYiOjkZISAguXbqExx57DLfeeismT55s0s/XKYqL6TE72zyiIJcDzs7CfdSdkEQhMhKwtxeiYGX0PFHQ06MHgMb6XDQ0ZMPZOQaMmc57NmfOHOzYsQN5eXmYN28eAGDr1q0oLCzE8ePHIZfLERQUpLVkdnsYO3YsEhMTsW/fPjzwwANYvnw57r//fpw8eRIHDx7ERx99hG+++QafffaZKT5W51Cp1I11djZgyrpKRUVA//703NNTjBS6E5Io9O0L+PsLUbAyrrmYgrnmKsybNw/bt2/Hjh07MGfOHABUMtvX1xdyuRzx8fG40g7f+pgxY/D1119DqVSisLAQiYmJGDVqFK5cuYLevXtj0aJFeOihh5CUlISioiKoVCrMnj0br7zyCpLMke3TEaRSFIDaj2wqpJECQKIgRgrdh6tXAT8/GiUIUbA6et5IwQDquQqNkMnsDRxtPOHh4aisrERAQAD8/f0BAPPnz8ftt9+OYcOGITY2tl2L2syaNQtHjhxBVFQUGGN4/fXX4efnh02bNmHt2rWQy+VwdnbG5s2bkZ2djYULF0LV1AC/+uqrJvtcnUKz956dbbrrKhQkApIoeHiIkUJ3IjMT6NePnvv7A+fOWdYeQQuuOVEw56zm06dPt3jt7e2NI0eOaD22qqpK7/uMMaxduxZr165tsX/BggVYsGBBm/OsZnSgiRRPAEwrCpIAaI4Uzp413fUF5iUzE4iIoOf+/sCvv1rWHkELzOY+Yoz1ZYzFM8bOMsbOMMb+T8sx4xlj5Yyx5KbtBXPZo76nKHXRZUiiwJhpRUGauCbcR90PzluOFPr0ITdjB5eeFZgec44UFACe5JwnMcZcABxnjP3EOW/dpfudc36bGe1oAZXPFqLQJUiiMGCAaWMKrUVBch9xTgIksF5KSoCaGgoyAzRSACiuEBJiObsEzZhtpMA5z+WcJzU9rwRwDkCAue5nLIwxMau5q5DcPJGR5h8pNDRQYyOwbqTMI82YAiCCzVZEl2QfMcaCAAwH8JeW3dczxk4yxg4wxsJ1nP8wY+wYY+xYYWGhCeyRQ6USomB2ioup5x4RARQUUMNtCrSJAiBcSN0BIQpWj9lFgTHmDGAngGWc84pWu5MA9OecRwF4F8B32q7BOd/AOY/lnMf6+PiYwCYxUugSiovJtdO3L7l2TPXDl0TBy4sexazm7oMQBavHrKLAKKq7E8BWzvmu1vs55xWc86qm5/sByBlj3ua0iewSotAlFBdTwx3Q5DU0lQupqIhmMTs40GtppCBEwfrJzKT5CVLnztubllQVomA1mDP7iAHYCOAc5/xNHcf4NR0HxtioJnuKtR1rSmQyW3CuAOfcJNcrKyvDBx980KFzp02bZl21ikxJcTE12FIlU1OKgrdG30G4j7oPUuaRlBAgkwG9ewtRsCLMOVIYDeA+ABM0Uk6nMcaWMMaWNB1zJ4AUxthJAOsB3MVN1VLrgQYwHJybZrEdfaKgMLB28P79++Hu7m4SO6yOkhLzjRQ0RUG4j7oPV6+qXUcSYlazVWHO7KM/OOeMcx7JOY9u2vZzzj/inH/UdMx7nPNwznkU5zyOc37YXPZoYuq5CitXrkR6ejqio6OxYsUKJCQkYMyYMZg+fTqGDh0KAJg5cyZGjBiB8PBwbNiwofncoKAgFBUVISMjA2FhYVi0aBHCw8MxefJk1GrJ3d6zZw+uu+46DB8+HDfffDPy8/MB0KS3hQsXYtiwYYiMjMTOnTsBAD/88ANiYmIQFRWFiRMnmuTzGo3kPvLwIFePqdJSxUih+6I5R0GiTx/Dayrk5gJPPQU0CrevuelxM5oNVM4GAHDuCpVqMGQyO6PS2g1UzsaaNWuQkpKC5KYbJyQkICkpCSkpKQgODgYAfPbZZ/D09ERtbS1GjhyJ2bNnw0sKlDaRlpaGbdu24ZNPPsHcuXOxc+dO3HvvvS2OufHGG/Hnn3+CMYZPP/0Ur7/+Ot544w28/PLLcHNza55VXVpaisLCQixatAiJiYkIDg5GSVf3pCVRYIxGC6YcKQwerH7t7Ex+aTFSsG4aG6nx1zZSOGygP/j118AbbwB33gnExZnPRkHPEwXjkAZI5vNUjRo1qlkQAGD9+vX49ttvAQBXr15FWlpaG1EIDg5GdHQ0AGDEiBHIyMhoc92srCzMmzcPubm5aGhoaL7Hzz//jO3btzcf5+HhgT179mDs2LHNx3hKPequoKGB1lGQPmNgoOlEobi45UiBMVH/qDuQnU1ZaNLENQl/fxL6hgbAzk77uWfO0OP580IUzEyPEwUDlbMB0KpmVVXnYWcXCHt7w2sbdAQnJ/UiPgkJCfj5559x5MgR9OrVC+PHj9daQtveXl2gz8bGRqv76LHHHsPy5csxffp0JCQkYNWqVWaxv9NIDbQkRAEBwJ9/dv66DQ20YI93qyQ1UerC+mmdjiohpaXm57cVDAlJFC5cMI9tgmauudLZAMCYDQCZyWIKLi4uqKys1Lm/vLwcHh4e6NWrF1JTU/FnJxrH8vJyBDQFbjdt2tT8/qRJk1osCVpaWoq4uDgkJibi8uXLANC17iOpxIU0UpDcR53NI5Cu21oUxEjB+jEkCrqCzZy3HCkIzMo1KQqAaecqeHl5YfTo0YiIiMCKFSva7J8yZQoUCgXCwsKwcuVKxHVi+Ltq1SrMmTMHI0aMgLdGw/jcc8+htLQUERERiIqKQnx8PHx8fLBhwwbccccdiIqKal78p0toLQqBgUB9fcvKqR2h9WxmCbHQjvWjubiOJoZEITubRoeAGCl0AT3OfWQsMplpJ7B99dVXLV6PHz+++bm9vT0OHDig9TwpbuDt7d28xjIAPPXUU1qPnzFjBmbMmNHmfWdn5xYjB4mpU6di6tSphsw3PdpGCgD9wFs36O1BnyiIuvzWTWYm/d169Wr5viFRkEYJsbFASgot3CS7ZvuzZuea/WZF/SMzo0sUOpuWqksUrlX3kULReZdcV6EtHRWgyWuMGRaFWbOAujqa6yAwG9e0KIhSF2ZEW6AZ6HwGkr6RQnk5oFR27vrdCc6BESOAZ5+1tCXGoUsUbG0BX1/9ouDrC9xwA70WLiSzck2LAqAE5ypLm9IzKS6m9EIpC8vPj4b8phKF1um10uueWjJEG2fPAqdOATpW97M6tM1mlvD31z2B7cwZIDxcPTdFBJvNyjUuCmKxHbOhOXENAORychOYQhTc3el6mlyLpS727qXH7tBzLi+nYLE+UdA2UuCcxC88nDoWzs7d4/N2Y65ZUZDJqFERcQUzIYmCJgEBpokpaAtUX4ulLiRRyM1VZ+dYK7rSUSV0icLVq0BlJYkCY8CgQUIUzMw1KwpipGBmpAqpmpii1IUhUbC2kUJREbB0qelXhSsuptIQERH0Oi3NtNc3NbrSUSX8/WnyWuuYkBRkDm9af2vwYOE+MjNCFCwkCs7Ozha5b5chVUjVxBSlLoqK2l4XsF730fffAx98ABw6ZNrr/vADpWYuX06vjek919QAixaZdmlUYzFmpKBSAa1XVmwtCoMGAVeuUBaSwCxcw6JAUzTESMFM6HIflZZ2rtfc3dxHUqN28aJpr7t3L8Vo5s0jt4oxovDHH8CnnwJbt5rWFmPIzKQ4kJ+OsjK65iqcOUPnSH/fwYMpzmDq71PQzDUsCsxkaakrV65sUWJi1apVWLduHaqqqjBx4kTExMRg2LBh+P777w1eS1eJbW0lsHWVy7Y4nOsWBaBzPVVdomCtIwVziEJjI40Ubr2VJoL162ecKEiTIxMSTGeLsWRm0khR16QzfaLQVH4eAI0UABFXMCM9bkbzsh+WITnPQO3sJpTKGjDGIJM56j0u2i8ab0/RXWlv3rx5WLZsGZYuXQoA+Oabb3Dw4EE4ODjg22+/haurK4qKihAXF4fp06eD6anXra3Etkql0loCW1u5bKugqooaLn2iMHBg+69bU0ObNlGQyykzxVpFIT3ddNc8fJhSb2+7jV4bG3xt+j/BH3/QpDfbLvz565qjINGnDz1qioJKRZlHDz6ofk8SBRFXMBs9ThTaA2PMJEtyDh8+HAUFBcjJyUFhYSE8PDzQt29fNDY24tlnn0ViYiJkMhmys7ORn58PP11DaGgvsV1YWKi1BLa2ctlWQeuJaxKdXZZTVzE8CWurlFpWpv6sphwp7N1Lc0BuvpleDxoEbNlCIzR9C4SkpJAQVFYCJ04AI0eaziZDZGYC48bp3i/9JjTnKmRmAtXV6ngCALi40KhCjBTMRo8TBX09+tbU1l6GUlkBZ+eoTt93zpw52LFjB/Ly8poLz23duhWFhYU4fvw45HI5goKCtJbMljC2xLbV07rEhURnS13oms0sYW1F8c6epcfBg4FLl0xXs2ffPmpgXVzo9aBBlJJaUEBxBm2oVDRqmT2bFqz57beuEwWlksRR30jB3p7+fpojhdZBZgmRlmpWrtmYAiAVxVOYZLQwb948bN++HTt27MCcOXMAUJlrX19fyOVyxMfH48qVK3qvoavEtq4S2NrKZVsFukTB2Rlwde34SMGQKHh4WNdIQWrUpk8HamtNsw5xejoV/pNcR4BxM30vXyYbJk2iRrUr4wq5uSQM+kQBaDtXQZcoiLRUs3JNiwKlpXJwruj0tcLDw1FZWYmAgAD4NwXN5s+fj2PHjmHYsGHYvHkzhgwZovcaukps6yqBra1ctlWgSxSAzs1V6G4jhTNnKBAsrY1tirjCvn30eOut6veMCb5KQeaICGD8eOD337uuTpShdFQJbaLg769OIpAYNIj+xzpbhl2glR7nPmoPLecqyPUfbARSwFfC29sbR3TUpamqqmrznr4S29pKYOsql21x9IlCZ+YqGDNSsDZRGDpUHVS/eBEYO7Zz19y7FwgLAwYMUL/Xrx/FGPSJgvS/GR5OrqcNG2gx8xEjOmePMRiauCbROlYg1TxqjTQyunABuP5609goaEaMFACTjBQEGkgNs7bAd2dKXRQVqddj1oYUaLaWUtJSo9avHwV4OztSqKwkt4+m6wgAbGyA0FDDI4XgYHLhSQHf337rnD3G0h5RyMujv59KRW4ybaIg0lLNihAFiAlsJqe4mIKg2hZhDwigH35HXBdFRdTw29ho3+/pSau7aVnbusspLSVXSHg4CUJQUOczkH76iVJ9W4sCYDj4mpKiLokREEAi0lVxhcxMKmLo6qr/OH9/WoO7pATIyKD0Y22iEBxM36mIK5iFHiMKHQkWi6J4akwRbG9G28Q1iYAAEoT8/PZfV9fENQlrmsDWOkg6YEDnRWHvXmpcpXUFNBk0iK6vTWwbGqgBlUQBME9c4dtvgdWrSbw0S5gbmqMgoTlXQVeQGaA5KSEhYqRgJnqEKDg4OKC4uLjdDRtjNgCXgSvrzWSZmVEqqefYSTjnKC4uhoODgwmMgn5RMDRX4epV3XVtDImCNZW6aN2ohYaS+6ij4qtSUZB5yhTtk84GD6b/BW0Zbhcu0GQ1TVEYN44a7lOnOmZPa/78E5g7F3juOWDyZBLosDDggQeApCTjREGa1ZyTo/7+NGczayLSUs1Gjwg0BwYGIisrC4Wti2npQ6UCqqrAK0oBVQFY32r9E3+skaIi6gVKPaxO4ODggECpwe4sJSVtJ65JaM5VaJ0nX1BADcnixcAbb7Q9t6iIXAe6sKZKqWfOkP9eagxDQ2lNgeLijq1RfewYfT/aXEdAy5m+ISEt90lB5mHD1O9pxhWGD2+/PZqUlgJ33UUxg/h4qtj611+0HThAdt97r+HraJa6OHOG/lfc3bUfO3gw8PPPYr1mM9AjREEulzfP9jVIYSHw3nu0lZSgwdcOdgUNFNQykDJqdURGkq+4qqrtYuiWpLhYd+Otr/7Re+/RDNatW4HXX28bOygu1j/hytrcR0OHqjsaUrZQenrHRGHfPmr8pkzRvl8z+NoqS615JrOUtQNQAx4SQnGFZcvab48E58DChdS7P3QI6N+fNmm2Nee0z8fH8LVai4I215HEoEHq9Zr792+7/+hRteDFxOhOThC0wWwSyxjryxiLZ4ydZYydYYz9n5ZjGGNsPWPsImPsFGMsxlz2ICOD6tr36we89BIwZgzwxx/Iems07bf2evStUSqpAeCcBM2a0Oc+8vEhn3BrUaiuBt5/n2bk5udTj1MTzruf+0izUQsNpceOxhUOHADi4vR/r25u2l0qKSnUiLYO/I8fDyQmUm+7o6xfT+XBX3tNu2AzRh0BbUkHrXFyogSF7GzdmUcS+ibsKRTA3XcDK1aQOHl6UlrwXXcB69bpXvZTAMC8MQUFgCc550MBxAFYyhhr7SCcCmBg0/YwgA/NZk1yMvDJJ8D8+VR+4LvvgNGjwQc0DbW7WynezEzKtAHUE5OsAaWSfNW6Gi+ZjNxdrdNSP/uMevhbt1LDsG1by/3V1fR5jREFS48UiotJ2DQbteBgaiA7kpZaXg4cP67ufWtD36pkmplHmowbRwLaan6N0Rw9Sg3v9OmdG21o4u9PBf/q6gyPFADtn/err+h73ryZgt6vvgpERVHcQ7LXWtKWrRCziQLnPJdzntT0vBLAOQABrQ6bAWAzJ/4E4M4Y8zeLQdOnUxDu00/Jb92E3G8gFE6A8oIVNazGkJqqfm5NoiDNE9AlCkDbWc0KBfDmm5RVM3EiMHMmsHOnWvQAwxPXAPLh29hYfqSgLXPGwYGC7B3pfPz+O/Xmx4/Xf5w2UaiuprpLmvEEic7MVygvp7Uc/PyAzz83XTzO35+K9QH6RcHPjzoPrT+vQgG88goQHU1xjJtvBlauBHbsIG/Be++RwB47Zhp7eyBdEqFhjAUBGA7gr1a7AgBc1XidhbbCAcbYw4yxY4yxY+0KJmsik6l9lhq4ud+I2gBAmZrUsetaCkkUAgPVjZA1oKtCqiatRWHnTvrB/utf9Pruu6nR+eEH9THGiAJj1lHqQlc6ZUfTUuPjqWCcodm7gwbRCFJznoZki7aRQv/+NH+ivfMVOAceeojutX27/r91e/H3V/fidWUeAeqRUWv30fbt5Ap+4QXtQnXffeSm+vhj09ncwzC7KDDGnAHsBLCMc96h1cU55xs457Gc81gfYwJW7cDFZSRqA22A9Esmva7ZOX+efozjxlnXSEFfiQsJSRQ4p+3118lHfPvttP/mm6nx/+or9TnGiAJgPaLg6qpOv5WQ0lLbS0ICCYKhlGHJz64pPJo1j7TR3rhCbS3wzDPU8/7vf7XPmegMUiZd376GJ7u1HhkplTRKGDYMmDFD+zmursA995B7srzcNDb3MMwqCoymDO8EsJVzvkvLIdkANOe+Bza912XIZHbgA/pBnl1B6Z3dhdRUypaKiKAMDGv5BzdGFAIDya1RXk694KQk4Kmn1KmFcjkwZw6wZw9lVgHGi4I1VEptnXkkMWAApWdWVhp/rdJScqcYch0B2hegSUkBHB11Z4ONG0d/M0OjTc6p5PaQIRRUvv9++puZGmk0r891JDF4MLmEpZHRN9/QZ3/hBf1pqosX02zpL7/svL09EHNmHzEAGwGc45y/qeOw3QDub8pCigNQzjk3QX3h9mEzOAZMBTRc6EYuJE1RAKzHhWTsSAGg0cLatZRx1DqP/e676ccuLWHa3UYK2ho1KQOpPaOFxERqkG+6yfCxUuE9zd5zSgoJlK7SIJLY6IsrHD1K2Xp33UWi++uvwKZN5pkf0B5RGDSIvpv0dBrpvPwynXfHHfrPGzECiI0FPvpIBJy1YM6RwmgA9wGYwBhLbtqmMcaWMMaWNB2zH8AlABcBfALgn2a0RycOEZTVUXt6jyVur4Zz48o/lJXRcYMHq0XBWlxI7RGFH36g7fHH27pGRo+mEYWUhVRURA2bm5v++1taFAoLadMnCu2JKyQk0Hdz3XWGj3V2JvdLa1HQFmSWCAo15r8dAAAgAElEQVSiNO3WcYXqagpwL1gAjBpFfvpPPqEgrTEC1VHaO1IAaHSwYwelsT7/vHFitXgxfTeHD3fc1h6K2Savcc7/AKA3JYFTXYql5rLBWHpF3Q7gEdSf/d2yhmzdShOB0tLox6oLyT0wZAj9oJ2drUcUSkroR6nPHyyJwssvU9DvkUfaHiOTUc/07bdJaKQ5CoayXCztPtJXs0dzApuxxMeTQNrbG3e8pp+9uJgmgumKJ0iMHw/s309pwdJM5JQU8tHb2QFPPw08+6xhH78piIsDHn2UsgUNIY2MUlMpwBwWBtx5p3H3uesu4MknKeA8enTH7e2BiPnhAGS9+0DpZAPVBQu7YD75hFLqDh3Sf5yUeTRkCDWe4eHWIwrFxdRb19dbk4KJ5eXAokW6Z5vecw99Hzt2GJ64JuHpSSOprlpApjX6RMHFBfD1NX6kUFwMnDxpXDxBQlMUDAWZJSZMoO/3H/+guIGvLwWTd+8mF9+aNV0jCADNzH/3Xf0jTQkXF/pf+vBD+qzPPafbTdYaZ2dyWX7zjVispxVCFACAMSiCekOeUYKGhgLL2JCRQf5jAPj7b/3Hnj9PZQuk4GFEhHXFFAz9oB0cqIG3sQGeeEL3cdHR5CLYto0aLWMaCklgLBV4P3OGXFy66lENGGD8SEHy87fHXTN4sHpVMmNF4Z57SABSU2mk9+OPNIq7/faOleToSgYNIuEaNIjmTbSHxYtpLszmzeaxrZsiRKEJNnAIHLOBsrIuWnikNVImRGioYVFITaXj5E2rxYWHU1ZLgYUETRNjRAGgwOWSJfqrZzJGAefERJqFbuxIAbBcXEEKMutyc4WGGj9SSEignrO+ek+tkTKQ0tJIFNzdDRdMlMtJAAYP7n7F5aTP255RgkRkJKX6WjLgbIWB7m72H2A+5ENi4ZAHlBX+0vU35xzYsoXcBDNmUAqivvRYKfNIwpoykPRVSNVk1y6aXWqIu++m76ew0PpFgXPDhdwGDKASH7rKg2sSHw/ceKNxdYMkNNNSpSBzd6v+2x7mzSM30N13d+z8JUvI3dZVCw5JZGRQx2jiRKsTBiEKTbBBQyBTArXnf+76m//9N/1j3ncfZXrU1+uuR6NQUE9Ts+KlNWUgGTtSMJZBg6jKJWCcKEjuo/YEm6XlHztLQQF9fn2iEBpK97t8Wf+1Cgvp79meeAJALkUbG7UoGHIddXcmTKAOlbY1Joxhzhz6n+nKGc67dlH11iNHSPh/sUBHVA9CFCSa0wXT0dDQgVXBOsOWLeRnv/NOEgVAtwvp8mVaTEVzpODnRz3knigKAPm8AfOMFDinNQo8PIBZs8iVcKmDs9v1BZklpAwkQy4kqefa3vRPaVWy+HgKuPd0Uegsjo6Udrtrl/ndr3V1wGOPAbNnU+ZUSgql4L72mnnv206EKEg0iUKvro4rNDRQIHXmTMrw6N+fyiDrEgXNzCMJxujH3xWiUFREgTltQ966OpopampRuPtu+k4iIw0f215R+OILSse8/npy2z3yCDXcAwdSqfWzZ4230xhRMHYCW0ICZciMGGH8/SUGDaKKoIAQBWN4+GHqaEVF0e9q2DAanY4aRaL800/GXaehgZIDTp6kkZ7mb+TiRSoJ8t57wPLlwB9/0L2WLaPFgo4fN89n6wBCFCT8/MCdnNArR46ysnjDx5uKAweoAbvvPnrNGP0z6hIFaY6CpvsIUIuCOf2TCgUNtxcs0G6fMRPXOkKfPjRZb+JEw8e2x32Um0s/0DFjSBguX6bvd/16+sF+8QU1CtqWuNTGmTN0fz8/3cd4eZH4GxopxMeTXVIyQXvQ5loU6CYsjGbWT5xIwjBwIM2l8fSkv/2tt1LRRn0UF1PNrvHjKWvO15dG/0FBJAYxMXSt3btpVUEpTrR4Mf0/rF1r7k9pND1i5TWTwBhYaChc8nOQXZbQdffdvJn+gSZPVr83ahQ1UhUVbfPDU1Pp+Na5/RERdHx2dttCbKbixRfVbo19+9rOsjWmQmpHMTZYKpdTD9uYkcKjj1IpjU8/VWfdDBpE22OP0XcdF0cTqQ4douvqw1DmkfQ5DBXGy8uj2bkPPGD4M2hDCjb36WOev0VPRFcdp7IyEoW5c2ly34IFbY+5eBGYNo2qxn7wAY1qc3JabpMm0UTMvn1bnuvmRsHudeuA1avV7kVLwjnvVtuIESO42Zg9mzcM8OHx8eB1dTnmu49EcTHndnacL1vW8v0DB6h+6K+/tj3nxhs5HzOm7fuJiXTOgQPmsfXgQc4Z43zhQrJh+PC2x8THkw2//GIeG4ylXz/OFyzQf8yOHWTrmjX6jzt4kHOZjPOZMzlXKnUfp1Jx7uHB+eLFhu2bO5fz0FDd+7dtI9v+/tvwtbTx6690/uTJHTtf0JKqKs5vvpm+03ffbbnv0CHOvb059/Li/I8/Onb9nBxqBx55pPO26gHAMW5EGyvcR5qEhsI2sxRQdlFc4ZtvyA95//0t35fy0rW5aFqno0pIfmxzxBWys2nFuvBw8onedhv531svqWku91F7MVTqoqSE4gUxMVTqQB+TJwNvvUUr9T33nPZjOKe4UGmpcTV7BgyglESFQvv+hAQaIQ4fbvha2pBGCsJ1ZBqcnKhi74wZNIJ89VV6/3//o+wnd3fKJOpouQx/f2oDPv/cKuYaCVHQJDQUrFGBXkXOXRNX2LKFGpHo6Jbve3lRw9FaFKQaQNpEwdOT/rk6IgpJSbpnAEvr3dbW0o+gVy8SBYBcXK3tk+y3JIaK4j35JH2PGzcal8r42GPk+3311bbllk+fpoZh/nz6O86da/h6oaH0vWZmat8fHw+MHdvxNMs+fYBVq6hshcA0ODjQ///8+VQH6pZb6G8dG0uCINVh6igrVlAq+vr1prG3MxgznLCmzazuoyb3R/pHo/iffw7SfkxjI22dJS1Nv/vi7rs5Dwxs+d6hQ3TO3r3az5k0ifP2fj+7dtE1nZ05/7//4zw9veX+lStp/5dfqt9TqTjv35/z6dNbHvvf/9KxNTXts8HUzJ7NeXi49n0HD5KNzz7bvms2NHB+002c29tzfuQI56WlnD/2GOc2Npx7enL+4YecKxTGXSshgWw4eLDtvuxs2rduXfvsE3QNSiXnS5bQ3+iuuzivrTXdte+4g3N3d84rKkx3TQ1gpPvI4o18ezezikJWFucAL159R1NcIbvtMZMnc+7rSw1gWVnH7/Wf/5CP/upV7fvfeov+PNkaNmzcSO9dvKj9nCee4NzR0fjGKTOT/OAxMZzfey/ntrZk08yZnP/2G+f79tH9Fi1qe+7SpZz36tXyR/HUU5w7OBh3b3Py0EOc+/u3fb+yksRs8OCO/ZiLijgfMIBzHx/aZDLyAxcVte86Tf9n/IMP2u7bupX2HT/efvsEXYNKxfnp0/pjTB3hr7/ob//GG6a9bhPGioJwH2ni7w84OsI51xEAUNY6C+nqVSoW5uBAQ8h+/WhR8Ly8ttfKz6d009dfp8yCDRuoAuUPP9Bwc8sWcjvoyhSSJrEdPap+LzWVUtl0ldWOiCA3j6HZsgBVEb33XsrP/vprsufKFaqOmZhIK3Ldfjul6L3zTtvzb7uN5iRolgcwx8S1jiC5j3jTAiwbN5LPdsgQ+oyffmp4aUtteHmRb1mpJL/9sWOUbdLez+zvT/dvnZb63Xe0TrW3N33vAutEmhdk6jpRo0ZRSutbb1l2FUhjlMOaNrOOFDjnPCKCq26/jScmuvHU1FY95DfeICVPS+M8KYmySBgjl8LixdT7v/12zgMCOFevQKx727RJtx01NeSa0HRzTJ+u2y3Cubqn8d13hj/nSy/RsZs3t91XXc35xx/TZ7lwQfv5tbU0Uli6tKV9kZGG721u1qyhz6b5d/Dx4fzOOznfubPz16+vp95iZwgP53zGDHqemUnPAfr+Opp1JOj+7N9P/wfPP8/5uXPGj/qNAEaOFBgd232IjY3lx44dM98NZs0Czp/H6a9DUV2dguuuSweT8s7j4qhnrTn7MC2NJp5s2kTBwyFDKKtF2qKiqBZNeTltFRX0qFAAU6fqr+wYE0O9UGlG5eDBNNtyxw7tx1dVUY35V14B/v1v3dc9dIgCmffcQyOEjjJjBs3evHyZek9S8bZff+34NU1BYiKt0zB8OI14xo+nv4s1FYabMYPqXS1eTFlNKhXNA1m2rGMT1gQ9A87pt/nHH/Ta0ZFGJVFRNKN//Hj9K+npgTF2nHMea4QNlu/9t2cz+0hhxQrO7ex4dubHPD4evKIiid6/fJnrDQyXllI+sylZvJhzNzfyXdbX08jh3//Wf05wMAXAdFFSQnn8ISGcl5d3zr4NG+g7OX2aXoeFUW9cYJjly9WjmGnT6P9LIOCckhqSkjj//HOKE06YQPMgAM6feabDl4WRIwUxo7k1oaFAQwO860fhAmxQWPgNXFyGUzoaoDvl0N3d9LaMGkXVG9PSqPlQKtuWt2iNvhpInFMPOieHRgudXU1r2jR63LeP7iutuiYwzJQpNAJ8/nkqhGhNoxiBZZHLaZSrOU+FcyrL0gX/JyLQ3JqmgmV2mSXw8JiIgoJvwDmnYOzIkerVzroCzYqp2grhaSM8nI7VFqj65BOq4bJ6tfranSEggP5x9+6lf9qSEusINHcHJk0CTp2iWlJCEASGYIzmn/j7m/1WQhRaI1WxTEuDr+9c1NVdQvWp3RRHMGZikikJC6PZlJqiYMxIQaGg0YXEpUs06WbxYirapavOS0e47Tbg8GH1DF0hCgJBt0aIQmsCAwF7e+DiRXh7zwRgg/otb9K+OXO61hYbG5ox+fffVL3T39+wy0dzwZ2CAuDxx2l08e23lG66a5dpU+luu42CpFu30mshCgJBt0bEFFojk1GJiYsXIZd7wcPjZjjsjgePiwPr37/r7Rk1iuYJtF5YRxeDB5OYrF0LPPQQzVv4xz+A//zH8Fq9HSE2lqq2Soufi5iCQNCtMarLyBj7P8aYKyM2MsaSGGOTDZ/ZTdFYXN2/cgyc0hpQP+MGy9gyahTFB06cMOw6AmhS1JAh5O6aPJlGDB9/bB5BAEhEp01Tu6vESEEg6NYY60d4kHNeAWAyAA8A9wFYYzarLI1U716lgtcv1QCAvBtrLWOLZkDYmJECQEHx48cpqGzsOZ1BKpAHCFEQCLo5xrqPpPSIaQC2cM7PMNaDUyZCQ8ntkpMDm517UT3cA3m2P6A/5+jyj923L9C7N5XNMLaBN6Z8symZNInS6BobhSgIBN0cY0cKxxljP4JE4SBjzAWAynxmWRipDO6ePcDp01DcMRV1dZdRWWmBdVSl5TkB49xHlsDVlWZhAm1XhBMIBN0KY0XhHwBWAhjJOa8BIAewUN8JjLHPGGMFjDGtM6kYY+MZY+WMseSm7YV2WW5OpLTU118HGEOv+54FY7YoLPyfZeyZPp0EoV8/y9zfGJ56iha97+gaAAKBwCowqvYRY2w0gGTOeTVj7F4AMQDe4ZzrXNGcMTYWQBWAzZzzNktAMcbGA3iKc35b6336MHvtI4BmDjs6kjtk3DggIQGnTk1FTU0qrrvuUte7kAQCgaCTGFv7yNiRwocAahhjUQCeBJAOYLO+EzjniQCMWD3dCrGxAUJC6HnThDUfn7moq8tAZaWZBUkgEAgsiLGioGgqqDQDwHuc8/cBuJjg/tczxk4yxg4wxnRGRxljDzPGjjHGjhUWFprgtkYwcCClW86eDQDw9p4JxuSWcyEJBAJBF2CsKFQyxp4BpaLuY4zJQHGFzpAEoD/nPArAuwC+03Ug53wD5zyWcx7r4+PTydsaySOP0Jq8vXsDAORyD3h43KyuhSQQCAQ9EGNFYR6AetB8hTwAgQDWdubGnPMKznlV0/P9AOSMMe/OXNOkTJtGq2Bp4OMzF/X1V4QLSSAQ9FiMEoUmIdgKwI0xdhuAOs653piCIRhjftJcB8bYqCZbijtzTXPj7T0DjMlRUPC1pU0RCAQCs2BsmYu5AP4GMAfAXAB/McbuNHDONgBHAAxmjGUxxv7BGFvCGFvSdMidAFIYYycBrAdwF7dyv4xc7gEvr9uRm/sJGhqKLG2OQCAQmBxjU1JPApjEOS9oeu0D4OemeECX0iUpqXqorj6Lo0eHISDgUQwcqGVBe4FAILBCTJ2SKpMEoYnidpzbo3ByGgp//0XIyfkANTVphk8QCASCboSxDfsPjLGDjLEHGGMPANgHYL/5zLJugoJWgTF7XLq00tKmCAQCgUkxNtC8AsAGAJFN2wbO+dPmNMyasbf3Q79+T6OoaBfKyw9Z2hyBQCAwGUbFFKwJS8cUJJTKavz11yDY2/dFTMwRUfpCIBBYNSaJKTDGKhljFVq2SsZYhenM7X7Y2DghOPhlVFb+JWY5CwSCHoNeUeCcu3DOXbVsLpxzA4sF93z8/BbAyWkYLl16BipVvaXNEQgEgk5zTWYQmQrGbDBgwDrU1V1CdvYHljZHIBAIOo0QhU7i6TkZHh6TceXKy2hsLLW0OQKBQNAphCiYgAED1kKhKMPly89Z2hSBQCDoFEIUTICzcyQCA/8POTkfoLj4B0ubIxAIBB1GiIKJCA5+Fb16hSM19QE0NHTRmg8CgUBgYoQomAgbGwcMHfoVFIpSnD//kFhzQSAQdEuEKJgQZ+dIhIS8iuLi3cjN/cTS5ggEAkG7EaJgYgIDl8HD42ZcvPgEamouWNocgUAgaBdCFEwMYzIMGfIFZDIHnDs3HypVo6VNEggEAqMRomAG7O0DMHjwBlRWHkNGxipLmyMQCARGI0TBTPj4zIaf30JkZr6K8vLDljZHIBAIjEKIghkJDX0H9vYBuHjx/8C5ytLmCAQCgUGEKJgRW1sXBAe/gsrKY6KSqkAg6BYIUTAzvXvfCyenSFy69CxUqgZLmyMQCAR6EaJgZhizQUjIa6iru4ScnI8sbY5AIBDoRYhCF+DpeQvc3SciI+MlKBTlljZHIBAIdCJEoQtgjGHAgNehUBQjM/M1S5sjEAgEOhGi0EW4uMTA13c+srLeQl1dlqXNEQgEAq0IUehCgoNfAecqZGS8YGlTBAKBQCtCFLoQR8cgBAQ8hry8L1BVddrS5ggEAkEbzCYKjLHPGGMFjLEUHfsZY2w9Y+wiY+wUYyzGXLZYE/37PwtbWzdcuvS0pU0RCASCNphzpPAFgCl69k8FMLBpexjAh2a0xWqQyz3Rr9+/UVJyAJmZr4l1FwQCgVVhNlHgnCcCKNFzyAwAmznxJwB3xpi/ueyxJgIDH4O392xcurQSZ87MgUJRaWmTBAKBAIBlYwoBAK5qvM5qeq/HI5PZIzz8fwgJWYuiom+RlDQK1dXnLG2WQCAQdI9AM2PsYcbYMcbYscLCnrH+MWMM/fo9haion9HYWILjx0eioOAbS5slEAiucSwpCtkA+mq8Dmx6rw2c8w2c81jOeayPj0+XGNdVeHjchNjYJDg7R+Ls2Xm4ePFJcK60tFkCgcDK4BxQdkHTYGv+W+hkN4BHGWPbAVwHoJxznmtBeyyGvX0AoqMTcPHicmRlvYm6ugyEhW2FjY2DpU0TCHoE9fVAaSlQUgLU1VHjKm0qVcvXCoX21wpFy32qpmr4nNMG0Hu1tbTV1amf19erz9fcpOtwrn7knI6vrm67rVwJrF5t3u/KbKLAGNsGYDwAb8ZYFoD/AJADAOf8IwD7AUwDcBFADYCF5rKlOyCT2WHQoPfg6BiK9PQncPr0NEREfAdbW1dLmyYQNKNSUeNUUwM0NFDj1dCgfs4YYG/fdquqAgoKgMJC9WNhIV2nvr7l1tAANDbSplConyuVdH2ZrOUjY2Sb1DBLj3V1aiGore3678rODnB0pM3eHpDLAVvblptMpv3zODkBvr70qLmNH29+u80mCpzzuw3s5wCWmuv+3ZW+fZfBzs4HqakPIDl5PCIjD8DOrrelzRJYmIYGaow1e5SaPUtA/QjQvrKyto1wURE1xHV16p5sXR01xtpQKoHKSqCigh6rqlrepzPY2VFDJwmHgwM92tnRZmtL7zk7U4NqY6P+bNLnl55LwqD5aGcHeHoCHh60Sc979aIG2MbGuE1qwDWfazbm0v2kzdGR7JZ1i4htWyzpPhLooHfv+bC19cSZM7Nx4sSNiIz8EY6OwZY2S9BJamup5yr1XsvLqZFtvZWVUUMubYWF9F5nYYwaRWdnarSkzdERcHFRN3CayGRAUBDtd3WlRxcXdWNuZ9eyIZdcH603qefr40Obry/Zoe2eAssiRMFK8fKaiqioX3D69K04cWI0IiMPwtl5mKXN6tEoFEB+PpCbq96qqtTuC82tspIa97Iy9VZeTj1XzV4jY/ReeTn1yA1hb0+Nr68vbTEx6sbU2Zl6q61dDZo9Us1G1t1d3QD7+ABeXuretkCgCyEKVoyb2/UYPvx3nDx5C06cGIPw8P/B03OSpc2yGhQK4MoVdSBP07ctBeqqqloG6iorW7pDpMeiIuqRG3KN2NqSK8PFhRpdyTURHEyNuY2N2qUjbYy1PFZyZbi5qXvezs7Um5bLu+a7Ewh0IUTBynFyCkdMzGGcPn0rTp2aioED30VAwCOWNqvLkNwRJSXAuXPAqVPq7cwZ3b5wbdjYUOMruUFcXalhDgykRtrfH+jThx6lzdW1ZYBQuDsEPR0hCt0AB4d+GD78MM6evRtpaf9ETc05DBjwJmSy7vnnKy4Gzp8HsrLaBkIl/3lFhXprbGx5vp8fEBkJPPYYEBZGDbymf1vaWmdu2NmJRl0gMET3bFWuQWxtXTBs2PdIT1+BrKy3UFubhqFDt8PW1s3SpjXT0KD2r0v+9tJSavxTU0kIUlPJVdMaLy91ELJ/f+qha25ubsCgQSQGvr5d/9kEgmsFIQrdCMZsEBr6Jnr1CkNa2j+RlDQaw4bt6bLMJJUKuHiR3DZXrrTdiot1n+vrCwweDMyaRY9DhgD9+tH7Xl7kmhEIBJZH/BS7IX36LIKj4wCcOTMbSUnXISLie7i5XW/Se5SUUM/+5EnakpOB06cpWCvh5ES9+v79gVGjgIAACqJKQVV3d9r8/MhnLxAIrB8hCt0UD48JiIn5C6dP34rk5JsQFrYJvr7z2nWNmhogLY0a/wsX6Ln0qNnrd3cHoqKAhx4CoqOBYcMo28bDQ/joBYKehhCFbkyvXoMwfPgRnDlzB86evQs1NWno3//fYK1a6uJi6uWnpFAGz4ULJARXr7a8XkAA+e1nz6ZHyYffr59o/AWCawUhCt0cOztvREX9hPPnH0JGxvPIzCxAVtYbSEqS4/RpEoNcjTKDrq7k0x83jh6lxn/gQHIHCQSCaxshCt2c/HwgIcEe8fGb8dNPb+HSJW8AgL09x9ChDJMmkbtH2vz9Ra9fIBDoRohCN4FzICcHOHGCgr7JyfT80iXa7+LCMGaMN+bPP4GAgH9i6NACREZugZvbDZY1XCAQdCuEKFgpjY3U6Ccm0vbnnzSxSyI0lOriPPwwcNNN9JzSOoejouJtnD17F06cGIvg4BfRr99KMCaK3ggEAsMIUbASqqqAo0eBw4dJBA4dUqd/DhoE3HYbNfzDh1Pw18VF97VcXa9DbGwyzp9fjMuXn0Np6S8IC/sS9vZ9uubDCASCbosQBQtx8SI1/EeO0Cjg9Gn1Sk7DhgEPPEDB4DFjKM+/vdjaumHo0G3Iy5uMtLTHcPRoJIYM+QLe3reZ9HMIBIKug3PeJrvQ1AhR6EI4B37+GXjtNeCXX+g9V1fguuuA554D4uLouakmejHG4O//IFxdb8DZs3chJeV2eHvPQp8+S+DhcTMY66argAgEPRTOOU7ln0JKQQqyKrJwteIqrlZcpeflV7F05FL8Z/x/zGqDEIUuQKkEdu4kMUhKogygNWvIJRQWZv4VmpychiAm5k9cufIKcnI+QlHRt3BwCIG//yL4+y8UK7uZiPb24irqK1CvqEejqhEKlQIKlQKNykb4OvnCw9HDjJYKNKmsr8S+tH24XHoZZXVlKK8vb36sqK8AANjKbCGXyenRRg57G3v0d+uPUM/Q5q2vW1/YymzBOUdFfQXyqvKQX52PvKo8NCobEeIRgoFeA+Hl6NXi/6S6oRq/XP4Fey/sxf60/ciuzG7e5+7gjkDXQPR17YsR/iMQ4x9j9u+DcVOtrddFxMbG8mPHjlnaDKOorga2bAHWrQPS0yk2sGIFcN99VMXTEqhU9Sgs3IWcnI9RXv4bGLOFt/dM9O//HJydozp83fyqfORW5SLaL9qE1loPKq5CZX0lSutKUVJbgitlV3Cx5CJtpfRYVFOE1RNWY1ncMr3X4pxj+cHlePuvt7Xud5I74fVJr2NJ7BLIOjCaS8pNwoPfP4ix/cdi9YTVcLHXE4CyMvKq8rAqYRUeHP4gRgWMMnh8dkU2Dl09hFDPUAzxHoJe8l5G3adR2YifLv2EL099ie9Sv0OtghZxtrOxg5u9G9wd3OHm4AZXe1cwsDbCXauoRUZZBuoU6pWT5DI5fJx8UFxTjHql7pru7g7uCPUMxUDPgSitK0X85XjUK+vhYueCyQMm47ZBt+G6gOsQ6Bpo0r8dY+w45zzW4HFCFExPWhrwwQfA55/TilsjRwIrVwIzZljXylfV1anIzf0EeXmfQ6EoR58+SxAc/BLkcq92XefrlK/xyL5HUNlQiXNLzyHUM9RMFhtPZX0lyurKUFFf0WKT28gR4x+Dvq59tfbqOec4U3gGB9IO4GD6QVwuu4zS2lKU15dDxVVtjvfp5YMBngMQ6hmKvKo8/HzpZ6yZuAZP3/i0VrtUXIVH9z+KD499iAVRCxDbJ7ZFD9SG2WDLqS04mH4QNwXdhI3TNyLYw/iCh9tOb8ODux+Ek9wJJbUlCHANwAfTPsDtg2/Xe169oh6XSi8hrSQNacVpuFhyEWklacipzIGKq6DiKnBweuQcbg5uGO43HDH+MYjxj0FU71bb/n0AABx6SURBVCg42XVu9mN1QzXGbxqPYznHYMNs8MyNz+D5cc/DzsauzbFKlRIfHvsQz/7yLCobKgEADAxB7kEI8wnDUO+h6OvWF3KZHHIb+n5tZbawYTY4fPUwtp/ZjqKaIng6emLu0LmYHzkfsX1i4WDrYLS9Kq5CbmWuunNQchF51XnwdvRGb+fe8HP2Q28nerSV2SK9NL3Fd5tWkgZ7G3tMGzgNtw68FWP6j9H6WU2FEIUuRqkE9u8H3nsP+PFHWpjlzjuBpUuBG26w7gljjY2lyMj4D7KzP4CtrRuCg19Bnz4PG0xjLaktwdL9S7E9ZTtG9hmJlIIUzAqbha13bO0iy7Xz4dEP8eiBR7U24hK9nXojtk8sRvYZiZEBI1GnqMOBtAP4If0HZFVkAQAifCMQ2TsSHg4etDmqH/u59cMAjwFwc1CXLleoFFjw3QJ8dforvDT+JTw/7vkW91RxFR7Z+wg2JG3Av274F9bcvEanMH124jMs/3E5lCqlUaMGpUqJZ355BmsPr8WYfmOwY+4OXCq9hEV7FiGlIAV3Dr0T66esh7+Lf/M5acVp2H1+N3Zf2I1DmYeg5MrmfZ6OnhjoORCBroGwldmCMQYZk4GBgTGGwupCJOUmobCG8qQZGAZ7D0aEbwRC3EMwwHMAQjxCMMBjQLNbRR9KlRKzvp6FfWn7sHnmZvx06SdsOrkJ0X7R2DJrCyJ8I5qPPZ1/Gov2LMJf2X9h8oDJeGHsC8irysPZwrM4W3QWZwvP4nzReZ29dQdbB0wfPB3zh83HlNApZm2IrQkhCl1EaSmwcSPw/vtARgbVD1q8GFi0qGNZQ9oorytHemm60f5EzjkqGypRUF3QvBVWF6KwphAhHiG4OeRmePfybnNeVdVpXEh7HH9eTcDhMh+k1ngjus8NmBA8ATcF3dSiQfnh4g948PsHUVhTiFXjVuHpG5/G878+jzWH1iB5cTKi/PS7ogqqC1DdUI0g9yCDfnilSonSulKtNrfmTMEZjNgwAjf0vQH3DLsHrvauLbaqhiocyzmGozlHcTT7KFKLUsFBvwFXe1dMCpmEKaFTMCV0CgJdAw3eT5utD+5+EJtPbsbzY5/Hi+NfBGMMKq7Cw3sexsYTG/HMjc9g9YTVBj/31fKreGjPQ/gx/UfcFHQT1k5ai8jekZDbtFyzs7S2FPfsugc/XPwBj8Q+grenvN3c0DUqG7Hu8Dq8+NuLcLB1wPNjn0dBdQF2X9iN1KJUAEBk70hMC52GCN8Icmt4DYSno+FsB845cipzkJSbRFteElKLUnG59DIaVeqVkWxltrgr4i68M+UdrdflnOPxA4/jvaPv4b2p72HpqKUAgO9Sv8PDex5GeX05Xr7pZfxz5D/x39//i7WH18LDwQNv3fIW7hl2j9bvUalSoqyuTO3yaXL/NCobEeAaAFd7V4Ofr6chRMHMpKQA775LMYPaWkofffRRchGZcp3djLIMTPlyCs4Xn8fTo5/G6gmrYSPT3YM/V3gO9+y6B8l5yTqPYWAY7j8ck0MmY9KASbg+8Hoczz2OHWd3YOe5nciqyIItAwa7AFdr5ahoWvoszDsME4InoE5Rh40nNiLcJxxbZm3BcP/hAKhxClkfgtF9R2PvPXt13j+/Kh/RH0cjryoPrvauiOwdiUjfSET5RWGY7zCU1ZUhpSAFKYUpSClIwdnCs6hT1GHFDSvw+qTXdV63QdmAuE/jkFWRhdOPnEZvZ8MB9Ir6CpzIPQEbmQ2uC7iuTYPbEZQqJRbvXYyNJzZi5eiVeGXCK3hoz0P4IvmLFkJhDJxzbDyxEcsPLkdlQyXkMjmG+gxFlF8UonpHIcg9CCt/XomMsgy8N+09PDziYa3XSStOw5J9S/Dr5V8hl8kxPmg8bh90O24ffDuC3IM6/Zk1UaqUyK7MRnpJOi6VXkJyXjI+Ov4RfHr54NPpn2LawGktjn/7z7fxxMEnsDxuOd645Y0W+wqqC7Bk7xJ8m/otHG0dUauoxcLohVg7aS28erXPzXmtI0TBTPz0E/Dqq0B8PODgANx7Ly0LGRlp+nudzDuJqVunolZRiymhU7A9ZTsmhUzCttnb2vwgOOf4IvkLPHrgUTjJnfBE3BMIcA2Ar5Nv8+bp6ImUghT8mP4jfkz/EUeyjkChUoCBgYPD3sYet4TegjlD52DagIkoL/gYGZlrkF4FXOLjcKIM+D3zEGoaa/Dk9U/i5Qkvt/HBrvljDZ755Rn8sfAPjO43us1nUnEVpm6disQriVg9YTXSS9JxMv8kTuWfavYNS/Rx6YMI3whE+EQgpyoH21O2483Jb+KJ65/Q+n39+5d/479//BffzvsWM4fM7OS33zlUXIWl+5bio+MfYajPUJwtPIsXx7+IF8a90KHr5VXlIf5yPE7mn6Qt7yRyq6jSoa+TL3bO3Ykb+92o9xqcc5zIO9HG7dUVnMg9gfu/ux8pBSlYFLMIb0x+Ay72Lth1bhfu/OZOzAqbhf/N+Z9WFxnnHFtObcHmk5vx7JhnMSF4Qpfa3lMwVhTAOe9W24gRI3hXUNNQw/Or8ptfl5VxvnAh5wDn/fpx/tprnBcVme/+v176lbu+6soD3wzkKfkpnHPOPz3+Kbd72Y4HvR3Ek3KSmo+tqKvg83fO51gFftMXN/Hsimyj7lFRV8F3p+7mK39ayb869RUvrytvc0xtbQZPSZnD4+PBDx/uz7NytvGCqgKd16xuqOZ+6/z4mM/GcJVK1Wb/mt/XcKwC//jYxy3eV6qU/FLJJf596vc8MSORF9cUt9ivUCr47K9nc6wC3356e5vrHso8xGUvyvjC7xYa9dm7ApVKxR/b/xjHKvBXfnvF5NcvqCrg8Zfj9f49rIm6xjr+9E9Pc9mLMh70dhB//+/3ucMrDjzu0zhe01BjafN6PACOcSPaWIs38u3dukIUlColn7hpIrd50Ybfs/Me/uG3J3jfvpzLZJw/8wzndXXqYxVKBT+QdoDft+s+vvC7hfzdv97lhzMP8+qGaq3XVqlUvKSmhF8svsjrFfVaj9l+eju3e9mOD31/KM8sy2yx76+sv3jgm4Hc4RUHvuXkFp6Uk8QHrh/IZS/K+EsJL3GFUmGy70GTkpJ4/vffkTw+HjwpaSwvKtrPVSql1mPf//t9jlXg+y/sb/H+4czD3OZFGz7nmzlaBcMQtY21fMxnY7jdy3b810u/Nr9fWV/JQ94J4UFvB2kVNkuiUql4VnmWpc2wKg5lHuKh60M5VoGHvBPSovMlMB/GioJwH2nho2Mf4ZF9j2BKyG345WICGmVVcMqdjNem/wv/nDoBjDGkFafhi+QvsOnkJmRXZsPT0RM2zKY5G0PGZAjzDkO0XzQalA3IqcxBblUucipzmnObJf9wtF80onpHIdovGkm5SXjqp6cwpt8YfH/X91onMRVUF2Du/+bityu/wVZmC18nX2ybvQ1j+4816/eiUimQm/sJrlx5CQ0NeXB0HIzAwMfRu/f9sLV1bj6uQdmAsPfD4GrviuMPH4eMyVBaW4rhHw8HYwzJi5M77L4orS3FjZ/fiKyKLPy+8HdE9o7E4j2L8UnSJ/jtgd8wpv8YU31cgRmpbqjGhuMbMGPIDIR4hFjanGsCq4gpMMamAHgHgA2ATznna1rtfwDAWgDSFL73OOef6rumuUXhStkVRHwYgSHOcSh880dcySvHmCc+wnnPt1FQnY8Y/xg4yZ3we+bvkDEZpoROwYPRD+K2QbfBzsYO2ZXZOJ5zHMdzjyMpNwmn8k/BUe6IPi59aHOmR3cHd1wovoDk/GQk5yUjryqv2YY7wu7A1ju26s2ZblQ24vn453G14iremfKOUZk5pkKlakBh4f+QlfU2KiuPwdbWHf7+D8HP70E4Og6ATGaHrae24t5v78W22dswL3we7vzfndh9fjcOPXjIqElJ+rhafhXXb7weHBzPjXkO/9z/T/zrhn/htUmvmegTCgQ9D4uLAqMk9wsAJgHIAnAUwN2c87MaxzwAIJZz/qix1zWnKHDOccuXt+D3jCNQvXcafV2CsGkTMHo0UKeow5envsRbf74FFVdhQdQC3B91P/q4mKbyaH5VPk7mn0RFfQVmDZmlN8PIWuCco6LiMLKy3kFh4U78f3t3HiVVdSdw/PurfaM3utlBQEhUFFAYXOKCmBiXjNFMHBM1R2fG4DjmTJxIBjXLRGec6OQkxsmJJ3GNZkzQMaLi6LggkhgXNIigohEQodGhu+m9u7q6lt/88V6XRdNAI1Xd1VW/zznvvHq3br26F173r9997/0uZAAhEBiHPzCJi174M70ZYfGc8/jOC/fwo8/9iCUnLMnLd2/YuYGT7jmJtkQbs8fOZs1lawj6hukxcWNGgGIICscDP1DVz7vb1wKo6g9z6lxKEQWFO9feyddXfB35n9s41nsFjz8Oo+2ut0Hp6dlGS8uzJBLb6enZRiKxjWe2beTba52TwBPqKlnx1ceoqc7fENfqratZ+uxS7vjLOzhq7FF5268xpWiwQaGQCfEmArlTw9cDxw5Q769E5GScs4p/UtXtA9QpuG2t2/nGim/B+ws5c8zlPPiAzVl8IEKhKYwf/7e7lc2eraxoWsifm9Zz3REh1r9xCrW1X2L69JuJRA4+FcYpU0/h5ctePuj9GGM+NtxZUlcAv1XVhIhcDtwL7HETsogsBhYDTJkyJe+NSKWUE/9jMQlJ8yXfXSxb7snrA2jlSkR48uInSaaTxPx+tm//Mdu23cyuXSuYOPFKpky5xjK0GlNkCpm0eQcwOWd7Eh9fUAZAVXepal+CkjuBeQPtSFVvV9X5qjq/rq4ur43s6YHjrvgV24P/y2flJh66Y7oFhDyK+CNUhirxeiNMnfo9jj32PcaNu5T6+v/kxRcn8PrrJ7N9+0+Ix7cMd1ONMRT2moIPZ0joNJxg8Cpwoaq+lVNnvKp+5L4+D1iqqsfta7/5vqaweEk9d/iPZHp0Nu995/lPlKrYHLiurndoaFhGU9NyurrWAxCNzqa29lxqas5k1Kj5ePaTRM0YM3jDfqHZbcRZwE9xbkm9W1VvFJEbcB6ieExEfgicA6SAZuAKVX1nX/vMZ1B4cM0qLnjwInzRNt7+x3XMHD0zL/s1ByYe30JT06M0NS2nre0FQPF6Y1RWnkx19SKqqk4lFpuz36ytxpi9K4qgUAj5CAqpTIrrn7+ef/v9jdA8k8cvWcbZ847OUwvNwejtbaK1dRWtratoaXmOePxdAHy+GiZO/AcmT/42Pl/5Zbg05mAVw91HRWlb2zYu/N2F/HH7H5H1l3LZhJ9x9rzY/j9ohkQgUMuYMeczZsz5ACQSH9LauorGxuXZ6UQPOeS7TJjw93g89lyCMflWVgPoD298mDm/mMP6nes5cef9BJ+8hx9cZwGhmAWDExg79iKOPPIhjjnmVaLR2WzadBVr1hzOzp2/QfcxkY4x5sCVzZnCfW/cxyWPONMf/uucZZx13KFcfTVMyM8DyWYIVFTMZ86cZ2lpeYYtW5ayceNFfPDBvxOLzcXvr80ugUAdfv9YIpHDCASGLv2HMaWgbILCeYedR/2iepacsIQLLwgQi8HSgafRNUVMRKipOZ3q6s/S0LCMHTtuo739ZZLJRtLp9j3q+/21RCJHEI0eQSRyONHokYwaNd+uSxizF2V3oXntWpg3D77/fbj++jw2zAy7TKaXZHIXyWQjicQOurvfobv7bbq6NtLd/RapVKtbU4hEDqeiYgGjRh1LRcUCotGj8HjsARVTuuzuo704+2x46SV4/32oHNrJp8wwUlV6e3fS2bmOjo5XaW9/hY6OV0gmmwDwekdRVXUqNTWfp7r6dMLhQwc9ZaYxI4HdfTSAF1+EJ56Am26ygFBuRIRgcBzB4BmMHn0G4ASKnp6ttLe/Qlvbapqbn2LXrscACIWmUV19OhUVf0EoNI1QaCrB4GQ7mzAlr2zOFFRh0SLYuBE2b7Zkd2ZPqko8vpmWlqdobn6a1tbnSKc7c2p4CAYnEwpNJRqdxahRxxCLHU00OstujzVFz84U+lm5Ep5/Hm691QKCGZiIEInMIBKZwcSJV5LJpNxU4Fvp6Xk/u47Ht7Bz56/58MPb3M/5iUZnEY3ORsRDOt1JOt2VXVSThMMz3Ivds4hGZxGJfMoCiSlKZRMUxo2DSy6Byy8f7paYkcLj8REOTyMcngacutt7qhni8S10dq6ls/N1OjrW0tKyEhEvXm/UXWIEAuMQEbq63qSp6RGciYgAvEQin6aq6mQqK0+hquoUgsHxQ91FY/ZQNsNHxgy3TCZBd/e7dHW9TXf3W3R0rKWt7Q+k0x0AhMOfoqrKCRAVFccTCk2zi90mb2z4yJgi4/EEicVmE4vNzpZlMik6O9fR2vo8bW2raWh4gI8+ugMAv38slZXHU1HhLJHIp/F4Ini9YUsOaArGgoIxw8jj8VFRMZ+KivnAElTTdHW9SVvbi7S3v0R7+0vusNPuRAJ4PGG83jAeTxiPJ4hIEI8nhMfjrEOhacRiRxGNOovfXz30HTQjjg0fGVPkensbaG9/mURiO+l0nEymm0wmTjrd7b7uIZNJ5Cw9ZDLdxOPv5TywB8HgJKLRo4jF5hCLzSUWm0s4PMPOOsqEDR8ZUyICgTHU1p5zwJ9TVRKJHXR1baCrawOdnevp6tpAS8szqKYA8Hgi7pnE4Yj43QSDCiiqGTweP35/HYHAWPz+Mdl1MDjRzjxKlAUFY0qUiBAKTSIUmsTo0WdmyzOZBF1dG+nsXEdX1xt0dq6juflpnGAggCDiAQTVXnp7G4H0Hvv3+aoIhaYTDh+aXXs8YVKpFndpza6DwUOorj6VysqTLZgUORs+Msbsk2qGVKqF3t4Gent3kkw2kEhsJx7fQk/PFuLxzfT0bEU1udvnvN4YPl81Xm8FPT2byWR6ACEWO5qqqlOpqlpIOHwofn8dfn+1DWMVmA0fGWPyQsSD3z8av3800ejhA9ZRTZNI1JPJJPD5qvH5qnZLCZLJJGhvf8WdUW8VO3b8jPr6H+d+Cz5fTU4K9Gp3P87i91fj9cbIZBLZayl9a1B8vtG7pU/3+2sJBsfj94+x23oPkJ0pGGOGXDodp6PjVRKJHSSTTe7SmH2dSrWQTDrDUAOlRAfnDiyvNwIIqVTLgHW83hjh8IzdlkBgPF5vLGeJuutR7rBZabIzBWNM0fJ6w1RVnTyouplMinS6jXS60739ds9nNTKZlBtImrKLM8S1mXh8E52d62lqenSPIa5cIj53KGsMgcAYd12HxxPd7VZfZwmgmiKTSaL68QLsdtbiTPhUi89XPWICjgUFY0xR83h8eDzO8NW+6gQCdQQCdXut4+Sy2kYy2eTmp8pdOkgmm0kmG7LXTuLxTfT2NmSHqA6O4PWOwuercpdKd4gtnFPn4+/w+WoIh2cQicwkHJ5BKDQdrze8524LwIKCMaYsOLmsphMOTz+gz6mqe1bgPAOi6jwPIuJDxI+IH4/HWYO6waWx35DYLlKpNveOLGfd99xJ/2seqkoy2Ugq1bxbeTA4mUmTrmLy5G8d7D/FPllQMMaYfRCR7C9+iO23vtcbJRSafNDfm0w2E49v2m0JBAqfNNGCgjHGFCG/vwa/fwEVFQuG9HtHxpUPY4wxQ8KCgjHGmCwLCsYYY7IKGhRE5AwReVdENonINQO8HxSRB9z3XxGRqYVsjzHGmH0rWFAQ58mSnwNnAkcAXxWRI/pV+zugRVVnALcANxeqPcYYY/avkGcKC4BNqrpFVXuBZcAX+9X5InCv+/oh4DSxRCXGGDNsChkUJgLbc7br3bIB66iT4L0N2Ptji8YYYwpqRFxoFpHFIvKaiLzW2Ng43M0xxpiSVciH13YAuY/1TXLLBqpTLyI+oBLY1X9Hqno7cDuAiDSKyAefsE21QNMn/OxIUy59LZd+gvW1FA1lPw8ZTKVCBoVXgZkiMg3nl/9XgAv71XkMuAR4Cfgy8JzuJ5e3qu4949V+iMhrg0kdWwrKpa/l0k+wvpaiYuxnwYKCqqZE5BvAU4AXuFtV3xKRG4DXVPUx4C7g1yKyCWjGCRzGGGOGSUFzH6nqE8AT/cq+n/O6Bzi/kG0wxhgzeCPiQnMe3T7cDRhC5dLXcuknWF9LUdH1c8RNx2mMMaZwyu1MwRhjzD6UTVDYXx6mkUxE7haRBhF5M6esRkSeEZH33HX1cLYxH0RksoisEpG3ReQtEfmmW15SfRWRkIisEZE33H5e75ZPc3OEbXJzhgWGu635IiJeEXldRB53t0uyryKyVUQ2iMg6EXnNLSuq47csgsIg8zCNZL8CzuhXdg2wUlVnAivd7ZEuBVytqkcAxwFXuv+PpdbXBLBIVecAc4EzROQ4nNxgt7i5wlpwcoeVim8CG3O2S7mvp6rq3JxbUYvq+C2LoMDg8jCNWKr6e5xbenPl5pW6Fzh3SBtVAKr6kaqudV934PwSmUiJ9VUdne6m310UWISTIwxKoJ99RGQScDZwp7stlGhf96Kojt9yCQqDycNUasaq6kfu6/8Dxg5nY/LNTbN+NPAKJdhXdzhlHdAAPANsBlrdHGFQWsfwT4F/BjLu9mhKt68KPC0ifxKRxW5ZUR2/NkdzGVBVFZGSuc1MRGLA74CrVLU9N7FuqfRVVdPAXBGpApYDhw1zkwpCRL4ANKjqn0Rk4XC3ZwicqKo7RGQM8IyIvJP7ZjEcv+VypjCYPEylZqeIjAdw1w3D3J68EBE/TkC4X1UfdotLsq8AqtoKrAKOB6rcHGFQOsfwZ4BzRGQrzrDuIuBWSrOvqOoOd92AE+wXUGTHb7kEhWweJvcuhq/g5F0qZX15pXDXjw5jW/LCHWu+C9ioqj/Jeauk+ioide4ZAiISBj6Hc/1kFU6OMCiBfgKo6rWqOklVp+L8XD6nqhdRgn0VkaiIjOp7DZwOvEmRHb9l8/CaiJyFM3bZl4fpxmFuUt6IyG+BhTgZF3cC/wI8AjwITAE+AP5aVftfjB5RRORE4A/ABj4ef74O57pCyfRVRGbjXHD04vzh9qCq3iAi03H+mq4BXgcuVtXE8LU0v9zhoyWq+oVS7Kvbp+Xupg/4jareKCKjKaLjt2yCgjHGmP0rl+EjY4wxg2BBwRhjTJYFBWOMMVkWFIwxxmRZUDDGGJNlQcGYISQiC/sygRpTjCwoGGOMybKgYMwARORid06DdSLySzdBXaeI3OLOcbBSROrcunNF5GURWS8iy/vy4YvIDBF51p0XYa2IHOruPiYiD4nIOyJyv+QmbzJmmFlQMKYfETkcuAD4jKrOBdLARUAUeE1VZwGrcZ4cB7gPWKqqs3Getu4rvx/4uTsvwglAXybMo4GrcOb2mI6T/8eYomBZUo3Z02nAPOBV94/4ME6SsgzwgFvnv4CHRaQSqFLV1W75vcB/uzluJqrqcgBV7QFw97dGVevd7XXAVOCFwnfLmP2zoGDMngS4V1Wv3a1Q5Hv96n3SHDG5OXzS2M+hKSI2fGTMnlYCX3Zz3vfNoXsIzs9LX+bOC4EXVLUNaBGRk9zyrwGr3Znh6kXkXHcfQRGJDGkvjPkE7C8UY/pR1bdF5Ls4M2R5gCRwJdAFLHDfa8C57gBOuuNfuL/0twB/45Z/DfiliNzg7uP8IeyGMZ+IZUk1ZpBEpFNVY8PdDmMKyYaPjDHGZNmZgjHGmCw7UzDGGJNlQcEYY0yWBQVjjDFZFhSMMcZkWVAwxhiTZUHBGGNM1v8Dr8Bs/Fh4pcwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.6511 - acc: 0.5693\n",
      "Loss: 1.6511325706697821 Accuracy: 0.56926274\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5895 - acc: 0.3242\n",
      "Epoch 00001: val_loss improved from inf to 1.61345, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/001-1.6134.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 2.5897 - acc: 0.3242 - val_loss: 1.6134 - val_acc: 0.4894\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6456 - acc: 0.5124\n",
      "Epoch 00002: val_loss improved from 1.61345 to 1.23194, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/002-1.2319.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.6458 - acc: 0.5124 - val_loss: 1.2319 - val_acc: 0.6191\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3913 - acc: 0.5863\n",
      "Epoch 00003: val_loss improved from 1.23194 to 1.11362, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/003-1.1136.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.3917 - acc: 0.5862 - val_loss: 1.1136 - val_acc: 0.6690\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2242 - acc: 0.6313\n",
      "Epoch 00004: val_loss improved from 1.11362 to 1.05804, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/004-1.0580.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.2246 - acc: 0.6312 - val_loss: 1.0580 - val_acc: 0.6806\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1001 - acc: 0.6635\n",
      "Epoch 00005: val_loss improved from 1.05804 to 0.92197, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/005-0.9220.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.1008 - acc: 0.6634 - val_loss: 0.9220 - val_acc: 0.7235\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0182 - acc: 0.6893\n",
      "Epoch 00006: val_loss improved from 0.92197 to 0.91255, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/006-0.9126.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.0184 - acc: 0.6893 - val_loss: 0.9126 - val_acc: 0.7207\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9418 - acc: 0.7119\n",
      "Epoch 00007: val_loss did not improve from 0.91255\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.9419 - acc: 0.7119 - val_loss: 1.0728 - val_acc: 0.6860\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7311\n",
      "Epoch 00008: val_loss did not improve from 0.91255\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.8823 - acc: 0.7311 - val_loss: 1.0325 - val_acc: 0.6851\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8462 - acc: 0.7397\n",
      "Epoch 00009: val_loss improved from 0.91255 to 0.87500, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/009-0.8750.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.8461 - acc: 0.7397 - val_loss: 0.8750 - val_acc: 0.7543\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7888 - acc: 0.7555\n",
      "Epoch 00010: val_loss did not improve from 0.87500\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.7889 - acc: 0.7555 - val_loss: 0.8913 - val_acc: 0.7452\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7705\n",
      "Epoch 00011: val_loss improved from 0.87500 to 0.76892, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/011-0.7689.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.7372 - acc: 0.7705 - val_loss: 0.7689 - val_acc: 0.7871\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7846\n",
      "Epoch 00012: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.6955 - acc: 0.7846 - val_loss: 0.9127 - val_acc: 0.7440\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6684 - acc: 0.7925\n",
      "Epoch 00013: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.6683 - acc: 0.7925 - val_loss: 0.8422 - val_acc: 0.7696\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6391 - acc: 0.7995\n",
      "Epoch 00014: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.6396 - acc: 0.7994 - val_loss: 0.8736 - val_acc: 0.7505\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6190 - acc: 0.8037\n",
      "Epoch 00015: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.6190 - acc: 0.8037 - val_loss: 0.7839 - val_acc: 0.7778\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8178\n",
      "Epoch 00016: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.5750 - acc: 0.8178 - val_loss: 0.7906 - val_acc: 0.7794\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.8214\n",
      "Epoch 00017: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.5603 - acc: 0.8214 - val_loss: 0.8808 - val_acc: 0.7568\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.8313\n",
      "Epoch 00018: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.5365 - acc: 0.8312 - val_loss: 0.9532 - val_acc: 0.7244\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8343\n",
      "Epoch 00019: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.5202 - acc: 0.8343 - val_loss: 0.8060 - val_acc: 0.7715\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4951 - acc: 0.8415\n",
      "Epoch 00020: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4951 - acc: 0.8415 - val_loss: 0.9869 - val_acc: 0.7345\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4790 - acc: 0.8483\n",
      "Epoch 00021: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4794 - acc: 0.8483 - val_loss: 0.7783 - val_acc: 0.7834\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8555\n",
      "Epoch 00022: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4476 - acc: 0.8555 - val_loss: 0.7859 - val_acc: 0.7936\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8613\n",
      "Epoch 00023: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4346 - acc: 0.8613 - val_loss: 0.9171 - val_acc: 0.7668\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8637\n",
      "Epoch 00024: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4235 - acc: 0.8637 - val_loss: 0.8615 - val_acc: 0.7668\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8692\n",
      "Epoch 00025: val_loss improved from 0.76892 to 0.75001, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv_checkpoint/025-0.7500.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.4033 - acc: 0.8692 - val_loss: 0.7500 - val_acc: 0.7976\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8751\n",
      "Epoch 00026: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3871 - acc: 0.8751 - val_loss: 0.8609 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8793\n",
      "Epoch 00027: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3775 - acc: 0.8793 - val_loss: 0.7904 - val_acc: 0.7939\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8832\n",
      "Epoch 00028: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3609 - acc: 0.8832 - val_loss: 0.8562 - val_acc: 0.7815\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8835\n",
      "Epoch 00029: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3529 - acc: 0.8835 - val_loss: 0.8178 - val_acc: 0.7894\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8870\n",
      "Epoch 00030: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3443 - acc: 0.8869 - val_loss: 0.8357 - val_acc: 0.7857\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8926\n",
      "Epoch 00031: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3282 - acc: 0.8927 - val_loss: 0.7578 - val_acc: 0.8113\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8932\n",
      "Epoch 00032: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3233 - acc: 0.8932 - val_loss: 0.7716 - val_acc: 0.8018\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8976\n",
      "Epoch 00033: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3091 - acc: 0.8976 - val_loss: 0.9449 - val_acc: 0.7747\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9030\n",
      "Epoch 00034: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2943 - acc: 0.9030 - val_loss: 1.0862 - val_acc: 0.7482\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9046\n",
      "Epoch 00035: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2888 - acc: 0.9045 - val_loss: 0.8687 - val_acc: 0.7794\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9048\n",
      "Epoch 00036: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2931 - acc: 0.9048 - val_loss: 0.8076 - val_acc: 0.7999\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9078\n",
      "Epoch 00037: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2801 - acc: 0.9078 - val_loss: 0.8277 - val_acc: 0.7887\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9147\n",
      "Epoch 00038: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2579 - acc: 0.9146 - val_loss: 0.8353 - val_acc: 0.7950\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9150\n",
      "Epoch 00039: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2604 - acc: 0.9150 - val_loss: 0.8285 - val_acc: 0.8004\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9198\n",
      "Epoch 00040: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2463 - acc: 0.9198 - val_loss: 0.8171 - val_acc: 0.8025\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9201\n",
      "Epoch 00041: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2450 - acc: 0.9201 - val_loss: 0.8403 - val_acc: 0.8018\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9207\n",
      "Epoch 00042: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2397 - acc: 0.9207 - val_loss: 0.7959 - val_acc: 0.8097\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9265\n",
      "Epoch 00043: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2294 - acc: 0.9265 - val_loss: 0.9171 - val_acc: 0.7945\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9279\n",
      "Epoch 00044: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2263 - acc: 0.9279 - val_loss: 0.8362 - val_acc: 0.7906\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9250\n",
      "Epoch 00045: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2284 - acc: 0.9250 - val_loss: 1.0299 - val_acc: 0.7566\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9294\n",
      "Epoch 00046: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2163 - acc: 0.9294 - val_loss: 0.8105 - val_acc: 0.8055\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9317\n",
      "Epoch 00047: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2104 - acc: 0.9317 - val_loss: 0.8337 - val_acc: 0.8008\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9322\n",
      "Epoch 00048: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2072 - acc: 0.9321 - val_loss: 0.9032 - val_acc: 0.7918\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9327\n",
      "Epoch 00049: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2101 - acc: 0.9327 - val_loss: 0.8230 - val_acc: 0.8048\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9348\n",
      "Epoch 00050: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2006 - acc: 0.9348 - val_loss: 0.8016 - val_acc: 0.8150\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9379\n",
      "Epoch 00051: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1900 - acc: 0.9379 - val_loss: 0.8146 - val_acc: 0.8053\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9385\n",
      "Epoch 00052: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1901 - acc: 0.9384 - val_loss: 0.8772 - val_acc: 0.7999\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9402\n",
      "Epoch 00053: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1892 - acc: 0.9402 - val_loss: 0.8426 - val_acc: 0.8013\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9412\n",
      "Epoch 00054: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1826 - acc: 0.9412 - val_loss: 0.7964 - val_acc: 0.8192\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9446\n",
      "Epoch 00055: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1745 - acc: 0.9446 - val_loss: 0.7890 - val_acc: 0.8209\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9463\n",
      "Epoch 00056: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1691 - acc: 0.9463 - val_loss: 0.8421 - val_acc: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9486\n",
      "Epoch 00057: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1638 - acc: 0.9486 - val_loss: 0.8391 - val_acc: 0.8095\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9448\n",
      "Epoch 00058: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1713 - acc: 0.9448 - val_loss: 0.8471 - val_acc: 0.8083\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9469\n",
      "Epoch 00059: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1665 - acc: 0.9468 - val_loss: 0.9366 - val_acc: 0.7913\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9463\n",
      "Epoch 00060: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1665 - acc: 0.9463 - val_loss: 0.8235 - val_acc: 0.8083\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9508\n",
      "Epoch 00061: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1539 - acc: 0.9508 - val_loss: 0.9248 - val_acc: 0.8004\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9489\n",
      "Epoch 00062: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1594 - acc: 0.9489 - val_loss: 0.8428 - val_acc: 0.8174\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9519\n",
      "Epoch 00063: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1516 - acc: 0.9518 - val_loss: 0.9087 - val_acc: 0.7999\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9490\n",
      "Epoch 00064: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1613 - acc: 0.9490 - val_loss: 0.8646 - val_acc: 0.8192\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9574\n",
      "Epoch 00065: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1402 - acc: 0.9574 - val_loss: 0.8107 - val_acc: 0.8155\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9555\n",
      "Epoch 00066: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1406 - acc: 0.9555 - val_loss: 0.9021 - val_acc: 0.8004\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9541\n",
      "Epoch 00067: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1454 - acc: 0.9541 - val_loss: 0.8636 - val_acc: 0.8146\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9563\n",
      "Epoch 00068: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1363 - acc: 0.9563 - val_loss: 0.7956 - val_acc: 0.8232\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9583\n",
      "Epoch 00069: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1367 - acc: 0.9583 - val_loss: 0.8674 - val_acc: 0.8104\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9595\n",
      "Epoch 00070: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1309 - acc: 0.9595 - val_loss: 0.8766 - val_acc: 0.8092\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9600\n",
      "Epoch 00071: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1278 - acc: 0.9600 - val_loss: 0.8551 - val_acc: 0.8150\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9581\n",
      "Epoch 00072: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1335 - acc: 0.9581 - val_loss: 0.8433 - val_acc: 0.8141\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9593\n",
      "Epoch 00073: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1296 - acc: 0.9593 - val_loss: 0.9249 - val_acc: 0.8095\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9605\n",
      "Epoch 00074: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1276 - acc: 0.9605 - val_loss: 0.8115 - val_acc: 0.8211\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9615\n",
      "Epoch 00075: val_loss did not improve from 0.75001\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1213 - acc: 0.9615 - val_loss: 0.8352 - val_acc: 0.8157\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.8928 - acc: 0.7583\n",
      "Loss: 0.8927627567685406 Accuracy: 0.7582554\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6300 - acc: 0.3294\n",
      "Epoch 00001: val_loss improved from inf to 1.66399, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/001-1.6640.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 2.6298 - acc: 0.3295 - val_loss: 1.6640 - val_acc: 0.4542\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5923 - acc: 0.5441\n",
      "Epoch 00002: val_loss improved from 1.66399 to 1.31332, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/002-1.3133.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 1.5925 - acc: 0.5440 - val_loss: 1.3133 - val_acc: 0.6110\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2763 - acc: 0.6278\n",
      "Epoch 00003: val_loss improved from 1.31332 to 1.01688, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/003-1.0169.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 1.2763 - acc: 0.6278 - val_loss: 1.0169 - val_acc: 0.6983\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1045 - acc: 0.6759\n",
      "Epoch 00004: val_loss improved from 1.01688 to 0.84792, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/004-0.8479.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 1.1048 - acc: 0.6758 - val_loss: 0.8479 - val_acc: 0.7489\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0195 - acc: 0.6948\n",
      "Epoch 00005: val_loss did not improve from 0.84792\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 1.0196 - acc: 0.6947 - val_loss: 0.9029 - val_acc: 0.7340\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9141 - acc: 0.7283\n",
      "Epoch 00006: val_loss improved from 0.84792 to 0.80703, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/006-0.8070.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.9141 - acc: 0.7283 - val_loss: 0.8070 - val_acc: 0.7580\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8468 - acc: 0.7467\n",
      "Epoch 00007: val_loss improved from 0.80703 to 0.69976, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/007-0.6998.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.8468 - acc: 0.7467 - val_loss: 0.6998 - val_acc: 0.7964\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7872 - acc: 0.7624\n",
      "Epoch 00008: val_loss did not improve from 0.69976\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.7873 - acc: 0.7624 - val_loss: 0.7510 - val_acc: 0.7813\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7454 - acc: 0.7772\n",
      "Epoch 00009: val_loss improved from 0.69976 to 0.68371, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/009-0.6837.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.7455 - acc: 0.7772 - val_loss: 0.6837 - val_acc: 0.7966\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.7862\n",
      "Epoch 00010: val_loss did not improve from 0.68371\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.7104 - acc: 0.7862 - val_loss: 0.7994 - val_acc: 0.7743\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6684 - acc: 0.7970\n",
      "Epoch 00011: val_loss did not improve from 0.68371\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.6686 - acc: 0.7970 - val_loss: 0.7018 - val_acc: 0.8060\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6610 - acc: 0.8031\n",
      "Epoch 00012: val_loss did not improve from 0.68371\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.6610 - acc: 0.8031 - val_loss: 0.7897 - val_acc: 0.7680\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6209 - acc: 0.8115\n",
      "Epoch 00013: val_loss did not improve from 0.68371\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.6209 - acc: 0.8115 - val_loss: 0.7145 - val_acc: 0.7973\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8193\n",
      "Epoch 00014: val_loss improved from 0.68371 to 0.65475, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/014-0.6548.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.5973 - acc: 0.8193 - val_loss: 0.6548 - val_acc: 0.8237\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8256\n",
      "Epoch 00015: val_loss did not improve from 0.65475\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.5741 - acc: 0.8255 - val_loss: 0.6620 - val_acc: 0.8167\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.8283\n",
      "Epoch 00016: val_loss improved from 0.65475 to 0.58155, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/016-0.5816.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.5668 - acc: 0.8283 - val_loss: 0.5816 - val_acc: 0.8362\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.8372\n",
      "Epoch 00017: val_loss improved from 0.58155 to 0.56232, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/017-0.5623.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.5262 - acc: 0.8372 - val_loss: 0.5623 - val_acc: 0.8486\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8432\n",
      "Epoch 00018: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.5086 - acc: 0.8431 - val_loss: 0.6301 - val_acc: 0.8211\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8497\n",
      "Epoch 00019: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4884 - acc: 0.8497 - val_loss: 0.6723 - val_acc: 0.8155\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8516\n",
      "Epoch 00020: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4769 - acc: 0.8516 - val_loss: 0.7026 - val_acc: 0.8088\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8588\n",
      "Epoch 00021: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4564 - acc: 0.8587 - val_loss: 0.5699 - val_acc: 0.8421\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8596\n",
      "Epoch 00022: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4551 - acc: 0.8596 - val_loss: 0.6338 - val_acc: 0.8304\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8670\n",
      "Epoch 00023: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4296 - acc: 0.8670 - val_loss: 0.6879 - val_acc: 0.8132\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8706\n",
      "Epoch 00024: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4146 - acc: 0.8705 - val_loss: 0.6076 - val_acc: 0.8293\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8747\n",
      "Epoch 00025: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.4058 - acc: 0.8746 - val_loss: 0.5733 - val_acc: 0.8479\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8757\n",
      "Epoch 00026: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3893 - acc: 0.8757 - val_loss: 0.5867 - val_acc: 0.8418\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8837\n",
      "Epoch 00027: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3687 - acc: 0.8836 - val_loss: 0.6495 - val_acc: 0.8332\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8829\n",
      "Epoch 00028: val_loss did not improve from 0.56232\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3743 - acc: 0.8829 - val_loss: 0.5996 - val_acc: 0.8388\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8886\n",
      "Epoch 00029: val_loss improved from 0.56232 to 0.56154, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/029-0.5615.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3548 - acc: 0.8886 - val_loss: 0.5615 - val_acc: 0.8474\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8918\n",
      "Epoch 00030: val_loss improved from 0.56154 to 0.54304, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/030-0.5430.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3405 - acc: 0.8918 - val_loss: 0.5430 - val_acc: 0.8602\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8966\n",
      "Epoch 00031: val_loss did not improve from 0.54304\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3236 - acc: 0.8966 - val_loss: 0.5558 - val_acc: 0.8491\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8950\n",
      "Epoch 00032: val_loss improved from 0.54304 to 0.53836, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/032-0.5384.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3261 - acc: 0.8950 - val_loss: 0.5384 - val_acc: 0.8593\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9008\n",
      "Epoch 00033: val_loss did not improve from 0.53836\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.3110 - acc: 0.9008 - val_loss: 0.5902 - val_acc: 0.8435\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9057\n",
      "Epoch 00034: val_loss did not improve from 0.53836\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2987 - acc: 0.9057 - val_loss: 0.6557 - val_acc: 0.8372\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9064\n",
      "Epoch 00035: val_loss improved from 0.53836 to 0.51572, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv_checkpoint/035-0.5157.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2893 - acc: 0.9064 - val_loss: 0.5157 - val_acc: 0.8661\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9086\n",
      "Epoch 00036: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2821 - acc: 0.9087 - val_loss: 0.6269 - val_acc: 0.8374\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9114\n",
      "Epoch 00037: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2731 - acc: 0.9113 - val_loss: 0.6958 - val_acc: 0.8302\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9114\n",
      "Epoch 00038: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2820 - acc: 0.9114 - val_loss: 0.6231 - val_acc: 0.8395\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9168\n",
      "Epoch 00039: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2561 - acc: 0.9168 - val_loss: 0.5826 - val_acc: 0.8549\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9187\n",
      "Epoch 00040: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2472 - acc: 0.9187 - val_loss: 0.5465 - val_acc: 0.8656\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9217\n",
      "Epoch 00041: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2420 - acc: 0.9217 - val_loss: 0.5255 - val_acc: 0.8696\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9262\n",
      "Epoch 00042: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2304 - acc: 0.9263 - val_loss: 0.5602 - val_acc: 0.8565\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9236\n",
      "Epoch 00043: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2299 - acc: 0.9236 - val_loss: 0.6061 - val_acc: 0.8411\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9264\n",
      "Epoch 00044: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2238 - acc: 0.9264 - val_loss: 0.5849 - val_acc: 0.8542\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9299\n",
      "Epoch 00045: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2141 - acc: 0.9298 - val_loss: 0.5938 - val_acc: 0.8532\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9287\n",
      "Epoch 00046: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2232 - acc: 0.9287 - val_loss: 0.5555 - val_acc: 0.8696\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9343\n",
      "Epoch 00047: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2023 - acc: 0.9343 - val_loss: 0.6725 - val_acc: 0.8395\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9335\n",
      "Epoch 00048: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2056 - acc: 0.9335 - val_loss: 0.5614 - val_acc: 0.8698\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9361\n",
      "Epoch 00049: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1943 - acc: 0.9360 - val_loss: 0.5524 - val_acc: 0.8654\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9290\n",
      "Epoch 00050: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.2193 - acc: 0.9290 - val_loss: 0.5491 - val_acc: 0.8607\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9411\n",
      "Epoch 00051: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1830 - acc: 0.9411 - val_loss: 0.5534 - val_acc: 0.8675\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9427\n",
      "Epoch 00052: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1769 - acc: 0.9427 - val_loss: 0.6170 - val_acc: 0.8491\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9428\n",
      "Epoch 00053: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1788 - acc: 0.9428 - val_loss: 0.6058 - val_acc: 0.8481\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9438\n",
      "Epoch 00054: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1728 - acc: 0.9438 - val_loss: 0.6213 - val_acc: 0.8607\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9434\n",
      "Epoch 00055: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1724 - acc: 0.9434 - val_loss: 0.5945 - val_acc: 0.8588\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9459\n",
      "Epoch 00056: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1634 - acc: 0.9459 - val_loss: 0.5460 - val_acc: 0.8714\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9484\n",
      "Epoch 00057: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1605 - acc: 0.9484 - val_loss: 0.6066 - val_acc: 0.8572\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9490\n",
      "Epoch 00058: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1548 - acc: 0.9489 - val_loss: 0.6668 - val_acc: 0.8432\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9488\n",
      "Epoch 00059: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1585 - acc: 0.9488 - val_loss: 0.7277 - val_acc: 0.8281\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9537\n",
      "Epoch 00060: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1450 - acc: 0.9537 - val_loss: 0.7157 - val_acc: 0.8276\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9544\n",
      "Epoch 00061: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1423 - acc: 0.9543 - val_loss: 0.6275 - val_acc: 0.8665\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9475\n",
      "Epoch 00062: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1601 - acc: 0.9475 - val_loss: 0.6401 - val_acc: 0.8581\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9539\n",
      "Epoch 00063: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1415 - acc: 0.9539 - val_loss: 0.6125 - val_acc: 0.8567\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9551\n",
      "Epoch 00064: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1379 - acc: 0.9551 - val_loss: 0.6163 - val_acc: 0.8570\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9550\n",
      "Epoch 00065: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1377 - acc: 0.9550 - val_loss: 0.5888 - val_acc: 0.8712\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9573\n",
      "Epoch 00066: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1310 - acc: 0.9572 - val_loss: 0.6973 - val_acc: 0.8512\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9564\n",
      "Epoch 00067: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1380 - acc: 0.9564 - val_loss: 0.5571 - val_acc: 0.8693\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9557\n",
      "Epoch 00068: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1334 - acc: 0.9557 - val_loss: 0.5337 - val_acc: 0.8789\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9594\n",
      "Epoch 00069: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1262 - acc: 0.9594 - val_loss: 0.5463 - val_acc: 0.8730\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9568\n",
      "Epoch 00070: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1310 - acc: 0.9568 - val_loss: 0.6100 - val_acc: 0.8654\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9596\n",
      "Epoch 00071: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1253 - acc: 0.9596 - val_loss: 0.5725 - val_acc: 0.8754\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9633\n",
      "Epoch 00072: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1151 - acc: 0.9633 - val_loss: 0.5978 - val_acc: 0.8675\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9629\n",
      "Epoch 00073: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1167 - acc: 0.9629 - val_loss: 0.6371 - val_acc: 0.8579\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9638\n",
      "Epoch 00074: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1148 - acc: 0.9638 - val_loss: 0.5937 - val_acc: 0.8740\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9652\n",
      "Epoch 00075: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1091 - acc: 0.9651 - val_loss: 0.6496 - val_acc: 0.8609\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9628\n",
      "Epoch 00076: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1169 - acc: 0.9627 - val_loss: 0.6244 - val_acc: 0.8619\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9648\n",
      "Epoch 00077: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1121 - acc: 0.9648 - val_loss: 0.6737 - val_acc: 0.8481\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9651\n",
      "Epoch 00078: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1074 - acc: 0.9651 - val_loss: 0.5799 - val_acc: 0.8703\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9672\n",
      "Epoch 00079: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1028 - acc: 0.9672 - val_loss: 0.5529 - val_acc: 0.8803\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9675\n",
      "Epoch 00080: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1034 - acc: 0.9675 - val_loss: 0.6142 - val_acc: 0.8605\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9662\n",
      "Epoch 00081: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1055 - acc: 0.9662 - val_loss: 0.6271 - val_acc: 0.8607\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9686\n",
      "Epoch 00082: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1021 - acc: 0.9686 - val_loss: 0.5728 - val_acc: 0.8735\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9689\n",
      "Epoch 00083: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.0979 - acc: 0.9689 - val_loss: 0.5788 - val_acc: 0.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9656\n",
      "Epoch 00084: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1079 - acc: 0.9656 - val_loss: 0.7313 - val_acc: 0.8435\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9692\n",
      "Epoch 00085: val_loss did not improve from 0.51572\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.0959 - acc: 0.9692 - val_loss: 0.5562 - val_acc: 0.8803\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.6380 - acc: 0.8330\n",
      "Loss: 0.6379768278120957 Accuracy: 0.8330218\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6868 - acc: 0.2865\n",
      "Epoch 00001: val_loss improved from inf to 1.91749, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/001-1.9175.hdf5\n",
      "36805/36805 [==============================] - 199s 5ms/sample - loss: 2.6868 - acc: 0.2865 - val_loss: 1.9175 - val_acc: 0.3324\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5281 - acc: 0.5302\n",
      "Epoch 00002: val_loss improved from 1.91749 to 1.05087, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/002-1.0509.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 1.5282 - acc: 0.5302 - val_loss: 1.0509 - val_acc: 0.6753\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1780 - acc: 0.6374\n",
      "Epoch 00003: val_loss improved from 1.05087 to 0.84236, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/003-0.8424.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 1.1782 - acc: 0.6374 - val_loss: 0.8424 - val_acc: 0.7633\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0078 - acc: 0.6895\n",
      "Epoch 00004: val_loss improved from 0.84236 to 0.74313, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/004-0.7431.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 1.0079 - acc: 0.6895 - val_loss: 0.7431 - val_acc: 0.7843\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7318\n",
      "Epoch 00005: val_loss did not improve from 0.74313\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.8837 - acc: 0.7317 - val_loss: 0.9743 - val_acc: 0.7321\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.7592\n",
      "Epoch 00006: val_loss improved from 0.74313 to 0.59111, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/006-0.5911.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.7913 - acc: 0.7592 - val_loss: 0.5911 - val_acc: 0.8360\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.7828\n",
      "Epoch 00007: val_loss improved from 0.59111 to 0.55940, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/007-0.5594.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.7169 - acc: 0.7828 - val_loss: 0.5594 - val_acc: 0.8393\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.8018\n",
      "Epoch 00008: val_loss did not improve from 0.55940\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.6565 - acc: 0.8018 - val_loss: 0.6128 - val_acc: 0.8293\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.8153\n",
      "Epoch 00009: val_loss did not improve from 0.55940\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.6125 - acc: 0.8153 - val_loss: 0.6054 - val_acc: 0.8281\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.8290\n",
      "Epoch 00010: val_loss improved from 0.55940 to 0.54016, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/010-0.5402.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.5732 - acc: 0.8290 - val_loss: 0.5402 - val_acc: 0.8404\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8383\n",
      "Epoch 00011: val_loss did not improve from 0.54016\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.5352 - acc: 0.8383 - val_loss: 0.6076 - val_acc: 0.8234\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8473\n",
      "Epoch 00012: val_loss improved from 0.54016 to 0.52177, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/012-0.5218.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.5068 - acc: 0.8473 - val_loss: 0.5218 - val_acc: 0.8502\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8534\n",
      "Epoch 00013: val_loss improved from 0.52177 to 0.43488, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/013-0.4349.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.4860 - acc: 0.8534 - val_loss: 0.4349 - val_acc: 0.8870\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8609\n",
      "Epoch 00014: val_loss did not improve from 0.43488\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.4606 - acc: 0.8609 - val_loss: 0.4403 - val_acc: 0.8717\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.8704\n",
      "Epoch 00015: val_loss did not improve from 0.43488\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.4299 - acc: 0.8703 - val_loss: 0.4427 - val_acc: 0.8775\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8728\n",
      "Epoch 00016: val_loss improved from 0.43488 to 0.41376, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/016-0.4138.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.4204 - acc: 0.8728 - val_loss: 0.4138 - val_acc: 0.8870\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8779\n",
      "Epoch 00017: val_loss did not improve from 0.41376\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.4022 - acc: 0.8779 - val_loss: 0.4977 - val_acc: 0.8544\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8828\n",
      "Epoch 00018: val_loss improved from 0.41376 to 0.40009, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/018-0.4001.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3851 - acc: 0.8828 - val_loss: 0.4001 - val_acc: 0.8982\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8880\n",
      "Epoch 00019: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3674 - acc: 0.8880 - val_loss: 0.4128 - val_acc: 0.8840\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8942\n",
      "Epoch 00020: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3489 - acc: 0.8942 - val_loss: 0.4019 - val_acc: 0.8926\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8913\n",
      "Epoch 00021: val_loss improved from 0.40009 to 0.39593, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/021-0.3959.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3473 - acc: 0.8913 - val_loss: 0.3959 - val_acc: 0.8928\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8984\n",
      "Epoch 00022: val_loss improved from 0.39593 to 0.34550, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/022-0.3455.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3279 - acc: 0.8984 - val_loss: 0.3455 - val_acc: 0.9110\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9032\n",
      "Epoch 00023: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3105 - acc: 0.9032 - val_loss: 0.3876 - val_acc: 0.8940\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9040\n",
      "Epoch 00024: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.3068 - acc: 0.9040 - val_loss: 0.4363 - val_acc: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9101\n",
      "Epoch 00025: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2922 - acc: 0.9101 - val_loss: 0.4313 - val_acc: 0.8898\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9117\n",
      "Epoch 00026: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2799 - acc: 0.9117 - val_loss: 0.5691 - val_acc: 0.8500\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9135\n",
      "Epoch 00027: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2772 - acc: 0.9135 - val_loss: 0.3512 - val_acc: 0.9031\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9158\n",
      "Epoch 00028: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2671 - acc: 0.9158 - val_loss: 0.4167 - val_acc: 0.8921\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9193\n",
      "Epoch 00029: val_loss did not improve from 0.34550\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2545 - acc: 0.9193 - val_loss: 0.3562 - val_acc: 0.9012\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9218\n",
      "Epoch 00030: val_loss improved from 0.34550 to 0.32897, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/030-0.3290.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2423 - acc: 0.9218 - val_loss: 0.3290 - val_acc: 0.9099\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9241\n",
      "Epoch 00031: val_loss did not improve from 0.32897\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2433 - acc: 0.9241 - val_loss: 0.5022 - val_acc: 0.8789\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9242\n",
      "Epoch 00032: val_loss did not improve from 0.32897\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2370 - acc: 0.9242 - val_loss: 0.3856 - val_acc: 0.9026\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9250\n",
      "Epoch 00033: val_loss did not improve from 0.32897\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2289 - acc: 0.9250 - val_loss: 0.3491 - val_acc: 0.9106\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9310\n",
      "Epoch 00034: val_loss did not improve from 0.32897\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2188 - acc: 0.9309 - val_loss: 0.3893 - val_acc: 0.8942\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9273\n",
      "Epoch 00035: val_loss improved from 0.32897 to 0.32523, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/035-0.3252.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2275 - acc: 0.9273 - val_loss: 0.3252 - val_acc: 0.9168\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9345\n",
      "Epoch 00036: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2018 - acc: 0.9345 - val_loss: 0.4342 - val_acc: 0.9015\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9351\n",
      "Epoch 00037: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.2027 - acc: 0.9350 - val_loss: 0.4880 - val_acc: 0.8856\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9353\n",
      "Epoch 00038: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1997 - acc: 0.9353 - val_loss: 0.4192 - val_acc: 0.9019\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9383\n",
      "Epoch 00039: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1866 - acc: 0.9384 - val_loss: 0.3462 - val_acc: 0.9159\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9420\n",
      "Epoch 00040: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1827 - acc: 0.9420 - val_loss: 0.4338 - val_acc: 0.8956\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9396\n",
      "Epoch 00041: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1849 - acc: 0.9396 - val_loss: 0.3999 - val_acc: 0.9075\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9423\n",
      "Epoch 00042: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1790 - acc: 0.9423 - val_loss: 0.4909 - val_acc: 0.8737\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9420\n",
      "Epoch 00043: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1776 - acc: 0.9420 - val_loss: 0.3576 - val_acc: 0.9080\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9455\n",
      "Epoch 00044: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1685 - acc: 0.9454 - val_loss: 0.3735 - val_acc: 0.9119\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9454\n",
      "Epoch 00045: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1659 - acc: 0.9454 - val_loss: 0.4381 - val_acc: 0.8975\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9471\n",
      "Epoch 00046: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1602 - acc: 0.9471 - val_loss: 0.3687 - val_acc: 0.9087\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9498\n",
      "Epoch 00047: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1545 - acc: 0.9498 - val_loss: 0.3852 - val_acc: 0.9131\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9505\n",
      "Epoch 00048: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1528 - acc: 0.9505 - val_loss: 0.3548 - val_acc: 0.9194\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9520\n",
      "Epoch 00049: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1459 - acc: 0.9520 - val_loss: 0.3416 - val_acc: 0.9222\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9514\n",
      "Epoch 00050: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1474 - acc: 0.9514 - val_loss: 0.3469 - val_acc: 0.9178\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9528\n",
      "Epoch 00051: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.1417 - acc: 0.9528 - val_loss: 0.3771 - val_acc: 0.9168\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9554\n",
      "Epoch 00052: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1355 - acc: 0.9554 - val_loss: 0.3424 - val_acc: 0.9217\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9555\n",
      "Epoch 00053: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1308 - acc: 0.9555 - val_loss: 0.3802 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9547\n",
      "Epoch 00054: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1388 - acc: 0.9547 - val_loss: 0.3877 - val_acc: 0.9140\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9498\n",
      "Epoch 00055: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1601 - acc: 0.9498 - val_loss: 0.3369 - val_acc: 0.9224\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9597\n",
      "Epoch 00056: val_loss did not improve from 0.32523\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1227 - acc: 0.9597 - val_loss: 0.3653 - val_acc: 0.9147\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9600\n",
      "Epoch 00057: val_loss improved from 0.32523 to 0.31698, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/057-0.3170.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1183 - acc: 0.9600 - val_loss: 0.3170 - val_acc: 0.9245\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9604\n",
      "Epoch 00058: val_loss did not improve from 0.31698\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1207 - acc: 0.9604 - val_loss: 0.3660 - val_acc: 0.9178\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9601\n",
      "Epoch 00059: val_loss did not improve from 0.31698\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1187 - acc: 0.9601 - val_loss: 0.3679 - val_acc: 0.9215\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9583\n",
      "Epoch 00060: val_loss did not improve from 0.31698\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1275 - acc: 0.9583 - val_loss: 0.3232 - val_acc: 0.9224\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9633\n",
      "Epoch 00061: val_loss improved from 0.31698 to 0.31099, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/061-0.3110.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1121 - acc: 0.9632 - val_loss: 0.3110 - val_acc: 0.9259\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9609\n",
      "Epoch 00062: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1158 - acc: 0.9609 - val_loss: 0.3654 - val_acc: 0.9180\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9645\n",
      "Epoch 00063: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1068 - acc: 0.9645 - val_loss: 0.3591 - val_acc: 0.9168\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9642\n",
      "Epoch 00064: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1079 - acc: 0.9641 - val_loss: 0.3342 - val_acc: 0.9245\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9657\n",
      "Epoch 00065: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1052 - acc: 0.9657 - val_loss: 0.3709 - val_acc: 0.9161\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9665\n",
      "Epoch 00066: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1026 - acc: 0.9665 - val_loss: 0.3391 - val_acc: 0.9231\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9664\n",
      "Epoch 00067: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0975 - acc: 0.9664 - val_loss: 0.3349 - val_acc: 0.9241\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9680\n",
      "Epoch 00068: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0935 - acc: 0.9680 - val_loss: 0.3583 - val_acc: 0.9192\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9686\n",
      "Epoch 00069: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0948 - acc: 0.9685 - val_loss: 0.4175 - val_acc: 0.9022\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9640\n",
      "Epoch 00070: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.1087 - acc: 0.9640 - val_loss: 0.4141 - val_acc: 0.9068\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9689\n",
      "Epoch 00071: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0953 - acc: 0.9689 - val_loss: 0.3267 - val_acc: 0.9245\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9716\n",
      "Epoch 00072: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0894 - acc: 0.9716 - val_loss: 0.3857 - val_acc: 0.9166\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9693\n",
      "Epoch 00073: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0923 - acc: 0.9693 - val_loss: 0.4044 - val_acc: 0.9113\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9708\n",
      "Epoch 00074: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0850 - acc: 0.9708 - val_loss: 0.3605 - val_acc: 0.9222\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9693\n",
      "Epoch 00075: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0915 - acc: 0.9693 - val_loss: 0.4611 - val_acc: 0.9052\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9723\n",
      "Epoch 00076: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0870 - acc: 0.9723 - val_loss: 0.4331 - val_acc: 0.9101\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9727\n",
      "Epoch 00077: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0826 - acc: 0.9726 - val_loss: 0.3446 - val_acc: 0.9185\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9706\n",
      "Epoch 00078: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0895 - acc: 0.9705 - val_loss: 0.6370 - val_acc: 0.8600\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9709\n",
      "Epoch 00079: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0832 - acc: 0.9709 - val_loss: 0.3975 - val_acc: 0.9133\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9726\n",
      "Epoch 00080: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0837 - acc: 0.9726 - val_loss: 0.3782 - val_acc: 0.9222\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9739\n",
      "Epoch 00081: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0767 - acc: 0.9738 - val_loss: 0.3862 - val_acc: 0.9199\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9736\n",
      "Epoch 00082: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0806 - acc: 0.9735 - val_loss: 0.3348 - val_acc: 0.9334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9754\n",
      "Epoch 00083: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0784 - acc: 0.9754 - val_loss: 0.3786 - val_acc: 0.9182\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9744\n",
      "Epoch 00084: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0784 - acc: 0.9744 - val_loss: 0.3359 - val_acc: 0.9297\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9768\n",
      "Epoch 00085: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0696 - acc: 0.9768 - val_loss: 0.3868 - val_acc: 0.9238\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9739\n",
      "Epoch 00086: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0786 - acc: 0.9739 - val_loss: 0.3796 - val_acc: 0.9199\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9770\n",
      "Epoch 00087: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0705 - acc: 0.9769 - val_loss: 0.3618 - val_acc: 0.9234\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9742\n",
      "Epoch 00088: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0748 - acc: 0.9741 - val_loss: 0.4027 - val_acc: 0.9173\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9757\n",
      "Epoch 00089: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0745 - acc: 0.9757 - val_loss: 0.3899 - val_acc: 0.9173\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9776\n",
      "Epoch 00090: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0679 - acc: 0.9776 - val_loss: 0.4121 - val_acc: 0.9150\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9768\n",
      "Epoch 00091: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0696 - acc: 0.9768 - val_loss: 0.3683 - val_acc: 0.9208\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9789\n",
      "Epoch 00092: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0653 - acc: 0.9788 - val_loss: 0.4234 - val_acc: 0.9073\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9769\n",
      "Epoch 00093: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0714 - acc: 0.9769 - val_loss: 0.3498 - val_acc: 0.9238\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9787\n",
      "Epoch 00094: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0662 - acc: 0.9787 - val_loss: 0.3502 - val_acc: 0.9215\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9799\n",
      "Epoch 00095: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0602 - acc: 0.9799 - val_loss: 0.3582 - val_acc: 0.9229\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9798\n",
      "Epoch 00096: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0634 - acc: 0.9798 - val_loss: 0.3880 - val_acc: 0.9231\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9796\n",
      "Epoch 00097: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0632 - acc: 0.9796 - val_loss: 0.4120 - val_acc: 0.9164\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9800\n",
      "Epoch 00098: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0601 - acc: 0.9800 - val_loss: 0.3993 - val_acc: 0.9145\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9776\n",
      "Epoch 00099: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0701 - acc: 0.9776 - val_loss: 0.3980 - val_acc: 0.9203\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9809\n",
      "Epoch 00100: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0579 - acc: 0.9809 - val_loss: 0.3439 - val_acc: 0.9257\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9819\n",
      "Epoch 00101: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0546 - acc: 0.9819 - val_loss: 0.3911 - val_acc: 0.9222\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9811\n",
      "Epoch 00102: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0573 - acc: 0.9810 - val_loss: 0.4406 - val_acc: 0.9143\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9781\n",
      "Epoch 00103: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0656 - acc: 0.9781 - val_loss: 0.3655 - val_acc: 0.9252\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9823\n",
      "Epoch 00104: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0526 - acc: 0.9823 - val_loss: 0.3805 - val_acc: 0.9203\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9819\n",
      "Epoch 00105: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0571 - acc: 0.9819 - val_loss: 0.3955 - val_acc: 0.9266\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9821\n",
      "Epoch 00106: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0551 - acc: 0.9821 - val_loss: 0.3592 - val_acc: 0.9269\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9815\n",
      "Epoch 00107: val_loss did not improve from 0.31099\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0575 - acc: 0.9815 - val_loss: 0.3744 - val_acc: 0.9252\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9825\n",
      "Epoch 00108: val_loss improved from 0.31099 to 0.29557, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv_checkpoint/108-0.2956.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0536 - acc: 0.9825 - val_loss: 0.2956 - val_acc: 0.9364\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9828\n",
      "Epoch 00109: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0526 - acc: 0.9827 - val_loss: 0.3531 - val_acc: 0.9266\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9814\n",
      "Epoch 00110: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0560 - acc: 0.9814 - val_loss: 0.3548 - val_acc: 0.9306\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9823\n",
      "Epoch 00111: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0557 - acc: 0.9823 - val_loss: 0.3471 - val_acc: 0.9338\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9847\n",
      "Epoch 00112: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0490 - acc: 0.9847 - val_loss: 0.3557 - val_acc: 0.9280\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9836\n",
      "Epoch 00113: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0515 - acc: 0.9835 - val_loss: 0.4621 - val_acc: 0.9066\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9791\n",
      "Epoch 00114: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0615 - acc: 0.9791 - val_loss: 0.3942 - val_acc: 0.9180\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9815\n",
      "Epoch 00115: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0554 - acc: 0.9815 - val_loss: 0.3366 - val_acc: 0.9294\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9839\n",
      "Epoch 00116: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0492 - acc: 0.9839 - val_loss: 0.3453 - val_acc: 0.9331\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9840\n",
      "Epoch 00117: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0479 - acc: 0.9840 - val_loss: 0.4065 - val_acc: 0.9208\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9844\n",
      "Epoch 00118: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0482 - acc: 0.9844 - val_loss: 0.4007 - val_acc: 0.9222\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9851\n",
      "Epoch 00119: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0455 - acc: 0.9851 - val_loss: 0.3509 - val_acc: 0.9236\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9847\n",
      "Epoch 00120: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0469 - acc: 0.9846 - val_loss: 0.3509 - val_acc: 0.9320\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9816\n",
      "Epoch 00121: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0554 - acc: 0.9816 - val_loss: 0.3543 - val_acc: 0.9306\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9859\n",
      "Epoch 00122: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0443 - acc: 0.9859 - val_loss: 0.3586 - val_acc: 0.9290\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9841\n",
      "Epoch 00123: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0489 - acc: 0.9841 - val_loss: 0.3404 - val_acc: 0.9324\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9842\n",
      "Epoch 00124: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0477 - acc: 0.9841 - val_loss: 0.3815 - val_acc: 0.9297\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9856\n",
      "Epoch 00125: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0477 - acc: 0.9856 - val_loss: 0.3632 - val_acc: 0.9273\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9865\n",
      "Epoch 00126: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0406 - acc: 0.9865 - val_loss: 0.4058 - val_acc: 0.9304\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9876\n",
      "Epoch 00127: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0384 - acc: 0.9876 - val_loss: 0.4611 - val_acc: 0.9189\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9854\n",
      "Epoch 00128: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0444 - acc: 0.9854 - val_loss: 0.3578 - val_acc: 0.9327\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9860\n",
      "Epoch 00129: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0444 - acc: 0.9860 - val_loss: 0.4178 - val_acc: 0.9189\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9872\n",
      "Epoch 00130: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0386 - acc: 0.9872 - val_loss: 0.3904 - val_acc: 0.9287\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9868\n",
      "Epoch 00131: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0405 - acc: 0.9867 - val_loss: 0.4103 - val_acc: 0.9210\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9853\n",
      "Epoch 00132: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0462 - acc: 0.9852 - val_loss: 0.3709 - val_acc: 0.9269\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9853\n",
      "Epoch 00133: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0462 - acc: 0.9853 - val_loss: 0.3282 - val_acc: 0.9362\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9878\n",
      "Epoch 00134: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0385 - acc: 0.9878 - val_loss: 0.4463 - val_acc: 0.9203\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9876\n",
      "Epoch 00135: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0384 - acc: 0.9876 - val_loss: 0.3779 - val_acc: 0.9243\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 00136: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0389 - acc: 0.9870 - val_loss: 0.4139 - val_acc: 0.9301\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9849\n",
      "Epoch 00137: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0457 - acc: 0.9849 - val_loss: 0.3899 - val_acc: 0.9264\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9860\n",
      "Epoch 00138: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0436 - acc: 0.9860 - val_loss: 0.3897 - val_acc: 0.9264\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9890\n",
      "Epoch 00139: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0342 - acc: 0.9891 - val_loss: 0.3558 - val_acc: 0.9334\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9889\n",
      "Epoch 00140: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0342 - acc: 0.9889 - val_loss: 0.3932 - val_acc: 0.9290\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9883\n",
      "Epoch 00141: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0360 - acc: 0.9883 - val_loss: 0.4248 - val_acc: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9866\n",
      "Epoch 00142: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0396 - acc: 0.9866 - val_loss: 0.3938 - val_acc: 0.9297\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9895\n",
      "Epoch 00143: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0333 - acc: 0.9894 - val_loss: 0.4796 - val_acc: 0.9215\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9804\n",
      "Epoch 00144: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0636 - acc: 0.9804 - val_loss: 0.3343 - val_acc: 0.9359\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9886\n",
      "Epoch 00145: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0335 - acc: 0.9886 - val_loss: 0.3608 - val_acc: 0.9294\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9872\n",
      "Epoch 00146: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0403 - acc: 0.9872 - val_loss: 0.3460 - val_acc: 0.9343\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 00147: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0300 - acc: 0.9900 - val_loss: 0.3752 - val_acc: 0.9297\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9885\n",
      "Epoch 00148: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0350 - acc: 0.9884 - val_loss: 0.4217 - val_acc: 0.9203\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9873\n",
      "Epoch 00149: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0393 - acc: 0.9873 - val_loss: 0.3460 - val_acc: 0.9364\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9910\n",
      "Epoch 00150: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0279 - acc: 0.9910 - val_loss: 0.3982 - val_acc: 0.9324\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9862\n",
      "Epoch 00151: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0430 - acc: 0.9862 - val_loss: 0.3972 - val_acc: 0.9315\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9896\n",
      "Epoch 00152: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0330 - acc: 0.9896 - val_loss: 0.4708 - val_acc: 0.9154\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9890\n",
      "Epoch 00153: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0328 - acc: 0.9890 - val_loss: 0.3675 - val_acc: 0.9320\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9903\n",
      "Epoch 00154: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0286 - acc: 0.9903 - val_loss: 0.4435 - val_acc: 0.9168\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9895\n",
      "Epoch 00155: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0331 - acc: 0.9895 - val_loss: 0.4407 - val_acc: 0.9133\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9870\n",
      "Epoch 00156: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0405 - acc: 0.9870 - val_loss: 0.3466 - val_acc: 0.9324\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9913\n",
      "Epoch 00157: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0276 - acc: 0.9913 - val_loss: 0.4107 - val_acc: 0.9259\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9888\n",
      "Epoch 00158: val_loss did not improve from 0.29557\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0338 - acc: 0.9888 - val_loss: 0.4456 - val_acc: 0.9201\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4392 - acc: 0.9076\n",
      "Loss: 0.4391809105625529 Accuracy: 0.9075805\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7064 - acc: 0.2674\n",
      "Epoch 00001: val_loss improved from inf to 1.64350, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/001-1.6435.hdf5\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 2.7067 - acc: 0.2674 - val_loss: 1.6435 - val_acc: 0.4589\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5298 - acc: 0.5194\n",
      "Epoch 00002: val_loss improved from 1.64350 to 0.86999, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/002-0.8700.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 1.5298 - acc: 0.5194 - val_loss: 0.8700 - val_acc: 0.7445\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1388 - acc: 0.6461\n",
      "Epoch 00003: val_loss improved from 0.86999 to 0.63487, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/003-0.6349.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 1.1387 - acc: 0.6461 - val_loss: 0.6349 - val_acc: 0.8220\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9148 - acc: 0.7185\n",
      "Epoch 00004: val_loss did not improve from 0.63487\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.9148 - acc: 0.7185 - val_loss: 0.6995 - val_acc: 0.8011\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7656 - acc: 0.7665\n",
      "Epoch 00005: val_loss did not improve from 0.63487\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.7656 - acc: 0.7666 - val_loss: 0.7008 - val_acc: 0.7904\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.7991\n",
      "Epoch 00006: val_loss improved from 0.63487 to 0.55153, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/006-0.5515.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.6612 - acc: 0.7991 - val_loss: 0.5515 - val_acc: 0.8416\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.8266\n",
      "Epoch 00007: val_loss improved from 0.55153 to 0.42123, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/007-0.4212.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.5742 - acc: 0.8266 - val_loss: 0.4212 - val_acc: 0.8796\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.8440\n",
      "Epoch 00008: val_loss did not improve from 0.42123\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.5203 - acc: 0.8440 - val_loss: 0.4634 - val_acc: 0.8689\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8559\n",
      "Epoch 00009: val_loss improved from 0.42123 to 0.37575, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/009-0.3758.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.4743 - acc: 0.8559 - val_loss: 0.3758 - val_acc: 0.8896\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8685\n",
      "Epoch 00010: val_loss improved from 0.37575 to 0.32282, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/010-0.3228.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.4326 - acc: 0.8685 - val_loss: 0.3228 - val_acc: 0.9110\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8795\n",
      "Epoch 00011: val_loss did not improve from 0.32282\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.3971 - acc: 0.8794 - val_loss: 0.3247 - val_acc: 0.9096\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8871\n",
      "Epoch 00012: val_loss did not improve from 0.32282\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.3683 - acc: 0.8871 - val_loss: 0.3827 - val_acc: 0.8903\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8931\n",
      "Epoch 00013: val_loss improved from 0.32282 to 0.27248, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/013-0.2725.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.3477 - acc: 0.8931 - val_loss: 0.2725 - val_acc: 0.9245\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8977\n",
      "Epoch 00014: val_loss did not improve from 0.27248\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.3288 - acc: 0.8977 - val_loss: 0.3300 - val_acc: 0.9101\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9049\n",
      "Epoch 00015: val_loss improved from 0.27248 to 0.25120, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/015-0.2512.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.3021 - acc: 0.9049 - val_loss: 0.2512 - val_acc: 0.9341\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9113\n",
      "Epoch 00016: val_loss did not improve from 0.25120\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2852 - acc: 0.9113 - val_loss: 0.2718 - val_acc: 0.9231\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9187\n",
      "Epoch 00017: val_loss did not improve from 0.25120\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2661 - acc: 0.9187 - val_loss: 0.2806 - val_acc: 0.9173\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9196\n",
      "Epoch 00018: val_loss improved from 0.25120 to 0.24690, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/018-0.2469.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2579 - acc: 0.9196 - val_loss: 0.2469 - val_acc: 0.9294\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9214\n",
      "Epoch 00019: val_loss improved from 0.24690 to 0.23448, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/019-0.2345.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2484 - acc: 0.9215 - val_loss: 0.2345 - val_acc: 0.9394\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9289\n",
      "Epoch 00020: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2256 - acc: 0.9289 - val_loss: 0.2346 - val_acc: 0.9341\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9307\n",
      "Epoch 00021: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2204 - acc: 0.9307 - val_loss: 0.2446 - val_acc: 0.9334\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9344\n",
      "Epoch 00022: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2068 - acc: 0.9344 - val_loss: 0.3672 - val_acc: 0.9094\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9346\n",
      "Epoch 00023: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2054 - acc: 0.9346 - val_loss: 0.3289 - val_acc: 0.9147\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9400\n",
      "Epoch 00024: val_loss improved from 0.23448 to 0.22309, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/024-0.2231.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1918 - acc: 0.9400 - val_loss: 0.2231 - val_acc: 0.9434\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9424\n",
      "Epoch 00025: val_loss improved from 0.22309 to 0.20729, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/025-0.2073.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1841 - acc: 0.9424 - val_loss: 0.2073 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9426\n",
      "Epoch 00026: val_loss improved from 0.20729 to 0.20641, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/026-0.2064.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1792 - acc: 0.9426 - val_loss: 0.2064 - val_acc: 0.9425\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9457\n",
      "Epoch 00027: val_loss did not improve from 0.20641\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1643 - acc: 0.9457 - val_loss: 0.2201 - val_acc: 0.9425\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9460\n",
      "Epoch 00028: val_loss improved from 0.20641 to 0.19333, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/028-0.1933.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1688 - acc: 0.9460 - val_loss: 0.1933 - val_acc: 0.9478\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9501\n",
      "Epoch 00029: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1527 - acc: 0.9501 - val_loss: 0.1994 - val_acc: 0.9509\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9487\n",
      "Epoch 00030: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1564 - acc: 0.9487 - val_loss: 0.2944 - val_acc: 0.9285\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9480\n",
      "Epoch 00031: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1624 - acc: 0.9480 - val_loss: 0.2215 - val_acc: 0.9478\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9548\n",
      "Epoch 00032: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1402 - acc: 0.9548 - val_loss: 0.2354 - val_acc: 0.9418\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9574\n",
      "Epoch 00033: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1330 - acc: 0.9574 - val_loss: 0.2009 - val_acc: 0.9492\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9583\n",
      "Epoch 00034: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1252 - acc: 0.9583 - val_loss: 0.2459 - val_acc: 0.9369\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9581\n",
      "Epoch 00035: val_loss did not improve from 0.19333\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1294 - acc: 0.9581 - val_loss: 0.2330 - val_acc: 0.9383\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9592\n",
      "Epoch 00036: val_loss improved from 0.19333 to 0.18537, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/036-0.1854.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1256 - acc: 0.9592 - val_loss: 0.1854 - val_acc: 0.9529\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9624\n",
      "Epoch 00037: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1172 - acc: 0.9624 - val_loss: 0.1974 - val_acc: 0.9534\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9612\n",
      "Epoch 00038: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1199 - acc: 0.9612 - val_loss: 0.1950 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9643\n",
      "Epoch 00039: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1089 - acc: 0.9643 - val_loss: 0.2388 - val_acc: 0.9371\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9633\n",
      "Epoch 00040: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1099 - acc: 0.9633 - val_loss: 0.2506 - val_acc: 0.9425\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9647\n",
      "Epoch 00041: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1058 - acc: 0.9647 - val_loss: 0.1935 - val_acc: 0.9520\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9663\n",
      "Epoch 00042: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1036 - acc: 0.9663 - val_loss: 0.1866 - val_acc: 0.9557\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9660\n",
      "Epoch 00043: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1033 - acc: 0.9659 - val_loss: 0.2783 - val_acc: 0.9390\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9664\n",
      "Epoch 00044: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1049 - acc: 0.9664 - val_loss: 0.2110 - val_acc: 0.9511\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9699\n",
      "Epoch 00045: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0918 - acc: 0.9699 - val_loss: 0.2196 - val_acc: 0.9469\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9706\n",
      "Epoch 00046: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0923 - acc: 0.9705 - val_loss: 0.2117 - val_acc: 0.9506\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9687\n",
      "Epoch 00047: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0962 - acc: 0.9687 - val_loss: 0.2098 - val_acc: 0.9502\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9720\n",
      "Epoch 00048: val_loss did not improve from 0.18537\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0852 - acc: 0.9720 - val_loss: 0.2036 - val_acc: 0.9492\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9738\n",
      "Epoch 00049: val_loss improved from 0.18537 to 0.18309, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv_checkpoint/049-0.1831.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0775 - acc: 0.9738 - val_loss: 0.1831 - val_acc: 0.9534\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9727\n",
      "Epoch 00050: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0814 - acc: 0.9727 - val_loss: 0.2233 - val_acc: 0.9464\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9725\n",
      "Epoch 00051: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0823 - acc: 0.9725 - val_loss: 0.2360 - val_acc: 0.9460\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9752\n",
      "Epoch 00052: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0742 - acc: 0.9752 - val_loss: 0.2957 - val_acc: 0.9378\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9777\n",
      "Epoch 00053: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0676 - acc: 0.9777 - val_loss: 0.2755 - val_acc: 0.9420\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9726\n",
      "Epoch 00054: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0853 - acc: 0.9726 - val_loss: 0.2469 - val_acc: 0.9429\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9781\n",
      "Epoch 00055: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0677 - acc: 0.9781 - val_loss: 0.2472 - val_acc: 0.9392\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9789\n",
      "Epoch 00056: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0640 - acc: 0.9789 - val_loss: 0.2541 - val_acc: 0.9429\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9786\n",
      "Epoch 00057: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0635 - acc: 0.9786 - val_loss: 0.2254 - val_acc: 0.9476\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9785\n",
      "Epoch 00058: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0671 - acc: 0.9785 - val_loss: 0.2265 - val_acc: 0.9513\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9807\n",
      "Epoch 00059: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0596 - acc: 0.9807 - val_loss: 0.2529 - val_acc: 0.9464\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9788\n",
      "Epoch 00060: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0611 - acc: 0.9788 - val_loss: 0.4328 - val_acc: 0.9068\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9787\n",
      "Epoch 00061: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0627 - acc: 0.9787 - val_loss: 0.2500 - val_acc: 0.9471\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9812\n",
      "Epoch 00062: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0574 - acc: 0.9813 - val_loss: 0.2148 - val_acc: 0.9509\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9828\n",
      "Epoch 00063: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0521 - acc: 0.9828 - val_loss: 0.2211 - val_acc: 0.9520\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9811\n",
      "Epoch 00064: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0559 - acc: 0.9811 - val_loss: 0.2186 - val_acc: 0.9488\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9832\n",
      "Epoch 00065: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0503 - acc: 0.9832 - val_loss: 0.2221 - val_acc: 0.9550\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9822\n",
      "Epoch 00066: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0566 - acc: 0.9822 - val_loss: 0.2742 - val_acc: 0.9366\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9814\n",
      "Epoch 00067: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0552 - acc: 0.9814 - val_loss: 0.2226 - val_acc: 0.9541\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9851\n",
      "Epoch 00068: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0474 - acc: 0.9851 - val_loss: 0.2467 - val_acc: 0.9483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9824\n",
      "Epoch 00069: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0517 - acc: 0.9824 - val_loss: 0.2152 - val_acc: 0.9497\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9844\n",
      "Epoch 00070: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0482 - acc: 0.9843 - val_loss: 0.3044 - val_acc: 0.9383\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9803\n",
      "Epoch 00071: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0603 - acc: 0.9803 - val_loss: 0.4051 - val_acc: 0.9166\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9852\n",
      "Epoch 00072: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0456 - acc: 0.9852 - val_loss: 0.2543 - val_acc: 0.9511\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9843\n",
      "Epoch 00073: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0467 - acc: 0.9843 - val_loss: 0.3257 - val_acc: 0.9341\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9852\n",
      "Epoch 00074: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0449 - acc: 0.9852 - val_loss: 0.2322 - val_acc: 0.9515\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9860\n",
      "Epoch 00075: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0425 - acc: 0.9860 - val_loss: 0.2511 - val_acc: 0.9481\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9853\n",
      "Epoch 00076: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0451 - acc: 0.9853 - val_loss: 0.3319 - val_acc: 0.9315\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9869\n",
      "Epoch 00077: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0423 - acc: 0.9869 - val_loss: 0.3758 - val_acc: 0.9276\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9859\n",
      "Epoch 00078: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0406 - acc: 0.9859 - val_loss: 0.2253 - val_acc: 0.9488\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9874\n",
      "Epoch 00079: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0390 - acc: 0.9873 - val_loss: 0.2447 - val_acc: 0.9506\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 00080: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0379 - acc: 0.9874 - val_loss: 0.3769 - val_acc: 0.9187\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 00081: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0397 - acc: 0.9870 - val_loss: 0.2148 - val_acc: 0.9529\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9871\n",
      "Epoch 00082: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0390 - acc: 0.9871 - val_loss: 0.2303 - val_acc: 0.9560\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9875\n",
      "Epoch 00083: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0374 - acc: 0.9875 - val_loss: 0.2393 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9847\n",
      "Epoch 00084: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0467 - acc: 0.9847 - val_loss: 0.2333 - val_acc: 0.9488\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9899\n",
      "Epoch 00085: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0306 - acc: 0.9899 - val_loss: 0.2260 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9883\n",
      "Epoch 00086: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0338 - acc: 0.9883 - val_loss: 0.2351 - val_acc: 0.9520\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9894\n",
      "Epoch 00087: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0341 - acc: 0.9894 - val_loss: 0.2037 - val_acc: 0.9581\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9886\n",
      "Epoch 00088: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0350 - acc: 0.9886 - val_loss: 0.2679 - val_acc: 0.9408\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9896\n",
      "Epoch 00089: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0315 - acc: 0.9896 - val_loss: 0.2143 - val_acc: 0.9592\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9885\n",
      "Epoch 00090: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0345 - acc: 0.9885 - val_loss: 0.3066 - val_acc: 0.9350\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9885\n",
      "Epoch 00091: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0348 - acc: 0.9885 - val_loss: 0.2727 - val_acc: 0.9469\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9896\n",
      "Epoch 00092: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0313 - acc: 0.9896 - val_loss: 0.2978 - val_acc: 0.9422\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9894\n",
      "Epoch 00093: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0326 - acc: 0.9894 - val_loss: 0.2369 - val_acc: 0.9541\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9907\n",
      "Epoch 00094: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0285 - acc: 0.9907 - val_loss: 0.2345 - val_acc: 0.9515\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9906\n",
      "Epoch 00095: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0290 - acc: 0.9906 - val_loss: 0.2316 - val_acc: 0.9564\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9888\n",
      "Epoch 00096: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0343 - acc: 0.9888 - val_loss: 0.1874 - val_acc: 0.9606\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9912\n",
      "Epoch 00097: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0264 - acc: 0.9912 - val_loss: 0.3082 - val_acc: 0.9406\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9895\n",
      "Epoch 00098: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0322 - acc: 0.9895 - val_loss: 0.2818 - val_acc: 0.9467\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9906\n",
      "Epoch 00099: val_loss did not improve from 0.18309\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0302 - acc: 0.9906 - val_loss: 0.2486 - val_acc: 0.9488\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2285 - acc: 0.9396\n",
      "Loss: 0.22854622391092194 Accuracy: 0.9395639\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6853 - acc: 0.2932\n",
      "Epoch 00001: val_loss improved from inf to 1.49243, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/001-1.4924.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 2.6851 - acc: 0.2933 - val_loss: 1.4924 - val_acc: 0.4917\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4059 - acc: 0.5709\n",
      "Epoch 00002: val_loss improved from 1.49243 to 0.68045, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/002-0.6805.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 1.4058 - acc: 0.5709 - val_loss: 0.6805 - val_acc: 0.7971\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.7031\n",
      "Epoch 00003: val_loss improved from 0.68045 to 0.47592, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/003-0.4759.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.9719 - acc: 0.7031 - val_loss: 0.4759 - val_acc: 0.8607\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7373 - acc: 0.7753\n",
      "Epoch 00004: val_loss did not improve from 0.47592\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.7373 - acc: 0.7752 - val_loss: 0.5159 - val_acc: 0.8451\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8164\n",
      "Epoch 00005: val_loss improved from 0.47592 to 0.33020, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/005-0.3302.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.5904 - acc: 0.8164 - val_loss: 0.3302 - val_acc: 0.9038\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.8466\n",
      "Epoch 00006: val_loss did not improve from 0.33020\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.5035 - acc: 0.8465 - val_loss: 0.5146 - val_acc: 0.8393\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8649\n",
      "Epoch 00007: val_loss improved from 0.33020 to 0.25004, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/007-0.2500.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.4415 - acc: 0.8649 - val_loss: 0.2500 - val_acc: 0.9313\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3825 - acc: 0.8809\n",
      "Epoch 00008: val_loss did not improve from 0.25004\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.3825 - acc: 0.8809 - val_loss: 0.3086 - val_acc: 0.9096\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8936\n",
      "Epoch 00009: val_loss did not improve from 0.25004\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.3434 - acc: 0.8935 - val_loss: 0.2762 - val_acc: 0.9208\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9041\n",
      "Epoch 00010: val_loss improved from 0.25004 to 0.22955, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/010-0.2296.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.3061 - acc: 0.9040 - val_loss: 0.2296 - val_acc: 0.9357\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9146\n",
      "Epoch 00011: val_loss did not improve from 0.22955\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.2743 - acc: 0.9146 - val_loss: 0.2413 - val_acc: 0.9324\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9232\n",
      "Epoch 00012: val_loss improved from 0.22955 to 0.17482, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/012-0.1748.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.2456 - acc: 0.9232 - val_loss: 0.1748 - val_acc: 0.9534\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9292\n",
      "Epoch 00013: val_loss did not improve from 0.17482\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.2259 - acc: 0.9291 - val_loss: 0.1890 - val_acc: 0.9513\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9310\n",
      "Epoch 00014: val_loss improved from 0.17482 to 0.16964, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/014-0.1696.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.2183 - acc: 0.9309 - val_loss: 0.1696 - val_acc: 0.9550\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9366\n",
      "Epoch 00015: val_loss did not improve from 0.16964\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.2035 - acc: 0.9366 - val_loss: 0.1962 - val_acc: 0.9432\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9406\n",
      "Epoch 00016: val_loss improved from 0.16964 to 0.16735, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/016-0.1673.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1855 - acc: 0.9406 - val_loss: 0.1673 - val_acc: 0.9483\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9449\n",
      "Epoch 00017: val_loss improved from 0.16735 to 0.16495, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/017-0.1649.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1749 - acc: 0.9448 - val_loss: 0.1649 - val_acc: 0.9541\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9488\n",
      "Epoch 00018: val_loss improved from 0.16495 to 0.14080, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/018-0.1408.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1645 - acc: 0.9488 - val_loss: 0.1408 - val_acc: 0.9616\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9518\n",
      "Epoch 00019: val_loss did not improve from 0.14080\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1509 - acc: 0.9518 - val_loss: 0.1637 - val_acc: 0.9539\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9555\n",
      "Epoch 00020: val_loss did not improve from 0.14080\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1390 - acc: 0.9554 - val_loss: 0.1427 - val_acc: 0.9613\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9529\n",
      "Epoch 00021: val_loss did not improve from 0.14080\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1473 - acc: 0.9529 - val_loss: 0.2340 - val_acc: 0.9380\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9560\n",
      "Epoch 00022: val_loss did not improve from 0.14080\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1370 - acc: 0.9560 - val_loss: 0.1613 - val_acc: 0.9532\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9608\n",
      "Epoch 00023: val_loss improved from 0.14080 to 0.13884, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/023-0.1388.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1187 - acc: 0.9608 - val_loss: 0.1388 - val_acc: 0.9611\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9633\n",
      "Epoch 00024: val_loss improved from 0.13884 to 0.12765, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/024-0.1276.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1125 - acc: 0.9633 - val_loss: 0.1276 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9676\n",
      "Epoch 00025: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1014 - acc: 0.9676 - val_loss: 0.1831 - val_acc: 0.9502\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9666\n",
      "Epoch 00026: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1033 - acc: 0.9666 - val_loss: 0.1422 - val_acc: 0.9623\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9701\n",
      "Epoch 00027: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0926 - acc: 0.9701 - val_loss: 0.1395 - val_acc: 0.9611\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9677\n",
      "Epoch 00028: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0975 - acc: 0.9677 - val_loss: 0.1681 - val_acc: 0.9576\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9719\n",
      "Epoch 00029: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0882 - acc: 0.9719 - val_loss: 0.1693 - val_acc: 0.9548\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9711\n",
      "Epoch 00030: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0887 - acc: 0.9711 - val_loss: 0.1616 - val_acc: 0.9529\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9722\n",
      "Epoch 00031: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0845 - acc: 0.9722 - val_loss: 0.2000 - val_acc: 0.9481\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9760\n",
      "Epoch 00032: val_loss did not improve from 0.12765\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0727 - acc: 0.9760 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9773\n",
      "Epoch 00033: val_loss improved from 0.12765 to 0.12667, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/033-0.1267.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0723 - acc: 0.9773 - val_loss: 0.1267 - val_acc: 0.9688\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9780\n",
      "Epoch 00034: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0709 - acc: 0.9779 - val_loss: 0.1948 - val_acc: 0.9557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9741\n",
      "Epoch 00035: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0806 - acc: 0.9741 - val_loss: 0.1477 - val_acc: 0.9602\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9798\n",
      "Epoch 00036: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0627 - acc: 0.9798 - val_loss: 0.3191 - val_acc: 0.9292\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9810\n",
      "Epoch 00037: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0588 - acc: 0.9810 - val_loss: 0.1774 - val_acc: 0.9620\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9793\n",
      "Epoch 00038: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0626 - acc: 0.9793 - val_loss: 0.1361 - val_acc: 0.9648\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9808\n",
      "Epoch 00039: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0605 - acc: 0.9808 - val_loss: 0.1385 - val_acc: 0.9630\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9846\n",
      "Epoch 00040: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0503 - acc: 0.9846 - val_loss: 0.1400 - val_acc: 0.9648\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9841\n",
      "Epoch 00041: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0504 - acc: 0.9840 - val_loss: 0.1600 - val_acc: 0.9613\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9797\n",
      "Epoch 00042: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0616 - acc: 0.9797 - val_loss: 0.1338 - val_acc: 0.9644\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9850\n",
      "Epoch 00043: val_loss did not improve from 0.12667\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0470 - acc: 0.9850 - val_loss: 0.1318 - val_acc: 0.9651\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9877\n",
      "Epoch 00044: val_loss improved from 0.12667 to 0.12019, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv_checkpoint/044-0.1202.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0398 - acc: 0.9877 - val_loss: 0.1202 - val_acc: 0.9681\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9868\n",
      "Epoch 00045: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0410 - acc: 0.9867 - val_loss: 0.2213 - val_acc: 0.9476\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9831\n",
      "Epoch 00046: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0521 - acc: 0.9830 - val_loss: 0.1579 - val_acc: 0.9623\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9835\n",
      "Epoch 00047: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0503 - acc: 0.9835 - val_loss: 0.1355 - val_acc: 0.9653\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9870\n",
      "Epoch 00048: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0419 - acc: 0.9870 - val_loss: 0.1716 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9842\n",
      "Epoch 00049: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0469 - acc: 0.9842 - val_loss: 0.1291 - val_acc: 0.9693\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9882\n",
      "Epoch 00050: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0387 - acc: 0.9882 - val_loss: 0.1773 - val_acc: 0.9546\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9894\n",
      "Epoch 00051: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0335 - acc: 0.9894 - val_loss: 0.1454 - val_acc: 0.9658\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9899\n",
      "Epoch 00052: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0327 - acc: 0.9899 - val_loss: 0.1983 - val_acc: 0.9529\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9823\n",
      "Epoch 00053: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0585 - acc: 0.9823 - val_loss: 0.1402 - val_acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9910\n",
      "Epoch 00054: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0300 - acc: 0.9910 - val_loss: 0.1440 - val_acc: 0.9672\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9907\n",
      "Epoch 00055: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0287 - acc: 0.9907 - val_loss: 0.1825 - val_acc: 0.9599\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9905\n",
      "Epoch 00056: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.2473 - val_acc: 0.9464\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9902\n",
      "Epoch 00057: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 0.1466 - val_acc: 0.9641\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9857\n",
      "Epoch 00058: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0450 - acc: 0.9857 - val_loss: 0.1331 - val_acc: 0.9630\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9912\n",
      "Epoch 00059: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0266 - acc: 0.9911 - val_loss: 0.1463 - val_acc: 0.9648\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9898\n",
      "Epoch 00060: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0322 - acc: 0.9898 - val_loss: 0.1344 - val_acc: 0.9648\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9932\n",
      "Epoch 00061: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.1620 - val_acc: 0.9632\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9915\n",
      "Epoch 00062: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0268 - acc: 0.9915 - val_loss: 0.1780 - val_acc: 0.9602\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9923\n",
      "Epoch 00063: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0243 - acc: 0.9923 - val_loss: 0.1585 - val_acc: 0.9653\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9882\n",
      "Epoch 00064: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0353 - acc: 0.9882 - val_loss: 0.1932 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9893\n",
      "Epoch 00065: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0338 - acc: 0.9893 - val_loss: 0.1523 - val_acc: 0.9616\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9936\n",
      "Epoch 00066: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0205 - acc: 0.9936 - val_loss: 0.1903 - val_acc: 0.9562\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9941\n",
      "Epoch 00067: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0206 - acc: 0.9941 - val_loss: 0.1835 - val_acc: 0.9609\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 00068: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0226 - acc: 0.9927 - val_loss: 0.1744 - val_acc: 0.9632\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9924\n",
      "Epoch 00069: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0240 - acc: 0.9924 - val_loss: 0.2468 - val_acc: 0.9497\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9887\n",
      "Epoch 00070: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0343 - acc: 0.9888 - val_loss: 0.1467 - val_acc: 0.9679\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9930\n",
      "Epoch 00071: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0220 - acc: 0.9930 - val_loss: 0.1775 - val_acc: 0.9616\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00072: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1406 - val_acc: 0.9695\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9924\n",
      "Epoch 00073: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0254 - acc: 0.9924 - val_loss: 0.1813 - val_acc: 0.9592\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9932\n",
      "Epoch 00074: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 0.2156 - val_acc: 0.9529\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9907\n",
      "Epoch 00075: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0286 - acc: 0.9907 - val_loss: 0.1960 - val_acc: 0.9625\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9953\n",
      "Epoch 00076: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0160 - acc: 0.9953 - val_loss: 0.1482 - val_acc: 0.9653\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00077: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0172 - acc: 0.9948 - val_loss: 0.2183 - val_acc: 0.9502\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9888\n",
      "Epoch 00078: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0376 - acc: 0.9888 - val_loss: 0.1572 - val_acc: 0.9660\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917\n",
      "Epoch 00079: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.1670 - val_acc: 0.9646\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9949\n",
      "Epoch 00080: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0162 - acc: 0.9949 - val_loss: 0.1625 - val_acc: 0.9653\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9957\n",
      "Epoch 00081: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0146 - acc: 0.9956 - val_loss: 0.1779 - val_acc: 0.9606\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9878\n",
      "Epoch 00082: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0377 - acc: 0.9878 - val_loss: 0.1460 - val_acc: 0.9681\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00083: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.1599 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00084: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.2221 - val_acc: 0.9543\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9956\n",
      "Epoch 00085: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0144 - acc: 0.9956 - val_loss: 0.1409 - val_acc: 0.9686\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00086: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1797 - val_acc: 0.9653\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9963\n",
      "Epoch 00087: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0127 - acc: 0.9963 - val_loss: 0.1562 - val_acc: 0.9683\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9937\n",
      "Epoch 00088: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0200 - acc: 0.9938 - val_loss: 0.1733 - val_acc: 0.9630\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 00089: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.1969 - val_acc: 0.9588\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9958\n",
      "Epoch 00090: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0131 - acc: 0.9957 - val_loss: 0.1966 - val_acc: 0.9634\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9927\n",
      "Epoch 00091: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0232 - acc: 0.9927 - val_loss: 0.1807 - val_acc: 0.9623\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9948\n",
      "Epoch 00092: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.0155 - acc: 0.9948 - val_loss: 0.1800 - val_acc: 0.9644\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 00093: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.1960 - val_acc: 0.9616\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9951\n",
      "Epoch 00094: val_loss did not improve from 0.12019\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0158 - acc: 0.9951 - val_loss: 0.1637 - val_acc: 0.9688\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1695 - acc: 0.9562\n",
      "Loss: 0.1694656056089874 Accuracy: 0.9561786\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2728 - acc: 0.4305\n",
      "Epoch 00001: val_loss improved from inf to 1.02150, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/001-1.0215.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 2.2731 - acc: 0.4304 - val_loss: 1.0215 - val_acc: 0.6751\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8923 - acc: 0.7383\n",
      "Epoch 00002: val_loss improved from 1.02150 to 0.38842, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/002-0.3884.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.8924 - acc: 0.7382 - val_loss: 0.3884 - val_acc: 0.8856\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.8273\n",
      "Epoch 00003: val_loss improved from 0.38842 to 0.28013, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/003-0.2801.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.5772 - acc: 0.8273 - val_loss: 0.2801 - val_acc: 0.9159\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4300 - acc: 0.8712\n",
      "Epoch 00004: val_loss improved from 0.28013 to 0.24607, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/004-0.2461.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.4300 - acc: 0.8712 - val_loss: 0.2461 - val_acc: 0.9273\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8957\n",
      "Epoch 00005: val_loss improved from 0.24607 to 0.20100, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/005-0.2010.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.3466 - acc: 0.8957 - val_loss: 0.2010 - val_acc: 0.9434\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9105\n",
      "Epoch 00006: val_loss improved from 0.20100 to 0.19404, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/006-0.1940.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.2906 - acc: 0.9105 - val_loss: 0.1940 - val_acc: 0.9471\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9227\n",
      "Epoch 00007: val_loss improved from 0.19404 to 0.16993, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/007-0.1699.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.2509 - acc: 0.9227 - val_loss: 0.1699 - val_acc: 0.9525\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9326\n",
      "Epoch 00008: val_loss did not improve from 0.16993\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.2189 - acc: 0.9326 - val_loss: 0.1901 - val_acc: 0.9397\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9359\n",
      "Epoch 00009: val_loss did not improve from 0.16993\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.2034 - acc: 0.9359 - val_loss: 0.1775 - val_acc: 0.9485\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9432\n",
      "Epoch 00010: val_loss improved from 0.16993 to 0.14311, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/010-0.1431.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1805 - acc: 0.9432 - val_loss: 0.1431 - val_acc: 0.9569\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9490\n",
      "Epoch 00011: val_loss did not improve from 0.14311\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1609 - acc: 0.9490 - val_loss: 0.1452 - val_acc: 0.9567\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9508\n",
      "Epoch 00012: val_loss improved from 0.14311 to 0.13982, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/012-0.1398.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1533 - acc: 0.9508 - val_loss: 0.1398 - val_acc: 0.9597\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9549\n",
      "Epoch 00013: val_loss did not improve from 0.13982\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1404 - acc: 0.9549 - val_loss: 0.1513 - val_acc: 0.9567\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9601\n",
      "Epoch 00014: val_loss did not improve from 0.13982\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1210 - acc: 0.9601 - val_loss: 0.1786 - val_acc: 0.9492\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9635\n",
      "Epoch 00015: val_loss improved from 0.13982 to 0.13390, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/015-0.1339.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.1139 - acc: 0.9635 - val_loss: 0.1339 - val_acc: 0.9630\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9663\n",
      "Epoch 00016: val_loss did not improve from 0.13390\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1064 - acc: 0.9663 - val_loss: 0.1688 - val_acc: 0.9525\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9674\n",
      "Epoch 00017: val_loss did not improve from 0.13390\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0997 - acc: 0.9673 - val_loss: 0.2530 - val_acc: 0.9331\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9685\n",
      "Epoch 00018: val_loss improved from 0.13390 to 0.11685, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/018-0.1168.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0978 - acc: 0.9684 - val_loss: 0.1168 - val_acc: 0.9669\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9735\n",
      "Epoch 00019: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0819 - acc: 0.9735 - val_loss: 0.1216 - val_acc: 0.9660\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9740\n",
      "Epoch 00020: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0801 - acc: 0.9741 - val_loss: 0.1522 - val_acc: 0.9569\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9751\n",
      "Epoch 00021: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0805 - acc: 0.9751 - val_loss: 0.1334 - val_acc: 0.9639\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9793\n",
      "Epoch 00022: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0662 - acc: 0.9792 - val_loss: 0.1270 - val_acc: 0.9641\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9755\n",
      "Epoch 00023: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0716 - acc: 0.9755 - val_loss: 0.1410 - val_acc: 0.9616\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9805\n",
      "Epoch 00024: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0607 - acc: 0.9805 - val_loss: 0.1249 - val_acc: 0.9632\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9817\n",
      "Epoch 00025: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0573 - acc: 0.9817 - val_loss: 0.1682 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9832\n",
      "Epoch 00026: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0547 - acc: 0.9832 - val_loss: 0.1661 - val_acc: 0.9578\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9813\n",
      "Epoch 00027: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0578 - acc: 0.9813 - val_loss: 0.1290 - val_acc: 0.9653\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9844\n",
      "Epoch 00028: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0487 - acc: 0.9844 - val_loss: 0.1196 - val_acc: 0.9695\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9856\n",
      "Epoch 00029: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0459 - acc: 0.9856 - val_loss: 0.1631 - val_acc: 0.9592\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9856\n",
      "Epoch 00030: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0439 - acc: 0.9856 - val_loss: 0.1327 - val_acc: 0.9665\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9826\n",
      "Epoch 00031: val_loss did not improve from 0.11685\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0536 - acc: 0.9826 - val_loss: 0.1171 - val_acc: 0.9662\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9876\n",
      "Epoch 00032: val_loss improved from 0.11685 to 0.11465, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/032-0.1147.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0396 - acc: 0.9876 - val_loss: 0.1147 - val_acc: 0.9697\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0439 - acc: 0.9860 - val_loss: 0.1181 - val_acc: 0.9688\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9878\n",
      "Epoch 00034: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0379 - acc: 0.9877 - val_loss: 0.1680 - val_acc: 0.9597\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9841\n",
      "Epoch 00035: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0498 - acc: 0.9841 - val_loss: 0.1232 - val_acc: 0.9662\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9913\n",
      "Epoch 00036: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0287 - acc: 0.9913 - val_loss: 0.1201 - val_acc: 0.9709\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9920\n",
      "Epoch 00037: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0267 - acc: 0.9920 - val_loss: 0.1201 - val_acc: 0.9681\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9897\n",
      "Epoch 00038: val_loss did not improve from 0.11465\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.1495 - val_acc: 0.9648\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9869\n",
      "Epoch 00039: val_loss improved from 0.11465 to 0.11348, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv_checkpoint/039-0.1135.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0407 - acc: 0.9869 - val_loss: 0.1135 - val_acc: 0.9711\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9913\n",
      "Epoch 00040: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0273 - acc: 0.9913 - val_loss: 0.2014 - val_acc: 0.9550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9918\n",
      "Epoch 00041: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0255 - acc: 0.9918 - val_loss: 0.1216 - val_acc: 0.9704\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9914\n",
      "Epoch 00042: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0263 - acc: 0.9914 - val_loss: 0.1280 - val_acc: 0.9713\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9913\n",
      "Epoch 00043: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0286 - acc: 0.9913 - val_loss: 0.1587 - val_acc: 0.9609\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9925\n",
      "Epoch 00044: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0245 - acc: 0.9925 - val_loss: 0.1331 - val_acc: 0.9674\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00045: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0247 - acc: 0.9919 - val_loss: 0.1287 - val_acc: 0.9641\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9901\n",
      "Epoch 00046: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0326 - acc: 0.9901 - val_loss: 0.1329 - val_acc: 0.9651\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9935\n",
      "Epoch 00047: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0216 - acc: 0.9935 - val_loss: 0.1489 - val_acc: 0.9637\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9915\n",
      "Epoch 00048: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.1181 - val_acc: 0.9727\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9947\n",
      "Epoch 00049: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0164 - acc: 0.9947 - val_loss: 0.1614 - val_acc: 0.9646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9927\n",
      "Epoch 00050: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0217 - acc: 0.9927 - val_loss: 0.1590 - val_acc: 0.9632\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9945\n",
      "Epoch 00051: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0186 - acc: 0.9944 - val_loss: 0.1470 - val_acc: 0.9665\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9891\n",
      "Epoch 00052: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0366 - acc: 0.9891 - val_loss: 0.1633 - val_acc: 0.9620\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9955\n",
      "Epoch 00053: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0156 - acc: 0.9955 - val_loss: 0.1196 - val_acc: 0.9688\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9928\n",
      "Epoch 00054: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0222 - acc: 0.9928 - val_loss: 0.1497 - val_acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9951\n",
      "Epoch 00055: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 0.1289 - val_acc: 0.9720\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9963\n",
      "Epoch 00056: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0123 - acc: 0.9963 - val_loss: 0.2005 - val_acc: 0.9571\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9895\n",
      "Epoch 00057: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0353 - acc: 0.9895 - val_loss: 0.1222 - val_acc: 0.9723\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0133 - acc: 0.9961 - val_loss: 0.1237 - val_acc: 0.9739\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1379 - val_acc: 0.9697\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.1462 - val_acc: 0.9686\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9945\n",
      "Epoch 00061: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0183 - acc: 0.9945 - val_loss: 0.1301 - val_acc: 0.9695\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 00062: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0135 - acc: 0.9960 - val_loss: 0.1463 - val_acc: 0.9711\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 00063: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.1535 - val_acc: 0.9644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9938\n",
      "Epoch 00064: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0194 - acc: 0.9938 - val_loss: 0.1445 - val_acc: 0.9697\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9948\n",
      "Epoch 00065: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0168 - acc: 0.9948 - val_loss: 0.1396 - val_acc: 0.9702\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 00066: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0138 - acc: 0.9955 - val_loss: 0.1137 - val_acc: 0.9739\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0129 - acc: 0.9963 - val_loss: 0.1637 - val_acc: 0.9632\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 00068: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0120 - acc: 0.9965 - val_loss: 0.1303 - val_acc: 0.9739\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9959\n",
      "Epoch 00069: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0131 - acc: 0.9959 - val_loss: 0.1948 - val_acc: 0.9583\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9946\n",
      "Epoch 00070: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0171 - acc: 0.9946 - val_loss: 0.1601 - val_acc: 0.9667\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9965\n",
      "Epoch 00071: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 0.1510 - val_acc: 0.9686\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9939\n",
      "Epoch 00072: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0180 - acc: 0.9939 - val_loss: 0.1224 - val_acc: 0.9732\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9968\n",
      "Epoch 00073: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0095 - acc: 0.9968 - val_loss: 0.1533 - val_acc: 0.9679\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9967\n",
      "Epoch 00074: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.1386 - val_acc: 0.9713\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 00075: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0127 - acc: 0.9958 - val_loss: 0.1444 - val_acc: 0.9697\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00076: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0116 - acc: 0.9965 - val_loss: 0.1368 - val_acc: 0.9732\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 00077: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0085 - acc: 0.9973 - val_loss: 0.1606 - val_acc: 0.9695\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9943\n",
      "Epoch 00078: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0169 - acc: 0.9943 - val_loss: 0.1371 - val_acc: 0.9695\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9976\n",
      "Epoch 00079: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0091 - acc: 0.9976 - val_loss: 0.1623 - val_acc: 0.9660\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 0.1730 - val_acc: 0.9674\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9974\n",
      "Epoch 00081: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0088 - acc: 0.9974 - val_loss: 0.1267 - val_acc: 0.9718\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9958\n",
      "Epoch 00082: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0125 - acc: 0.9958 - val_loss: 0.1469 - val_acc: 0.9669\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00083: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.1425 - val_acc: 0.9695\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9966\n",
      "Epoch 00084: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0107 - acc: 0.9966 - val_loss: 0.1386 - val_acc: 0.9681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00085: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1657 - val_acc: 0.9672\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9973\n",
      "Epoch 00086: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.1570 - val_acc: 0.9641\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9965\n",
      "Epoch 00087: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0128 - acc: 0.9965 - val_loss: 0.1249 - val_acc: 0.9716\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 00088: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0049 - acc: 0.9985 - val_loss: 0.1585 - val_acc: 0.9676\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9964\n",
      "Epoch 00089: val_loss did not improve from 0.11348\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0096 - acc: 0.9964 - val_loss: 0.1605 - val_acc: 0.9651\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1614 - acc: 0.9605\n",
      "Loss: 0.16137579948499195 Accuracy: 0.96054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_128_DO_075_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_128_DO_BN(conv_num=i)\n",
    "\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.6511 - acc: 0.5693\n",
      "Loss: 1.6511325706697821 Accuracy: 0.56926274\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.8928 - acc: 0.7583\n",
      "Loss: 0.8927627567685406 Accuracy: 0.7582554\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.6380 - acc: 0.8330\n",
      "Loss: 0.6379768278120957 Accuracy: 0.8330218\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.4392 - acc: 0.9076\n",
      "Loss: 0.4391809105625529 Accuracy: 0.9075805\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2285 - acc: 0.9396\n",
      "Loss: 0.22854622391092194 Accuracy: 0.9395639\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1695 - acc: 0.9562\n",
      "Loss: 0.1694656056089874 Accuracy: 0.9561786\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 512)            655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1614 - acc: 0.9605\n",
      "Loss: 0.16137579948499195 Accuracy: 0.96054\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_128_DO_075_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.9526 - acc: 0.6457\n",
      "Loss: 1.9525565086990626 Accuracy: 0.64569056\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9606 - acc: 0.7867\n",
      "Loss: 0.9605783773236062 Accuracy: 0.7867082\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.6856 - acc: 0.8449\n",
      "Loss: 0.6855898995389572 Accuracy: 0.84485984\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.5683 - acc: 0.8970\n",
      "Loss: 0.5683296093069257 Accuracy: 0.8969886\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3205 - acc: 0.9302\n",
      "Loss: 0.3204554096947454 Accuracy: 0.93021804\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1893 - acc: 0.9580\n",
      "Loss: 0.18926691873915888 Accuracy: 0.95804775\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 512)            655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2095 - acc: 0.9595\n",
      "Loss: 0.2094678941997095 Accuracy: 0.95950156\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
