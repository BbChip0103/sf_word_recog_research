{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 64\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 354,864\n",
      "Trainable params: 354,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 157,776\n",
      "Trainable params: 157,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 95,344\n",
      "Trainable params: 95,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 70,016\n",
      "Trainable params: 70,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 67,728\n",
      "Trainable params: 67,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 67,744\n",
      "Trainable params: 67,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1920 - acc: 0.2929\n",
      "Epoch 00001: val_loss improved from inf to 1.67022, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/001-1.6702.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.1919 - acc: 0.2929 - val_loss: 1.6702 - val_acc: 0.4631\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6194 - acc: 0.4785\n",
      "Epoch 00002: val_loss improved from 1.67022 to 1.51259, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/002-1.5126.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.6194 - acc: 0.4785 - val_loss: 1.5126 - val_acc: 0.5174\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4378 - acc: 0.5492\n",
      "Epoch 00003: val_loss improved from 1.51259 to 1.36347, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/003-1.3635.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4379 - acc: 0.5491 - val_loss: 1.3635 - val_acc: 0.5740\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2873 - acc: 0.5981\n",
      "Epoch 00004: val_loss improved from 1.36347 to 1.24870, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/004-1.2487.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2873 - acc: 0.5981 - val_loss: 1.2487 - val_acc: 0.6117\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1783 - acc: 0.6346\n",
      "Epoch 00005: val_loss improved from 1.24870 to 1.19243, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/005-1.1924.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1783 - acc: 0.6346 - val_loss: 1.1924 - val_acc: 0.6327\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0975 - acc: 0.6587\n",
      "Epoch 00006: val_loss improved from 1.19243 to 1.13477, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/006-1.1348.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0976 - acc: 0.6587 - val_loss: 1.1348 - val_acc: 0.6560\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0291 - acc: 0.6830\n",
      "Epoch 00007: val_loss improved from 1.13477 to 1.10003, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/007-1.1000.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0290 - acc: 0.6830 - val_loss: 1.1000 - val_acc: 0.6618\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9643 - acc: 0.7032\n",
      "Epoch 00008: val_loss did not improve from 1.10003\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9642 - acc: 0.7032 - val_loss: 1.1135 - val_acc: 0.6497\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7173\n",
      "Epoch 00009: val_loss improved from 1.10003 to 1.08874, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/009-1.0887.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9075 - acc: 0.7173 - val_loss: 1.0887 - val_acc: 0.6674\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8594 - acc: 0.7372\n",
      "Epoch 00010: val_loss improved from 1.08874 to 1.02779, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/010-1.0278.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8595 - acc: 0.7371 - val_loss: 1.0278 - val_acc: 0.6760\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8087 - acc: 0.7525\n",
      "Epoch 00011: val_loss improved from 1.02779 to 1.01974, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/011-1.0197.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8087 - acc: 0.7525 - val_loss: 1.0197 - val_acc: 0.6907\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7606 - acc: 0.7660\n",
      "Epoch 00012: val_loss improved from 1.01974 to 1.01664, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/012-1.0166.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7606 - acc: 0.7660 - val_loss: 1.0166 - val_acc: 0.6888\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7284 - acc: 0.7747\n",
      "Epoch 00013: val_loss improved from 1.01664 to 0.98275, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/013-0.9827.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7285 - acc: 0.7747 - val_loss: 0.9827 - val_acc: 0.7023\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6881 - acc: 0.7881\n",
      "Epoch 00014: val_loss did not improve from 0.98275\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6882 - acc: 0.7881 - val_loss: 1.0184 - val_acc: 0.6921\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.7974\n",
      "Epoch 00015: val_loss did not improve from 0.98275\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6562 - acc: 0.7974 - val_loss: 0.9975 - val_acc: 0.7114\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.8052\n",
      "Epoch 00016: val_loss improved from 0.98275 to 0.97873, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/016-0.9787.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6253 - acc: 0.8052 - val_loss: 0.9787 - val_acc: 0.7205\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8130\n",
      "Epoch 00017: val_loss improved from 0.97873 to 0.97577, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/017-0.9758.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5953 - acc: 0.8130 - val_loss: 0.9758 - val_acc: 0.7100\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8242\n",
      "Epoch 00018: val_loss improved from 0.97577 to 0.97372, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_4_conv_checkpoint/018-0.9737.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5650 - acc: 0.8242 - val_loss: 0.9737 - val_acc: 0.7193\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8318\n",
      "Epoch 00019: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5398 - acc: 0.8318 - val_loss: 0.9898 - val_acc: 0.7102\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8388\n",
      "Epoch 00020: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5120 - acc: 0.8387 - val_loss: 0.9869 - val_acc: 0.7195\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8426\n",
      "Epoch 00021: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4953 - acc: 0.8426 - val_loss: 1.0046 - val_acc: 0.7193\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8485\n",
      "Epoch 00022: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4767 - acc: 0.8485 - val_loss: 0.9922 - val_acc: 0.7265\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.8528\n",
      "Epoch 00023: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4571 - acc: 0.8528 - val_loss: 0.9880 - val_acc: 0.7177\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8594\n",
      "Epoch 00024: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4420 - acc: 0.8594 - val_loss: 0.9956 - val_acc: 0.7223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8647\n",
      "Epoch 00025: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4244 - acc: 0.8647 - val_loss: 0.9917 - val_acc: 0.7321\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8653\n",
      "Epoch 00026: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4187 - acc: 0.8653 - val_loss: 0.9939 - val_acc: 0.7242\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8698\n",
      "Epoch 00027: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4027 - acc: 0.8698 - val_loss: 1.0240 - val_acc: 0.7198\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8752\n",
      "Epoch 00028: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3843 - acc: 0.8752 - val_loss: 1.0112 - val_acc: 0.7242\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8801\n",
      "Epoch 00029: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3689 - acc: 0.8802 - val_loss: 1.0335 - val_acc: 0.7228\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8811\n",
      "Epoch 00030: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3644 - acc: 0.8811 - val_loss: 1.0193 - val_acc: 0.7314\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8872\n",
      "Epoch 00031: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3498 - acc: 0.8872 - val_loss: 1.0187 - val_acc: 0.7338\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8888\n",
      "Epoch 00032: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3411 - acc: 0.8888 - val_loss: 1.0204 - val_acc: 0.7352\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8895\n",
      "Epoch 00033: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3376 - acc: 0.8895 - val_loss: 1.0261 - val_acc: 0.7349\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.8944\n",
      "Epoch 00034: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3237 - acc: 0.8944 - val_loss: 1.0231 - val_acc: 0.7361\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8967\n",
      "Epoch 00035: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3130 - acc: 0.8968 - val_loss: 1.0580 - val_acc: 0.7235\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8984\n",
      "Epoch 00036: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3103 - acc: 0.8984 - val_loss: 1.0644 - val_acc: 0.7205\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9000\n",
      "Epoch 00037: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3035 - acc: 0.9000 - val_loss: 1.0416 - val_acc: 0.7368\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9027\n",
      "Epoch 00038: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2994 - acc: 0.9027 - val_loss: 1.0273 - val_acc: 0.7482\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9065\n",
      "Epoch 00039: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2884 - acc: 0.9065 - val_loss: 1.0712 - val_acc: 0.7307\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9061\n",
      "Epoch 00040: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2857 - acc: 0.9061 - val_loss: 1.0997 - val_acc: 0.7321\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9087\n",
      "Epoch 00041: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2753 - acc: 0.9087 - val_loss: 1.0684 - val_acc: 0.7384\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9121\n",
      "Epoch 00042: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2708 - acc: 0.9121 - val_loss: 1.0499 - val_acc: 0.7447\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9127\n",
      "Epoch 00043: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2654 - acc: 0.9128 - val_loss: 1.0608 - val_acc: 0.7473\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9138\n",
      "Epoch 00044: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2633 - acc: 0.9138 - val_loss: 1.0740 - val_acc: 0.7475\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9143\n",
      "Epoch 00045: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2595 - acc: 0.9143 - val_loss: 1.1005 - val_acc: 0.7426\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9151\n",
      "Epoch 00046: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2544 - acc: 0.9151 - val_loss: 1.1172 - val_acc: 0.7454\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9181\n",
      "Epoch 00047: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2445 - acc: 0.9181 - val_loss: 1.0535 - val_acc: 0.7494\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9207\n",
      "Epoch 00048: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2418 - acc: 0.9207 - val_loss: 1.0763 - val_acc: 0.7368\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9199\n",
      "Epoch 00049: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2421 - acc: 0.9199 - val_loss: 1.0807 - val_acc: 0.7466\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9184\n",
      "Epoch 00050: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2453 - acc: 0.9184 - val_loss: 1.0687 - val_acc: 0.7496\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9231\n",
      "Epoch 00051: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2324 - acc: 0.9231 - val_loss: 1.0600 - val_acc: 0.7552\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9238\n",
      "Epoch 00052: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2289 - acc: 0.9238 - val_loss: 1.0870 - val_acc: 0.7454\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9252\n",
      "Epoch 00053: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2280 - acc: 0.9252 - val_loss: 1.0968 - val_acc: 0.7391\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9272\n",
      "Epoch 00054: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2210 - acc: 0.9272 - val_loss: 1.0628 - val_acc: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9256\n",
      "Epoch 00055: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2234 - acc: 0.9256 - val_loss: 1.0548 - val_acc: 0.7556\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9272\n",
      "Epoch 00056: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2208 - acc: 0.9272 - val_loss: 1.0699 - val_acc: 0.7556\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9284\n",
      "Epoch 00057: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2161 - acc: 0.9284 - val_loss: 1.1000 - val_acc: 0.7466\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9327\n",
      "Epoch 00058: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2081 - acc: 0.9327 - val_loss: 1.0835 - val_acc: 0.7498\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9332\n",
      "Epoch 00059: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2063 - acc: 0.9332 - val_loss: 1.0807 - val_acc: 0.7554\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9320\n",
      "Epoch 00060: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2041 - acc: 0.9320 - val_loss: 1.0838 - val_acc: 0.7552\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9327\n",
      "Epoch 00061: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2031 - acc: 0.9326 - val_loss: 1.1217 - val_acc: 0.7591\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9333\n",
      "Epoch 00062: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1989 - acc: 0.9333 - val_loss: 1.1038 - val_acc: 0.7584\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9342\n",
      "Epoch 00063: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1978 - acc: 0.9342 - val_loss: 1.1188 - val_acc: 0.7547\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9356\n",
      "Epoch 00064: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1962 - acc: 0.9356 - val_loss: 1.1066 - val_acc: 0.7589\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9347\n",
      "Epoch 00065: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1952 - acc: 0.9347 - val_loss: 1.1018 - val_acc: 0.7543\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9344\n",
      "Epoch 00066: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1957 - acc: 0.9344 - val_loss: 1.1157 - val_acc: 0.7512\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9388\n",
      "Epoch 00067: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1832 - acc: 0.9388 - val_loss: 1.0736 - val_acc: 0.7601\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9376\n",
      "Epoch 00068: val_loss did not improve from 0.97372\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1903 - acc: 0.9376 - val_loss: 1.1114 - val_acc: 0.7582\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 562us/sample - loss: 1.0616 - acc: 0.6816\n",
      "Loss: 1.0616259873594203 Accuracy: 0.68161994\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3388 - acc: 0.2279\n",
      "Epoch 00001: val_loss improved from inf to 1.82587, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/001-1.8259.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.3388 - acc: 0.2279 - val_loss: 1.8259 - val_acc: 0.4135\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7483 - acc: 0.4254\n",
      "Epoch 00002: val_loss improved from 1.82587 to 1.50217, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/002-1.5022.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.7483 - acc: 0.4254 - val_loss: 1.5022 - val_acc: 0.5283\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5470 - acc: 0.4997\n",
      "Epoch 00003: val_loss improved from 1.50217 to 1.37204, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/003-1.3720.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5470 - acc: 0.4997 - val_loss: 1.3720 - val_acc: 0.5800\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4134 - acc: 0.5487\n",
      "Epoch 00004: val_loss improved from 1.37204 to 1.25614, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/004-1.2561.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4134 - acc: 0.5488 - val_loss: 1.2561 - val_acc: 0.6184\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3097 - acc: 0.5833\n",
      "Epoch 00005: val_loss improved from 1.25614 to 1.15408, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/005-1.1541.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.3097 - acc: 0.5833 - val_loss: 1.1541 - val_acc: 0.6560\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2090 - acc: 0.6225\n",
      "Epoch 00006: val_loss improved from 1.15408 to 1.08406, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/006-1.0841.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2089 - acc: 0.6226 - val_loss: 1.0841 - val_acc: 0.6830\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6499\n",
      "Epoch 00007: val_loss improved from 1.08406 to 1.04106, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/007-1.0411.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1258 - acc: 0.6499 - val_loss: 1.0411 - val_acc: 0.6862\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0579 - acc: 0.6741\n",
      "Epoch 00008: val_loss improved from 1.04106 to 0.97606, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/008-0.9761.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0579 - acc: 0.6741 - val_loss: 0.9761 - val_acc: 0.6969\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0076 - acc: 0.6897\n",
      "Epoch 00009: val_loss improved from 0.97606 to 0.93666, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/009-0.9367.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0078 - acc: 0.6897 - val_loss: 0.9367 - val_acc: 0.7184\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.7040\n",
      "Epoch 00010: val_loss improved from 0.93666 to 0.90010, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/010-0.9001.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9607 - acc: 0.7040 - val_loss: 0.9001 - val_acc: 0.7286\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9212 - acc: 0.7182\n",
      "Epoch 00011: val_loss improved from 0.90010 to 0.87197, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/011-0.8720.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9212 - acc: 0.7182 - val_loss: 0.8720 - val_acc: 0.7393\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8854 - acc: 0.7291\n",
      "Epoch 00012: val_loss improved from 0.87197 to 0.87136, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/012-0.8714.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8856 - acc: 0.7291 - val_loss: 0.8714 - val_acc: 0.7426\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8558 - acc: 0.7390\n",
      "Epoch 00013: val_loss improved from 0.87136 to 0.85264, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/013-0.8526.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8557 - acc: 0.7391 - val_loss: 0.8526 - val_acc: 0.7438\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8251 - acc: 0.7494\n",
      "Epoch 00014: val_loss improved from 0.85264 to 0.83259, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/014-0.8326.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8252 - acc: 0.7494 - val_loss: 0.8326 - val_acc: 0.7545\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7977 - acc: 0.7558\n",
      "Epoch 00015: val_loss improved from 0.83259 to 0.80449, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/015-0.8045.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7977 - acc: 0.7558 - val_loss: 0.8045 - val_acc: 0.7603\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7610\n",
      "Epoch 00016: val_loss did not improve from 0.80449\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7784 - acc: 0.7610 - val_loss: 0.9285 - val_acc: 0.7272\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7560 - acc: 0.7692\n",
      "Epoch 00017: val_loss improved from 0.80449 to 0.76684, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/017-0.7668.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7560 - acc: 0.7692 - val_loss: 0.7668 - val_acc: 0.7761\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.7761\n",
      "Epoch 00018: val_loss improved from 0.76684 to 0.74928, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/018-0.7493.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7363 - acc: 0.7761 - val_loss: 0.7493 - val_acc: 0.7829\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7840\n",
      "Epoch 00019: val_loss improved from 0.74928 to 0.74729, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/019-0.7473.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7120 - acc: 0.7841 - val_loss: 0.7473 - val_acc: 0.7759\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6989 - acc: 0.7876\n",
      "Epoch 00020: val_loss improved from 0.74729 to 0.72556, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/020-0.7256.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6990 - acc: 0.7875 - val_loss: 0.7256 - val_acc: 0.7897\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.7946\n",
      "Epoch 00021: val_loss did not improve from 0.72556\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6778 - acc: 0.7946 - val_loss: 0.7286 - val_acc: 0.7838\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6678 - acc: 0.7966\n",
      "Epoch 00022: val_loss improved from 0.72556 to 0.71282, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/022-0.7128.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6677 - acc: 0.7966 - val_loss: 0.7128 - val_acc: 0.7890\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.8010\n",
      "Epoch 00023: val_loss did not improve from 0.71282\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6576 - acc: 0.8010 - val_loss: 0.7340 - val_acc: 0.7927\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.8077\n",
      "Epoch 00024: val_loss improved from 0.71282 to 0.71122, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/024-0.7112.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6376 - acc: 0.8078 - val_loss: 0.7112 - val_acc: 0.7918\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6216 - acc: 0.8118\n",
      "Epoch 00025: val_loss did not improve from 0.71122\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6216 - acc: 0.8118 - val_loss: 0.7124 - val_acc: 0.7920\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.8123\n",
      "Epoch 00026: val_loss improved from 0.71122 to 0.69140, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/026-0.6914.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6093 - acc: 0.8124 - val_loss: 0.6914 - val_acc: 0.7985\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5974 - acc: 0.8180\n",
      "Epoch 00027: val_loss improved from 0.69140 to 0.67842, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/027-0.6784.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5974 - acc: 0.8180 - val_loss: 0.6784 - val_acc: 0.8020\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8212\n",
      "Epoch 00028: val_loss did not improve from 0.67842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5879 - acc: 0.8212 - val_loss: 0.7034 - val_acc: 0.7948\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8235\n",
      "Epoch 00029: val_loss did not improve from 0.67842\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5767 - acc: 0.8235 - val_loss: 0.6807 - val_acc: 0.8102\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8286\n",
      "Epoch 00030: val_loss improved from 0.67842 to 0.67700, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/030-0.6770.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5651 - acc: 0.8286 - val_loss: 0.6770 - val_acc: 0.8050\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.8308\n",
      "Epoch 00031: val_loss did not improve from 0.67700\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5536 - acc: 0.8308 - val_loss: 0.6850 - val_acc: 0.8013\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8336\n",
      "Epoch 00032: val_loss improved from 0.67700 to 0.66468, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/032-0.6647.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5456 - acc: 0.8336 - val_loss: 0.6647 - val_acc: 0.8071\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8346\n",
      "Epoch 00033: val_loss did not improve from 0.66468\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5377 - acc: 0.8346 - val_loss: 0.6699 - val_acc: 0.8083\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8411\n",
      "Epoch 00034: val_loss improved from 0.66468 to 0.65636, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/034-0.6564.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5220 - acc: 0.8411 - val_loss: 0.6564 - val_acc: 0.8109\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8410\n",
      "Epoch 00035: val_loss did not improve from 0.65636\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5167 - acc: 0.8410 - val_loss: 0.6716 - val_acc: 0.8092\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8459\n",
      "Epoch 00036: val_loss did not improve from 0.65636\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5061 - acc: 0.8459 - val_loss: 0.6827 - val_acc: 0.8008\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8497\n",
      "Epoch 00037: val_loss improved from 0.65636 to 0.65556, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/037-0.6556.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4953 - acc: 0.8497 - val_loss: 0.6556 - val_acc: 0.8137\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8451\n",
      "Epoch 00038: val_loss did not improve from 0.65556\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4938 - acc: 0.8451 - val_loss: 0.6649 - val_acc: 0.8125\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8498\n",
      "Epoch 00039: val_loss did not improve from 0.65556\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4856 - acc: 0.8498 - val_loss: 0.6900 - val_acc: 0.8046\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8513\n",
      "Epoch 00040: val_loss did not improve from 0.65556\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4800 - acc: 0.8513 - val_loss: 0.6650 - val_acc: 0.8120\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4719 - acc: 0.8528\n",
      "Epoch 00041: val_loss did not improve from 0.65556\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4718 - acc: 0.8528 - val_loss: 0.6697 - val_acc: 0.8083\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.8534\n",
      "Epoch 00042: val_loss improved from 0.65556 to 0.65283, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/042-0.6528.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4697 - acc: 0.8534 - val_loss: 0.6528 - val_acc: 0.8155\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4579 - acc: 0.8570\n",
      "Epoch 00043: val_loss improved from 0.65283 to 0.65037, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/043-0.6504.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4579 - acc: 0.8570 - val_loss: 0.6504 - val_acc: 0.8143\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8570\n",
      "Epoch 00044: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4570 - acc: 0.8570 - val_loss: 0.6586 - val_acc: 0.8150\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8610\n",
      "Epoch 00045: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4481 - acc: 0.8610 - val_loss: 0.6682 - val_acc: 0.8137\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.8622\n",
      "Epoch 00046: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4472 - acc: 0.8622 - val_loss: 0.6785 - val_acc: 0.8102\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8617\n",
      "Epoch 00047: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4379 - acc: 0.8617 - val_loss: 0.6604 - val_acc: 0.8106\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8650\n",
      "Epoch 00048: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4316 - acc: 0.8650 - val_loss: 0.7092 - val_acc: 0.8085\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8646\n",
      "Epoch 00049: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4266 - acc: 0.8646 - val_loss: 0.6652 - val_acc: 0.8162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8681\n",
      "Epoch 00050: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4209 - acc: 0.8681 - val_loss: 0.6714 - val_acc: 0.8176\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8690\n",
      "Epoch 00051: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4152 - acc: 0.8690 - val_loss: 0.6623 - val_acc: 0.8192\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8707\n",
      "Epoch 00052: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4096 - acc: 0.8707 - val_loss: 0.6516 - val_acc: 0.8223\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8710\n",
      "Epoch 00053: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4054 - acc: 0.8710 - val_loss: 0.6560 - val_acc: 0.8195\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8739\n",
      "Epoch 00054: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4001 - acc: 0.8739 - val_loss: 0.6585 - val_acc: 0.8258\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8720\n",
      "Epoch 00055: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3994 - acc: 0.8720 - val_loss: 0.6608 - val_acc: 0.8167\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8749\n",
      "Epoch 00056: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3922 - acc: 0.8749 - val_loss: 0.6602 - val_acc: 0.8220\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8772\n",
      "Epoch 00057: val_loss did not improve from 0.65037\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3899 - acc: 0.8772 - val_loss: 0.6634 - val_acc: 0.8297\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8796\n",
      "Epoch 00058: val_loss improved from 0.65037 to 0.64855, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/058-0.6485.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3821 - acc: 0.8796 - val_loss: 0.6485 - val_acc: 0.8251\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8773\n",
      "Epoch 00059: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3849 - acc: 0.8773 - val_loss: 0.6554 - val_acc: 0.8295\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8788\n",
      "Epoch 00060: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3796 - acc: 0.8788 - val_loss: 0.6770 - val_acc: 0.8281\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8792\n",
      "Epoch 00061: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3725 - acc: 0.8792 - val_loss: 0.6676 - val_acc: 0.8206\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8832\n",
      "Epoch 00062: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3666 - acc: 0.8832 - val_loss: 0.7030 - val_acc: 0.8183\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8827\n",
      "Epoch 00063: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3674 - acc: 0.8828 - val_loss: 0.6622 - val_acc: 0.8283\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8840\n",
      "Epoch 00064: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3653 - acc: 0.8840 - val_loss: 0.6518 - val_acc: 0.8267\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8848\n",
      "Epoch 00065: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3594 - acc: 0.8848 - val_loss: 0.6682 - val_acc: 0.8239\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8820\n",
      "Epoch 00066: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3630 - acc: 0.8819 - val_loss: 0.6662 - val_acc: 0.8253\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8871\n",
      "Epoch 00067: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3566 - acc: 0.8871 - val_loss: 0.6569 - val_acc: 0.8288\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8886\n",
      "Epoch 00068: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3489 - acc: 0.8886 - val_loss: 0.6645 - val_acc: 0.8260\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8905\n",
      "Epoch 00069: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3438 - acc: 0.8905 - val_loss: 0.6952 - val_acc: 0.8241\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8885\n",
      "Epoch 00070: val_loss did not improve from 0.64855\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3423 - acc: 0.8885 - val_loss: 0.6733 - val_acc: 0.8234\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8891\n",
      "Epoch 00071: val_loss improved from 0.64855 to 0.64714, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_5_conv_checkpoint/071-0.6471.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3417 - acc: 0.8891 - val_loss: 0.6471 - val_acc: 0.8295\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8898\n",
      "Epoch 00072: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3409 - acc: 0.8898 - val_loss: 0.6736 - val_acc: 0.8302\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8922\n",
      "Epoch 00073: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3345 - acc: 0.8922 - val_loss: 0.6711 - val_acc: 0.8227\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8916\n",
      "Epoch 00074: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3364 - acc: 0.8916 - val_loss: 0.7007 - val_acc: 0.8150\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8933\n",
      "Epoch 00075: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3325 - acc: 0.8933 - val_loss: 0.6623 - val_acc: 0.8269\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8944\n",
      "Epoch 00076: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3295 - acc: 0.8944 - val_loss: 0.6800 - val_acc: 0.8248\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8956\n",
      "Epoch 00077: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3235 - acc: 0.8956 - val_loss: 0.6727 - val_acc: 0.8295\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8969\n",
      "Epoch 00078: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3243 - acc: 0.8969 - val_loss: 0.6694 - val_acc: 0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8960\n",
      "Epoch 00079: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3165 - acc: 0.8960 - val_loss: 0.6672 - val_acc: 0.8316\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8953\n",
      "Epoch 00080: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3192 - acc: 0.8953 - val_loss: 0.6722 - val_acc: 0.8309\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8958\n",
      "Epoch 00081: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3207 - acc: 0.8957 - val_loss: 0.6797 - val_acc: 0.8232\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8965\n",
      "Epoch 00082: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3194 - acc: 0.8965 - val_loss: 0.6719 - val_acc: 0.8272\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.8984\n",
      "Epoch 00083: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3159 - acc: 0.8984 - val_loss: 0.6605 - val_acc: 0.8332\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.8988\n",
      "Epoch 00084: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3080 - acc: 0.8988 - val_loss: 0.6697 - val_acc: 0.8302\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9003\n",
      "Epoch 00085: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3096 - acc: 0.9002 - val_loss: 0.6811 - val_acc: 0.8253\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9017\n",
      "Epoch 00086: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3044 - acc: 0.9017 - val_loss: 0.6894 - val_acc: 0.8276\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9022\n",
      "Epoch 00087: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2987 - acc: 0.9022 - val_loss: 0.6796 - val_acc: 0.8286\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9026\n",
      "Epoch 00088: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3012 - acc: 0.9025 - val_loss: 0.6916 - val_acc: 0.8300\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9010\n",
      "Epoch 00089: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3025 - acc: 0.9010 - val_loss: 0.6638 - val_acc: 0.8286\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9026\n",
      "Epoch 00090: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2990 - acc: 0.9026 - val_loss: 0.6927 - val_acc: 0.8265\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9060\n",
      "Epoch 00091: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2898 - acc: 0.9060 - val_loss: 0.6980 - val_acc: 0.8253\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9064\n",
      "Epoch 00092: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2928 - acc: 0.9064 - val_loss: 0.6695 - val_acc: 0.8295\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9063\n",
      "Epoch 00093: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2921 - acc: 0.9063 - val_loss: 0.6758 - val_acc: 0.8304\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9074\n",
      "Epoch 00094: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2883 - acc: 0.9073 - val_loss: 0.6986 - val_acc: 0.8234\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9058\n",
      "Epoch 00095: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2872 - acc: 0.9058 - val_loss: 0.6939 - val_acc: 0.8274\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9052\n",
      "Epoch 00096: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2888 - acc: 0.9052 - val_loss: 0.6954 - val_acc: 0.8314\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9084\n",
      "Epoch 00097: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2829 - acc: 0.9084 - val_loss: 0.6648 - val_acc: 0.8332\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9070\n",
      "Epoch 00098: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2854 - acc: 0.9069 - val_loss: 0.6988 - val_acc: 0.8234\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9073\n",
      "Epoch 00099: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2816 - acc: 0.9073 - val_loss: 0.6868 - val_acc: 0.8286\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9084\n",
      "Epoch 00100: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2815 - acc: 0.9084 - val_loss: 0.7214 - val_acc: 0.8272\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9084\n",
      "Epoch 00101: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2764 - acc: 0.9084 - val_loss: 0.6930 - val_acc: 0.8288\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9081\n",
      "Epoch 00102: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2786 - acc: 0.9081 - val_loss: 0.6678 - val_acc: 0.8344\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9091\n",
      "Epoch 00103: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2765 - acc: 0.9091 - val_loss: 0.6764 - val_acc: 0.8297\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9104\n",
      "Epoch 00104: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2711 - acc: 0.9104 - val_loss: 0.6911 - val_acc: 0.8334\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9126\n",
      "Epoch 00105: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2697 - acc: 0.9125 - val_loss: 0.7021 - val_acc: 0.8237\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9129\n",
      "Epoch 00106: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2668 - acc: 0.9129 - val_loss: 0.6896 - val_acc: 0.8318\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9113\n",
      "Epoch 00107: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2689 - acc: 0.9113 - val_loss: 0.6814 - val_acc: 0.8358\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9124\n",
      "Epoch 00108: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2706 - acc: 0.9124 - val_loss: 0.6737 - val_acc: 0.8355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9111\n",
      "Epoch 00109: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2671 - acc: 0.9110 - val_loss: 0.6880 - val_acc: 0.8344\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9129\n",
      "Epoch 00110: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2700 - acc: 0.9129 - val_loss: 0.7004 - val_acc: 0.8279\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9132\n",
      "Epoch 00111: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2653 - acc: 0.9132 - val_loss: 0.7003 - val_acc: 0.8281\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9160\n",
      "Epoch 00112: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2553 - acc: 0.9160 - val_loss: 0.6993 - val_acc: 0.8332\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9152\n",
      "Epoch 00113: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2602 - acc: 0.9152 - val_loss: 0.6945 - val_acc: 0.8297\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9152\n",
      "Epoch 00114: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2535 - acc: 0.9152 - val_loss: 0.6907 - val_acc: 0.8339\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9162\n",
      "Epoch 00115: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2537 - acc: 0.9163 - val_loss: 0.7001 - val_acc: 0.8351\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9171\n",
      "Epoch 00116: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2550 - acc: 0.9171 - val_loss: 0.6929 - val_acc: 0.8344\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9158\n",
      "Epoch 00117: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2575 - acc: 0.9158 - val_loss: 0.6733 - val_acc: 0.8400\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9163\n",
      "Epoch 00118: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2565 - acc: 0.9163 - val_loss: 0.6884 - val_acc: 0.8404\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9156\n",
      "Epoch 00119: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2547 - acc: 0.9156 - val_loss: 0.6969 - val_acc: 0.8348\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9187\n",
      "Epoch 00120: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2490 - acc: 0.9187 - val_loss: 0.6767 - val_acc: 0.8390\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9193\n",
      "Epoch 00121: val_loss did not improve from 0.64714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2513 - acc: 0.9193 - val_loss: 0.7124 - val_acc: 0.8307\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 590us/sample - loss: 0.7404 - acc: 0.8019\n",
      "Loss: 0.740428773425945 Accuracy: 0.80186915\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5216 - acc: 0.1753\n",
      "Epoch 00001: val_loss improved from inf to 2.00508, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/001-2.0051.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.5215 - acc: 0.1754 - val_loss: 2.0051 - val_acc: 0.3701\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9047 - acc: 0.3735\n",
      "Epoch 00002: val_loss improved from 2.00508 to 1.60065, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/002-1.6006.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.9047 - acc: 0.3736 - val_loss: 1.6006 - val_acc: 0.5015\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6576 - acc: 0.4539\n",
      "Epoch 00003: val_loss improved from 1.60065 to 1.43807, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/003-1.4381.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.6575 - acc: 0.4540 - val_loss: 1.4381 - val_acc: 0.5402\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5224 - acc: 0.4988\n",
      "Epoch 00004: val_loss improved from 1.43807 to 1.31708, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/004-1.3171.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5223 - acc: 0.4988 - val_loss: 1.3171 - val_acc: 0.5816\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4037 - acc: 0.5456\n",
      "Epoch 00005: val_loss improved from 1.31708 to 1.21440, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/005-1.2144.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4038 - acc: 0.5456 - val_loss: 1.2144 - val_acc: 0.6320\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3129 - acc: 0.5832\n",
      "Epoch 00006: val_loss improved from 1.21440 to 1.09716, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/006-1.0972.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3128 - acc: 0.5832 - val_loss: 1.0972 - val_acc: 0.6709\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2343 - acc: 0.6100\n",
      "Epoch 00007: val_loss improved from 1.09716 to 1.03229, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/007-1.0323.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2343 - acc: 0.6101 - val_loss: 1.0323 - val_acc: 0.6897\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1721 - acc: 0.6359\n",
      "Epoch 00008: val_loss improved from 1.03229 to 0.99262, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/008-0.9926.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1720 - acc: 0.6359 - val_loss: 0.9926 - val_acc: 0.7018\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.6561\n",
      "Epoch 00009: val_loss improved from 0.99262 to 0.92471, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/009-0.9247.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1083 - acc: 0.6561 - val_loss: 0.9247 - val_acc: 0.7333\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0573 - acc: 0.6732\n",
      "Epoch 00010: val_loss improved from 0.92471 to 0.91545, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/010-0.9155.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0572 - acc: 0.6733 - val_loss: 0.9155 - val_acc: 0.7256\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0187 - acc: 0.6853\n",
      "Epoch 00011: val_loss improved from 0.91545 to 0.85586, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/011-0.8559.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0189 - acc: 0.6853 - val_loss: 0.8559 - val_acc: 0.7580\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9706 - acc: 0.7049\n",
      "Epoch 00012: val_loss improved from 0.85586 to 0.83157, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/012-0.8316.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9706 - acc: 0.7049 - val_loss: 0.8316 - val_acc: 0.7603\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.7163\n",
      "Epoch 00013: val_loss improved from 0.83157 to 0.80049, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/013-0.8005.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9331 - acc: 0.7163 - val_loss: 0.8005 - val_acc: 0.7664\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.7250\n",
      "Epoch 00014: val_loss improved from 0.80049 to 0.76076, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/014-0.7608.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9059 - acc: 0.7250 - val_loss: 0.7608 - val_acc: 0.7773\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8714 - acc: 0.7367\n",
      "Epoch 00015: val_loss did not improve from 0.76076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8714 - acc: 0.7367 - val_loss: 0.7614 - val_acc: 0.7766\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8483 - acc: 0.7427\n",
      "Epoch 00016: val_loss improved from 0.76076 to 0.71426, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/016-0.7143.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8483 - acc: 0.7427 - val_loss: 0.7143 - val_acc: 0.7908\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8167 - acc: 0.7549\n",
      "Epoch 00017: val_loss improved from 0.71426 to 0.70083, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/017-0.7008.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8167 - acc: 0.7549 - val_loss: 0.7008 - val_acc: 0.7943\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7882 - acc: 0.7621\n",
      "Epoch 00018: val_loss improved from 0.70083 to 0.66214, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/018-0.6621.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7882 - acc: 0.7622 - val_loss: 0.6621 - val_acc: 0.8164\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7676 - acc: 0.7696\n",
      "Epoch 00019: val_loss improved from 0.66214 to 0.64760, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/019-0.6476.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7675 - acc: 0.7697 - val_loss: 0.6476 - val_acc: 0.8164\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7742\n",
      "Epoch 00020: val_loss did not improve from 0.64760\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7415 - acc: 0.7742 - val_loss: 0.6535 - val_acc: 0.8202\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7241 - acc: 0.7814\n",
      "Epoch 00021: val_loss improved from 0.64760 to 0.64506, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/021-0.6451.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7241 - acc: 0.7814 - val_loss: 0.6451 - val_acc: 0.8102\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7870\n",
      "Epoch 00022: val_loss improved from 0.64506 to 0.60890, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/022-0.6089.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7096 - acc: 0.7870 - val_loss: 0.6089 - val_acc: 0.8286\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6906 - acc: 0.7905\n",
      "Epoch 00023: val_loss improved from 0.60890 to 0.60769, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/023-0.6077.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6910 - acc: 0.7905 - val_loss: 0.6077 - val_acc: 0.8295\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6714 - acc: 0.7984\n",
      "Epoch 00024: val_loss improved from 0.60769 to 0.58529, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/024-0.5853.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6714 - acc: 0.7984 - val_loss: 0.5853 - val_acc: 0.8351\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.8005\n",
      "Epoch 00025: val_loss improved from 0.58529 to 0.56947, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/025-0.5695.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6607 - acc: 0.8005 - val_loss: 0.5695 - val_acc: 0.8430\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.8045\n",
      "Epoch 00026: val_loss improved from 0.56947 to 0.56147, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/026-0.5615.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6495 - acc: 0.8045 - val_loss: 0.5615 - val_acc: 0.8456\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.8133\n",
      "Epoch 00027: val_loss did not improve from 0.56147\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6333 - acc: 0.8133 - val_loss: 0.5851 - val_acc: 0.8372\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.8143\n",
      "Epoch 00028: val_loss did not improve from 0.56147\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6224 - acc: 0.8143 - val_loss: 0.5626 - val_acc: 0.8404\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.8163\n",
      "Epoch 00029: val_loss improved from 0.56147 to 0.56145, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/029-0.5614.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6091 - acc: 0.8163 - val_loss: 0.5614 - val_acc: 0.8449\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.8215\n",
      "Epoch 00030: val_loss did not improve from 0.56145\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5970 - acc: 0.8215 - val_loss: 0.5647 - val_acc: 0.8509\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5882 - acc: 0.8224\n",
      "Epoch 00031: val_loss improved from 0.56145 to 0.53739, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/031-0.5374.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5881 - acc: 0.8224 - val_loss: 0.5374 - val_acc: 0.8488\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.8261\n",
      "Epoch 00032: val_loss improved from 0.53739 to 0.53234, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/032-0.5323.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5775 - acc: 0.8261 - val_loss: 0.5323 - val_acc: 0.8509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.8300\n",
      "Epoch 00033: val_loss improved from 0.53234 to 0.51694, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/033-0.5169.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5659 - acc: 0.8299 - val_loss: 0.5169 - val_acc: 0.8595\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8318\n",
      "Epoch 00034: val_loss improved from 0.51694 to 0.50467, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/034-0.5047.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5572 - acc: 0.8318 - val_loss: 0.5047 - val_acc: 0.8626\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8352\n",
      "Epoch 00035: val_loss improved from 0.50467 to 0.49489, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/035-0.4949.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5485 - acc: 0.8352 - val_loss: 0.4949 - val_acc: 0.8621\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8385\n",
      "Epoch 00036: val_loss did not improve from 0.49489\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5387 - acc: 0.8385 - val_loss: 0.4982 - val_acc: 0.8626\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8397\n",
      "Epoch 00037: val_loss improved from 0.49489 to 0.47939, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/037-0.4794.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5318 - acc: 0.8397 - val_loss: 0.4794 - val_acc: 0.8735\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8413\n",
      "Epoch 00038: val_loss did not improve from 0.47939\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5317 - acc: 0.8413 - val_loss: 0.5026 - val_acc: 0.8630\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8439\n",
      "Epoch 00039: val_loss did not improve from 0.47939\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5155 - acc: 0.8439 - val_loss: 0.4813 - val_acc: 0.8719\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8477\n",
      "Epoch 00040: val_loss improved from 0.47939 to 0.46724, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/040-0.4672.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5026 - acc: 0.8477 - val_loss: 0.4672 - val_acc: 0.8712\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8483\n",
      "Epoch 00041: val_loss did not improve from 0.46724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4991 - acc: 0.8484 - val_loss: 0.4704 - val_acc: 0.8707\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.8524\n",
      "Epoch 00042: val_loss did not improve from 0.46724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4896 - acc: 0.8524 - val_loss: 0.5042 - val_acc: 0.8600\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4844 - acc: 0.8529\n",
      "Epoch 00043: val_loss improved from 0.46724 to 0.45902, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/043-0.4590.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4844 - acc: 0.8529 - val_loss: 0.4590 - val_acc: 0.8751\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.8522\n",
      "Epoch 00044: val_loss improved from 0.45902 to 0.45351, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/044-0.4535.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4895 - acc: 0.8522 - val_loss: 0.4535 - val_acc: 0.8770\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8548\n",
      "Epoch 00045: val_loss did not improve from 0.45351\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4796 - acc: 0.8548 - val_loss: 0.4616 - val_acc: 0.8733\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8577\n",
      "Epoch 00046: val_loss improved from 0.45351 to 0.44068, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/046-0.4407.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4726 - acc: 0.8577 - val_loss: 0.4407 - val_acc: 0.8819\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8570\n",
      "Epoch 00047: val_loss did not improve from 0.44068\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4659 - acc: 0.8570 - val_loss: 0.4604 - val_acc: 0.8749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8600\n",
      "Epoch 00048: val_loss improved from 0.44068 to 0.43904, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/048-0.4390.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4589 - acc: 0.8600 - val_loss: 0.4390 - val_acc: 0.8814\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.8619\n",
      "Epoch 00049: val_loss did not improve from 0.43904\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4566 - acc: 0.8619 - val_loss: 0.4528 - val_acc: 0.8793\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8621\n",
      "Epoch 00050: val_loss did not improve from 0.43904\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4520 - acc: 0.8622 - val_loss: 0.4473 - val_acc: 0.8749\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8640\n",
      "Epoch 00051: val_loss did not improve from 0.43904\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4450 - acc: 0.8640 - val_loss: 0.4565 - val_acc: 0.8735\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8646\n",
      "Epoch 00052: val_loss improved from 0.43904 to 0.42405, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/052-0.4241.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4446 - acc: 0.8646 - val_loss: 0.4241 - val_acc: 0.8810\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.8669\n",
      "Epoch 00053: val_loss did not improve from 0.42405\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4390 - acc: 0.8669 - val_loss: 0.4322 - val_acc: 0.8856\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8658\n",
      "Epoch 00054: val_loss did not improve from 0.42405\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4407 - acc: 0.8658 - val_loss: 0.4263 - val_acc: 0.8826\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8695\n",
      "Epoch 00055: val_loss did not improve from 0.42405\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4280 - acc: 0.8695 - val_loss: 0.4610 - val_acc: 0.8803\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8715\n",
      "Epoch 00056: val_loss improved from 0.42405 to 0.42037, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/056-0.4204.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4175 - acc: 0.8715 - val_loss: 0.4204 - val_acc: 0.8868\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8717\n",
      "Epoch 00057: val_loss improved from 0.42037 to 0.41376, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/057-0.4138.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4245 - acc: 0.8717 - val_loss: 0.4138 - val_acc: 0.8912\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8721\n",
      "Epoch 00058: val_loss did not improve from 0.41376\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4183 - acc: 0.8721 - val_loss: 0.4313 - val_acc: 0.8847\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8739\n",
      "Epoch 00059: val_loss did not improve from 0.41376\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4156 - acc: 0.8739 - val_loss: 0.4227 - val_acc: 0.8866\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8745\n",
      "Epoch 00060: val_loss improved from 0.41376 to 0.41121, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/060-0.4112.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4095 - acc: 0.8744 - val_loss: 0.4112 - val_acc: 0.8905\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8759\n",
      "Epoch 00061: val_loss improved from 0.41121 to 0.40974, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/061-0.4097.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4073 - acc: 0.8759 - val_loss: 0.4097 - val_acc: 0.8908\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8776\n",
      "Epoch 00062: val_loss did not improve from 0.40974\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3989 - acc: 0.8776 - val_loss: 0.4270 - val_acc: 0.8840\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8777\n",
      "Epoch 00063: val_loss improved from 0.40974 to 0.40032, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/063-0.4003.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4007 - acc: 0.8777 - val_loss: 0.4003 - val_acc: 0.8961\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8774\n",
      "Epoch 00064: val_loss improved from 0.40032 to 0.39830, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/064-0.3983.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3995 - acc: 0.8774 - val_loss: 0.3983 - val_acc: 0.8926\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8787\n",
      "Epoch 00065: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3947 - acc: 0.8787 - val_loss: 0.4055 - val_acc: 0.8903\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8795\n",
      "Epoch 00066: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3910 - acc: 0.8795 - val_loss: 0.4510 - val_acc: 0.8749\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8792\n",
      "Epoch 00067: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3889 - acc: 0.8792 - val_loss: 0.4151 - val_acc: 0.8854\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8805\n",
      "Epoch 00068: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3864 - acc: 0.8805 - val_loss: 0.4025 - val_acc: 0.8942\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8849\n",
      "Epoch 00069: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3762 - acc: 0.8849 - val_loss: 0.4068 - val_acc: 0.8940\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8836\n",
      "Epoch 00070: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3802 - acc: 0.8836 - val_loss: 0.4213 - val_acc: 0.8868\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8829\n",
      "Epoch 00071: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3768 - acc: 0.8829 - val_loss: 0.4110 - val_acc: 0.8926\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8835\n",
      "Epoch 00072: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3710 - acc: 0.8834 - val_loss: 0.4140 - val_acc: 0.8894\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8866\n",
      "Epoch 00073: val_loss did not improve from 0.39830\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3691 - acc: 0.8866 - val_loss: 0.4205 - val_acc: 0.8847\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8870\n",
      "Epoch 00074: val_loss improved from 0.39830 to 0.38187, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/074-0.3819.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3657 - acc: 0.8870 - val_loss: 0.3819 - val_acc: 0.9024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8896\n",
      "Epoch 00075: val_loss improved from 0.38187 to 0.38159, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/075-0.3816.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3588 - acc: 0.8895 - val_loss: 0.3816 - val_acc: 0.8989\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8877\n",
      "Epoch 00076: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3621 - acc: 0.8877 - val_loss: 0.3964 - val_acc: 0.8968\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8895\n",
      "Epoch 00077: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3587 - acc: 0.8895 - val_loss: 0.4028 - val_acc: 0.8954\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8898\n",
      "Epoch 00078: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3548 - acc: 0.8898 - val_loss: 0.3887 - val_acc: 0.8956\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8890\n",
      "Epoch 00079: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3554 - acc: 0.8890 - val_loss: 0.3830 - val_acc: 0.9010\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8901\n",
      "Epoch 00080: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3560 - acc: 0.8901 - val_loss: 0.3830 - val_acc: 0.8996\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8902\n",
      "Epoch 00081: val_loss did not improve from 0.38159\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3523 - acc: 0.8902 - val_loss: 0.4165 - val_acc: 0.8905\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8911\n",
      "Epoch 00082: val_loss improved from 0.38159 to 0.37800, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/082-0.3780.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3500 - acc: 0.8911 - val_loss: 0.3780 - val_acc: 0.8989\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8942\n",
      "Epoch 00083: val_loss did not improve from 0.37800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3431 - acc: 0.8941 - val_loss: 0.3971 - val_acc: 0.8984\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8937\n",
      "Epoch 00084: val_loss did not improve from 0.37800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3424 - acc: 0.8937 - val_loss: 0.3978 - val_acc: 0.8963\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8941\n",
      "Epoch 00085: val_loss did not improve from 0.37800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3397 - acc: 0.8941 - val_loss: 0.3925 - val_acc: 0.8982\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8937\n",
      "Epoch 00086: val_loss did not improve from 0.37800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3367 - acc: 0.8937 - val_loss: 0.3855 - val_acc: 0.8959\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8951\n",
      "Epoch 00087: val_loss improved from 0.37800 to 0.37656, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/087-0.3766.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3370 - acc: 0.8951 - val_loss: 0.3766 - val_acc: 0.9010\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8949\n",
      "Epoch 00088: val_loss improved from 0.37656 to 0.36917, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/088-0.3692.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3356 - acc: 0.8949 - val_loss: 0.3692 - val_acc: 0.9031\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8976\n",
      "Epoch 00089: val_loss did not improve from 0.36917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3294 - acc: 0.8976 - val_loss: 0.3750 - val_acc: 0.8987\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8977\n",
      "Epoch 00090: val_loss improved from 0.36917 to 0.36577, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/090-0.3658.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3234 - acc: 0.8977 - val_loss: 0.3658 - val_acc: 0.9022\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8963\n",
      "Epoch 00091: val_loss did not improve from 0.36577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3271 - acc: 0.8963 - val_loss: 0.3800 - val_acc: 0.8994\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.8966\n",
      "Epoch 00092: val_loss did not improve from 0.36577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3303 - acc: 0.8966 - val_loss: 0.3795 - val_acc: 0.8991\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.9007\n",
      "Epoch 00093: val_loss did not improve from 0.36577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3231 - acc: 0.9007 - val_loss: 0.3845 - val_acc: 0.9005\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8968\n",
      "Epoch 00094: val_loss did not improve from 0.36577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3243 - acc: 0.8969 - val_loss: 0.3746 - val_acc: 0.9043\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8976\n",
      "Epoch 00095: val_loss did not improve from 0.36577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3203 - acc: 0.8976 - val_loss: 0.3707 - val_acc: 0.9059\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.8984\n",
      "Epoch 00096: val_loss improved from 0.36577 to 0.36540, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/096-0.3654.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3226 - acc: 0.8984 - val_loss: 0.3654 - val_acc: 0.9033\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8988\n",
      "Epoch 00097: val_loss did not improve from 0.36540\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3148 - acc: 0.8988 - val_loss: 0.3763 - val_acc: 0.9033\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8990\n",
      "Epoch 00098: val_loss did not improve from 0.36540\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3212 - acc: 0.8991 - val_loss: 0.3958 - val_acc: 0.8915\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9002\n",
      "Epoch 00099: val_loss improved from 0.36540 to 0.36216, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/099-0.3622.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3196 - acc: 0.9003 - val_loss: 0.3622 - val_acc: 0.9029\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9010\n",
      "Epoch 00100: val_loss did not improve from 0.36216\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3131 - acc: 0.9010 - val_loss: 0.3717 - val_acc: 0.9040\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9029\n",
      "Epoch 00101: val_loss did not improve from 0.36216\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3085 - acc: 0.9029 - val_loss: 0.3632 - val_acc: 0.9066\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9025\n",
      "Epoch 00102: val_loss did not improve from 0.36216\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3057 - acc: 0.9025 - val_loss: 0.3749 - val_acc: 0.9033\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9035\n",
      "Epoch 00103: val_loss did not improve from 0.36216\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3065 - acc: 0.9035 - val_loss: 0.3685 - val_acc: 0.9047\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9025\n",
      "Epoch 00104: val_loss improved from 0.36216 to 0.35792, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/104-0.3579.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3066 - acc: 0.9025 - val_loss: 0.3579 - val_acc: 0.9057\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9012\n",
      "Epoch 00105: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3115 - acc: 0.9011 - val_loss: 0.3658 - val_acc: 0.9071\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9037\n",
      "Epoch 00106: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3040 - acc: 0.9037 - val_loss: 0.3674 - val_acc: 0.9031\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9067\n",
      "Epoch 00107: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2983 - acc: 0.9066 - val_loss: 0.3639 - val_acc: 0.9068\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9039\n",
      "Epoch 00108: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3096 - acc: 0.9039 - val_loss: 0.3593 - val_acc: 0.9052\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9072\n",
      "Epoch 00109: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2930 - acc: 0.9072 - val_loss: 0.3628 - val_acc: 0.9047\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9039\n",
      "Epoch 00110: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3038 - acc: 0.9040 - val_loss: 0.3748 - val_acc: 0.9047\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9071\n",
      "Epoch 00111: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2949 - acc: 0.9071 - val_loss: 0.3792 - val_acc: 0.9040\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9077\n",
      "Epoch 00112: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2927 - acc: 0.9077 - val_loss: 0.3626 - val_acc: 0.9082\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9070\n",
      "Epoch 00113: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2915 - acc: 0.9070 - val_loss: 0.3617 - val_acc: 0.9094\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9095\n",
      "Epoch 00114: val_loss did not improve from 0.35792\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2888 - acc: 0.9096 - val_loss: 0.3596 - val_acc: 0.9075\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9106\n",
      "Epoch 00115: val_loss improved from 0.35792 to 0.35623, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/115-0.3562.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2843 - acc: 0.9106 - val_loss: 0.3562 - val_acc: 0.9096\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9095\n",
      "Epoch 00116: val_loss did not improve from 0.35623\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2881 - acc: 0.9094 - val_loss: 0.3667 - val_acc: 0.9075\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9092\n",
      "Epoch 00117: val_loss did not improve from 0.35623\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2869 - acc: 0.9092 - val_loss: 0.3756 - val_acc: 0.9066\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9080\n",
      "Epoch 00118: val_loss did not improve from 0.35623\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2878 - acc: 0.9080 - val_loss: 0.3705 - val_acc: 0.9082\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9108\n",
      "Epoch 00119: val_loss improved from 0.35623 to 0.34970, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/119-0.3497.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2848 - acc: 0.9108 - val_loss: 0.3497 - val_acc: 0.9108\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9085\n",
      "Epoch 00120: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2882 - acc: 0.9084 - val_loss: 0.3630 - val_acc: 0.9068\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9105\n",
      "Epoch 00121: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2825 - acc: 0.9105 - val_loss: 0.3497 - val_acc: 0.9101\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9105\n",
      "Epoch 00122: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2836 - acc: 0.9105 - val_loss: 0.3748 - val_acc: 0.9036\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9112\n",
      "Epoch 00123: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2780 - acc: 0.9112 - val_loss: 0.3519 - val_acc: 0.9110\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9110\n",
      "Epoch 00124: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2785 - acc: 0.9110 - val_loss: 0.3629 - val_acc: 0.9106\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9133\n",
      "Epoch 00125: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2713 - acc: 0.9132 - val_loss: 0.3678 - val_acc: 0.9085\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9109\n",
      "Epoch 00126: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2756 - acc: 0.9109 - val_loss: 0.3594 - val_acc: 0.9099\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9132\n",
      "Epoch 00127: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2723 - acc: 0.9131 - val_loss: 0.3662 - val_acc: 0.9129\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9112\n",
      "Epoch 00128: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2734 - acc: 0.9112 - val_loss: 0.3517 - val_acc: 0.9075\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9130\n",
      "Epoch 00129: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2720 - acc: 0.9130 - val_loss: 0.3710 - val_acc: 0.9029\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9140\n",
      "Epoch 00130: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2696 - acc: 0.9140 - val_loss: 0.3562 - val_acc: 0.9099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9124\n",
      "Epoch 00131: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2751 - acc: 0.9124 - val_loss: 0.3668 - val_acc: 0.9115\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9134\n",
      "Epoch 00132: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2705 - acc: 0.9134 - val_loss: 0.3576 - val_acc: 0.9096\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9138\n",
      "Epoch 00133: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2721 - acc: 0.9138 - val_loss: 0.3624 - val_acc: 0.9082\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9144\n",
      "Epoch 00134: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2690 - acc: 0.9144 - val_loss: 0.3658 - val_acc: 0.9043\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9144\n",
      "Epoch 00135: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2652 - acc: 0.9144 - val_loss: 0.3570 - val_acc: 0.9078\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9138\n",
      "Epoch 00136: val_loss did not improve from 0.34970\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2687 - acc: 0.9138 - val_loss: 0.3647 - val_acc: 0.9080\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9159\n",
      "Epoch 00137: val_loss improved from 0.34970 to 0.34866, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/137-0.3487.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2600 - acc: 0.9159 - val_loss: 0.3487 - val_acc: 0.9103\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9147\n",
      "Epoch 00138: val_loss did not improve from 0.34866\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2642 - acc: 0.9147 - val_loss: 0.3633 - val_acc: 0.9103\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9146\n",
      "Epoch 00139: val_loss did not improve from 0.34866\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2637 - acc: 0.9146 - val_loss: 0.3592 - val_acc: 0.9117\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9162\n",
      "Epoch 00140: val_loss improved from 0.34866 to 0.34566, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/140-0.3457.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2603 - acc: 0.9162 - val_loss: 0.3457 - val_acc: 0.9136\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9170\n",
      "Epoch 00141: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2574 - acc: 0.9170 - val_loss: 0.3564 - val_acc: 0.9126\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9167\n",
      "Epoch 00142: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2614 - acc: 0.9167 - val_loss: 0.3689 - val_acc: 0.9087\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9162\n",
      "Epoch 00143: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2591 - acc: 0.9163 - val_loss: 0.3487 - val_acc: 0.9145\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9170\n",
      "Epoch 00144: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2586 - acc: 0.9170 - val_loss: 0.3696 - val_acc: 0.9057\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9166\n",
      "Epoch 00145: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2545 - acc: 0.9166 - val_loss: 0.3517 - val_acc: 0.9143\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9169\n",
      "Epoch 00146: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2531 - acc: 0.9169 - val_loss: 0.3602 - val_acc: 0.9108\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9171\n",
      "Epoch 00147: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2582 - acc: 0.9172 - val_loss: 0.3663 - val_acc: 0.9110\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9199\n",
      "Epoch 00148: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2528 - acc: 0.9199 - val_loss: 0.3565 - val_acc: 0.9147\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9199\n",
      "Epoch 00149: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2545 - acc: 0.9199 - val_loss: 0.3460 - val_acc: 0.9171\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9206\n",
      "Epoch 00150: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2493 - acc: 0.9206 - val_loss: 0.3570 - val_acc: 0.9140\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9204\n",
      "Epoch 00151: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2466 - acc: 0.9203 - val_loss: 0.3480 - val_acc: 0.9147\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9200\n",
      "Epoch 00152: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2484 - acc: 0.9200 - val_loss: 0.3463 - val_acc: 0.9138\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9192\n",
      "Epoch 00153: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2510 - acc: 0.9192 - val_loss: 0.3602 - val_acc: 0.9140\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9186\n",
      "Epoch 00154: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2558 - acc: 0.9186 - val_loss: 0.3542 - val_acc: 0.9143\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9222\n",
      "Epoch 00155: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2416 - acc: 0.9222 - val_loss: 0.3568 - val_acc: 0.9122\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9201\n",
      "Epoch 00156: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2457 - acc: 0.9201 - val_loss: 0.3546 - val_acc: 0.9133\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9214\n",
      "Epoch 00157: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2459 - acc: 0.9214 - val_loss: 0.3559 - val_acc: 0.9140\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9198\n",
      "Epoch 00158: val_loss did not improve from 0.34566\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2462 - acc: 0.9197 - val_loss: 0.3746 - val_acc: 0.9133\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9206\n",
      "Epoch 00159: val_loss improved from 0.34566 to 0.34235, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/159-0.3424.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2426 - acc: 0.9206 - val_loss: 0.3424 - val_acc: 0.9131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9192\n",
      "Epoch 00160: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2469 - acc: 0.9191 - val_loss: 0.3515 - val_acc: 0.9108\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9237\n",
      "Epoch 00161: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2369 - acc: 0.9237 - val_loss: 0.3562 - val_acc: 0.9117\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9221\n",
      "Epoch 00162: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2400 - acc: 0.9220 - val_loss: 0.3577 - val_acc: 0.9159\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9203\n",
      "Epoch 00163: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2404 - acc: 0.9203 - val_loss: 0.3788 - val_acc: 0.9136\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9223\n",
      "Epoch 00164: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2382 - acc: 0.9223 - val_loss: 0.3564 - val_acc: 0.9122\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9208\n",
      "Epoch 00165: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2368 - acc: 0.9209 - val_loss: 0.3575 - val_acc: 0.9129\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9217\n",
      "Epoch 00166: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2393 - acc: 0.9217 - val_loss: 0.3474 - val_acc: 0.9122\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9227\n",
      "Epoch 00167: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2348 - acc: 0.9227 - val_loss: 0.3461 - val_acc: 0.9166\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9246\n",
      "Epoch 00168: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2372 - acc: 0.9246 - val_loss: 0.3521 - val_acc: 0.9175\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9241\n",
      "Epoch 00169: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2358 - acc: 0.9241 - val_loss: 0.3461 - val_acc: 0.9133\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9232\n",
      "Epoch 00170: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2371 - acc: 0.9232 - val_loss: 0.3633 - val_acc: 0.9138\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9220\n",
      "Epoch 00171: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2360 - acc: 0.9220 - val_loss: 0.3760 - val_acc: 0.9124\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9234\n",
      "Epoch 00172: val_loss did not improve from 0.34235\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2368 - acc: 0.9234 - val_loss: 0.3611 - val_acc: 0.9140\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9249\n",
      "Epoch 00173: val_loss improved from 0.34235 to 0.34111, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/173-0.3411.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2323 - acc: 0.9249 - val_loss: 0.3411 - val_acc: 0.9164\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9240\n",
      "Epoch 00174: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2339 - acc: 0.9241 - val_loss: 0.3562 - val_acc: 0.9101\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9245\n",
      "Epoch 00175: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2308 - acc: 0.9245 - val_loss: 0.3437 - val_acc: 0.9140\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9257\n",
      "Epoch 00176: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2286 - acc: 0.9257 - val_loss: 0.3637 - val_acc: 0.9099\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9264\n",
      "Epoch 00177: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2301 - acc: 0.9264 - val_loss: 0.3435 - val_acc: 0.9180\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9255\n",
      "Epoch 00178: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2265 - acc: 0.9255 - val_loss: 0.3545 - val_acc: 0.9136\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9246\n",
      "Epoch 00179: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2295 - acc: 0.9247 - val_loss: 0.3686 - val_acc: 0.9103\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9261\n",
      "Epoch 00180: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2263 - acc: 0.9261 - val_loss: 0.3619 - val_acc: 0.9119\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9241\n",
      "Epoch 00181: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2285 - acc: 0.9241 - val_loss: 0.3455 - val_acc: 0.9157\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9278\n",
      "Epoch 00182: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2227 - acc: 0.9278 - val_loss: 0.3559 - val_acc: 0.9150\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9276\n",
      "Epoch 00183: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2228 - acc: 0.9275 - val_loss: 0.3506 - val_acc: 0.9122\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9257\n",
      "Epoch 00184: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2281 - acc: 0.9257 - val_loss: 0.3552 - val_acc: 0.9152\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9289\n",
      "Epoch 00185: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2170 - acc: 0.9289 - val_loss: 0.3542 - val_acc: 0.9140\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9269\n",
      "Epoch 00186: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2211 - acc: 0.9269 - val_loss: 0.3653 - val_acc: 0.9178\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9275\n",
      "Epoch 00187: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2191 - acc: 0.9275 - val_loss: 0.3507 - val_acc: 0.9147\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9267\n",
      "Epoch 00188: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2246 - acc: 0.9267 - val_loss: 0.3464 - val_acc: 0.9161\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9277\n",
      "Epoch 00189: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2227 - acc: 0.9277 - val_loss: 0.3588 - val_acc: 0.9143\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9281\n",
      "Epoch 00190: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2194 - acc: 0.9281 - val_loss: 0.3765 - val_acc: 0.9126\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9287\n",
      "Epoch 00191: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2198 - acc: 0.9287 - val_loss: 0.3516 - val_acc: 0.9157\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9275\n",
      "Epoch 00192: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2200 - acc: 0.9275 - val_loss: 0.3578 - val_acc: 0.9159\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9289\n",
      "Epoch 00193: val_loss did not improve from 0.34111\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2164 - acc: 0.9289 - val_loss: 0.3459 - val_acc: 0.9175\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9299\n",
      "Epoch 00194: val_loss improved from 0.34111 to 0.34062, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/194-0.3406.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2169 - acc: 0.9299 - val_loss: 0.3406 - val_acc: 0.9187\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9286\n",
      "Epoch 00195: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2223 - acc: 0.9286 - val_loss: 0.3718 - val_acc: 0.9122\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9308\n",
      "Epoch 00196: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2128 - acc: 0.9308 - val_loss: 0.3598 - val_acc: 0.9124\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9305\n",
      "Epoch 00197: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2111 - acc: 0.9305 - val_loss: 0.3505 - val_acc: 0.9171\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9322\n",
      "Epoch 00198: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2076 - acc: 0.9322 - val_loss: 0.3528 - val_acc: 0.9173\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9294\n",
      "Epoch 00199: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2175 - acc: 0.9294 - val_loss: 0.3593 - val_acc: 0.9168\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9278\n",
      "Epoch 00200: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2178 - acc: 0.9278 - val_loss: 0.3480 - val_acc: 0.9175\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9289\n",
      "Epoch 00201: val_loss did not improve from 0.34062\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2150 - acc: 0.9289 - val_loss: 0.3465 - val_acc: 0.9175\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9301\n",
      "Epoch 00202: val_loss improved from 0.34062 to 0.33577, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_6_conv_checkpoint/202-0.3358.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2109 - acc: 0.9301 - val_loss: 0.3358 - val_acc: 0.9194\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9302\n",
      "Epoch 00203: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2104 - acc: 0.9302 - val_loss: 0.3412 - val_acc: 0.9154\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9311\n",
      "Epoch 00204: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2110 - acc: 0.9311 - val_loss: 0.3741 - val_acc: 0.9150\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9329\n",
      "Epoch 00205: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2057 - acc: 0.9329 - val_loss: 0.3774 - val_acc: 0.9136\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9282\n",
      "Epoch 00206: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2113 - acc: 0.9282 - val_loss: 0.3541 - val_acc: 0.9171\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9331\n",
      "Epoch 00207: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2042 - acc: 0.9331 - val_loss: 0.3697 - val_acc: 0.9133\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9319\n",
      "Epoch 00208: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2125 - acc: 0.9319 - val_loss: 0.3581 - val_acc: 0.9178\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9306\n",
      "Epoch 00209: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2082 - acc: 0.9306 - val_loss: 0.3693 - val_acc: 0.9173\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9330\n",
      "Epoch 00210: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2062 - acc: 0.9330 - val_loss: 0.3711 - val_acc: 0.9101\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9316\n",
      "Epoch 00211: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2057 - acc: 0.9316 - val_loss: 0.3514 - val_acc: 0.9164\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9335\n",
      "Epoch 00212: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2056 - acc: 0.9335 - val_loss: 0.3606 - val_acc: 0.9199\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9334\n",
      "Epoch 00213: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2042 - acc: 0.9334 - val_loss: 0.3519 - val_acc: 0.9143\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9332\n",
      "Epoch 00214: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2042 - acc: 0.9332 - val_loss: 0.3465 - val_acc: 0.9150\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9314\n",
      "Epoch 00215: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2055 - acc: 0.9314 - val_loss: 0.3725 - val_acc: 0.9154\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9333\n",
      "Epoch 00216: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2009 - acc: 0.9333 - val_loss: 0.3778 - val_acc: 0.9108\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9317\n",
      "Epoch 00217: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2071 - acc: 0.9317 - val_loss: 0.3490 - val_acc: 0.9175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9339\n",
      "Epoch 00218: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2004 - acc: 0.9339 - val_loss: 0.3858 - val_acc: 0.9152\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9346\n",
      "Epoch 00219: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2028 - acc: 0.9346 - val_loss: 0.3715 - val_acc: 0.9129\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9342\n",
      "Epoch 00220: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1989 - acc: 0.9342 - val_loss: 0.3541 - val_acc: 0.9175\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9330\n",
      "Epoch 00221: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2002 - acc: 0.9330 - val_loss: 0.3778 - val_acc: 0.9129\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9342\n",
      "Epoch 00222: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1963 - acc: 0.9342 - val_loss: 0.3663 - val_acc: 0.9154\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9348\n",
      "Epoch 00223: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1967 - acc: 0.9348 - val_loss: 0.3606 - val_acc: 0.9185\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9346\n",
      "Epoch 00224: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1980 - acc: 0.9346 - val_loss: 0.3462 - val_acc: 0.9192\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9334\n",
      "Epoch 00225: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2000 - acc: 0.9334 - val_loss: 0.3462 - val_acc: 0.9168\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9345\n",
      "Epoch 00226: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1957 - acc: 0.9345 - val_loss: 0.3560 - val_acc: 0.9199\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9317\n",
      "Epoch 00227: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2005 - acc: 0.9317 - val_loss: 0.3672 - val_acc: 0.9175\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9360\n",
      "Epoch 00228: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1950 - acc: 0.9360 - val_loss: 0.3557 - val_acc: 0.9180\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9351\n",
      "Epoch 00229: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1936 - acc: 0.9351 - val_loss: 0.3562 - val_acc: 0.9180\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9365\n",
      "Epoch 00230: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1922 - acc: 0.9365 - val_loss: 0.3548 - val_acc: 0.9196\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9386\n",
      "Epoch 00231: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1918 - acc: 0.9386 - val_loss: 0.3538 - val_acc: 0.9201\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9352\n",
      "Epoch 00232: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1932 - acc: 0.9352 - val_loss: 0.3595 - val_acc: 0.9171\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9359\n",
      "Epoch 00233: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1921 - acc: 0.9359 - val_loss: 0.3405 - val_acc: 0.9171\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9338\n",
      "Epoch 00234: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1955 - acc: 0.9338 - val_loss: 0.3460 - val_acc: 0.9152\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9374\n",
      "Epoch 00235: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1909 - acc: 0.9374 - val_loss: 0.3630 - val_acc: 0.9203\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9364\n",
      "Epoch 00236: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1933 - acc: 0.9364 - val_loss: 0.3653 - val_acc: 0.9192\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9369\n",
      "Epoch 00237: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1922 - acc: 0.9369 - val_loss: 0.3594 - val_acc: 0.9168\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9393\n",
      "Epoch 00238: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1866 - acc: 0.9392 - val_loss: 0.3709 - val_acc: 0.9143\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9370\n",
      "Epoch 00239: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1876 - acc: 0.9370 - val_loss: 0.3555 - val_acc: 0.9210\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9384\n",
      "Epoch 00240: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1874 - acc: 0.9384 - val_loss: 0.3591 - val_acc: 0.9150\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9373\n",
      "Epoch 00241: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1892 - acc: 0.9372 - val_loss: 0.3398 - val_acc: 0.9222\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9382\n",
      "Epoch 00242: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1888 - acc: 0.9382 - val_loss: 0.3585 - val_acc: 0.9189\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9371\n",
      "Epoch 00243: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1894 - acc: 0.9371 - val_loss: 0.3709 - val_acc: 0.9173\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9376\n",
      "Epoch 00244: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1874 - acc: 0.9376 - val_loss: 0.3638 - val_acc: 0.9178\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9386\n",
      "Epoch 00245: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1858 - acc: 0.9386 - val_loss: 0.3473 - val_acc: 0.9201\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9379\n",
      "Epoch 00246: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1854 - acc: 0.9379 - val_loss: 0.3669 - val_acc: 0.9192\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9376\n",
      "Epoch 00247: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1873 - acc: 0.9376 - val_loss: 0.3670 - val_acc: 0.9182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9379\n",
      "Epoch 00248: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1878 - acc: 0.9379 - val_loss: 0.3622 - val_acc: 0.9201\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9385\n",
      "Epoch 00249: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1850 - acc: 0.9385 - val_loss: 0.3496 - val_acc: 0.9192\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9363\n",
      "Epoch 00250: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1898 - acc: 0.9363 - val_loss: 0.3416 - val_acc: 0.9201\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9389\n",
      "Epoch 00251: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1834 - acc: 0.9389 - val_loss: 0.3568 - val_acc: 0.9196\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9392\n",
      "Epoch 00252: val_loss did not improve from 0.33577\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1838 - acc: 0.9391 - val_loss: 0.3796 - val_acc: 0.9171\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 602us/sample - loss: 0.4032 - acc: 0.8933\n",
      "Loss: 0.40324922873099894 Accuracy: 0.8932503\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6266 - acc: 0.1268\n",
      "Epoch 00001: val_loss improved from inf to 2.26001, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/001-2.2600.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.6265 - acc: 0.1269 - val_loss: 2.2600 - val_acc: 0.2758\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2006 - acc: 0.2656\n",
      "Epoch 00002: val_loss improved from 2.26001 to 1.85434, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/002-1.8543.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.2006 - acc: 0.2656 - val_loss: 1.8543 - val_acc: 0.4239\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9341 - acc: 0.3518\n",
      "Epoch 00003: val_loss improved from 1.85434 to 1.68637, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/003-1.6864.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.9340 - acc: 0.3519 - val_loss: 1.6864 - val_acc: 0.4638\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7715 - acc: 0.4035\n",
      "Epoch 00004: val_loss improved from 1.68637 to 1.46970, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/004-1.4697.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.7716 - acc: 0.4035 - val_loss: 1.4697 - val_acc: 0.5332\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6599 - acc: 0.4394\n",
      "Epoch 00005: val_loss improved from 1.46970 to 1.37138, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/005-1.3714.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.6599 - acc: 0.4394 - val_loss: 1.3714 - val_acc: 0.5556\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5754 - acc: 0.4708\n",
      "Epoch 00006: val_loss improved from 1.37138 to 1.30584, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/006-1.3058.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5754 - acc: 0.4708 - val_loss: 1.3058 - val_acc: 0.5898\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4998 - acc: 0.4976\n",
      "Epoch 00007: val_loss improved from 1.30584 to 1.24895, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/007-1.2490.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4998 - acc: 0.4976 - val_loss: 1.2490 - val_acc: 0.6119\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4366 - acc: 0.5233\n",
      "Epoch 00008: val_loss improved from 1.24895 to 1.17784, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/008-1.1778.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4365 - acc: 0.5233 - val_loss: 1.1778 - val_acc: 0.6415\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3750 - acc: 0.5480\n",
      "Epoch 00009: val_loss improved from 1.17784 to 1.14090, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/009-1.1409.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3749 - acc: 0.5481 - val_loss: 1.1409 - val_acc: 0.6424\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3262 - acc: 0.5700\n",
      "Epoch 00010: val_loss improved from 1.14090 to 1.08387, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/010-1.0839.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3262 - acc: 0.5699 - val_loss: 1.0839 - val_acc: 0.6664\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2718 - acc: 0.5901\n",
      "Epoch 00011: val_loss improved from 1.08387 to 1.03542, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/011-1.0354.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2717 - acc: 0.5901 - val_loss: 1.0354 - val_acc: 0.6874\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2172 - acc: 0.6087\n",
      "Epoch 00012: val_loss improved from 1.03542 to 0.97520, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/012-0.9752.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2172 - acc: 0.6087 - val_loss: 0.9752 - val_acc: 0.7098\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1775 - acc: 0.6268\n",
      "Epoch 00013: val_loss improved from 0.97520 to 0.94351, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/013-0.9435.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1775 - acc: 0.6268 - val_loss: 0.9435 - val_acc: 0.7230\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1379 - acc: 0.6399\n",
      "Epoch 00014: val_loss improved from 0.94351 to 0.94251, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/014-0.9425.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1380 - acc: 0.6400 - val_loss: 0.9425 - val_acc: 0.7249\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1042 - acc: 0.6516\n",
      "Epoch 00015: val_loss improved from 0.94251 to 0.87574, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/015-0.8757.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1042 - acc: 0.6516 - val_loss: 0.8757 - val_acc: 0.7456\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0743 - acc: 0.6618\n",
      "Epoch 00016: val_loss improved from 0.87574 to 0.84066, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/016-0.8407.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0743 - acc: 0.6618 - val_loss: 0.8407 - val_acc: 0.7489\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0472 - acc: 0.6709\n",
      "Epoch 00017: val_loss did not improve from 0.84066\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0471 - acc: 0.6709 - val_loss: 0.8611 - val_acc: 0.7475\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0205 - acc: 0.6798\n",
      "Epoch 00018: val_loss improved from 0.84066 to 0.80802, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/018-0.8080.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0205 - acc: 0.6798 - val_loss: 0.8080 - val_acc: 0.7633\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9905 - acc: 0.6926\n",
      "Epoch 00019: val_loss improved from 0.80802 to 0.79507, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/019-0.7951.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9906 - acc: 0.6926 - val_loss: 0.7951 - val_acc: 0.7768\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9694 - acc: 0.6994\n",
      "Epoch 00020: val_loss improved from 0.79507 to 0.76809, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/020-0.7681.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9694 - acc: 0.6994 - val_loss: 0.7681 - val_acc: 0.7841\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9418 - acc: 0.7068\n",
      "Epoch 00021: val_loss did not improve from 0.76809\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9417 - acc: 0.7068 - val_loss: 0.7740 - val_acc: 0.7706\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9282 - acc: 0.7140\n",
      "Epoch 00022: val_loss improved from 0.76809 to 0.72217, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/022-0.7222.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9281 - acc: 0.7140 - val_loss: 0.7222 - val_acc: 0.7932\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7210\n",
      "Epoch 00023: val_loss did not improve from 0.72217\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9033 - acc: 0.7210 - val_loss: 0.7251 - val_acc: 0.7952\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8883 - acc: 0.7254\n",
      "Epoch 00024: val_loss improved from 0.72217 to 0.65273, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/024-0.6527.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8883 - acc: 0.7254 - val_loss: 0.6527 - val_acc: 0.8088\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8716 - acc: 0.7319\n",
      "Epoch 00025: val_loss did not improve from 0.65273\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8716 - acc: 0.7319 - val_loss: 0.6565 - val_acc: 0.8120\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8443 - acc: 0.7404\n",
      "Epoch 00026: val_loss did not improve from 0.65273\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8444 - acc: 0.7404 - val_loss: 0.6865 - val_acc: 0.7918\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.7464\n",
      "Epoch 00027: val_loss improved from 0.65273 to 0.60380, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/027-0.6038.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8310 - acc: 0.7464 - val_loss: 0.6038 - val_acc: 0.8288\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8265 - acc: 0.7460\n",
      "Epoch 00028: val_loss did not improve from 0.60380\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8266 - acc: 0.7459 - val_loss: 0.6514 - val_acc: 0.8125\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7487\n",
      "Epoch 00029: val_loss improved from 0.60380 to 0.57684, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/029-0.5768.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8145 - acc: 0.7488 - val_loss: 0.5768 - val_acc: 0.8318\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7905 - acc: 0.7602\n",
      "Epoch 00030: val_loss improved from 0.57684 to 0.57362, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/030-0.5736.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7905 - acc: 0.7602 - val_loss: 0.5736 - val_acc: 0.8358\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.7612\n",
      "Epoch 00031: val_loss improved from 0.57362 to 0.55392, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/031-0.5539.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7737 - acc: 0.7611 - val_loss: 0.5539 - val_acc: 0.8418\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7658\n",
      "Epoch 00032: val_loss improved from 0.55392 to 0.54945, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/032-0.5494.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7659 - acc: 0.7658 - val_loss: 0.5494 - val_acc: 0.8458\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7547 - acc: 0.7709\n",
      "Epoch 00033: val_loss did not improve from 0.54945\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7548 - acc: 0.7709 - val_loss: 0.5707 - val_acc: 0.8323\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7357 - acc: 0.7752\n",
      "Epoch 00034: val_loss improved from 0.54945 to 0.53505, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/034-0.5351.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7357 - acc: 0.7752 - val_loss: 0.5351 - val_acc: 0.8458\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7305 - acc: 0.7755\n",
      "Epoch 00035: val_loss did not improve from 0.53505\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7305 - acc: 0.7755 - val_loss: 0.5640 - val_acc: 0.8297\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7813\n",
      "Epoch 00036: val_loss improved from 0.53505 to 0.50821, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/036-0.5082.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7216 - acc: 0.7813 - val_loss: 0.5082 - val_acc: 0.8502\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7198 - acc: 0.7817\n",
      "Epoch 00037: val_loss did not improve from 0.50821\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7198 - acc: 0.7817 - val_loss: 0.5613 - val_acc: 0.8393\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7094 - acc: 0.7821\n",
      "Epoch 00038: val_loss did not improve from 0.50821\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7093 - acc: 0.7822 - val_loss: 0.5298 - val_acc: 0.8460\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.7879\n",
      "Epoch 00039: val_loss improved from 0.50821 to 0.48988, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/039-0.4899.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6936 - acc: 0.7879 - val_loss: 0.4899 - val_acc: 0.8581\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.7868\n",
      "Epoch 00040: val_loss did not improve from 0.48988\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6931 - acc: 0.7867 - val_loss: 0.4899 - val_acc: 0.8619\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6771 - acc: 0.7936\n",
      "Epoch 00041: val_loss did not improve from 0.48988\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6771 - acc: 0.7936 - val_loss: 0.5241 - val_acc: 0.8493\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7980\n",
      "Epoch 00042: val_loss improved from 0.48988 to 0.48351, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/042-0.4835.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6659 - acc: 0.7980 - val_loss: 0.4835 - val_acc: 0.8542\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.7981\n",
      "Epoch 00043: val_loss improved from 0.48351 to 0.45199, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/043-0.4520.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6696 - acc: 0.7981 - val_loss: 0.4520 - val_acc: 0.8714\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6593 - acc: 0.7985\n",
      "Epoch 00044: val_loss did not improve from 0.45199\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6592 - acc: 0.7985 - val_loss: 0.4584 - val_acc: 0.8656\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.8014\n",
      "Epoch 00045: val_loss improved from 0.45199 to 0.44225, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/045-0.4422.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6463 - acc: 0.8014 - val_loss: 0.4422 - val_acc: 0.8726\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.8059\n",
      "Epoch 00046: val_loss did not improve from 0.44225\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6457 - acc: 0.8059 - val_loss: 0.4622 - val_acc: 0.8703\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.8056\n",
      "Epoch 00047: val_loss improved from 0.44225 to 0.42838, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/047-0.4284.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6368 - acc: 0.8055 - val_loss: 0.4284 - val_acc: 0.8800\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6331 - acc: 0.8087\n",
      "Epoch 00048: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6335 - acc: 0.8087 - val_loss: 0.4663 - val_acc: 0.8682\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.8086\n",
      "Epoch 00049: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6332 - acc: 0.8086 - val_loss: 0.4443 - val_acc: 0.8707\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8091\n",
      "Epoch 00050: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6204 - acc: 0.8091 - val_loss: 0.4297 - val_acc: 0.8768\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.8102\n",
      "Epoch 00051: val_loss did not improve from 0.42838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6243 - acc: 0.8102 - val_loss: 0.4369 - val_acc: 0.8742\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8123\n",
      "Epoch 00052: val_loss improved from 0.42838 to 0.41416, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/052-0.4142.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6164 - acc: 0.8124 - val_loss: 0.4142 - val_acc: 0.8800\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6077 - acc: 0.8139\n",
      "Epoch 00053: val_loss did not improve from 0.41416\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6077 - acc: 0.8139 - val_loss: 0.4655 - val_acc: 0.8689\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8174\n",
      "Epoch 00054: val_loss improved from 0.41416 to 0.40360, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/054-0.4036.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5967 - acc: 0.8174 - val_loss: 0.4036 - val_acc: 0.8826\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8173\n",
      "Epoch 00055: val_loss did not improve from 0.40360\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5953 - acc: 0.8173 - val_loss: 0.4061 - val_acc: 0.8868\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5896 - acc: 0.8200\n",
      "Epoch 00056: val_loss did not improve from 0.40360\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5896 - acc: 0.8200 - val_loss: 0.4211 - val_acc: 0.8791\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8212\n",
      "Epoch 00057: val_loss did not improve from 0.40360\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5844 - acc: 0.8212 - val_loss: 0.4068 - val_acc: 0.8845\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8213\n",
      "Epoch 00058: val_loss improved from 0.40360 to 0.39338, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/058-0.3934.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5842 - acc: 0.8213 - val_loss: 0.3934 - val_acc: 0.8942\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.8217\n",
      "Epoch 00059: val_loss did not improve from 0.39338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5798 - acc: 0.8217 - val_loss: 0.4259 - val_acc: 0.8810\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8240\n",
      "Epoch 00060: val_loss improved from 0.39338 to 0.38862, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/060-0.3886.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5744 - acc: 0.8240 - val_loss: 0.3886 - val_acc: 0.8947\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8259\n",
      "Epoch 00061: val_loss improved from 0.38862 to 0.38779, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/061-0.3878.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5680 - acc: 0.8258 - val_loss: 0.3878 - val_acc: 0.8919\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.8265\n",
      "Epoch 00062: val_loss improved from 0.38779 to 0.37536, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/062-0.3754.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5673 - acc: 0.8265 - val_loss: 0.3754 - val_acc: 0.8935\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8280\n",
      "Epoch 00063: val_loss improved from 0.37536 to 0.37499, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/063-0.3750.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5588 - acc: 0.8280 - val_loss: 0.3750 - val_acc: 0.8949\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8279\n",
      "Epoch 00064: val_loss improved from 0.37499 to 0.37239, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/064-0.3724.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5617 - acc: 0.8280 - val_loss: 0.3724 - val_acc: 0.8961\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.8301\n",
      "Epoch 00065: val_loss did not improve from 0.37239\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5515 - acc: 0.8301 - val_loss: 0.3749 - val_acc: 0.8928\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8320\n",
      "Epoch 00066: val_loss improved from 0.37239 to 0.36179, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/066-0.3618.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5472 - acc: 0.8320 - val_loss: 0.3618 - val_acc: 0.8961\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8301\n",
      "Epoch 00067: val_loss did not improve from 0.36179\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5480 - acc: 0.8301 - val_loss: 0.3990 - val_acc: 0.8849\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8330\n",
      "Epoch 00068: val_loss did not improve from 0.36179\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5429 - acc: 0.8330 - val_loss: 0.3693 - val_acc: 0.8970\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.8343\n",
      "Epoch 00069: val_loss did not improve from 0.36179\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5486 - acc: 0.8343 - val_loss: 0.3764 - val_acc: 0.8924\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8371\n",
      "Epoch 00070: val_loss did not improve from 0.36179\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5353 - acc: 0.8372 - val_loss: 0.3645 - val_acc: 0.8961\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8363\n",
      "Epoch 00071: val_loss improved from 0.36179 to 0.34763, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/071-0.3476.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5308 - acc: 0.8363 - val_loss: 0.3476 - val_acc: 0.8991\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8386\n",
      "Epoch 00072: val_loss did not improve from 0.34763\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5322 - acc: 0.8386 - val_loss: 0.3678 - val_acc: 0.8954\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.8370\n",
      "Epoch 00073: val_loss did not improve from 0.34763\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5264 - acc: 0.8370 - val_loss: 0.3525 - val_acc: 0.9015\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8391\n",
      "Epoch 00074: val_loss improved from 0.34763 to 0.34555, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/074-0.3456.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5259 - acc: 0.8391 - val_loss: 0.3456 - val_acc: 0.8984\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8389\n",
      "Epoch 00075: val_loss did not improve from 0.34555\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5271 - acc: 0.8390 - val_loss: 0.3482 - val_acc: 0.9008\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8401\n",
      "Epoch 00076: val_loss did not improve from 0.34555\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5203 - acc: 0.8401 - val_loss: 0.3480 - val_acc: 0.9066\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8418\n",
      "Epoch 00077: val_loss improved from 0.34555 to 0.33630, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/077-0.3363.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5138 - acc: 0.8418 - val_loss: 0.3363 - val_acc: 0.9040\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8420\n",
      "Epoch 00078: val_loss did not improve from 0.33630\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5133 - acc: 0.8420 - val_loss: 0.3465 - val_acc: 0.9033\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8442\n",
      "Epoch 00079: val_loss did not improve from 0.33630\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5062 - acc: 0.8442 - val_loss: 0.3538 - val_acc: 0.9005\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8439\n",
      "Epoch 00080: val_loss improved from 0.33630 to 0.33622, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/080-0.3362.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5090 - acc: 0.8439 - val_loss: 0.3362 - val_acc: 0.9087\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8453\n",
      "Epoch 00081: val_loss improved from 0.33622 to 0.32697, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/081-0.3270.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5031 - acc: 0.8453 - val_loss: 0.3270 - val_acc: 0.9054\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.8463\n",
      "Epoch 00082: val_loss did not improve from 0.32697\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5003 - acc: 0.8464 - val_loss: 0.3294 - val_acc: 0.9054\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8447\n",
      "Epoch 00083: val_loss did not improve from 0.32697\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5014 - acc: 0.8447 - val_loss: 0.3353 - val_acc: 0.9075\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8488\n",
      "Epoch 00084: val_loss did not improve from 0.32697\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4926 - acc: 0.8488 - val_loss: 0.3374 - val_acc: 0.9096\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8488\n",
      "Epoch 00085: val_loss did not improve from 0.32697\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4936 - acc: 0.8487 - val_loss: 0.3412 - val_acc: 0.9054\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.8466\n",
      "Epoch 00086: val_loss did not improve from 0.32697\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4944 - acc: 0.8466 - val_loss: 0.3593 - val_acc: 0.9005\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8509\n",
      "Epoch 00087: val_loss improved from 0.32697 to 0.31871, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/087-0.3187.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4849 - acc: 0.8509 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8494\n",
      "Epoch 00088: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4854 - acc: 0.8494 - val_loss: 0.3296 - val_acc: 0.9124\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8487\n",
      "Epoch 00089: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4876 - acc: 0.8487 - val_loss: 0.3459 - val_acc: 0.9038\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8537\n",
      "Epoch 00090: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4737 - acc: 0.8537 - val_loss: 0.3255 - val_acc: 0.9092\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8504\n",
      "Epoch 00091: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4839 - acc: 0.8505 - val_loss: 0.3215 - val_acc: 0.9094\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8523\n",
      "Epoch 00092: val_loss did not improve from 0.31871\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4774 - acc: 0.8523 - val_loss: 0.3189 - val_acc: 0.9071\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.8524\n",
      "Epoch 00093: val_loss improved from 0.31871 to 0.31615, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/093-0.3162.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4755 - acc: 0.8524 - val_loss: 0.3162 - val_acc: 0.9106\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8558\n",
      "Epoch 00094: val_loss did not improve from 0.31615\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4734 - acc: 0.8558 - val_loss: 0.3219 - val_acc: 0.9110\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8552\n",
      "Epoch 00095: val_loss did not improve from 0.31615\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4714 - acc: 0.8552 - val_loss: 0.3263 - val_acc: 0.9089\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8567\n",
      "Epoch 00096: val_loss improved from 0.31615 to 0.30592, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/096-0.3059.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4663 - acc: 0.8566 - val_loss: 0.3059 - val_acc: 0.9115\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.8560\n",
      "Epoch 00097: val_loss did not improve from 0.30592\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4645 - acc: 0.8559 - val_loss: 0.3221 - val_acc: 0.9071\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4603 - acc: 0.8595\n",
      "Epoch 00098: val_loss did not improve from 0.30592\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4603 - acc: 0.8595 - val_loss: 0.3137 - val_acc: 0.9140\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8570\n",
      "Epoch 00099: val_loss improved from 0.30592 to 0.30243, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/099-0.3024.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4596 - acc: 0.8570 - val_loss: 0.3024 - val_acc: 0.9166\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8557\n",
      "Epoch 00100: val_loss did not improve from 0.30243\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4626 - acc: 0.8556 - val_loss: 0.3076 - val_acc: 0.9138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8576\n",
      "Epoch 00101: val_loss did not improve from 0.30243\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4580 - acc: 0.8576 - val_loss: 0.3372 - val_acc: 0.9050\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8588\n",
      "Epoch 00102: val_loss did not improve from 0.30243\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4585 - acc: 0.8589 - val_loss: 0.3090 - val_acc: 0.9115\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8611\n",
      "Epoch 00103: val_loss did not improve from 0.30243\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4508 - acc: 0.8612 - val_loss: 0.3127 - val_acc: 0.9171\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.8594\n",
      "Epoch 00104: val_loss did not improve from 0.30243\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4561 - acc: 0.8594 - val_loss: 0.3053 - val_acc: 0.9164\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8615\n",
      "Epoch 00105: val_loss improved from 0.30243 to 0.29363, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/105-0.2936.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4479 - acc: 0.8615 - val_loss: 0.2936 - val_acc: 0.9227\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8614\n",
      "Epoch 00106: val_loss did not improve from 0.29363\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4457 - acc: 0.8614 - val_loss: 0.3089 - val_acc: 0.9108\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8594\n",
      "Epoch 00107: val_loss did not improve from 0.29363\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4508 - acc: 0.8594 - val_loss: 0.3145 - val_acc: 0.9140\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8614\n",
      "Epoch 00108: val_loss did not improve from 0.29363\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4442 - acc: 0.8614 - val_loss: 0.2992 - val_acc: 0.9171\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8624\n",
      "Epoch 00109: val_loss improved from 0.29363 to 0.29167, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/109-0.2917.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4397 - acc: 0.8624 - val_loss: 0.2917 - val_acc: 0.9171\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8639\n",
      "Epoch 00110: val_loss improved from 0.29167 to 0.29056, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/110-0.2906.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4419 - acc: 0.8639 - val_loss: 0.2906 - val_acc: 0.9192\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8634\n",
      "Epoch 00111: val_loss improved from 0.29056 to 0.28739, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/111-0.2874.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4405 - acc: 0.8634 - val_loss: 0.2874 - val_acc: 0.9189\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8660\n",
      "Epoch 00112: val_loss did not improve from 0.28739\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4370 - acc: 0.8660 - val_loss: 0.2888 - val_acc: 0.9220\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8620\n",
      "Epoch 00113: val_loss did not improve from 0.28739\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4369 - acc: 0.8620 - val_loss: 0.3023 - val_acc: 0.9213\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8651\n",
      "Epoch 00114: val_loss improved from 0.28739 to 0.28534, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/114-0.2853.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4339 - acc: 0.8651 - val_loss: 0.2853 - val_acc: 0.9250\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8666\n",
      "Epoch 00115: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4335 - acc: 0.8665 - val_loss: 0.2864 - val_acc: 0.9210\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8685\n",
      "Epoch 00116: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4270 - acc: 0.8685 - val_loss: 0.2894 - val_acc: 0.9210\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8661\n",
      "Epoch 00117: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4330 - acc: 0.8661 - val_loss: 0.2935 - val_acc: 0.9152\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.8651\n",
      "Epoch 00118: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4326 - acc: 0.8651 - val_loss: 0.2866 - val_acc: 0.9231\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8687\n",
      "Epoch 00119: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4243 - acc: 0.8687 - val_loss: 0.2917 - val_acc: 0.9199\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8685\n",
      "Epoch 00120: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4187 - acc: 0.8685 - val_loss: 0.2856 - val_acc: 0.9180\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8687\n",
      "Epoch 00121: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4271 - acc: 0.8687 - val_loss: 0.2931 - val_acc: 0.9178\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8712\n",
      "Epoch 00122: val_loss did not improve from 0.28534\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4166 - acc: 0.8712 - val_loss: 0.2873 - val_acc: 0.9194\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8721\n",
      "Epoch 00123: val_loss improved from 0.28534 to 0.28346, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/123-0.2835.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4168 - acc: 0.8721 - val_loss: 0.2835 - val_acc: 0.9227\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8702\n",
      "Epoch 00124: val_loss did not improve from 0.28346\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4216 - acc: 0.8702 - val_loss: 0.2926 - val_acc: 0.9154\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8699\n",
      "Epoch 00125: val_loss improved from 0.28346 to 0.27894, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/125-0.2789.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4169 - acc: 0.8699 - val_loss: 0.2789 - val_acc: 0.9231\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8714\n",
      "Epoch 00126: val_loss improved from 0.27894 to 0.27257, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/126-0.2726.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4118 - acc: 0.8714 - val_loss: 0.2726 - val_acc: 0.9231\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8698\n",
      "Epoch 00127: val_loss did not improve from 0.27257\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4084 - acc: 0.8698 - val_loss: 0.2945 - val_acc: 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8718\n",
      "Epoch 00128: val_loss improved from 0.27257 to 0.27075, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/128-0.2707.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4136 - acc: 0.8718 - val_loss: 0.2707 - val_acc: 0.9250\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8707\n",
      "Epoch 00129: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4130 - acc: 0.8707 - val_loss: 0.2823 - val_acc: 0.9217\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8729\n",
      "Epoch 00130: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4052 - acc: 0.8729 - val_loss: 0.2799 - val_acc: 0.9241\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8752\n",
      "Epoch 00131: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4011 - acc: 0.8752 - val_loss: 0.2717 - val_acc: 0.9229\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8740\n",
      "Epoch 00132: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4036 - acc: 0.8740 - val_loss: 0.2787 - val_acc: 0.9255\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8730\n",
      "Epoch 00133: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4064 - acc: 0.8730 - val_loss: 0.2738 - val_acc: 0.9245\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8750\n",
      "Epoch 00134: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4039 - acc: 0.8750 - val_loss: 0.2793 - val_acc: 0.9266\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8765\n",
      "Epoch 00135: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3960 - acc: 0.8765 - val_loss: 0.2734 - val_acc: 0.9266\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8747\n",
      "Epoch 00136: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4002 - acc: 0.8747 - val_loss: 0.2735 - val_acc: 0.9231\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8765\n",
      "Epoch 00137: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3966 - acc: 0.8765 - val_loss: 0.2774 - val_acc: 0.9252\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8751\n",
      "Epoch 00138: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3986 - acc: 0.8751 - val_loss: 0.2732 - val_acc: 0.9245\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8762\n",
      "Epoch 00139: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3962 - acc: 0.8762 - val_loss: 0.2743 - val_acc: 0.9245\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8775\n",
      "Epoch 00140: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3909 - acc: 0.8775 - val_loss: 0.2725 - val_acc: 0.9241\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8778\n",
      "Epoch 00141: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3946 - acc: 0.8778 - val_loss: 0.2716 - val_acc: 0.9285\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8781\n",
      "Epoch 00142: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3859 - acc: 0.8781 - val_loss: 0.2732 - val_acc: 0.9264\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8783\n",
      "Epoch 00143: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3907 - acc: 0.8784 - val_loss: 0.2744 - val_acc: 0.9276\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8777\n",
      "Epoch 00144: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3893 - acc: 0.8777 - val_loss: 0.2819 - val_acc: 0.9241\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8797\n",
      "Epoch 00145: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3821 - acc: 0.8797 - val_loss: 0.2849 - val_acc: 0.9229\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8789\n",
      "Epoch 00146: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3844 - acc: 0.8789 - val_loss: 0.2710 - val_acc: 0.9259\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8785\n",
      "Epoch 00147: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3864 - acc: 0.8785 - val_loss: 0.2877 - val_acc: 0.9245\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8800\n",
      "Epoch 00148: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3813 - acc: 0.8800 - val_loss: 0.2781 - val_acc: 0.9234\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8775\n",
      "Epoch 00149: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3859 - acc: 0.8775 - val_loss: 0.2850 - val_acc: 0.9229\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8808\n",
      "Epoch 00150: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3824 - acc: 0.8807 - val_loss: 0.2708 - val_acc: 0.9259\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8770\n",
      "Epoch 00151: val_loss improved from 0.27075 to 0.27071, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/151-0.2707.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3871 - acc: 0.8770 - val_loss: 0.2707 - val_acc: 0.9222\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8820\n",
      "Epoch 00152: val_loss improved from 0.27071 to 0.26367, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/152-0.2637.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3775 - acc: 0.8820 - val_loss: 0.2637 - val_acc: 0.9304\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8818\n",
      "Epoch 00153: val_loss improved from 0.26367 to 0.26088, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/153-0.2609.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3743 - acc: 0.8817 - val_loss: 0.2609 - val_acc: 0.9292\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8819\n",
      "Epoch 00154: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3757 - acc: 0.8819 - val_loss: 0.2728 - val_acc: 0.9280\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8790\n",
      "Epoch 00155: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3840 - acc: 0.8790 - val_loss: 0.2766 - val_acc: 0.9257\n",
      "Epoch 156/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8819\n",
      "Epoch 00156: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3746 - acc: 0.8819 - val_loss: 0.2620 - val_acc: 0.9311\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8836\n",
      "Epoch 00157: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3674 - acc: 0.8836 - val_loss: 0.2625 - val_acc: 0.9320\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8854\n",
      "Epoch 00158: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3654 - acc: 0.8854 - val_loss: 0.2650 - val_acc: 0.9294\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8842\n",
      "Epoch 00159: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3729 - acc: 0.8842 - val_loss: 0.2610 - val_acc: 0.9317\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8846\n",
      "Epoch 00160: val_loss did not improve from 0.26088\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3696 - acc: 0.8846 - val_loss: 0.2616 - val_acc: 0.9259\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8859\n",
      "Epoch 00161: val_loss improved from 0.26088 to 0.25800, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/161-0.2580.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3645 - acc: 0.8859 - val_loss: 0.2580 - val_acc: 0.9280\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8858\n",
      "Epoch 00162: val_loss did not improve from 0.25800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3615 - acc: 0.8858 - val_loss: 0.2692 - val_acc: 0.9290\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8842\n",
      "Epoch 00163: val_loss did not improve from 0.25800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3659 - acc: 0.8842 - val_loss: 0.2609 - val_acc: 0.9327\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8869\n",
      "Epoch 00164: val_loss improved from 0.25800 to 0.25724, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/164-0.2572.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3671 - acc: 0.8869 - val_loss: 0.2572 - val_acc: 0.9299\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8877\n",
      "Epoch 00165: val_loss did not improve from 0.25724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3558 - acc: 0.8877 - val_loss: 0.2713 - val_acc: 0.9285\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8877\n",
      "Epoch 00166: val_loss improved from 0.25724 to 0.25482, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/166-0.2548.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3601 - acc: 0.8877 - val_loss: 0.2548 - val_acc: 0.9343\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8859\n",
      "Epoch 00167: val_loss improved from 0.25482 to 0.25424, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/167-0.2542.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3604 - acc: 0.8859 - val_loss: 0.2542 - val_acc: 0.9294\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8884\n",
      "Epoch 00168: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3588 - acc: 0.8884 - val_loss: 0.2586 - val_acc: 0.9357\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8890\n",
      "Epoch 00169: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3569 - acc: 0.8890 - val_loss: 0.2597 - val_acc: 0.9322\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8892\n",
      "Epoch 00170: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3506 - acc: 0.8893 - val_loss: 0.2686 - val_acc: 0.9299\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8900\n",
      "Epoch 00171: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3514 - acc: 0.8900 - val_loss: 0.2579 - val_acc: 0.9315\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8894\n",
      "Epoch 00172: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3525 - acc: 0.8894 - val_loss: 0.2599 - val_acc: 0.9308\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8893\n",
      "Epoch 00173: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3502 - acc: 0.8893 - val_loss: 0.2629 - val_acc: 0.9334\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8908\n",
      "Epoch 00174: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3502 - acc: 0.8908 - val_loss: 0.2575 - val_acc: 0.9285\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8905\n",
      "Epoch 00175: val_loss did not improve from 0.25424\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3468 - acc: 0.8905 - val_loss: 0.2649 - val_acc: 0.9304\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8927\n",
      "Epoch 00176: val_loss improved from 0.25424 to 0.25366, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/176-0.2537.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3443 - acc: 0.8927 - val_loss: 0.2537 - val_acc: 0.9320\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8894\n",
      "Epoch 00177: val_loss did not improve from 0.25366\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3465 - acc: 0.8894 - val_loss: 0.2657 - val_acc: 0.9308\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8908\n",
      "Epoch 00178: val_loss did not improve from 0.25366\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3440 - acc: 0.8908 - val_loss: 0.2644 - val_acc: 0.9308\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8909\n",
      "Epoch 00179: val_loss did not improve from 0.25366\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3453 - acc: 0.8909 - val_loss: 0.2640 - val_acc: 0.9257\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8902\n",
      "Epoch 00180: val_loss did not improve from 0.25366\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3456 - acc: 0.8902 - val_loss: 0.2570 - val_acc: 0.9297\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8912\n",
      "Epoch 00181: val_loss did not improve from 0.25366\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3422 - acc: 0.8912 - val_loss: 0.2542 - val_acc: 0.9336\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8918\n",
      "Epoch 00182: val_loss improved from 0.25366 to 0.25223, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/182-0.2522.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3408 - acc: 0.8918 - val_loss: 0.2522 - val_acc: 0.9350\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8916\n",
      "Epoch 00183: val_loss did not improve from 0.25223\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3477 - acc: 0.8916 - val_loss: 0.2763 - val_acc: 0.9304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8949\n",
      "Epoch 00184: val_loss did not improve from 0.25223\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3379 - acc: 0.8949 - val_loss: 0.2737 - val_acc: 0.9283\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8914\n",
      "Epoch 00185: val_loss improved from 0.25223 to 0.25197, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/185-0.2520.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3434 - acc: 0.8914 - val_loss: 0.2520 - val_acc: 0.9359\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8943\n",
      "Epoch 00186: val_loss improved from 0.25197 to 0.24838, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/186-0.2484.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3394 - acc: 0.8943 - val_loss: 0.2484 - val_acc: 0.9364\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8936\n",
      "Epoch 00187: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3401 - acc: 0.8936 - val_loss: 0.2566 - val_acc: 0.9336\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8928\n",
      "Epoch 00188: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3405 - acc: 0.8928 - val_loss: 0.2589 - val_acc: 0.9331\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8947\n",
      "Epoch 00189: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3318 - acc: 0.8947 - val_loss: 0.2545 - val_acc: 0.9359\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8943\n",
      "Epoch 00190: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3337 - acc: 0.8944 - val_loss: 0.2677 - val_acc: 0.9338\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8921\n",
      "Epoch 00191: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3412 - acc: 0.8921 - val_loss: 0.2512 - val_acc: 0.9350\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8914\n",
      "Epoch 00192: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3404 - acc: 0.8914 - val_loss: 0.2593 - val_acc: 0.9348\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8941\n",
      "Epoch 00193: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3305 - acc: 0.8941 - val_loss: 0.2514 - val_acc: 0.9345\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8943\n",
      "Epoch 00194: val_loss did not improve from 0.24838\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3310 - acc: 0.8943 - val_loss: 0.2622 - val_acc: 0.9306\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8960\n",
      "Epoch 00195: val_loss improved from 0.24838 to 0.24818, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/195-0.2482.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3287 - acc: 0.8960 - val_loss: 0.2482 - val_acc: 0.9366\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8971\n",
      "Epoch 00196: val_loss did not improve from 0.24818\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3235 - acc: 0.8971 - val_loss: 0.2514 - val_acc: 0.9336\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8945\n",
      "Epoch 00197: val_loss did not improve from 0.24818\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3310 - acc: 0.8944 - val_loss: 0.2547 - val_acc: 0.9329\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8958\n",
      "Epoch 00198: val_loss did not improve from 0.24818\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3280 - acc: 0.8959 - val_loss: 0.2582 - val_acc: 0.9315\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8994\n",
      "Epoch 00199: val_loss did not improve from 0.24818\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3235 - acc: 0.8994 - val_loss: 0.2599 - val_acc: 0.9320\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8970\n",
      "Epoch 00200: val_loss improved from 0.24818 to 0.24288, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/200-0.2429.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3274 - acc: 0.8970 - val_loss: 0.2429 - val_acc: 0.9373\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8968\n",
      "Epoch 00201: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3211 - acc: 0.8968 - val_loss: 0.2515 - val_acc: 0.9364\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8974\n",
      "Epoch 00202: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3279 - acc: 0.8974 - val_loss: 0.2675 - val_acc: 0.9287\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8978\n",
      "Epoch 00203: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3240 - acc: 0.8978 - val_loss: 0.2511 - val_acc: 0.9348\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8963\n",
      "Epoch 00204: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3258 - acc: 0.8963 - val_loss: 0.2593 - val_acc: 0.9299\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8978\n",
      "Epoch 00205: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3198 - acc: 0.8978 - val_loss: 0.2455 - val_acc: 0.9376\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8991\n",
      "Epoch 00206: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3202 - acc: 0.8991 - val_loss: 0.2469 - val_acc: 0.9348\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8979\n",
      "Epoch 00207: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3235 - acc: 0.8979 - val_loss: 0.2491 - val_acc: 0.9324\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9003\n",
      "Epoch 00208: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3153 - acc: 0.9003 - val_loss: 0.2568 - val_acc: 0.9385\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.8983\n",
      "Epoch 00209: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3204 - acc: 0.8983 - val_loss: 0.2531 - val_acc: 0.9373\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.9011\n",
      "Epoch 00210: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3170 - acc: 0.9011 - val_loss: 0.2477 - val_acc: 0.9336\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9017\n",
      "Epoch 00211: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3135 - acc: 0.9017 - val_loss: 0.2509 - val_acc: 0.9343\n",
      "Epoch 212/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8994\n",
      "Epoch 00212: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3117 - acc: 0.8994 - val_loss: 0.2532 - val_acc: 0.9355\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9004\n",
      "Epoch 00213: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3136 - acc: 0.9004 - val_loss: 0.2492 - val_acc: 0.9343\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8990\n",
      "Epoch 00214: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3208 - acc: 0.8991 - val_loss: 0.2476 - val_acc: 0.9373\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9004\n",
      "Epoch 00215: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3110 - acc: 0.9004 - val_loss: 0.2557 - val_acc: 0.9376\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9020\n",
      "Epoch 00216: val_loss improved from 0.24288 to 0.23999, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/216-0.2400.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3082 - acc: 0.9020 - val_loss: 0.2400 - val_acc: 0.9373\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9016\n",
      "Epoch 00217: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3090 - acc: 0.9016 - val_loss: 0.2595 - val_acc: 0.9322\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9012\n",
      "Epoch 00218: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3101 - acc: 0.9012 - val_loss: 0.2586 - val_acc: 0.9373\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9010\n",
      "Epoch 00219: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3125 - acc: 0.9010 - val_loss: 0.2590 - val_acc: 0.9378\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9036\n",
      "Epoch 00220: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3065 - acc: 0.9035 - val_loss: 0.2608 - val_acc: 0.9324\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9025\n",
      "Epoch 00221: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3087 - acc: 0.9025 - val_loss: 0.2447 - val_acc: 0.9357\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9016\n",
      "Epoch 00222: val_loss did not improve from 0.23999\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3078 - acc: 0.9016 - val_loss: 0.2543 - val_acc: 0.9336\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9026\n",
      "Epoch 00223: val_loss improved from 0.23999 to 0.23668, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/223-0.2367.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3068 - acc: 0.9026 - val_loss: 0.2367 - val_acc: 0.9385\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9024\n",
      "Epoch 00224: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3085 - acc: 0.9025 - val_loss: 0.2674 - val_acc: 0.9308\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9001\n",
      "Epoch 00225: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3089 - acc: 0.9001 - val_loss: 0.2418 - val_acc: 0.9371\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9042\n",
      "Epoch 00226: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3046 - acc: 0.9042 - val_loss: 0.2525 - val_acc: 0.9345\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.9027\n",
      "Epoch 00227: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3049 - acc: 0.9027 - val_loss: 0.2485 - val_acc: 0.9364\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9059\n",
      "Epoch 00228: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2998 - acc: 0.9059 - val_loss: 0.2405 - val_acc: 0.9392\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9043\n",
      "Epoch 00229: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3030 - acc: 0.9043 - val_loss: 0.2392 - val_acc: 0.9376\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9026\n",
      "Epoch 00230: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3032 - acc: 0.9026 - val_loss: 0.2388 - val_acc: 0.9394\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9040\n",
      "Epoch 00231: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2995 - acc: 0.9041 - val_loss: 0.2407 - val_acc: 0.9390\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9062\n",
      "Epoch 00232: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2994 - acc: 0.9061 - val_loss: 0.2500 - val_acc: 0.9364\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9035\n",
      "Epoch 00233: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3023 - acc: 0.9035 - val_loss: 0.2432 - val_acc: 0.9399\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9047\n",
      "Epoch 00234: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2973 - acc: 0.9047 - val_loss: 0.2596 - val_acc: 0.9369\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9045\n",
      "Epoch 00235: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3014 - acc: 0.9045 - val_loss: 0.2436 - val_acc: 0.9357\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9067\n",
      "Epoch 00236: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2952 - acc: 0.9067 - val_loss: 0.2415 - val_acc: 0.9390\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9041\n",
      "Epoch 00237: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2988 - acc: 0.9041 - val_loss: 0.2442 - val_acc: 0.9392\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9048\n",
      "Epoch 00238: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2961 - acc: 0.9048 - val_loss: 0.2454 - val_acc: 0.9404\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9041\n",
      "Epoch 00239: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3009 - acc: 0.9041 - val_loss: 0.2404 - val_acc: 0.9380\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9059\n",
      "Epoch 00240: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2960 - acc: 0.9059 - val_loss: 0.2503 - val_acc: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9060\n",
      "Epoch 00241: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2888 - acc: 0.9060 - val_loss: 0.2537 - val_acc: 0.9373\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9030\n",
      "Epoch 00242: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3006 - acc: 0.9029 - val_loss: 0.2482 - val_acc: 0.9380\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9068\n",
      "Epoch 00243: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2932 - acc: 0.9069 - val_loss: 0.2511 - val_acc: 0.9425\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9067\n",
      "Epoch 00244: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2933 - acc: 0.9067 - val_loss: 0.2454 - val_acc: 0.9397\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9063\n",
      "Epoch 00245: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2951 - acc: 0.9063 - val_loss: 0.2508 - val_acc: 0.9394\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9077\n",
      "Epoch 00246: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2914 - acc: 0.9077 - val_loss: 0.2514 - val_acc: 0.9350\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9072\n",
      "Epoch 00247: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2866 - acc: 0.9072 - val_loss: 0.2399 - val_acc: 0.9394\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9061\n",
      "Epoch 00248: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2926 - acc: 0.9061 - val_loss: 0.2541 - val_acc: 0.9371\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9031\n",
      "Epoch 00249: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2998 - acc: 0.9031 - val_loss: 0.2392 - val_acc: 0.9418\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9067\n",
      "Epoch 00250: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2908 - acc: 0.9067 - val_loss: 0.2418 - val_acc: 0.9411\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9061\n",
      "Epoch 00251: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2932 - acc: 0.9061 - val_loss: 0.2508 - val_acc: 0.9390\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9089\n",
      "Epoch 00252: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2874 - acc: 0.9090 - val_loss: 0.2557 - val_acc: 0.9385\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9082\n",
      "Epoch 00253: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2871 - acc: 0.9082 - val_loss: 0.2400 - val_acc: 0.9383\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9089\n",
      "Epoch 00254: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2876 - acc: 0.9089 - val_loss: 0.2417 - val_acc: 0.9397\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9068\n",
      "Epoch 00255: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2891 - acc: 0.9068 - val_loss: 0.2411 - val_acc: 0.9385\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9099\n",
      "Epoch 00256: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2842 - acc: 0.9099 - val_loss: 0.2398 - val_acc: 0.9415\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9086\n",
      "Epoch 00257: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2894 - acc: 0.9086 - val_loss: 0.2423 - val_acc: 0.9392\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9121\n",
      "Epoch 00258: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2749 - acc: 0.9121 - val_loss: 0.2407 - val_acc: 0.9390\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9093\n",
      "Epoch 00259: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2837 - acc: 0.9093 - val_loss: 0.2439 - val_acc: 0.9408\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9104\n",
      "Epoch 00260: val_loss did not improve from 0.23668\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2791 - acc: 0.9104 - val_loss: 0.2407 - val_acc: 0.9413\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9057\n",
      "Epoch 00261: val_loss improved from 0.23668 to 0.22990, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_7_conv_checkpoint/261-0.2299.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2864 - acc: 0.9057 - val_loss: 0.2299 - val_acc: 0.9394\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9090\n",
      "Epoch 00262: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2820 - acc: 0.9090 - val_loss: 0.2413 - val_acc: 0.9406\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9095\n",
      "Epoch 00263: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2781 - acc: 0.9095 - val_loss: 0.2412 - val_acc: 0.9390\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9092\n",
      "Epoch 00264: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2819 - acc: 0.9093 - val_loss: 0.2516 - val_acc: 0.9341\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9081\n",
      "Epoch 00265: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2850 - acc: 0.9081 - val_loss: 0.2528 - val_acc: 0.9392\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9082\n",
      "Epoch 00266: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2821 - acc: 0.9082 - val_loss: 0.2413 - val_acc: 0.9390\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9110\n",
      "Epoch 00267: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2761 - acc: 0.9110 - val_loss: 0.2460 - val_acc: 0.9380\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9099\n",
      "Epoch 00268: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2772 - acc: 0.9099 - val_loss: 0.2408 - val_acc: 0.9385\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9105\n",
      "Epoch 00269: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2777 - acc: 0.9106 - val_loss: 0.2537 - val_acc: 0.9385\n",
      "Epoch 270/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9108\n",
      "Epoch 00270: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2766 - acc: 0.9108 - val_loss: 0.2379 - val_acc: 0.9408\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9113\n",
      "Epoch 00271: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2767 - acc: 0.9113 - val_loss: 0.2393 - val_acc: 0.9371\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9117\n",
      "Epoch 00272: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2709 - acc: 0.9117 - val_loss: 0.2492 - val_acc: 0.9359\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9108\n",
      "Epoch 00273: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2761 - acc: 0.9108 - val_loss: 0.2381 - val_acc: 0.9401\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9138\n",
      "Epoch 00274: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2662 - acc: 0.9138 - val_loss: 0.2438 - val_acc: 0.9392\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9095\n",
      "Epoch 00275: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2764 - acc: 0.9095 - val_loss: 0.2575 - val_acc: 0.9364\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9124\n",
      "Epoch 00276: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2719 - acc: 0.9124 - val_loss: 0.2500 - val_acc: 0.9357\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9122\n",
      "Epoch 00277: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2740 - acc: 0.9122 - val_loss: 0.2401 - val_acc: 0.9408\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9128\n",
      "Epoch 00278: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2751 - acc: 0.9128 - val_loss: 0.2436 - val_acc: 0.9406\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9137\n",
      "Epoch 00279: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2706 - acc: 0.9137 - val_loss: 0.2686 - val_acc: 0.9392\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9126\n",
      "Epoch 00280: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2699 - acc: 0.9126 - val_loss: 0.2424 - val_acc: 0.9371\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9111\n",
      "Epoch 00281: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2752 - acc: 0.9111 - val_loss: 0.2425 - val_acc: 0.9418\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9152\n",
      "Epoch 00282: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2662 - acc: 0.9152 - val_loss: 0.2438 - val_acc: 0.9425\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9136\n",
      "Epoch 00283: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2705 - acc: 0.9136 - val_loss: 0.2501 - val_acc: 0.9387\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9137\n",
      "Epoch 00284: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2665 - acc: 0.9137 - val_loss: 0.2671 - val_acc: 0.9376\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9123\n",
      "Epoch 00285: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2696 - acc: 0.9123 - val_loss: 0.2602 - val_acc: 0.9362\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9149\n",
      "Epoch 00286: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2646 - acc: 0.9149 - val_loss: 0.2444 - val_acc: 0.9434\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9158\n",
      "Epoch 00287: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2621 - acc: 0.9157 - val_loss: 0.2430 - val_acc: 0.9429\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9143\n",
      "Epoch 00288: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2691 - acc: 0.9144 - val_loss: 0.2359 - val_acc: 0.9406\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9140\n",
      "Epoch 00289: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2639 - acc: 0.9141 - val_loss: 0.2485 - val_acc: 0.9411\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9126\n",
      "Epoch 00290: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2654 - acc: 0.9126 - val_loss: 0.2392 - val_acc: 0.9399\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9156\n",
      "Epoch 00291: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2668 - acc: 0.9156 - val_loss: 0.2370 - val_acc: 0.9427\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9141\n",
      "Epoch 00292: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2630 - acc: 0.9141 - val_loss: 0.2502 - val_acc: 0.9373\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9132\n",
      "Epoch 00293: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2642 - acc: 0.9132 - val_loss: 0.2440 - val_acc: 0.9406\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9140\n",
      "Epoch 00294: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2609 - acc: 0.9140 - val_loss: 0.2382 - val_acc: 0.9420\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9177\n",
      "Epoch 00295: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2582 - acc: 0.9176 - val_loss: 0.2356 - val_acc: 0.9399\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9156\n",
      "Epoch 00296: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2597 - acc: 0.9156 - val_loss: 0.2340 - val_acc: 0.9404\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9156\n",
      "Epoch 00297: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2612 - acc: 0.9156 - val_loss: 0.2458 - val_acc: 0.9394\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9144\n",
      "Epoch 00298: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2653 - acc: 0.9144 - val_loss: 0.2402 - val_acc: 0.9425\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9128\n",
      "Epoch 00299: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2651 - acc: 0.9128 - val_loss: 0.2334 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9144\n",
      "Epoch 00300: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2613 - acc: 0.9144 - val_loss: 0.2404 - val_acc: 0.9385\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9153\n",
      "Epoch 00301: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2584 - acc: 0.9153 - val_loss: 0.2381 - val_acc: 0.9408\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9168\n",
      "Epoch 00302: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2604 - acc: 0.9169 - val_loss: 0.2343 - val_acc: 0.9394\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9160\n",
      "Epoch 00303: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2591 - acc: 0.9160 - val_loss: 0.2543 - val_acc: 0.9413\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9148\n",
      "Epoch 00304: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2596 - acc: 0.9148 - val_loss: 0.2523 - val_acc: 0.9378\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9161\n",
      "Epoch 00305: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2572 - acc: 0.9161 - val_loss: 0.2460 - val_acc: 0.9418\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9191\n",
      "Epoch 00306: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2533 - acc: 0.9191 - val_loss: 0.2589 - val_acc: 0.9383\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9172\n",
      "Epoch 00307: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2569 - acc: 0.9172 - val_loss: 0.2475 - val_acc: 0.9411\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9164\n",
      "Epoch 00308: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2564 - acc: 0.9163 - val_loss: 0.2417 - val_acc: 0.9362\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9190\n",
      "Epoch 00309: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2522 - acc: 0.9190 - val_loss: 0.2531 - val_acc: 0.9383\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9163\n",
      "Epoch 00310: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2589 - acc: 0.9163 - val_loss: 0.2380 - val_acc: 0.9390\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9176\n",
      "Epoch 00311: val_loss did not improve from 0.22990\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2546 - acc: 0.9176 - val_loss: 0.2540 - val_acc: 0.9378\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 570us/sample - loss: 0.2918 - acc: 0.9190\n",
      "Loss: 0.291794709649289 Accuracy: 0.9190031\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7390 - acc: 0.0857\n",
      "Epoch 00001: val_loss improved from inf to 2.67261, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/001-2.6726.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.7391 - acc: 0.0856 - val_loss: 2.6726 - val_acc: 0.1491\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5541 - acc: 0.1746\n",
      "Epoch 00002: val_loss improved from 2.67261 to 2.19912, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/002-2.1991.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.5539 - acc: 0.1747 - val_loss: 2.1991 - val_acc: 0.3359\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1908 - acc: 0.2884\n",
      "Epoch 00003: val_loss improved from 2.19912 to 1.82371, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/003-1.8237.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.1908 - acc: 0.2884 - val_loss: 1.8237 - val_acc: 0.4389\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9559 - acc: 0.3571\n",
      "Epoch 00004: val_loss improved from 1.82371 to 1.62970, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/004-1.6297.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.9559 - acc: 0.3571 - val_loss: 1.6297 - val_acc: 0.5106\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7937 - acc: 0.4063\n",
      "Epoch 00005: val_loss improved from 1.62970 to 1.44977, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/005-1.4498.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.7936 - acc: 0.4063 - val_loss: 1.4498 - val_acc: 0.5642\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6856 - acc: 0.4405\n",
      "Epoch 00006: val_loss improved from 1.44977 to 1.36878, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/006-1.3688.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.6857 - acc: 0.4405 - val_loss: 1.3688 - val_acc: 0.5809\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5909 - acc: 0.4723\n",
      "Epoch 00007: val_loss improved from 1.36878 to 1.26922, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/007-1.2692.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5908 - acc: 0.4723 - val_loss: 1.2692 - val_acc: 0.6091\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5195 - acc: 0.4948\n",
      "Epoch 00008: val_loss improved from 1.26922 to 1.20353, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/008-1.2035.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5194 - acc: 0.4949 - val_loss: 1.2035 - val_acc: 0.6375\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4552 - acc: 0.5213\n",
      "Epoch 00009: val_loss improved from 1.20353 to 1.15850, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/009-1.1585.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4554 - acc: 0.5213 - val_loss: 1.1585 - val_acc: 0.6578\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4045 - acc: 0.5376\n",
      "Epoch 00010: val_loss improved from 1.15850 to 1.08110, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/010-1.0811.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4045 - acc: 0.5376 - val_loss: 1.0811 - val_acc: 0.6699\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3539 - acc: 0.5573\n",
      "Epoch 00011: val_loss improved from 1.08110 to 1.02737, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/011-1.0274.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3539 - acc: 0.5573 - val_loss: 1.0274 - val_acc: 0.6848\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3132 - acc: 0.5685\n",
      "Epoch 00012: val_loss improved from 1.02737 to 0.99145, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/012-0.9914.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3132 - acc: 0.5684 - val_loss: 0.9914 - val_acc: 0.7016\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2740 - acc: 0.5834\n",
      "Epoch 00013: val_loss improved from 0.99145 to 0.96342, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/013-0.9634.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2739 - acc: 0.5834 - val_loss: 0.9634 - val_acc: 0.7154\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2283 - acc: 0.5984\n",
      "Epoch 00014: val_loss improved from 0.96342 to 0.94938, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/014-0.9494.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2283 - acc: 0.5984 - val_loss: 0.9494 - val_acc: 0.7084\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1978 - acc: 0.6092\n",
      "Epoch 00015: val_loss improved from 0.94938 to 0.90735, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/015-0.9074.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1978 - acc: 0.6092 - val_loss: 0.9074 - val_acc: 0.7237\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6197\n",
      "Epoch 00016: val_loss improved from 0.90735 to 0.87627, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/016-0.8763.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1688 - acc: 0.6197 - val_loss: 0.8763 - val_acc: 0.7400\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1458 - acc: 0.6285\n",
      "Epoch 00017: val_loss improved from 0.87627 to 0.84850, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/017-0.8485.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1457 - acc: 0.6284 - val_loss: 0.8485 - val_acc: 0.7596\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1139 - acc: 0.6408\n",
      "Epoch 00018: val_loss improved from 0.84850 to 0.81481, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/018-0.8148.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1138 - acc: 0.6408 - val_loss: 0.8148 - val_acc: 0.7610\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0933 - acc: 0.6461\n",
      "Epoch 00019: val_loss improved from 0.81481 to 0.77848, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/019-0.7785.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0934 - acc: 0.6461 - val_loss: 0.7785 - val_acc: 0.7743\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6565\n",
      "Epoch 00020: val_loss did not improve from 0.77848\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0706 - acc: 0.6565 - val_loss: 0.8566 - val_acc: 0.7577\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0586 - acc: 0.6604\n",
      "Epoch 00021: val_loss improved from 0.77848 to 0.73796, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/021-0.7380.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0586 - acc: 0.6604 - val_loss: 0.7380 - val_acc: 0.7834\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0319 - acc: 0.6694\n",
      "Epoch 00022: val_loss improved from 0.73796 to 0.70955, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/022-0.7096.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0319 - acc: 0.6694 - val_loss: 0.7096 - val_acc: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.6811\n",
      "Epoch 00023: val_loss improved from 0.70955 to 0.70449, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/023-0.7045.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0019 - acc: 0.6810 - val_loss: 0.7045 - val_acc: 0.8074\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9989 - acc: 0.6811\n",
      "Epoch 00024: val_loss did not improve from 0.70449\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9989 - acc: 0.6810 - val_loss: 0.7117 - val_acc: 0.8027\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9790 - acc: 0.6908\n",
      "Epoch 00025: val_loss improved from 0.70449 to 0.67095, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/025-0.6710.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9791 - acc: 0.6908 - val_loss: 0.6710 - val_acc: 0.8239\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9504 - acc: 0.6978\n",
      "Epoch 00026: val_loss improved from 0.67095 to 0.65845, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/026-0.6585.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9503 - acc: 0.6978 - val_loss: 0.6585 - val_acc: 0.8160\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.7045\n",
      "Epoch 00027: val_loss improved from 0.65845 to 0.64099, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/027-0.6410.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9351 - acc: 0.7045 - val_loss: 0.6410 - val_acc: 0.8328\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9249 - acc: 0.7079\n",
      "Epoch 00028: val_loss did not improve from 0.64099\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9249 - acc: 0.7079 - val_loss: 0.6630 - val_acc: 0.8174\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9145 - acc: 0.7135\n",
      "Epoch 00029: val_loss improved from 0.64099 to 0.62165, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/029-0.6216.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9145 - acc: 0.7135 - val_loss: 0.6216 - val_acc: 0.8321\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9027 - acc: 0.7178\n",
      "Epoch 00030: val_loss improved from 0.62165 to 0.61307, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/030-0.6131.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9026 - acc: 0.7178 - val_loss: 0.6131 - val_acc: 0.8418\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7194\n",
      "Epoch 00031: val_loss improved from 0.61307 to 0.59892, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/031-0.5989.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8914 - acc: 0.7194 - val_loss: 0.5989 - val_acc: 0.8388\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8782 - acc: 0.7246\n",
      "Epoch 00032: val_loss improved from 0.59892 to 0.58030, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/032-0.5803.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8782 - acc: 0.7246 - val_loss: 0.5803 - val_acc: 0.8472\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8613 - acc: 0.7314\n",
      "Epoch 00033: val_loss improved from 0.58030 to 0.57706, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/033-0.5771.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8613 - acc: 0.7314 - val_loss: 0.5771 - val_acc: 0.8535\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7320\n",
      "Epoch 00034: val_loss improved from 0.57706 to 0.56369, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/034-0.5637.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8484 - acc: 0.7320 - val_loss: 0.5637 - val_acc: 0.8484\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7387\n",
      "Epoch 00035: val_loss improved from 0.56369 to 0.55746, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/035-0.5575.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8352 - acc: 0.7387 - val_loss: 0.5575 - val_acc: 0.8467\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8311 - acc: 0.7389\n",
      "Epoch 00036: val_loss improved from 0.55746 to 0.53749, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/036-0.5375.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8311 - acc: 0.7389 - val_loss: 0.5375 - val_acc: 0.8567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7455\n",
      "Epoch 00037: val_loss improved from 0.53749 to 0.53010, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/037-0.5301.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8175 - acc: 0.7456 - val_loss: 0.5301 - val_acc: 0.8546\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8063 - acc: 0.7487\n",
      "Epoch 00038: val_loss improved from 0.53010 to 0.52263, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/038-0.5226.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8062 - acc: 0.7487 - val_loss: 0.5226 - val_acc: 0.8595\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7970 - acc: 0.7500\n",
      "Epoch 00039: val_loss did not improve from 0.52263\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7970 - acc: 0.7501 - val_loss: 0.5274 - val_acc: 0.8544\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7903 - acc: 0.7507\n",
      "Epoch 00040: val_loss improved from 0.52263 to 0.49741, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/040-0.4974.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7902 - acc: 0.7507 - val_loss: 0.4974 - val_acc: 0.8677\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7537\n",
      "Epoch 00041: val_loss improved from 0.49741 to 0.49018, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/041-0.4902.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7825 - acc: 0.7537 - val_loss: 0.4902 - val_acc: 0.8698\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7568\n",
      "Epoch 00042: val_loss did not improve from 0.49018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7791 - acc: 0.7567 - val_loss: 0.5056 - val_acc: 0.8703\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7675 - acc: 0.7608\n",
      "Epoch 00043: val_loss did not improve from 0.49018\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7675 - acc: 0.7608 - val_loss: 0.4993 - val_acc: 0.8651\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7544 - acc: 0.7640\n",
      "Epoch 00044: val_loss improved from 0.49018 to 0.46868, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/044-0.4687.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7544 - acc: 0.7640 - val_loss: 0.4687 - val_acc: 0.8735\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7678\n",
      "Epoch 00045: val_loss did not improve from 0.46868\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7486 - acc: 0.7678 - val_loss: 0.4961 - val_acc: 0.8682\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7464 - acc: 0.7662\n",
      "Epoch 00046: val_loss improved from 0.46868 to 0.46473, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/046-0.4647.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7463 - acc: 0.7663 - val_loss: 0.4647 - val_acc: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7329 - acc: 0.7736\n",
      "Epoch 00047: val_loss did not improve from 0.46473\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7328 - acc: 0.7736 - val_loss: 0.4938 - val_acc: 0.8591\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7253 - acc: 0.7745\n",
      "Epoch 00048: val_loss improved from 0.46473 to 0.45052, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/048-0.4505.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7253 - acc: 0.7745 - val_loss: 0.4505 - val_acc: 0.8805\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7749\n",
      "Epoch 00049: val_loss did not improve from 0.45052\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7201 - acc: 0.7749 - val_loss: 0.4630 - val_acc: 0.8735\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7179 - acc: 0.7785\n",
      "Epoch 00050: val_loss did not improve from 0.45052\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7181 - acc: 0.7785 - val_loss: 0.4836 - val_acc: 0.8742\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.7776\n",
      "Epoch 00051: val_loss improved from 0.45052 to 0.44211, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/051-0.4421.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7108 - acc: 0.7776 - val_loss: 0.4421 - val_acc: 0.8779\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6985 - acc: 0.7815\n",
      "Epoch 00052: val_loss improved from 0.44211 to 0.43127, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/052-0.4313.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6986 - acc: 0.7815 - val_loss: 0.4313 - val_acc: 0.8821\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6986 - acc: 0.7837\n",
      "Epoch 00053: val_loss improved from 0.43127 to 0.41922, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/053-0.4192.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6987 - acc: 0.7836 - val_loss: 0.4192 - val_acc: 0.8877\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6877 - acc: 0.7851\n",
      "Epoch 00054: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6879 - acc: 0.7850 - val_loss: 0.4225 - val_acc: 0.8854\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.7900\n",
      "Epoch 00055: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6789 - acc: 0.7900 - val_loss: 0.4304 - val_acc: 0.8817\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.7860\n",
      "Epoch 00056: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6816 - acc: 0.7860 - val_loss: 0.4499 - val_acc: 0.8747\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.7933\n",
      "Epoch 00057: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6660 - acc: 0.7933 - val_loss: 0.4313 - val_acc: 0.8814\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6632 - acc: 0.7934\n",
      "Epoch 00058: val_loss improved from 0.41922 to 0.40998, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/058-0.4100.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6631 - acc: 0.7934 - val_loss: 0.4100 - val_acc: 0.8938\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.7949\n",
      "Epoch 00059: val_loss improved from 0.40998 to 0.39563, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/059-0.3956.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6648 - acc: 0.7949 - val_loss: 0.3956 - val_acc: 0.8898\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.7979\n",
      "Epoch 00060: val_loss did not improve from 0.39563\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6502 - acc: 0.7978 - val_loss: 0.4003 - val_acc: 0.8908\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6454 - acc: 0.8011\n",
      "Epoch 00061: val_loss did not improve from 0.39563\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6454 - acc: 0.8011 - val_loss: 0.3972 - val_acc: 0.8921\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.7962\n",
      "Epoch 00062: val_loss improved from 0.39563 to 0.38575, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/062-0.3858.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6487 - acc: 0.7961 - val_loss: 0.3858 - val_acc: 0.8963\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6347 - acc: 0.8036\n",
      "Epoch 00063: val_loss did not improve from 0.38575\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6347 - acc: 0.8036 - val_loss: 0.3946 - val_acc: 0.8905\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.8002\n",
      "Epoch 00064: val_loss improved from 0.38575 to 0.37169, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/064-0.3717.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6370 - acc: 0.8002 - val_loss: 0.3717 - val_acc: 0.9012\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6261 - acc: 0.8055\n",
      "Epoch 00065: val_loss improved from 0.37169 to 0.36472, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/065-0.3647.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6260 - acc: 0.8055 - val_loss: 0.3647 - val_acc: 0.8994\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.8039\n",
      "Epoch 00066: val_loss did not improve from 0.36472\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6245 - acc: 0.8039 - val_loss: 0.3810 - val_acc: 0.8954\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.8075\n",
      "Epoch 00067: val_loss did not improve from 0.36472\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6218 - acc: 0.8075 - val_loss: 0.3699 - val_acc: 0.8975\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8102\n",
      "Epoch 00068: val_loss did not improve from 0.36472\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6106 - acc: 0.8103 - val_loss: 0.4022 - val_acc: 0.8889\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8091\n",
      "Epoch 00069: val_loss did not improve from 0.36472\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6148 - acc: 0.8091 - val_loss: 0.3677 - val_acc: 0.8973\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.8136\n",
      "Epoch 00070: val_loss improved from 0.36472 to 0.35518, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/070-0.3552.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6040 - acc: 0.8137 - val_loss: 0.3552 - val_acc: 0.9022\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.8119\n",
      "Epoch 00071: val_loss did not improve from 0.35518\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5987 - acc: 0.8119 - val_loss: 0.3643 - val_acc: 0.9045\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8148\n",
      "Epoch 00072: val_loss improved from 0.35518 to 0.35333, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/072-0.3533.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5935 - acc: 0.8148 - val_loss: 0.3533 - val_acc: 0.9036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8152\n",
      "Epoch 00073: val_loss improved from 0.35333 to 0.34499, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/073-0.3450.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5931 - acc: 0.8152 - val_loss: 0.3450 - val_acc: 0.9066\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5909 - acc: 0.8189\n",
      "Epoch 00074: val_loss did not improve from 0.34499\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5909 - acc: 0.8190 - val_loss: 0.3490 - val_acc: 0.9026\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5863 - acc: 0.8177\n",
      "Epoch 00075: val_loss did not improve from 0.34499\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5863 - acc: 0.8177 - val_loss: 0.3514 - val_acc: 0.8989\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.8213\n",
      "Epoch 00076: val_loss improved from 0.34499 to 0.33882, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/076-0.3388.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5741 - acc: 0.8213 - val_loss: 0.3388 - val_acc: 0.9038\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.8223\n",
      "Epoch 00077: val_loss improved from 0.33882 to 0.33453, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/077-0.3345.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5721 - acc: 0.8223 - val_loss: 0.3345 - val_acc: 0.9089\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5693 - acc: 0.8220\n",
      "Epoch 00078: val_loss improved from 0.33453 to 0.32715, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/078-0.3271.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5693 - acc: 0.8220 - val_loss: 0.3271 - val_acc: 0.9073\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8253\n",
      "Epoch 00079: val_loss did not improve from 0.32715\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5652 - acc: 0.8253 - val_loss: 0.3441 - val_acc: 0.9043\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8279\n",
      "Epoch 00080: val_loss improved from 0.32715 to 0.32538, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/080-0.3254.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5573 - acc: 0.8279 - val_loss: 0.3254 - val_acc: 0.9124\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8245\n",
      "Epoch 00081: val_loss did not improve from 0.32538\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5626 - acc: 0.8245 - val_loss: 0.3394 - val_acc: 0.9071\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8274\n",
      "Epoch 00082: val_loss improved from 0.32538 to 0.32489, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/082-0.3249.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5544 - acc: 0.8274 - val_loss: 0.3249 - val_acc: 0.9096\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5515 - acc: 0.8301\n",
      "Epoch 00083: val_loss improved from 0.32489 to 0.32325, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/083-0.3232.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5514 - acc: 0.8301 - val_loss: 0.3232 - val_acc: 0.9082\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.8345\n",
      "Epoch 00084: val_loss improved from 0.32325 to 0.31432, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/084-0.3143.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5447 - acc: 0.8345 - val_loss: 0.3143 - val_acc: 0.9126\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.8335\n",
      "Epoch 00085: val_loss did not improve from 0.31432\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5437 - acc: 0.8335 - val_loss: 0.3158 - val_acc: 0.9126\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8306\n",
      "Epoch 00086: val_loss improved from 0.31432 to 0.31238, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/086-0.3124.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5483 - acc: 0.8305 - val_loss: 0.3124 - val_acc: 0.9140\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8316\n",
      "Epoch 00087: val_loss did not improve from 0.31238\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5405 - acc: 0.8316 - val_loss: 0.3241 - val_acc: 0.9050\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8327\n",
      "Epoch 00088: val_loss did not improve from 0.31238\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5324 - acc: 0.8327 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8355\n",
      "Epoch 00089: val_loss did not improve from 0.31238\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5325 - acc: 0.8355 - val_loss: 0.3260 - val_acc: 0.9082\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8312\n",
      "Epoch 00090: val_loss improved from 0.31238 to 0.30752, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/090-0.3075.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5368 - acc: 0.8312 - val_loss: 0.3075 - val_acc: 0.9115\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8379\n",
      "Epoch 00091: val_loss improved from 0.30752 to 0.29617, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/091-0.2962.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5198 - acc: 0.8379 - val_loss: 0.2962 - val_acc: 0.9150\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8383\n",
      "Epoch 00092: val_loss did not improve from 0.29617\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5163 - acc: 0.8383 - val_loss: 0.3079 - val_acc: 0.9117\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8382\n",
      "Epoch 00093: val_loss did not improve from 0.29617\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5201 - acc: 0.8382 - val_loss: 0.3064 - val_acc: 0.9166\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8428\n",
      "Epoch 00094: val_loss did not improve from 0.29617\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5099 - acc: 0.8428 - val_loss: 0.3071 - val_acc: 0.9113\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.8417\n",
      "Epoch 00095: val_loss improved from 0.29617 to 0.29345, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/095-0.2935.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5080 - acc: 0.8417 - val_loss: 0.2935 - val_acc: 0.9166\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8401\n",
      "Epoch 00096: val_loss did not improve from 0.29345\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5073 - acc: 0.8401 - val_loss: 0.3155 - val_acc: 0.9096\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8468\n",
      "Epoch 00097: val_loss improved from 0.29345 to 0.29060, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/097-0.2906.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4972 - acc: 0.8468 - val_loss: 0.2906 - val_acc: 0.9182\n",
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8438\n",
      "Epoch 00098: val_loss improved from 0.29060 to 0.28761, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/098-0.2876.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5037 - acc: 0.8437 - val_loss: 0.2876 - val_acc: 0.9194\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8445\n",
      "Epoch 00099: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4936 - acc: 0.8445 - val_loss: 0.2981 - val_acc: 0.9173\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8464\n",
      "Epoch 00100: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4934 - acc: 0.8464 - val_loss: 0.2885 - val_acc: 0.9161\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8471\n",
      "Epoch 00101: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4892 - acc: 0.8471 - val_loss: 0.2978 - val_acc: 0.9113\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8475\n",
      "Epoch 00102: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4833 - acc: 0.8475 - val_loss: 0.2888 - val_acc: 0.9206\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8453\n",
      "Epoch 00103: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4894 - acc: 0.8453 - val_loss: 0.2922 - val_acc: 0.9189\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8489\n",
      "Epoch 00104: val_loss improved from 0.28761 to 0.28695, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/104-0.2870.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4850 - acc: 0.8489 - val_loss: 0.2870 - val_acc: 0.9206\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8462\n",
      "Epoch 00105: val_loss improved from 0.28695 to 0.28499, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/105-0.2850.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4859 - acc: 0.8462 - val_loss: 0.2850 - val_acc: 0.9206\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8520\n",
      "Epoch 00106: val_loss improved from 0.28499 to 0.28353, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/106-0.2835.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4770 - acc: 0.8519 - val_loss: 0.2835 - val_acc: 0.9171\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8509\n",
      "Epoch 00107: val_loss improved from 0.28353 to 0.27834, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/107-0.2783.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4786 - acc: 0.8509 - val_loss: 0.2783 - val_acc: 0.9245\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8536\n",
      "Epoch 00108: val_loss improved from 0.27834 to 0.27787, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/108-0.2779.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4703 - acc: 0.8536 - val_loss: 0.2779 - val_acc: 0.9203\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.8518\n",
      "Epoch 00109: val_loss did not improve from 0.27787\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4687 - acc: 0.8518 - val_loss: 0.2963 - val_acc: 0.9180\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8552\n",
      "Epoch 00110: val_loss improved from 0.27787 to 0.27434, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/110-0.2743.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4654 - acc: 0.8552 - val_loss: 0.2743 - val_acc: 0.9196\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8541\n",
      "Epoch 00111: val_loss did not improve from 0.27434\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4613 - acc: 0.8541 - val_loss: 0.2940 - val_acc: 0.9164\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4726 - acc: 0.8549\n",
      "Epoch 00112: val_loss did not improve from 0.27434\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4727 - acc: 0.8549 - val_loss: 0.2747 - val_acc: 0.9245\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.8505\n",
      "Epoch 00113: val_loss did not improve from 0.27434\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4723 - acc: 0.8505 - val_loss: 0.2810 - val_acc: 0.9168\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8584\n",
      "Epoch 00114: val_loss did not improve from 0.27434\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4599 - acc: 0.8584 - val_loss: 0.2775 - val_acc: 0.9201\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8546\n",
      "Epoch 00115: val_loss improved from 0.27434 to 0.26574, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/115-0.2657.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4608 - acc: 0.8546 - val_loss: 0.2657 - val_acc: 0.9257\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8574\n",
      "Epoch 00116: val_loss did not improve from 0.26574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4567 - acc: 0.8575 - val_loss: 0.2722 - val_acc: 0.9245\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8592\n",
      "Epoch 00117: val_loss did not improve from 0.26574\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4509 - acc: 0.8592 - val_loss: 0.2751 - val_acc: 0.9245\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8587\n",
      "Epoch 00118: val_loss did not improve from 0.26574\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4505 - acc: 0.8587 - val_loss: 0.2697 - val_acc: 0.9234\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8569\n",
      "Epoch 00119: val_loss improved from 0.26574 to 0.26325, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/119-0.2632.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4488 - acc: 0.8569 - val_loss: 0.2632 - val_acc: 0.9276\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8592\n",
      "Epoch 00120: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4490 - acc: 0.8592 - val_loss: 0.2702 - val_acc: 0.9259\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.8600\n",
      "Epoch 00121: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4509 - acc: 0.8600 - val_loss: 0.2665 - val_acc: 0.9220\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8620\n",
      "Epoch 00122: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4415 - acc: 0.8621 - val_loss: 0.2698 - val_acc: 0.9271\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8614\n",
      "Epoch 00123: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4431 - acc: 0.8614 - val_loss: 0.2696 - val_acc: 0.9248\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.8641\n",
      "Epoch 00124: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4357 - acc: 0.8641 - val_loss: 0.2757 - val_acc: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8629\n",
      "Epoch 00125: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4352 - acc: 0.8629 - val_loss: 0.2778 - val_acc: 0.9227\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8644\n",
      "Epoch 00126: val_loss did not improve from 0.26325\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4350 - acc: 0.8644 - val_loss: 0.2638 - val_acc: 0.9276\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8630\n",
      "Epoch 00127: val_loss improved from 0.26325 to 0.26288, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/127-0.2629.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4343 - acc: 0.8630 - val_loss: 0.2629 - val_acc: 0.9269\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8642\n",
      "Epoch 00128: val_loss did not improve from 0.26288\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4346 - acc: 0.8643 - val_loss: 0.2787 - val_acc: 0.9236\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8661\n",
      "Epoch 00129: val_loss improved from 0.26288 to 0.25861, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/129-0.2586.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4284 - acc: 0.8661 - val_loss: 0.2586 - val_acc: 0.9276\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.8643\n",
      "Epoch 00130: val_loss improved from 0.25861 to 0.25721, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/130-0.2572.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4292 - acc: 0.8643 - val_loss: 0.2572 - val_acc: 0.9287\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8641\n",
      "Epoch 00131: val_loss improved from 0.25721 to 0.25556, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/131-0.2556.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4272 - acc: 0.8641 - val_loss: 0.2556 - val_acc: 0.9278\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8643\n",
      "Epoch 00132: val_loss did not improve from 0.25556\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4302 - acc: 0.8644 - val_loss: 0.2622 - val_acc: 0.9280\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8667\n",
      "Epoch 00133: val_loss improved from 0.25556 to 0.25553, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/133-0.2555.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4247 - acc: 0.8667 - val_loss: 0.2555 - val_acc: 0.9299\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8692\n",
      "Epoch 00134: val_loss did not improve from 0.25553\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4176 - acc: 0.8692 - val_loss: 0.2704 - val_acc: 0.9283\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8647\n",
      "Epoch 00135: val_loss improved from 0.25553 to 0.25195, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/135-0.2519.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4222 - acc: 0.8647 - val_loss: 0.2519 - val_acc: 0.9315\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8697\n",
      "Epoch 00136: val_loss did not improve from 0.25195\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4177 - acc: 0.8697 - val_loss: 0.2766 - val_acc: 0.9210\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8667\n",
      "Epoch 00137: val_loss improved from 0.25195 to 0.24804, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/137-0.2480.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4205 - acc: 0.8668 - val_loss: 0.2480 - val_acc: 0.9304\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8719\n",
      "Epoch 00138: val_loss did not improve from 0.24804\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4111 - acc: 0.8719 - val_loss: 0.2588 - val_acc: 0.9294\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8682\n",
      "Epoch 00139: val_loss did not improve from 0.24804\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4122 - acc: 0.8682 - val_loss: 0.2502 - val_acc: 0.9283\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8687\n",
      "Epoch 00140: val_loss did not improve from 0.24804\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4160 - acc: 0.8687 - val_loss: 0.2566 - val_acc: 0.9259\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8723\n",
      "Epoch 00141: val_loss did not improve from 0.24804\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4050 - acc: 0.8723 - val_loss: 0.3004 - val_acc: 0.9182\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8704\n",
      "Epoch 00142: val_loss did not improve from 0.24804\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4121 - acc: 0.8704 - val_loss: 0.2498 - val_acc: 0.9285\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8742\n",
      "Epoch 00143: val_loss improved from 0.24804 to 0.24394, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/143-0.2439.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4028 - acc: 0.8742 - val_loss: 0.2439 - val_acc: 0.9294\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8733\n",
      "Epoch 00144: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3999 - acc: 0.8733 - val_loss: 0.2753 - val_acc: 0.9243\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8749\n",
      "Epoch 00145: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3965 - acc: 0.8749 - val_loss: 0.2580 - val_acc: 0.9315\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8726\n",
      "Epoch 00146: val_loss improved from 0.24394 to 0.24317, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/146-0.2432.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4041 - acc: 0.8725 - val_loss: 0.2432 - val_acc: 0.9306\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8737\n",
      "Epoch 00147: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4009 - acc: 0.8737 - val_loss: 0.2491 - val_acc: 0.9322\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8758\n",
      "Epoch 00148: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3997 - acc: 0.8758 - val_loss: 0.2498 - val_acc: 0.9297\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8762\n",
      "Epoch 00149: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3953 - acc: 0.8762 - val_loss: 0.2531 - val_acc: 0.9292\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8760\n",
      "Epoch 00150: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3953 - acc: 0.8759 - val_loss: 0.2569 - val_acc: 0.9283\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8732\n",
      "Epoch 00151: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3994 - acc: 0.8732 - val_loss: 0.2506 - val_acc: 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8769\n",
      "Epoch 00152: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3908 - acc: 0.8769 - val_loss: 0.2688 - val_acc: 0.9245\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8768\n",
      "Epoch 00153: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3918 - acc: 0.8768 - val_loss: 0.2489 - val_acc: 0.9324\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8773\n",
      "Epoch 00154: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3907 - acc: 0.8773 - val_loss: 0.2553 - val_acc: 0.9308\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8783\n",
      "Epoch 00155: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3862 - acc: 0.8783 - val_loss: 0.2436 - val_acc: 0.9334\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8784\n",
      "Epoch 00156: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3819 - acc: 0.8784 - val_loss: 0.2571 - val_acc: 0.9287\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8764\n",
      "Epoch 00157: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3856 - acc: 0.8764 - val_loss: 0.2580 - val_acc: 0.9308\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8781\n",
      "Epoch 00158: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3832 - acc: 0.8781 - val_loss: 0.2550 - val_acc: 0.9308\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8801\n",
      "Epoch 00159: val_loss did not improve from 0.24317\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3807 - acc: 0.8801 - val_loss: 0.2511 - val_acc: 0.9299\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8784\n",
      "Epoch 00160: val_loss improved from 0.24317 to 0.23641, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/160-0.2364.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3818 - acc: 0.8784 - val_loss: 0.2364 - val_acc: 0.9338\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3782 - acc: 0.8799\n",
      "Epoch 00161: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3782 - acc: 0.8799 - val_loss: 0.2429 - val_acc: 0.9331\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8824\n",
      "Epoch 00162: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3685 - acc: 0.8824 - val_loss: 0.2446 - val_acc: 0.9327\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8803\n",
      "Epoch 00163: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3763 - acc: 0.8803 - val_loss: 0.2457 - val_acc: 0.9324\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8818\n",
      "Epoch 00164: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3766 - acc: 0.8818 - val_loss: 0.2473 - val_acc: 0.9315\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8823\n",
      "Epoch 00165: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3720 - acc: 0.8822 - val_loss: 0.2448 - val_acc: 0.9311\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8812\n",
      "Epoch 00166: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3722 - acc: 0.8812 - val_loss: 0.2509 - val_acc: 0.9308\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8846\n",
      "Epoch 00167: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3678 - acc: 0.8846 - val_loss: 0.2424 - val_acc: 0.9331\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8838\n",
      "Epoch 00168: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3662 - acc: 0.8838 - val_loss: 0.2498 - val_acc: 0.9311\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8849\n",
      "Epoch 00169: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3627 - acc: 0.8849 - val_loss: 0.2406 - val_acc: 0.9322\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8842\n",
      "Epoch 00170: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3689 - acc: 0.8842 - val_loss: 0.2501 - val_acc: 0.9320\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8850\n",
      "Epoch 00171: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3670 - acc: 0.8850 - val_loss: 0.2405 - val_acc: 0.9343\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8824\n",
      "Epoch 00172: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3661 - acc: 0.8824 - val_loss: 0.2426 - val_acc: 0.9320\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8844\n",
      "Epoch 00173: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3626 - acc: 0.8844 - val_loss: 0.2561 - val_acc: 0.9331\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8836\n",
      "Epoch 00174: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3641 - acc: 0.8837 - val_loss: 0.2442 - val_acc: 0.9334\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8871\n",
      "Epoch 00175: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3571 - acc: 0.8871 - val_loss: 0.2381 - val_acc: 0.9350\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8843\n",
      "Epoch 00176: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3624 - acc: 0.8843 - val_loss: 0.2444 - val_acc: 0.9334\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8860\n",
      "Epoch 00177: val_loss did not improve from 0.23641\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3563 - acc: 0.8860 - val_loss: 0.2611 - val_acc: 0.9297\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8874\n",
      "Epoch 00178: val_loss improved from 0.23641 to 0.23377, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/178-0.2338.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3559 - acc: 0.8874 - val_loss: 0.2338 - val_acc: 0.9355\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8874\n",
      "Epoch 00179: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3536 - acc: 0.8874 - val_loss: 0.2410 - val_acc: 0.9352\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8886\n",
      "Epoch 00180: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3553 - acc: 0.8887 - val_loss: 0.2447 - val_acc: 0.9322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.8898\n",
      "Epoch 00181: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3513 - acc: 0.8898 - val_loss: 0.2356 - val_acc: 0.9355\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8873\n",
      "Epoch 00182: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3544 - acc: 0.8873 - val_loss: 0.2552 - val_acc: 0.9297\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8897\n",
      "Epoch 00183: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3482 - acc: 0.8897 - val_loss: 0.2444 - val_acc: 0.9343\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8896\n",
      "Epoch 00184: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3459 - acc: 0.8897 - val_loss: 0.2417 - val_acc: 0.9355\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8877\n",
      "Epoch 00185: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3500 - acc: 0.8877 - val_loss: 0.2431 - val_acc: 0.9357\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8880\n",
      "Epoch 00186: val_loss did not improve from 0.23377\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3500 - acc: 0.8880 - val_loss: 0.2431 - val_acc: 0.9334\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8906\n",
      "Epoch 00187: val_loss improved from 0.23377 to 0.23158, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/187-0.2316.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3461 - acc: 0.8906 - val_loss: 0.2316 - val_acc: 0.9366\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8910\n",
      "Epoch 00188: val_loss did not improve from 0.23158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3405 - acc: 0.8910 - val_loss: 0.2342 - val_acc: 0.9366\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8910\n",
      "Epoch 00189: val_loss did not improve from 0.23158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3387 - acc: 0.8909 - val_loss: 0.2394 - val_acc: 0.9343\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8899\n",
      "Epoch 00190: val_loss did not improve from 0.23158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3438 - acc: 0.8900 - val_loss: 0.2431 - val_acc: 0.9336\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8907\n",
      "Epoch 00191: val_loss did not improve from 0.23158\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3415 - acc: 0.8907 - val_loss: 0.2368 - val_acc: 0.9371\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8918\n",
      "Epoch 00192: val_loss improved from 0.23158 to 0.23075, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/192-0.2308.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3380 - acc: 0.8918 - val_loss: 0.2308 - val_acc: 0.9350\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8920\n",
      "Epoch 00193: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3372 - acc: 0.8920 - val_loss: 0.2445 - val_acc: 0.9341\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8925\n",
      "Epoch 00194: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3321 - acc: 0.8925 - val_loss: 0.2363 - val_acc: 0.9373\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8929\n",
      "Epoch 00195: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3416 - acc: 0.8929 - val_loss: 0.2415 - val_acc: 0.9345\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8953\n",
      "Epoch 00196: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3299 - acc: 0.8953 - val_loss: 0.2474 - val_acc: 0.9315\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8922\n",
      "Epoch 00197: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3317 - acc: 0.8922 - val_loss: 0.2411 - val_acc: 0.9338\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8945\n",
      "Epoch 00198: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3322 - acc: 0.8945 - val_loss: 0.2473 - val_acc: 0.9331\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8938\n",
      "Epoch 00199: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3289 - acc: 0.8938 - val_loss: 0.2444 - val_acc: 0.9348\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8948\n",
      "Epoch 00200: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3306 - acc: 0.8948 - val_loss: 0.2355 - val_acc: 0.9355\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8931\n",
      "Epoch 00201: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3298 - acc: 0.8931 - val_loss: 0.2329 - val_acc: 0.9399\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8941\n",
      "Epoch 00202: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3275 - acc: 0.8941 - val_loss: 0.2380 - val_acc: 0.9390\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8946\n",
      "Epoch 00203: val_loss did not improve from 0.23075\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3318 - acc: 0.8947 - val_loss: 0.2434 - val_acc: 0.9334\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8963\n",
      "Epoch 00204: val_loss improved from 0.23075 to 0.22915, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/204-0.2292.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3254 - acc: 0.8963 - val_loss: 0.2292 - val_acc: 0.9362\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8963\n",
      "Epoch 00205: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3262 - acc: 0.8963 - val_loss: 0.2355 - val_acc: 0.9355\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8961\n",
      "Epoch 00206: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3232 - acc: 0.8962 - val_loss: 0.2344 - val_acc: 0.9364\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.8985\n",
      "Epoch 00207: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3170 - acc: 0.8985 - val_loss: 0.2415 - val_acc: 0.9334\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8933\n",
      "Epoch 00208: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3306 - acc: 0.8932 - val_loss: 0.2396 - val_acc: 0.9322\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8980\n",
      "Epoch 00209: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3145 - acc: 0.8980 - val_loss: 0.2436 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8962\n",
      "Epoch 00210: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3209 - acc: 0.8962 - val_loss: 0.2452 - val_acc: 0.9352\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8966\n",
      "Epoch 00211: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3228 - acc: 0.8966 - val_loss: 0.2426 - val_acc: 0.9343\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8944\n",
      "Epoch 00212: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3231 - acc: 0.8944 - val_loss: 0.2401 - val_acc: 0.9366\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8994\n",
      "Epoch 00213: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3148 - acc: 0.8994 - val_loss: 0.2446 - val_acc: 0.9327\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8989\n",
      "Epoch 00214: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3176 - acc: 0.8989 - val_loss: 0.2501 - val_acc: 0.9366\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.8964\n",
      "Epoch 00215: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3189 - acc: 0.8965 - val_loss: 0.2392 - val_acc: 0.9320\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8981\n",
      "Epoch 00216: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3196 - acc: 0.8981 - val_loss: 0.2387 - val_acc: 0.9327\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8985\n",
      "Epoch 00217: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3138 - acc: 0.8985 - val_loss: 0.2479 - val_acc: 0.9352\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8990\n",
      "Epoch 00218: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3148 - acc: 0.8990 - val_loss: 0.2396 - val_acc: 0.9341\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.8995\n",
      "Epoch 00219: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3124 - acc: 0.8994 - val_loss: 0.2332 - val_acc: 0.9355\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9011\n",
      "Epoch 00220: val_loss improved from 0.22915 to 0.22767, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/220-0.2277.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3094 - acc: 0.9012 - val_loss: 0.2277 - val_acc: 0.9371\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9014\n",
      "Epoch 00221: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3052 - acc: 0.9014 - val_loss: 0.2285 - val_acc: 0.9366\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8992\n",
      "Epoch 00222: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3143 - acc: 0.8992 - val_loss: 0.2296 - val_acc: 0.9373\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9000\n",
      "Epoch 00223: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3071 - acc: 0.9000 - val_loss: 0.2309 - val_acc: 0.9387\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9007\n",
      "Epoch 00224: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3081 - acc: 0.9007 - val_loss: 0.2452 - val_acc: 0.9369\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9005\n",
      "Epoch 00225: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3066 - acc: 0.9005 - val_loss: 0.2477 - val_acc: 0.9338\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9002\n",
      "Epoch 00226: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3063 - acc: 0.9002 - val_loss: 0.2343 - val_acc: 0.9376\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8984\n",
      "Epoch 00227: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3094 - acc: 0.8984 - val_loss: 0.2404 - val_acc: 0.9380\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9021\n",
      "Epoch 00228: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3021 - acc: 0.9022 - val_loss: 0.2576 - val_acc: 0.9306\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9022\n",
      "Epoch 00229: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3019 - acc: 0.9021 - val_loss: 0.2511 - val_acc: 0.9336\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9038\n",
      "Epoch 00230: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2958 - acc: 0.9038 - val_loss: 0.2509 - val_acc: 0.9359\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9043\n",
      "Epoch 00231: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2981 - acc: 0.9043 - val_loss: 0.2347 - val_acc: 0.9401\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9036\n",
      "Epoch 00232: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3002 - acc: 0.9036 - val_loss: 0.2333 - val_acc: 0.9390\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9013\n",
      "Epoch 00233: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3010 - acc: 0.9013 - val_loss: 0.2331 - val_acc: 0.9385\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9020\n",
      "Epoch 00234: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3007 - acc: 0.9020 - val_loss: 0.2321 - val_acc: 0.9385\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9019\n",
      "Epoch 00235: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2993 - acc: 0.9019 - val_loss: 0.2329 - val_acc: 0.9373\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9027\n",
      "Epoch 00236: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2977 - acc: 0.9027 - val_loss: 0.2373 - val_acc: 0.9378\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9024\n",
      "Epoch 00237: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3021 - acc: 0.9025 - val_loss: 0.2530 - val_acc: 0.9371\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9038\n",
      "Epoch 00238: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2989 - acc: 0.9038 - val_loss: 0.2322 - val_acc: 0.9383\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9064\n",
      "Epoch 00239: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2915 - acc: 0.9064 - val_loss: 0.2364 - val_acc: 0.9385\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9017\n",
      "Epoch 00240: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2976 - acc: 0.9017 - val_loss: 0.2482 - val_acc: 0.9362\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9047\n",
      "Epoch 00241: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2927 - acc: 0.9047 - val_loss: 0.2358 - val_acc: 0.9399\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9026\n",
      "Epoch 00242: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3002 - acc: 0.9026 - val_loss: 0.2307 - val_acc: 0.9387\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9085\n",
      "Epoch 00243: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2861 - acc: 0.9085 - val_loss: 0.2637 - val_acc: 0.9343\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9078\n",
      "Epoch 00244: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2888 - acc: 0.9077 - val_loss: 0.2295 - val_acc: 0.9380\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9041\n",
      "Epoch 00245: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2906 - acc: 0.9041 - val_loss: 0.2427 - val_acc: 0.9357\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9062\n",
      "Epoch 00246: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2918 - acc: 0.9062 - val_loss: 0.2354 - val_acc: 0.9371\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9067\n",
      "Epoch 00247: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2913 - acc: 0.9067 - val_loss: 0.2635 - val_acc: 0.9324\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9054\n",
      "Epoch 00248: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2915 - acc: 0.9054 - val_loss: 0.2467 - val_acc: 0.9385\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9075\n",
      "Epoch 00249: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2849 - acc: 0.9075 - val_loss: 0.2415 - val_acc: 0.9380\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9070\n",
      "Epoch 00250: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2828 - acc: 0.9070 - val_loss: 0.2425 - val_acc: 0.9380\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9090\n",
      "Epoch 00251: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2798 - acc: 0.9090 - val_loss: 0.2350 - val_acc: 0.9383\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9058\n",
      "Epoch 00252: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2885 - acc: 0.9058 - val_loss: 0.2397 - val_acc: 0.9357\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9090\n",
      "Epoch 00253: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2792 - acc: 0.9090 - val_loss: 0.2352 - val_acc: 0.9399\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9072\n",
      "Epoch 00254: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2859 - acc: 0.9072 - val_loss: 0.2311 - val_acc: 0.9401\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9055\n",
      "Epoch 00255: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2840 - acc: 0.9055 - val_loss: 0.2447 - val_acc: 0.9387\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9076\n",
      "Epoch 00256: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2813 - acc: 0.9076 - val_loss: 0.2399 - val_acc: 0.9362\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9060\n",
      "Epoch 00257: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2885 - acc: 0.9060 - val_loss: 0.2396 - val_acc: 0.9369\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9086\n",
      "Epoch 00258: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2791 - acc: 0.9086 - val_loss: 0.2399 - val_acc: 0.9366\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9083\n",
      "Epoch 00259: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2814 - acc: 0.9083 - val_loss: 0.2386 - val_acc: 0.9397\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9085\n",
      "Epoch 00260: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2759 - acc: 0.9085 - val_loss: 0.2390 - val_acc: 0.9359\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9070\n",
      "Epoch 00261: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2819 - acc: 0.9070 - val_loss: 0.2419 - val_acc: 0.9366\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9100\n",
      "Epoch 00262: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2766 - acc: 0.9100 - val_loss: 0.2484 - val_acc: 0.9387\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9076\n",
      "Epoch 00263: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2807 - acc: 0.9076 - val_loss: 0.2378 - val_acc: 0.9399\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9096\n",
      "Epoch 00264: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2743 - acc: 0.9096 - val_loss: 0.2573 - val_acc: 0.9350\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9090\n",
      "Epoch 00265: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2805 - acc: 0.9090 - val_loss: 0.2456 - val_acc: 0.9371\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9083\n",
      "Epoch 00266: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2817 - acc: 0.9083 - val_loss: 0.2463 - val_acc: 0.9385\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9102\n",
      "Epoch 00267: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2724 - acc: 0.9102 - val_loss: 0.2554 - val_acc: 0.9341\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9107\n",
      "Epoch 00268: val_loss did not improve from 0.22767\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2740 - acc: 0.9107 - val_loss: 0.2468 - val_acc: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9107\n",
      "Epoch 00269: val_loss improved from 0.22767 to 0.22550, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_8_conv_checkpoint/269-0.2255.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2771 - acc: 0.9107 - val_loss: 0.2255 - val_acc: 0.9380\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9098\n",
      "Epoch 00270: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2762 - acc: 0.9098 - val_loss: 0.2531 - val_acc: 0.9359\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9108\n",
      "Epoch 00271: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2714 - acc: 0.9108 - val_loss: 0.2430 - val_acc: 0.9357\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9095\n",
      "Epoch 00272: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2780 - acc: 0.9095 - val_loss: 0.2443 - val_acc: 0.9378\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9121\n",
      "Epoch 00273: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2689 - acc: 0.9122 - val_loss: 0.2375 - val_acc: 0.9392\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9096\n",
      "Epoch 00274: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2711 - acc: 0.9096 - val_loss: 0.2480 - val_acc: 0.9366\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9133\n",
      "Epoch 00275: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2683 - acc: 0.9133 - val_loss: 0.2385 - val_acc: 0.9376\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9127\n",
      "Epoch 00276: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2683 - acc: 0.9127 - val_loss: 0.2553 - val_acc: 0.9352\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9110\n",
      "Epoch 00277: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2729 - acc: 0.9110 - val_loss: 0.2437 - val_acc: 0.9329\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9128\n",
      "Epoch 00278: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2632 - acc: 0.9128 - val_loss: 0.2365 - val_acc: 0.9390\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9127\n",
      "Epoch 00279: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2675 - acc: 0.9127 - val_loss: 0.2448 - val_acc: 0.9357\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9129\n",
      "Epoch 00280: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2677 - acc: 0.9129 - val_loss: 0.2521 - val_acc: 0.9364\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9161\n",
      "Epoch 00281: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2583 - acc: 0.9162 - val_loss: 0.2477 - val_acc: 0.9364\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9122\n",
      "Epoch 00282: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2687 - acc: 0.9122 - val_loss: 0.2347 - val_acc: 0.9376\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9129\n",
      "Epoch 00283: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2672 - acc: 0.9129 - val_loss: 0.2381 - val_acc: 0.9378\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9122\n",
      "Epoch 00284: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2657 - acc: 0.9122 - val_loss: 0.2703 - val_acc: 0.9362\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9133\n",
      "Epoch 00285: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2620 - acc: 0.9134 - val_loss: 0.2496 - val_acc: 0.9369\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9160\n",
      "Epoch 00286: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2573 - acc: 0.9160 - val_loss: 0.2407 - val_acc: 0.9380\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9108\n",
      "Epoch 00287: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2672 - acc: 0.9107 - val_loss: 0.2395 - val_acc: 0.9392\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9142\n",
      "Epoch 00288: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2648 - acc: 0.9142 - val_loss: 0.2541 - val_acc: 0.9348\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9145\n",
      "Epoch 00289: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2579 - acc: 0.9145 - val_loss: 0.2530 - val_acc: 0.9324\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9170\n",
      "Epoch 00290: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2589 - acc: 0.9170 - val_loss: 0.2398 - val_acc: 0.9364\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9150\n",
      "Epoch 00291: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2586 - acc: 0.9150 - val_loss: 0.2362 - val_acc: 0.9371\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9161\n",
      "Epoch 00292: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2552 - acc: 0.9161 - val_loss: 0.2535 - val_acc: 0.9338\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9125\n",
      "Epoch 00293: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2617 - acc: 0.9125 - val_loss: 0.2471 - val_acc: 0.9357\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9162\n",
      "Epoch 00294: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2539 - acc: 0.9163 - val_loss: 0.2444 - val_acc: 0.9362\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9159\n",
      "Epoch 00295: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2575 - acc: 0.9159 - val_loss: 0.2524 - val_acc: 0.9345\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9151\n",
      "Epoch 00296: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2604 - acc: 0.9151 - val_loss: 0.2450 - val_acc: 0.9345\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9163\n",
      "Epoch 00297: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2541 - acc: 0.9163 - val_loss: 0.2471 - val_acc: 0.9387\n",
      "Epoch 298/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9164\n",
      "Epoch 00298: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2525 - acc: 0.9164 - val_loss: 0.2504 - val_acc: 0.9387\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9154\n",
      "Epoch 00299: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2545 - acc: 0.9154 - val_loss: 0.2501 - val_acc: 0.9380\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9172\n",
      "Epoch 00300: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2536 - acc: 0.9172 - val_loss: 0.2464 - val_acc: 0.9364\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9163\n",
      "Epoch 00301: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2545 - acc: 0.9163 - val_loss: 0.2556 - val_acc: 0.9350\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9162\n",
      "Epoch 00302: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2560 - acc: 0.9162 - val_loss: 0.2443 - val_acc: 0.9373\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9151\n",
      "Epoch 00303: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2575 - acc: 0.9151 - val_loss: 0.2400 - val_acc: 0.9392\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9180\n",
      "Epoch 00304: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2481 - acc: 0.9181 - val_loss: 0.2401 - val_acc: 0.9366\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9168\n",
      "Epoch 00305: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2541 - acc: 0.9168 - val_loss: 0.2402 - val_acc: 0.9397\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9169\n",
      "Epoch 00306: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2511 - acc: 0.9169 - val_loss: 0.2553 - val_acc: 0.9399\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9193\n",
      "Epoch 00307: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2461 - acc: 0.9193 - val_loss: 0.2480 - val_acc: 0.9387\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9183\n",
      "Epoch 00308: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2484 - acc: 0.9183 - val_loss: 0.2489 - val_acc: 0.9366\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9176\n",
      "Epoch 00309: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2517 - acc: 0.9176 - val_loss: 0.2444 - val_acc: 0.9383\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9175\n",
      "Epoch 00310: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2503 - acc: 0.9175 - val_loss: 0.2456 - val_acc: 0.9366\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9174\n",
      "Epoch 00311: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2508 - acc: 0.9173 - val_loss: 0.2466 - val_acc: 0.9331\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9158\n",
      "Epoch 00312: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2504 - acc: 0.9158 - val_loss: 0.2580 - val_acc: 0.9390\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9208\n",
      "Epoch 00313: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2444 - acc: 0.9207 - val_loss: 0.2553 - val_acc: 0.9406\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9188\n",
      "Epoch 00314: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2447 - acc: 0.9188 - val_loss: 0.2515 - val_acc: 0.9366\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9193\n",
      "Epoch 00315: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2413 - acc: 0.9193 - val_loss: 0.2551 - val_acc: 0.9352\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9174\n",
      "Epoch 00316: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2507 - acc: 0.9174 - val_loss: 0.2432 - val_acc: 0.9406\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9186\n",
      "Epoch 00317: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2430 - acc: 0.9185 - val_loss: 0.2578 - val_acc: 0.9399\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9187\n",
      "Epoch 00318: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2470 - acc: 0.9187 - val_loss: 0.2624 - val_acc: 0.9345\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9193\n",
      "Epoch 00319: val_loss did not improve from 0.22550\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2461 - acc: 0.9193 - val_loss: 0.2597 - val_acc: 0.9413\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 585us/sample - loss: 0.2784 - acc: 0.9211\n",
      "Loss: 0.278405804023812 Accuracy: 0.92107993\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6741 - acc: 0.1110\n",
      "Epoch 00001: val_loss improved from inf to 2.40911, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/001-2.4091.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.6742 - acc: 0.1110 - val_loss: 2.4091 - val_acc: 0.2432\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4087 - acc: 0.2074\n",
      "Epoch 00002: val_loss improved from 2.40911 to 2.08391, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/002-2.0839.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.4086 - acc: 0.2074 - val_loss: 2.0839 - val_acc: 0.3573\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1762 - acc: 0.2779\n",
      "Epoch 00003: val_loss improved from 2.08391 to 1.81384, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/003-1.8138.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.1761 - acc: 0.2779 - val_loss: 1.8138 - val_acc: 0.4342\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9880 - acc: 0.3315\n",
      "Epoch 00004: val_loss improved from 1.81384 to 1.62349, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/004-1.6235.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.9880 - acc: 0.3315 - val_loss: 1.6235 - val_acc: 0.4894\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8399 - acc: 0.3778\n",
      "Epoch 00005: val_loss improved from 1.62349 to 1.47702, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/005-1.4770.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.8399 - acc: 0.3778 - val_loss: 1.4770 - val_acc: 0.5365\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7330 - acc: 0.4177\n",
      "Epoch 00006: val_loss improved from 1.47702 to 1.38527, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/006-1.3853.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.7331 - acc: 0.4177 - val_loss: 1.3853 - val_acc: 0.5600\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6644 - acc: 0.4402\n",
      "Epoch 00007: val_loss improved from 1.38527 to 1.28270, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/007-1.2827.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.6644 - acc: 0.4402 - val_loss: 1.2827 - val_acc: 0.5991\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5801 - acc: 0.4642\n",
      "Epoch 00008: val_loss improved from 1.28270 to 1.21701, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/008-1.2170.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5801 - acc: 0.4642 - val_loss: 1.2170 - val_acc: 0.6152\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5235 - acc: 0.4851\n",
      "Epoch 00009: val_loss improved from 1.21701 to 1.15916, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/009-1.1592.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5236 - acc: 0.4850 - val_loss: 1.1592 - val_acc: 0.6373\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4698 - acc: 0.5025\n",
      "Epoch 00010: val_loss improved from 1.15916 to 1.09362, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/010-1.0936.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4697 - acc: 0.5025 - val_loss: 1.0936 - val_acc: 0.6536\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4311 - acc: 0.5131\n",
      "Epoch 00011: val_loss improved from 1.09362 to 1.04762, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/011-1.0476.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4311 - acc: 0.5131 - val_loss: 1.0476 - val_acc: 0.6683\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3895 - acc: 0.5281\n",
      "Epoch 00012: val_loss improved from 1.04762 to 0.99729, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/012-0.9973.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3894 - acc: 0.5281 - val_loss: 0.9973 - val_acc: 0.6811\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3449 - acc: 0.5442\n",
      "Epoch 00013: val_loss improved from 0.99729 to 0.99236, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/013-0.9924.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3448 - acc: 0.5442 - val_loss: 0.9924 - val_acc: 0.6853\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3193 - acc: 0.5517\n",
      "Epoch 00014: val_loss improved from 0.99236 to 0.95894, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/014-0.9589.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3192 - acc: 0.5517 - val_loss: 0.9589 - val_acc: 0.7028\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2849 - acc: 0.5647\n",
      "Epoch 00015: val_loss improved from 0.95894 to 0.94286, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/015-0.9429.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2849 - acc: 0.5647 - val_loss: 0.9429 - val_acc: 0.7130\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2393 - acc: 0.5807\n",
      "Epoch 00016: val_loss improved from 0.94286 to 0.91782, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/016-0.9178.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2392 - acc: 0.5807 - val_loss: 0.9178 - val_acc: 0.7160\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2329 - acc: 0.5841\n",
      "Epoch 00017: val_loss improved from 0.91782 to 0.84935, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/017-0.8494.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2328 - acc: 0.5841 - val_loss: 0.8494 - val_acc: 0.7414\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2038 - acc: 0.5961\n",
      "Epoch 00018: val_loss improved from 0.84935 to 0.81297, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/018-0.8130.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2038 - acc: 0.5961 - val_loss: 0.8130 - val_acc: 0.7510\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1766 - acc: 0.6048\n",
      "Epoch 00019: val_loss did not improve from 0.81297\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1765 - acc: 0.6049 - val_loss: 0.8440 - val_acc: 0.7342\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1546 - acc: 0.6130\n",
      "Epoch 00020: val_loss improved from 0.81297 to 0.78029, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/020-0.7803.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1546 - acc: 0.6130 - val_loss: 0.7803 - val_acc: 0.7594\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1214 - acc: 0.6196\n",
      "Epoch 00021: val_loss improved from 0.78029 to 0.75743, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/021-0.7574.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1213 - acc: 0.6196 - val_loss: 0.7574 - val_acc: 0.7657\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6322\n",
      "Epoch 00022: val_loss improved from 0.75743 to 0.74374, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/022-0.7437.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0950 - acc: 0.6322 - val_loss: 0.7437 - val_acc: 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0831 - acc: 0.6370\n",
      "Epoch 00023: val_loss did not improve from 0.74374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0830 - acc: 0.6370 - val_loss: 0.7485 - val_acc: 0.7727\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0578 - acc: 0.6454\n",
      "Epoch 00024: val_loss improved from 0.74374 to 0.67473, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/024-0.6747.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0578 - acc: 0.6454 - val_loss: 0.6747 - val_acc: 0.7915\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0379 - acc: 0.6540\n",
      "Epoch 00025: val_loss improved from 0.67473 to 0.64781, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/025-0.6478.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0379 - acc: 0.6540 - val_loss: 0.6478 - val_acc: 0.7952\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0223 - acc: 0.6552\n",
      "Epoch 00026: val_loss improved from 0.64781 to 0.62817, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/026-0.6282.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0222 - acc: 0.6552 - val_loss: 0.6282 - val_acc: 0.8071\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9973 - acc: 0.6662\n",
      "Epoch 00027: val_loss did not improve from 0.62817\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9973 - acc: 0.6663 - val_loss: 0.6284 - val_acc: 0.8074\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9859 - acc: 0.6683\n",
      "Epoch 00028: val_loss improved from 0.62817 to 0.61506, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/028-0.6151.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9858 - acc: 0.6683 - val_loss: 0.6151 - val_acc: 0.8113\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9696 - acc: 0.6746\n",
      "Epoch 00029: val_loss improved from 0.61506 to 0.59356, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/029-0.5936.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9695 - acc: 0.6746 - val_loss: 0.5936 - val_acc: 0.8181\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9594 - acc: 0.6790\n",
      "Epoch 00030: val_loss improved from 0.59356 to 0.57386, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/030-0.5739.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9594 - acc: 0.6790 - val_loss: 0.5739 - val_acc: 0.8248\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9436 - acc: 0.6847\n",
      "Epoch 00031: val_loss improved from 0.57386 to 0.56780, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/031-0.5678.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9435 - acc: 0.6847 - val_loss: 0.5678 - val_acc: 0.8297\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9248 - acc: 0.6941\n",
      "Epoch 00032: val_loss improved from 0.56780 to 0.54747, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/032-0.5475.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9248 - acc: 0.6941 - val_loss: 0.5475 - val_acc: 0.8316\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9144 - acc: 0.6970\n",
      "Epoch 00033: val_loss did not improve from 0.54747\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9145 - acc: 0.6971 - val_loss: 0.5620 - val_acc: 0.8232\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.6990\n",
      "Epoch 00034: val_loss improved from 0.54747 to 0.53160, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/034-0.5316.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9049 - acc: 0.6991 - val_loss: 0.5316 - val_acc: 0.8360\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8890 - acc: 0.7030\n",
      "Epoch 00035: val_loss did not improve from 0.53160\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8891 - acc: 0.7030 - val_loss: 0.5407 - val_acc: 0.8297\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7081\n",
      "Epoch 00036: val_loss did not improve from 0.53160\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8773 - acc: 0.7081 - val_loss: 0.5334 - val_acc: 0.8348\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8706 - acc: 0.7106\n",
      "Epoch 00037: val_loss improved from 0.53160 to 0.49202, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/037-0.4920.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8706 - acc: 0.7106 - val_loss: 0.4920 - val_acc: 0.8500\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8579 - acc: 0.7146\n",
      "Epoch 00038: val_loss did not improve from 0.49202\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8578 - acc: 0.7146 - val_loss: 0.4991 - val_acc: 0.8488\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8433 - acc: 0.7177\n",
      "Epoch 00039: val_loss did not improve from 0.49202\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8432 - acc: 0.7177 - val_loss: 0.5028 - val_acc: 0.8428\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8302 - acc: 0.7226\n",
      "Epoch 00040: val_loss improved from 0.49202 to 0.47945, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/040-0.4795.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8302 - acc: 0.7225 - val_loss: 0.4795 - val_acc: 0.8488\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8245 - acc: 0.7250\n",
      "Epoch 00041: val_loss improved from 0.47945 to 0.47408, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/041-0.4741.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8245 - acc: 0.7250 - val_loss: 0.4741 - val_acc: 0.8556\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7262\n",
      "Epoch 00042: val_loss improved from 0.47408 to 0.46493, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/042-0.4649.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8227 - acc: 0.7262 - val_loss: 0.4649 - val_acc: 0.8567\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.7319\n",
      "Epoch 00043: val_loss improved from 0.46493 to 0.44654, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/043-0.4465.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8091 - acc: 0.7319 - val_loss: 0.4465 - val_acc: 0.8612\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7875 - acc: 0.7348\n",
      "Epoch 00044: val_loss improved from 0.44654 to 0.44485, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/044-0.4449.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7874 - acc: 0.7348 - val_loss: 0.4449 - val_acc: 0.8614\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7837 - acc: 0.7360\n",
      "Epoch 00045: val_loss did not improve from 0.44485\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7836 - acc: 0.7360 - val_loss: 0.4485 - val_acc: 0.8577\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7827 - acc: 0.7386\n",
      "Epoch 00046: val_loss did not improve from 0.44485\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7826 - acc: 0.7386 - val_loss: 0.4492 - val_acc: 0.8623\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7809 - acc: 0.7379\n",
      "Epoch 00047: val_loss improved from 0.44485 to 0.44154, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/047-0.4415.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7809 - acc: 0.7379 - val_loss: 0.4415 - val_acc: 0.8658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7454\n",
      "Epoch 00048: val_loss improved from 0.44154 to 0.42331, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/048-0.4233.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7659 - acc: 0.7453 - val_loss: 0.4233 - val_acc: 0.8714\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7477\n",
      "Epoch 00049: val_loss improved from 0.42331 to 0.41267, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/049-0.4127.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7557 - acc: 0.7477 - val_loss: 0.4127 - val_acc: 0.8726\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7530 - acc: 0.7482\n",
      "Epoch 00050: val_loss did not improve from 0.41267\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7529 - acc: 0.7482 - val_loss: 0.4287 - val_acc: 0.8651\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7456 - acc: 0.7513\n",
      "Epoch 00051: val_loss did not improve from 0.41267\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7455 - acc: 0.7513 - val_loss: 0.4147 - val_acc: 0.8682\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7539\n",
      "Epoch 00052: val_loss improved from 0.41267 to 0.41259, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/052-0.4126.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7393 - acc: 0.7539 - val_loss: 0.4126 - val_acc: 0.8714\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7287 - acc: 0.7571\n",
      "Epoch 00053: val_loss improved from 0.41259 to 0.40060, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/053-0.4006.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7288 - acc: 0.7570 - val_loss: 0.4006 - val_acc: 0.8765\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.7616\n",
      "Epoch 00054: val_loss did not improve from 0.40060\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7170 - acc: 0.7616 - val_loss: 0.4084 - val_acc: 0.8726\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7148 - acc: 0.7586\n",
      "Epoch 00055: val_loss improved from 0.40060 to 0.38893, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/055-0.3889.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7148 - acc: 0.7586 - val_loss: 0.3889 - val_acc: 0.8852\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7124 - acc: 0.7630\n",
      "Epoch 00056: val_loss improved from 0.38893 to 0.38453, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/056-0.3845.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7125 - acc: 0.7630 - val_loss: 0.3845 - val_acc: 0.8796\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.7645\n",
      "Epoch 00057: val_loss did not improve from 0.38453\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7039 - acc: 0.7645 - val_loss: 0.4030 - val_acc: 0.8740\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.7674\n",
      "Epoch 00058: val_loss did not improve from 0.38453\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6932 - acc: 0.7674 - val_loss: 0.4027 - val_acc: 0.8779\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.7667\n",
      "Epoch 00059: val_loss improved from 0.38453 to 0.37923, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/059-0.3792.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7044 - acc: 0.7667 - val_loss: 0.3792 - val_acc: 0.8847\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.7693\n",
      "Epoch 00060: val_loss improved from 0.37923 to 0.37187, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/060-0.3719.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6917 - acc: 0.7693 - val_loss: 0.3719 - val_acc: 0.8803\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.7717\n",
      "Epoch 00061: val_loss did not improve from 0.37187\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6796 - acc: 0.7717 - val_loss: 0.3746 - val_acc: 0.8838\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.7777\n",
      "Epoch 00062: val_loss did not improve from 0.37187\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6647 - acc: 0.7777 - val_loss: 0.3921 - val_acc: 0.8838\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6682 - acc: 0.7760\n",
      "Epoch 00063: val_loss did not improve from 0.37187\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6681 - acc: 0.7761 - val_loss: 0.3790 - val_acc: 0.8805\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6670 - acc: 0.7775\n",
      "Epoch 00064: val_loss improved from 0.37187 to 0.36899, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/064-0.3690.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6670 - acc: 0.7775 - val_loss: 0.3690 - val_acc: 0.8838\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.7806\n",
      "Epoch 00065: val_loss improved from 0.36899 to 0.35667, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/065-0.3567.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6611 - acc: 0.7806 - val_loss: 0.3567 - val_acc: 0.8915\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6557 - acc: 0.7834\n",
      "Epoch 00066: val_loss improved from 0.35667 to 0.35432, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/066-0.3543.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6557 - acc: 0.7834 - val_loss: 0.3543 - val_acc: 0.8894\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6558 - acc: 0.7815\n",
      "Epoch 00067: val_loss did not improve from 0.35432\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6558 - acc: 0.7815 - val_loss: 0.3591 - val_acc: 0.8901\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6481 - acc: 0.7839\n",
      "Epoch 00068: val_loss did not improve from 0.35432\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6482 - acc: 0.7838 - val_loss: 0.3567 - val_acc: 0.8905\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.7871\n",
      "Epoch 00069: val_loss improved from 0.35432 to 0.34734, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/069-0.3473.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6424 - acc: 0.7871 - val_loss: 0.3473 - val_acc: 0.8917\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6411 - acc: 0.7869\n",
      "Epoch 00070: val_loss did not improve from 0.34734\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6411 - acc: 0.7869 - val_loss: 0.3540 - val_acc: 0.8998\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.7883\n",
      "Epoch 00071: val_loss improved from 0.34734 to 0.34602, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/071-0.3460.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6366 - acc: 0.7883 - val_loss: 0.3460 - val_acc: 0.8996\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6298 - acc: 0.7914\n",
      "Epoch 00072: val_loss improved from 0.34602 to 0.34021, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/072-0.3402.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6298 - acc: 0.7914 - val_loss: 0.3402 - val_acc: 0.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.7916\n",
      "Epoch 00073: val_loss did not improve from 0.34021\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6245 - acc: 0.7916 - val_loss: 0.3458 - val_acc: 0.8968\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6152 - acc: 0.7969\n",
      "Epoch 00074: val_loss did not improve from 0.34021\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6152 - acc: 0.7969 - val_loss: 0.3453 - val_acc: 0.8938\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6190 - acc: 0.7936\n",
      "Epoch 00075: val_loss did not improve from 0.34021\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6190 - acc: 0.7936 - val_loss: 0.3452 - val_acc: 0.8994\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.7977\n",
      "Epoch 00076: val_loss did not improve from 0.34021\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6101 - acc: 0.7978 - val_loss: 0.3567 - val_acc: 0.8908\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.7995\n",
      "Epoch 00077: val_loss improved from 0.34021 to 0.32966, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/077-0.3297.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6009 - acc: 0.7995 - val_loss: 0.3297 - val_acc: 0.9031\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5990 - acc: 0.7998\n",
      "Epoch 00078: val_loss did not improve from 0.32966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5991 - acc: 0.7998 - val_loss: 0.3607 - val_acc: 0.8938\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.8018\n",
      "Epoch 00079: val_loss did not improve from 0.32966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6047 - acc: 0.8018 - val_loss: 0.3329 - val_acc: 0.9005\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.8014\n",
      "Epoch 00080: val_loss did not improve from 0.32966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5975 - acc: 0.8013 - val_loss: 0.3313 - val_acc: 0.9003\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8024\n",
      "Epoch 00081: val_loss improved from 0.32966 to 0.32145, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/081-0.3214.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5992 - acc: 0.8024 - val_loss: 0.3214 - val_acc: 0.9064\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8052\n",
      "Epoch 00082: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5861 - acc: 0.8052 - val_loss: 0.3383 - val_acc: 0.9045\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.8058\n",
      "Epoch 00083: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5833 - acc: 0.8058 - val_loss: 0.3288 - val_acc: 0.9059\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5814 - acc: 0.8076\n",
      "Epoch 00084: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5816 - acc: 0.8076 - val_loss: 0.3338 - val_acc: 0.8991\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.8087\n",
      "Epoch 00085: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5802 - acc: 0.8087 - val_loss: 0.3224 - val_acc: 0.9061\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8111\n",
      "Epoch 00086: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5707 - acc: 0.8111 - val_loss: 0.3310 - val_acc: 0.9031\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8111\n",
      "Epoch 00087: val_loss improved from 0.32145 to 0.31704, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/087-0.3170.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5678 - acc: 0.8112 - val_loss: 0.3170 - val_acc: 0.9064\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8109\n",
      "Epoch 00088: val_loss did not improve from 0.31704\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5691 - acc: 0.8109 - val_loss: 0.3319 - val_acc: 0.9026\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8149\n",
      "Epoch 00089: val_loss improved from 0.31704 to 0.31686, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/089-0.3169.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5616 - acc: 0.8149 - val_loss: 0.3169 - val_acc: 0.9057\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.8122\n",
      "Epoch 00090: val_loss improved from 0.31686 to 0.31247, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/090-0.3125.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5642 - acc: 0.8122 - val_loss: 0.3125 - val_acc: 0.9075\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8175\n",
      "Epoch 00091: val_loss did not improve from 0.31247\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5571 - acc: 0.8175 - val_loss: 0.3195 - val_acc: 0.9075\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8176\n",
      "Epoch 00092: val_loss did not improve from 0.31247\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5524 - acc: 0.8176 - val_loss: 0.3158 - val_acc: 0.9106\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.8188\n",
      "Epoch 00093: val_loss improved from 0.31247 to 0.30165, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/093-0.3016.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5523 - acc: 0.8188 - val_loss: 0.3016 - val_acc: 0.9117\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8180\n",
      "Epoch 00094: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5471 - acc: 0.8180 - val_loss: 0.3020 - val_acc: 0.9110\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.8215\n",
      "Epoch 00095: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5403 - acc: 0.8215 - val_loss: 0.3189 - val_acc: 0.9038\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.8220\n",
      "Epoch 00096: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5414 - acc: 0.8220 - val_loss: 0.3044 - val_acc: 0.9103\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8223\n",
      "Epoch 00097: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5439 - acc: 0.8223 - val_loss: 0.3057 - val_acc: 0.9122\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8229\n",
      "Epoch 00098: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5348 - acc: 0.8229 - val_loss: 0.3206 - val_acc: 0.9096\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.8257\n",
      "Epoch 00099: val_loss improved from 0.30165 to 0.29850, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/099-0.2985.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5333 - acc: 0.8257 - val_loss: 0.2985 - val_acc: 0.9108\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.8289\n",
      "Epoch 00100: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5193 - acc: 0.8289 - val_loss: 0.3045 - val_acc: 0.9085\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8254\n",
      "Epoch 00101: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5245 - acc: 0.8253 - val_loss: 0.3024 - val_acc: 0.9110\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8258\n",
      "Epoch 00102: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5225 - acc: 0.8258 - val_loss: 0.3127 - val_acc: 0.9080\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8284\n",
      "Epoch 00103: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5151 - acc: 0.8284 - val_loss: 0.3044 - val_acc: 0.9119\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8295\n",
      "Epoch 00104: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5150 - acc: 0.8295 - val_loss: 0.3078 - val_acc: 0.9106\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.8311\n",
      "Epoch 00105: val_loss did not improve from 0.29850\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5119 - acc: 0.8312 - val_loss: 0.3109 - val_acc: 0.9094\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.8318\n",
      "Epoch 00106: val_loss improved from 0.29850 to 0.29811, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/106-0.2981.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5081 - acc: 0.8318 - val_loss: 0.2981 - val_acc: 0.9124\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8327\n",
      "Epoch 00107: val_loss did not improve from 0.29811\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5097 - acc: 0.8327 - val_loss: 0.3016 - val_acc: 0.9122\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8301\n",
      "Epoch 00108: val_loss improved from 0.29811 to 0.29078, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/108-0.2908.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5135 - acc: 0.8301 - val_loss: 0.2908 - val_acc: 0.9124\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8342\n",
      "Epoch 00109: val_loss did not improve from 0.29078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5027 - acc: 0.8342 - val_loss: 0.2989 - val_acc: 0.9126\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8352\n",
      "Epoch 00110: val_loss did not improve from 0.29078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4977 - acc: 0.8352 - val_loss: 0.2934 - val_acc: 0.9143\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8352\n",
      "Epoch 00111: val_loss improved from 0.29078 to 0.29067, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/111-0.2907.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4962 - acc: 0.8353 - val_loss: 0.2907 - val_acc: 0.9145\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8386\n",
      "Epoch 00112: val_loss did not improve from 0.29067\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4942 - acc: 0.8386 - val_loss: 0.3089 - val_acc: 0.9117\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8354\n",
      "Epoch 00113: val_loss did not improve from 0.29067\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4993 - acc: 0.8353 - val_loss: 0.3033 - val_acc: 0.9119\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8401\n",
      "Epoch 00114: val_loss improved from 0.29067 to 0.28836, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/114-0.2884.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4875 - acc: 0.8401 - val_loss: 0.2884 - val_acc: 0.9145\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8395\n",
      "Epoch 00115: val_loss did not improve from 0.28836\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4899 - acc: 0.8395 - val_loss: 0.3224 - val_acc: 0.9068\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4879 - acc: 0.8406\n",
      "Epoch 00116: val_loss improved from 0.28836 to 0.28018, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/116-0.2802.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4878 - acc: 0.8406 - val_loss: 0.2802 - val_acc: 0.9117\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8409\n",
      "Epoch 00117: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4793 - acc: 0.8409 - val_loss: 0.2882 - val_acc: 0.9180\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8406\n",
      "Epoch 00118: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4833 - acc: 0.8406 - val_loss: 0.2824 - val_acc: 0.9187\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4844 - acc: 0.8391\n",
      "Epoch 00119: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4843 - acc: 0.8391 - val_loss: 0.2898 - val_acc: 0.9131\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8442\n",
      "Epoch 00120: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4731 - acc: 0.8442 - val_loss: 0.2996 - val_acc: 0.9136\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8417\n",
      "Epoch 00121: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4801 - acc: 0.8416 - val_loss: 0.2932 - val_acc: 0.9131\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8423\n",
      "Epoch 00122: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4750 - acc: 0.8424 - val_loss: 0.2857 - val_acc: 0.9164\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.8465\n",
      "Epoch 00123: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4723 - acc: 0.8465 - val_loss: 0.2895 - val_acc: 0.9185\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.8443\n",
      "Epoch 00124: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4722 - acc: 0.8443 - val_loss: 0.2853 - val_acc: 0.9145\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8455\n",
      "Epoch 00125: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4635 - acc: 0.8455 - val_loss: 0.2893 - val_acc: 0.9143\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8463\n",
      "Epoch 00126: val_loss did not improve from 0.28018\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4644 - acc: 0.8463 - val_loss: 0.2936 - val_acc: 0.9140\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8461\n",
      "Epoch 00127: val_loss improved from 0.28018 to 0.27508, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/127-0.2751.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4628 - acc: 0.8461 - val_loss: 0.2751 - val_acc: 0.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.8479\n",
      "Epoch 00128: val_loss did not improve from 0.27508\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4613 - acc: 0.8479 - val_loss: 0.2822 - val_acc: 0.9203\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8503\n",
      "Epoch 00129: val_loss did not improve from 0.27508\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4503 - acc: 0.8502 - val_loss: 0.2787 - val_acc: 0.9213\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8480\n",
      "Epoch 00130: val_loss improved from 0.27508 to 0.27374, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/130-0.2737.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4580 - acc: 0.8480 - val_loss: 0.2737 - val_acc: 0.9201\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8472\n",
      "Epoch 00131: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4553 - acc: 0.8472 - val_loss: 0.2867 - val_acc: 0.9150\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8524\n",
      "Epoch 00132: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4500 - acc: 0.8524 - val_loss: 0.2892 - val_acc: 0.9192\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.8522\n",
      "Epoch 00133: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4509 - acc: 0.8522 - val_loss: 0.2777 - val_acc: 0.9215\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8528\n",
      "Epoch 00134: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4452 - acc: 0.8527 - val_loss: 0.2806 - val_acc: 0.9171\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8535\n",
      "Epoch 00135: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4450 - acc: 0.8535 - val_loss: 0.2840 - val_acc: 0.9182\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8536\n",
      "Epoch 00136: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4450 - acc: 0.8536 - val_loss: 0.2764 - val_acc: 0.9199\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8522\n",
      "Epoch 00137: val_loss did not improve from 0.27374\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4488 - acc: 0.8522 - val_loss: 0.2780 - val_acc: 0.9220\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8557\n",
      "Epoch 00138: val_loss improved from 0.27374 to 0.26524, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/138-0.2652.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4400 - acc: 0.8557 - val_loss: 0.2652 - val_acc: 0.9217\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8546\n",
      "Epoch 00139: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4392 - acc: 0.8546 - val_loss: 0.2774 - val_acc: 0.9192\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8549\n",
      "Epoch 00140: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4371 - acc: 0.8549 - val_loss: 0.2848 - val_acc: 0.9192\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8555\n",
      "Epoch 00141: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4350 - acc: 0.8556 - val_loss: 0.2903 - val_acc: 0.9157\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8573\n",
      "Epoch 00142: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4312 - acc: 0.8573 - val_loss: 0.2739 - val_acc: 0.9229\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8587\n",
      "Epoch 00143: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4324 - acc: 0.8587 - val_loss: 0.2836 - val_acc: 0.9206\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8574\n",
      "Epoch 00144: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4323 - acc: 0.8574 - val_loss: 0.2753 - val_acc: 0.9168\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8584\n",
      "Epoch 00145: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4276 - acc: 0.8584 - val_loss: 0.2771 - val_acc: 0.9203\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8582\n",
      "Epoch 00146: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4280 - acc: 0.8582 - val_loss: 0.2775 - val_acc: 0.9203\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8608\n",
      "Epoch 00147: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4213 - acc: 0.8608 - val_loss: 0.3007 - val_acc: 0.9192\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8568\n",
      "Epoch 00148: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4281 - acc: 0.8568 - val_loss: 0.2757 - val_acc: 0.9217\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8579\n",
      "Epoch 00149: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4286 - acc: 0.8579 - val_loss: 0.2784 - val_acc: 0.9206\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8604\n",
      "Epoch 00150: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4260 - acc: 0.8603 - val_loss: 0.2803 - val_acc: 0.9220\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8602\n",
      "Epoch 00151: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4204 - acc: 0.8602 - val_loss: 0.2706 - val_acc: 0.9234\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8615\n",
      "Epoch 00152: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4140 - acc: 0.8616 - val_loss: 0.2682 - val_acc: 0.9234\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8630\n",
      "Epoch 00153: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4149 - acc: 0.8631 - val_loss: 0.2700 - val_acc: 0.9234\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8633\n",
      "Epoch 00154: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4103 - acc: 0.8633 - val_loss: 0.2847 - val_acc: 0.9217\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8639\n",
      "Epoch 00155: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4112 - acc: 0.8639 - val_loss: 0.2972 - val_acc: 0.9175\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8626\n",
      "Epoch 00156: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4130 - acc: 0.8626 - val_loss: 0.2786 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8630\n",
      "Epoch 00157: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4131 - acc: 0.8630 - val_loss: 0.2843 - val_acc: 0.9194\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8612\n",
      "Epoch 00158: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4130 - acc: 0.8612 - val_loss: 0.2852 - val_acc: 0.9196\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8658\n",
      "Epoch 00159: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3984 - acc: 0.8659 - val_loss: 0.2787 - val_acc: 0.9241\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8649\n",
      "Epoch 00160: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4041 - acc: 0.8649 - val_loss: 0.2687 - val_acc: 0.9229\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8674\n",
      "Epoch 00161: val_loss improved from 0.26524 to 0.26443, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/161-0.2644.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4038 - acc: 0.8674 - val_loss: 0.2644 - val_acc: 0.9222\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8660\n",
      "Epoch 00162: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4019 - acc: 0.8661 - val_loss: 0.2805 - val_acc: 0.9213\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8666\n",
      "Epoch 00163: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4007 - acc: 0.8666 - val_loss: 0.2862 - val_acc: 0.9182\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3962 - acc: 0.8680\n",
      "Epoch 00164: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3962 - acc: 0.8681 - val_loss: 0.2787 - val_acc: 0.9220\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8667\n",
      "Epoch 00165: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3966 - acc: 0.8667 - val_loss: 0.2711 - val_acc: 0.9231\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8692\n",
      "Epoch 00166: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3949 - acc: 0.8692 - val_loss: 0.2769 - val_acc: 0.9224\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8704\n",
      "Epoch 00167: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3909 - acc: 0.8704 - val_loss: 0.2816 - val_acc: 0.9185\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8706\n",
      "Epoch 00168: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3942 - acc: 0.8706 - val_loss: 0.2894 - val_acc: 0.9222\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8709\n",
      "Epoch 00169: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3899 - acc: 0.8709 - val_loss: 0.2786 - val_acc: 0.9248\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8688\n",
      "Epoch 00170: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3901 - acc: 0.8688 - val_loss: 0.2685 - val_acc: 0.9229\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8732\n",
      "Epoch 00171: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3816 - acc: 0.8732 - val_loss: 0.2891 - val_acc: 0.9236\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8698\n",
      "Epoch 00172: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3856 - acc: 0.8699 - val_loss: 0.2686 - val_acc: 0.9252\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8708\n",
      "Epoch 00173: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3878 - acc: 0.8709 - val_loss: 0.2737 - val_acc: 0.9255\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8701\n",
      "Epoch 00174: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3876 - acc: 0.8701 - val_loss: 0.2668 - val_acc: 0.9238\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8708\n",
      "Epoch 00175: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3854 - acc: 0.8708 - val_loss: 0.2726 - val_acc: 0.9224\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8700\n",
      "Epoch 00176: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3915 - acc: 0.8700 - val_loss: 0.2788 - val_acc: 0.9250\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8745\n",
      "Epoch 00177: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3752 - acc: 0.8745 - val_loss: 0.2661 - val_acc: 0.9255\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8738\n",
      "Epoch 00178: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3827 - acc: 0.8738 - val_loss: 0.2680 - val_acc: 0.9255\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8720\n",
      "Epoch 00179: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3801 - acc: 0.8720 - val_loss: 0.2745 - val_acc: 0.9273\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8738\n",
      "Epoch 00180: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3779 - acc: 0.8738 - val_loss: 0.2715 - val_acc: 0.9264\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8726\n",
      "Epoch 00181: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3778 - acc: 0.8726 - val_loss: 0.2651 - val_acc: 0.9264\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8740\n",
      "Epoch 00182: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3764 - acc: 0.8741 - val_loss: 0.2906 - val_acc: 0.9192\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8764\n",
      "Epoch 00183: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3648 - acc: 0.8764 - val_loss: 0.2682 - val_acc: 0.9266\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8776\n",
      "Epoch 00184: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3696 - acc: 0.8776 - val_loss: 0.2778 - val_acc: 0.9213\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8745\n",
      "Epoch 00185: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3748 - acc: 0.8745 - val_loss: 0.2952 - val_acc: 0.9220\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8756\n",
      "Epoch 00186: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3723 - acc: 0.8756 - val_loss: 0.2776 - val_acc: 0.9262\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8781\n",
      "Epoch 00187: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3675 - acc: 0.8781 - val_loss: 0.2756 - val_acc: 0.9257\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8804\n",
      "Epoch 00188: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3628 - acc: 0.8804 - val_loss: 0.2905 - val_acc: 0.9196\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8783\n",
      "Epoch 00189: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3642 - acc: 0.8784 - val_loss: 0.2912 - val_acc: 0.9236\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8793\n",
      "Epoch 00190: val_loss did not improve from 0.26443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3613 - acc: 0.8793 - val_loss: 0.2795 - val_acc: 0.9252\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8785\n",
      "Epoch 00191: val_loss improved from 0.26443 to 0.26215, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_9_conv_checkpoint/191-0.2621.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3650 - acc: 0.8785 - val_loss: 0.2621 - val_acc: 0.9276\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8789\n",
      "Epoch 00192: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3567 - acc: 0.8789 - val_loss: 0.2709 - val_acc: 0.9290\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8789\n",
      "Epoch 00193: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3605 - acc: 0.8790 - val_loss: 0.2850 - val_acc: 0.9250\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8808\n",
      "Epoch 00194: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3537 - acc: 0.8808 - val_loss: 0.2980 - val_acc: 0.9222\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8816\n",
      "Epoch 00195: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3528 - acc: 0.8816 - val_loss: 0.2825 - val_acc: 0.9243\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8807\n",
      "Epoch 00196: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3565 - acc: 0.8807 - val_loss: 0.2768 - val_acc: 0.9264\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8821\n",
      "Epoch 00197: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3527 - acc: 0.8821 - val_loss: 0.2713 - val_acc: 0.9280\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8825\n",
      "Epoch 00198: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3505 - acc: 0.8825 - val_loss: 0.2774 - val_acc: 0.9269\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8799\n",
      "Epoch 00199: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3577 - acc: 0.8799 - val_loss: 0.2634 - val_acc: 0.9262\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8813\n",
      "Epoch 00200: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3505 - acc: 0.8813 - val_loss: 0.2719 - val_acc: 0.9259\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8835\n",
      "Epoch 00201: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3484 - acc: 0.8835 - val_loss: 0.2752 - val_acc: 0.9259\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8826\n",
      "Epoch 00202: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3443 - acc: 0.8827 - val_loss: 0.2724 - val_acc: 0.9257\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8821\n",
      "Epoch 00203: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3486 - acc: 0.8820 - val_loss: 0.2682 - val_acc: 0.9273\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8814\n",
      "Epoch 00204: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3497 - acc: 0.8814 - val_loss: 0.2805 - val_acc: 0.9250\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8838\n",
      "Epoch 00205: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3481 - acc: 0.8838 - val_loss: 0.2826 - val_acc: 0.9236\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8790\n",
      "Epoch 00206: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3536 - acc: 0.8790 - val_loss: 0.2752 - val_acc: 0.9262\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8843\n",
      "Epoch 00207: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3439 - acc: 0.8843 - val_loss: 0.2886 - val_acc: 0.9231\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8864\n",
      "Epoch 00208: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3372 - acc: 0.8864 - val_loss: 0.3041 - val_acc: 0.9231\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8849\n",
      "Epoch 00209: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3401 - acc: 0.8849 - val_loss: 0.2939 - val_acc: 0.9201\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8862\n",
      "Epoch 00210: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3317 - acc: 0.8862 - val_loss: 0.3220 - val_acc: 0.9220\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3393 - acc: 0.8867\n",
      "Epoch 00211: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3392 - acc: 0.8867 - val_loss: 0.2874 - val_acc: 0.9262\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8857\n",
      "Epoch 00212: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3383 - acc: 0.8857 - val_loss: 0.2756 - val_acc: 0.9271\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8861\n",
      "Epoch 00213: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3384 - acc: 0.8860 - val_loss: 0.2739 - val_acc: 0.9222\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8868\n",
      "Epoch 00214: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3347 - acc: 0.8869 - val_loss: 0.2811 - val_acc: 0.9264\n",
      "Epoch 215/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8847\n",
      "Epoch 00215: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3395 - acc: 0.8847 - val_loss: 0.2782 - val_acc: 0.9292\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8888\n",
      "Epoch 00216: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3277 - acc: 0.8888 - val_loss: 0.2725 - val_acc: 0.9266\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8867\n",
      "Epoch 00217: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3339 - acc: 0.8867 - val_loss: 0.2783 - val_acc: 0.9250\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8862\n",
      "Epoch 00218: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3321 - acc: 0.8862 - val_loss: 0.2774 - val_acc: 0.9290\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8874\n",
      "Epoch 00219: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3354 - acc: 0.8874 - val_loss: 0.3002 - val_acc: 0.9248\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8887\n",
      "Epoch 00220: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3287 - acc: 0.8887 - val_loss: 0.3013 - val_acc: 0.9236\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8880\n",
      "Epoch 00221: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3287 - acc: 0.8880 - val_loss: 0.2875 - val_acc: 0.9276\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8904\n",
      "Epoch 00222: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3259 - acc: 0.8904 - val_loss: 0.2804 - val_acc: 0.9311\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8893\n",
      "Epoch 00223: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3253 - acc: 0.8893 - val_loss: 0.2778 - val_acc: 0.9257\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8898\n",
      "Epoch 00224: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3232 - acc: 0.8898 - val_loss: 0.2733 - val_acc: 0.9278\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8889\n",
      "Epoch 00225: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3221 - acc: 0.8888 - val_loss: 0.2922 - val_acc: 0.9266\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8867\n",
      "Epoch 00226: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3329 - acc: 0.8867 - val_loss: 0.2848 - val_acc: 0.9280\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.8926\n",
      "Epoch 00227: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3150 - acc: 0.8925 - val_loss: 0.2927 - val_acc: 0.9266\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8918\n",
      "Epoch 00228: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3224 - acc: 0.8918 - val_loss: 0.2774 - val_acc: 0.9276\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8924\n",
      "Epoch 00229: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3156 - acc: 0.8924 - val_loss: 0.2758 - val_acc: 0.9306\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8935\n",
      "Epoch 00230: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3146 - acc: 0.8935 - val_loss: 0.2952 - val_acc: 0.9248\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8896\n",
      "Epoch 00231: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3219 - acc: 0.8896 - val_loss: 0.2860 - val_acc: 0.9278\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8908\n",
      "Epoch 00232: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3198 - acc: 0.8908 - val_loss: 0.3056 - val_acc: 0.9203\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8911\n",
      "Epoch 00233: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3148 - acc: 0.8912 - val_loss: 0.2891 - val_acc: 0.9271\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8909\n",
      "Epoch 00234: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3167 - acc: 0.8909 - val_loss: 0.2940 - val_acc: 0.9266\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8945\n",
      "Epoch 00235: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3121 - acc: 0.8946 - val_loss: 0.2919 - val_acc: 0.9222\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8937\n",
      "Epoch 00236: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3113 - acc: 0.8937 - val_loss: 0.2825 - val_acc: 0.9269\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8930\n",
      "Epoch 00237: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3145 - acc: 0.8930 - val_loss: 0.2862 - val_acc: 0.9278\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.8929\n",
      "Epoch 00238: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3069 - acc: 0.8929 - val_loss: 0.2936 - val_acc: 0.9248\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8938\n",
      "Epoch 00239: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3134 - acc: 0.8938 - val_loss: 0.2916 - val_acc: 0.9285\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8954\n",
      "Epoch 00240: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3085 - acc: 0.8954 - val_loss: 0.2807 - val_acc: 0.9301\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.8959\n",
      "Epoch 00241: val_loss did not improve from 0.26215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3059 - acc: 0.8959 - val_loss: 0.3038 - val_acc: 0.9245\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 601us/sample - loss: 0.3387 - acc: 0.9053\n",
      "Loss: 0.338672388107482 Accuracy: 0.90529597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_4_ch_64_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_64_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 354,864\n",
      "Trainable params: 354,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 588us/sample - loss: 1.0616 - acc: 0.6816\n",
      "Loss: 1.0616259873594203 Accuracy: 0.68161994\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 157,776\n",
      "Trainable params: 157,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 576us/sample - loss: 0.7404 - acc: 0.8019\n",
      "Loss: 0.740428773425945 Accuracy: 0.80186915\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 95,344\n",
      "Trainable params: 95,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 624us/sample - loss: 0.4032 - acc: 0.8933\n",
      "Loss: 0.40324922873099894 Accuracy: 0.8932503\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 70,016\n",
      "Trainable params: 70,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 602us/sample - loss: 0.2918 - acc: 0.9190\n",
      "Loss: 0.291794709649289 Accuracy: 0.9190031\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 67,728\n",
      "Trainable params: 67,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 657us/sample - loss: 0.2784 - acc: 0.9211\n",
      "Loss: 0.278405804023812 Accuracy: 0.92107993\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 67,744\n",
      "Trainable params: 67,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 632us/sample - loss: 0.3387 - acc: 0.9053\n",
      "Loss: 0.338672388107482 Accuracy: 0.90529597\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_4_ch_64_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_64_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 354,864\n",
      "Trainable params: 354,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 638us/sample - loss: 1.2565 - acc: 0.7248\n",
      "Loss: 1.2565261681131856 Accuracy: 0.7248183\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 157,776\n",
      "Trainable params: 157,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 668us/sample - loss: 0.8160 - acc: 0.8050\n",
      "Loss: 0.8160230188973844 Accuracy: 0.80498445\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 95,344\n",
      "Trainable params: 95,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 636us/sample - loss: 0.4538 - acc: 0.8920\n",
      "Loss: 0.45384855102032023 Accuracy: 0.89200413\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 70,016\n",
      "Trainable params: 70,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.2981 - acc: 0.9215\n",
      "Loss: 0.2981047599238894 Accuracy: 0.9214953\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 67,728\n",
      "Trainable params: 67,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 701us/sample - loss: 0.3022 - acc: 0.9202\n",
      "Loss: 0.3021761265839619 Accuracy: 0.9202492\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 67,744\n",
      "Trainable params: 67,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 686us/sample - loss: 0.4013 - acc: 0.9055\n",
      "Loss: 0.4013017707029483 Accuracy: 0.90550363\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
