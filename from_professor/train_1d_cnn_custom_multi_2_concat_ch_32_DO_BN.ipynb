{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 75808)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 75808)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1212944     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,223,824\n",
      "Trainable params: 1,223,632\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 25248)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 25248)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           403984      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,144\n",
      "Trainable params: 419,888\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10464)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10464)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           167440      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 194,160\n",
      "Trainable params: 193,776\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5504)         0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5504)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           88080       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 135,600\n",
      "Trainable params: 135,088\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1792)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1792)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           28688       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 97,008\n",
      "Trainable params: 96,368\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 576)          0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 576)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           9232        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 98,352\n",
      "Trainable params: 97,584\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3777 - acc: 0.3264\n",
      "Epoch 00001: val_loss improved from inf to 1.70210, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv_checkpoint/001-1.7021.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 2.3768 - acc: 0.3265 - val_loss: 1.7021 - val_acc: 0.4305\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5551 - acc: 0.5193\n",
      "Epoch 00002: val_loss improved from 1.70210 to 1.50501, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv_checkpoint/002-1.5050.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.5554 - acc: 0.5192 - val_loss: 1.5050 - val_acc: 0.5257\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2680 - acc: 0.6000\n",
      "Epoch 00003: val_loss improved from 1.50501 to 1.35907, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv_checkpoint/003-1.3591.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.2677 - acc: 0.6002 - val_loss: 1.3591 - val_acc: 0.5658\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.6509\n",
      "Epoch 00004: val_loss improved from 1.35907 to 1.27448, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv_checkpoint/004-1.2745.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0963 - acc: 0.6509 - val_loss: 1.2745 - val_acc: 0.6091\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9502 - acc: 0.6960\n",
      "Epoch 00005: val_loss did not improve from 1.27448\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9505 - acc: 0.6959 - val_loss: 1.4141 - val_acc: 0.5723\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7318\n",
      "Epoch 00006: val_loss did not improve from 1.27448\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8312 - acc: 0.7320 - val_loss: 1.6046 - val_acc: 0.5344\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7416 - acc: 0.7596\n",
      "Epoch 00007: val_loss did not improve from 1.27448\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7417 - acc: 0.7596 - val_loss: 1.3406 - val_acc: 0.6045\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6624 - acc: 0.7840\n",
      "Epoch 00008: val_loss improved from 1.27448 to 1.23915, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv_checkpoint/008-1.2392.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.6626 - acc: 0.7839 - val_loss: 1.2392 - val_acc: 0.6487\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6153 - acc: 0.7986\n",
      "Epoch 00009: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.6158 - acc: 0.7984 - val_loss: 1.3724 - val_acc: 0.6152\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8215\n",
      "Epoch 00010: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.5463 - acc: 0.8215 - val_loss: 1.2866 - val_acc: 0.6396\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8342\n",
      "Epoch 00011: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.5001 - acc: 0.8342 - val_loss: 1.3243 - val_acc: 0.6238\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8437\n",
      "Epoch 00012: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.4706 - acc: 0.8437 - val_loss: 1.5861 - val_acc: 0.5914\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8598\n",
      "Epoch 00013: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.4231 - acc: 0.8597 - val_loss: 1.8477 - val_acc: 0.5558\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8674\n",
      "Epoch 00014: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.4001 - acc: 0.8674 - val_loss: 1.4176 - val_acc: 0.6345\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8730\n",
      "Epoch 00015: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.3777 - acc: 0.8730 - val_loss: 1.7109 - val_acc: 0.5935\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8813\n",
      "Epoch 00016: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.3510 - acc: 0.8813 - val_loss: 1.5594 - val_acc: 0.6124\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8937\n",
      "Epoch 00017: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.3233 - acc: 0.8937 - val_loss: 1.5552 - val_acc: 0.6061\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8969\n",
      "Epoch 00018: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.3085 - acc: 0.8969 - val_loss: 1.7376 - val_acc: 0.5833\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9015\n",
      "Epoch 00019: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.2958 - acc: 0.9016 - val_loss: 1.8591 - val_acc: 0.5705\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9109\n",
      "Epoch 00020: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.2718 - acc: 0.9107 - val_loss: 1.6581 - val_acc: 0.6124\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9113\n",
      "Epoch 00021: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.2655 - acc: 0.9111 - val_loss: 1.5177 - val_acc: 0.6350\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9131\n",
      "Epoch 00022: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.2609 - acc: 0.9132 - val_loss: 1.5511 - val_acc: 0.6273\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9208\n",
      "Epoch 00023: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2406 - acc: 0.9208 - val_loss: 1.4694 - val_acc: 0.6655\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9238\n",
      "Epoch 00024: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.2297 - acc: 0.9238 - val_loss: 1.6018 - val_acc: 0.6399\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9283\n",
      "Epoch 00025: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.2178 - acc: 0.9283 - val_loss: 1.9295 - val_acc: 0.5828\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9319\n",
      "Epoch 00026: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.2059 - acc: 0.9319 - val_loss: 1.6197 - val_acc: 0.6431\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9308\n",
      "Epoch 00027: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.2072 - acc: 0.9308 - val_loss: 1.7193 - val_acc: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9361\n",
      "Epoch 00028: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1972 - acc: 0.9359 - val_loss: 1.9064 - val_acc: 0.6184\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9329\n",
      "Epoch 00029: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.1976 - acc: 0.9330 - val_loss: 1.5489 - val_acc: 0.6613\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9380\n",
      "Epoch 00030: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1865 - acc: 0.9380 - val_loss: 1.7770 - val_acc: 0.6348\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9435\n",
      "Epoch 00031: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1724 - acc: 0.9435 - val_loss: 1.6412 - val_acc: 0.6513\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9428\n",
      "Epoch 00032: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.1730 - acc: 0.9429 - val_loss: 1.9348 - val_acc: 0.6122\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9451\n",
      "Epoch 00033: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1676 - acc: 0.9450 - val_loss: 2.3026 - val_acc: 0.5472\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9466\n",
      "Epoch 00034: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1603 - acc: 0.9466 - val_loss: 1.6707 - val_acc: 0.6587\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9467\n",
      "Epoch 00035: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1582 - acc: 0.9467 - val_loss: 2.2233 - val_acc: 0.5933\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9502\n",
      "Epoch 00036: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1531 - acc: 0.9501 - val_loss: 1.7295 - val_acc: 0.6427\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9502\n",
      "Epoch 00037: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1513 - acc: 0.9503 - val_loss: 1.7520 - val_acc: 0.6487\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9524\n",
      "Epoch 00038: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1443 - acc: 0.9524 - val_loss: 3.0007 - val_acc: 0.5190\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9517\n",
      "Epoch 00039: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1491 - acc: 0.9517 - val_loss: 1.8656 - val_acc: 0.6292\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9566\n",
      "Epoch 00040: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.1355 - acc: 0.9566 - val_loss: 1.6156 - val_acc: 0.6709\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9569\n",
      "Epoch 00041: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.1311 - acc: 0.9569 - val_loss: 1.6738 - val_acc: 0.6711\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9583\n",
      "Epoch 00042: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1288 - acc: 0.9583 - val_loss: 1.9002 - val_acc: 0.6403\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9594\n",
      "Epoch 00043: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1260 - acc: 0.9594 - val_loss: 1.8997 - val_acc: 0.6541\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9590\n",
      "Epoch 00044: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1296 - acc: 0.9589 - val_loss: 1.9027 - val_acc: 0.6427\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9612\n",
      "Epoch 00045: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1232 - acc: 0.9611 - val_loss: 2.0132 - val_acc: 0.6052\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9604\n",
      "Epoch 00046: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1209 - acc: 0.9603 - val_loss: 2.0270 - val_acc: 0.6259\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9652\n",
      "Epoch 00047: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1094 - acc: 0.9652 - val_loss: 1.8370 - val_acc: 0.6476\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9645\n",
      "Epoch 00048: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1098 - acc: 0.9645 - val_loss: 1.6716 - val_acc: 0.6678\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9604\n",
      "Epoch 00049: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1189 - acc: 0.9604 - val_loss: 1.8239 - val_acc: 0.6555\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9634\n",
      "Epoch 00050: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1125 - acc: 0.9633 - val_loss: 1.7539 - val_acc: 0.6620\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9653\n",
      "Epoch 00051: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.1083 - acc: 0.9654 - val_loss: 2.1485 - val_acc: 0.6222\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9670\n",
      "Epoch 00052: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1050 - acc: 0.9670 - val_loss: 1.8580 - val_acc: 0.6415\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9645\n",
      "Epoch 00053: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.1097 - acc: 0.9645 - val_loss: 1.9558 - val_acc: 0.6280\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9677\n",
      "Epoch 00054: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1020 - acc: 0.9677 - val_loss: 2.6193 - val_acc: 0.5751\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9669\n",
      "Epoch 00055: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1050 - acc: 0.9669 - val_loss: 2.1599 - val_acc: 0.6133\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9682\n",
      "Epoch 00056: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.1014 - acc: 0.9681 - val_loss: 2.1620 - val_acc: 0.6063\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9703\n",
      "Epoch 00057: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.0938 - acc: 0.9704 - val_loss: 1.8165 - val_acc: 0.6627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9713\n",
      "Epoch 00058: val_loss did not improve from 1.23915\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.0910 - acc: 0.9713 - val_loss: 1.9585 - val_acc: 0.6466\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 370us/sample - loss: 1.3305 - acc: 0.6191\n",
      "Loss: 1.3304850657285807 Accuracy: 0.61910695\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3246 - acc: 0.3160\n",
      "Epoch 00001: val_loss improved from inf to 1.76226, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/001-1.7623.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 2.3245 - acc: 0.3160 - val_loss: 1.7623 - val_acc: 0.4074\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5547 - acc: 0.5122\n",
      "Epoch 00002: val_loss improved from 1.76226 to 1.25202, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/002-1.2520.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.5548 - acc: 0.5122 - val_loss: 1.2520 - val_acc: 0.6012\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3089 - acc: 0.5932\n",
      "Epoch 00003: val_loss improved from 1.25202 to 1.21016, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/003-1.2102.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.3087 - acc: 0.5932 - val_loss: 1.2102 - val_acc: 0.6329\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1470 - acc: 0.6418\n",
      "Epoch 00004: val_loss improved from 1.21016 to 1.02848, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/004-1.0285.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.1469 - acc: 0.6418 - val_loss: 1.0285 - val_acc: 0.6890\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0414 - acc: 0.6754\n",
      "Epoch 00005: val_loss did not improve from 1.02848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.0414 - acc: 0.6753 - val_loss: 1.0299 - val_acc: 0.6976\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9570 - acc: 0.7031\n",
      "Epoch 00006: val_loss did not improve from 1.02848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.9568 - acc: 0.7031 - val_loss: 1.0489 - val_acc: 0.6942\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8965 - acc: 0.7238\n",
      "Epoch 00007: val_loss improved from 1.02848 to 0.91343, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/007-0.9134.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8965 - acc: 0.7238 - val_loss: 0.9134 - val_acc: 0.7405\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8321 - acc: 0.7437\n",
      "Epoch 00008: val_loss did not improve from 0.91343\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8326 - acc: 0.7436 - val_loss: 1.1416 - val_acc: 0.6851\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.7592\n",
      "Epoch 00009: val_loss did not improve from 0.91343\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7764 - acc: 0.7592 - val_loss: 0.9633 - val_acc: 0.7130\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7383 - acc: 0.7717\n",
      "Epoch 00010: val_loss did not improve from 0.91343\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7382 - acc: 0.7717 - val_loss: 1.0178 - val_acc: 0.7179\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.7792\n",
      "Epoch 00011: val_loss did not improve from 0.91343\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7043 - acc: 0.7793 - val_loss: 0.9239 - val_acc: 0.7314\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.7918\n",
      "Epoch 00012: val_loss did not improve from 0.91343\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.6672 - acc: 0.7918 - val_loss: 1.0126 - val_acc: 0.7044\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.8016\n",
      "Epoch 00013: val_loss improved from 0.91343 to 0.87534, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/013-0.8753.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.6375 - acc: 0.8016 - val_loss: 0.8753 - val_acc: 0.7519\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.8091\n",
      "Epoch 00014: val_loss did not improve from 0.87534\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6075 - acc: 0.8090 - val_loss: 1.0668 - val_acc: 0.6909\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5826 - acc: 0.8167\n",
      "Epoch 00015: val_loss did not improve from 0.87534\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.5824 - acc: 0.8168 - val_loss: 0.9212 - val_acc: 0.7349\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8250\n",
      "Epoch 00016: val_loss improved from 0.87534 to 0.85901, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/016-0.8590.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5521 - acc: 0.8251 - val_loss: 0.8590 - val_acc: 0.7570\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.8302\n",
      "Epoch 00017: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.5300 - acc: 0.8302 - val_loss: 0.8867 - val_acc: 0.7449\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.8382\n",
      "Epoch 00018: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.5122 - acc: 0.8382 - val_loss: 0.9350 - val_acc: 0.7414\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.8413\n",
      "Epoch 00019: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4977 - acc: 0.8412 - val_loss: 0.9464 - val_acc: 0.7391\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.8474\n",
      "Epoch 00020: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.4753 - acc: 0.8474 - val_loss: 0.9205 - val_acc: 0.7484\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8504\n",
      "Epoch 00021: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4627 - acc: 0.8503 - val_loss: 1.0677 - val_acc: 0.7079\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8565\n",
      "Epoch 00022: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4466 - acc: 0.8565 - val_loss: 0.8684 - val_acc: 0.7536\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8583\n",
      "Epoch 00023: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.4408 - acc: 0.8583 - val_loss: 1.0238 - val_acc: 0.7191\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8663\n",
      "Epoch 00024: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.4153 - acc: 0.8663 - val_loss: 0.8878 - val_acc: 0.7522\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8694\n",
      "Epoch 00025: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4044 - acc: 0.8694 - val_loss: 0.9447 - val_acc: 0.7470\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8686\n",
      "Epoch 00026: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.4017 - acc: 0.8686 - val_loss: 0.9908 - val_acc: 0.7352\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8752\n",
      "Epoch 00027: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3833 - acc: 0.8752 - val_loss: 0.9907 - val_acc: 0.7391\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8783\n",
      "Epoch 00028: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3691 - acc: 0.8783 - val_loss: 1.1263 - val_acc: 0.7186\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8820\n",
      "Epoch 00029: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.3647 - acc: 0.8817 - val_loss: 1.0848 - val_acc: 0.7084\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8890\n",
      "Epoch 00030: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.3436 - acc: 0.8890 - val_loss: 0.9779 - val_acc: 0.7400\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8929\n",
      "Epoch 00031: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.3324 - acc: 0.8929 - val_loss: 1.0571 - val_acc: 0.7207\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8903\n",
      "Epoch 00032: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3358 - acc: 0.8903 - val_loss: 1.2912 - val_acc: 0.6848\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8947\n",
      "Epoch 00033: val_loss did not improve from 0.85901\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3266 - acc: 0.8946 - val_loss: 0.8824 - val_acc: 0.7675\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8958\n",
      "Epoch 00034: val_loss improved from 0.85901 to 0.84421, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv_checkpoint/034-0.8442.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3165 - acc: 0.8958 - val_loss: 0.8442 - val_acc: 0.7850\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.8999\n",
      "Epoch 00035: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.3052 - acc: 0.8999 - val_loss: 1.0240 - val_acc: 0.7284\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9023\n",
      "Epoch 00036: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.3006 - acc: 0.9023 - val_loss: 1.2303 - val_acc: 0.7056\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9043\n",
      "Epoch 00037: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2960 - acc: 0.9041 - val_loss: 0.9256 - val_acc: 0.7633\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9042\n",
      "Epoch 00038: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2939 - acc: 0.9042 - val_loss: 1.5104 - val_acc: 0.6352\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9089\n",
      "Epoch 00039: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2835 - acc: 0.9087 - val_loss: 0.9873 - val_acc: 0.7459\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9069\n",
      "Epoch 00040: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2845 - acc: 0.9068 - val_loss: 1.2433 - val_acc: 0.7018\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9101\n",
      "Epoch 00041: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2674 - acc: 0.9102 - val_loss: 1.0643 - val_acc: 0.7312\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9132\n",
      "Epoch 00042: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.2625 - acc: 0.9131 - val_loss: 1.1234 - val_acc: 0.7142\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9183\n",
      "Epoch 00043: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2542 - acc: 0.9184 - val_loss: 1.0071 - val_acc: 0.7442\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9170\n",
      "Epoch 00044: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2532 - acc: 0.9170 - val_loss: 1.0512 - val_acc: 0.7475\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9176\n",
      "Epoch 00045: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.2505 - acc: 0.9176 - val_loss: 0.9015 - val_acc: 0.7750\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9216\n",
      "Epoch 00046: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2413 - acc: 0.9216 - val_loss: 1.0622 - val_acc: 0.7417\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9206\n",
      "Epoch 00047: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2425 - acc: 0.9205 - val_loss: 0.9339 - val_acc: 0.7626\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9228\n",
      "Epoch 00048: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2354 - acc: 0.9227 - val_loss: 0.9164 - val_acc: 0.7759\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9241\n",
      "Epoch 00049: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2308 - acc: 0.9241 - val_loss: 1.1652 - val_acc: 0.7221\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9271\n",
      "Epoch 00050: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2244 - acc: 0.9271 - val_loss: 0.9756 - val_acc: 0.7577\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9277\n",
      "Epoch 00051: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2187 - acc: 0.9278 - val_loss: 1.1112 - val_acc: 0.7358\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9295\n",
      "Epoch 00052: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2188 - acc: 0.9294 - val_loss: 1.1616 - val_acc: 0.7270\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9296\n",
      "Epoch 00053: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.2190 - acc: 0.9296 - val_loss: 1.1226 - val_acc: 0.7382\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9325\n",
      "Epoch 00054: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2111 - acc: 0.9324 - val_loss: 0.8871 - val_acc: 0.7850\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9303\n",
      "Epoch 00055: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2112 - acc: 0.9303 - val_loss: 1.0960 - val_acc: 0.7386\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9324\n",
      "Epoch 00056: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2060 - acc: 0.9324 - val_loss: 1.0065 - val_acc: 0.7715\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9361\n",
      "Epoch 00057: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1982 - acc: 0.9361 - val_loss: 0.9324 - val_acc: 0.7782\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9343\n",
      "Epoch 00058: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.1990 - acc: 0.9343 - val_loss: 1.1510 - val_acc: 0.7261\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9346\n",
      "Epoch 00059: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1971 - acc: 0.9346 - val_loss: 0.9491 - val_acc: 0.7747\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9369\n",
      "Epoch 00060: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.1910 - acc: 0.9369 - val_loss: 0.9688 - val_acc: 0.7701\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9375\n",
      "Epoch 00061: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.1926 - acc: 0.9375 - val_loss: 1.0085 - val_acc: 0.7715\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9389\n",
      "Epoch 00062: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.1909 - acc: 0.9388 - val_loss: 0.9036 - val_acc: 0.7827\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9402\n",
      "Epoch 00063: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.1830 - acc: 0.9401 - val_loss: 0.9346 - val_acc: 0.7901\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9405\n",
      "Epoch 00064: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1830 - acc: 0.9405 - val_loss: 0.9740 - val_acc: 0.7794\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9437\n",
      "Epoch 00065: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1746 - acc: 0.9438 - val_loss: 0.9449 - val_acc: 0.7857\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9445\n",
      "Epoch 00066: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1731 - acc: 0.9445 - val_loss: 0.9497 - val_acc: 0.7766\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9425\n",
      "Epoch 00067: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.1728 - acc: 0.9425 - val_loss: 1.1093 - val_acc: 0.7424\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9453\n",
      "Epoch 00068: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1709 - acc: 0.9453 - val_loss: 0.9374 - val_acc: 0.7806\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9471\n",
      "Epoch 00069: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.1652 - acc: 0.9471 - val_loss: 0.9469 - val_acc: 0.7841\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9458\n",
      "Epoch 00070: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.1693 - acc: 0.9458 - val_loss: 0.9787 - val_acc: 0.7743\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9470\n",
      "Epoch 00071: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1619 - acc: 0.9470 - val_loss: 0.9315 - val_acc: 0.7806\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9490\n",
      "Epoch 00072: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1579 - acc: 0.9490 - val_loss: 1.0702 - val_acc: 0.7619\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9491\n",
      "Epoch 00073: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1584 - acc: 0.9491 - val_loss: 1.1984 - val_acc: 0.7345\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9477\n",
      "Epoch 00074: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.1616 - acc: 0.9477 - val_loss: 1.0142 - val_acc: 0.7559\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9495\n",
      "Epoch 00075: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1572 - acc: 0.9495 - val_loss: 1.1588 - val_acc: 0.7480\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9530\n",
      "Epoch 00076: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.1482 - acc: 0.9529 - val_loss: 0.9711 - val_acc: 0.7745\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9507\n",
      "Epoch 00077: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1516 - acc: 0.9506 - val_loss: 1.1106 - val_acc: 0.7531\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9530\n",
      "Epoch 00078: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1488 - acc: 0.9529 - val_loss: 1.1637 - val_acc: 0.7452\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9514\n",
      "Epoch 00079: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1515 - acc: 0.9514 - val_loss: 0.9500 - val_acc: 0.7939\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9516\n",
      "Epoch 00080: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.1494 - acc: 0.9516 - val_loss: 0.9846 - val_acc: 0.7703\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9532\n",
      "Epoch 00081: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1453 - acc: 0.9532 - val_loss: 1.1165 - val_acc: 0.7540\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9554\n",
      "Epoch 00082: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.1397 - acc: 0.9554 - val_loss: 1.0140 - val_acc: 0.7743\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9546\n",
      "Epoch 00083: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1433 - acc: 0.9545 - val_loss: 0.9541 - val_acc: 0.7852\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9543\n",
      "Epoch 00084: val_loss did not improve from 0.84421\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1449 - acc: 0.9543 - val_loss: 1.0715 - val_acc: 0.7738\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 397us/sample - loss: 0.9603 - acc: 0.7400\n",
      "Loss: 0.9603418133340521 Accuracy: 0.7399792\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3778 - acc: 0.2944\n",
      "Epoch 00001: val_loss improved from inf to 1.68693, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/001-1.6869.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 2.3776 - acc: 0.2944 - val_loss: 1.6869 - val_acc: 0.4284\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5755 - acc: 0.5011\n",
      "Epoch 00002: val_loss improved from 1.68693 to 1.21004, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/002-1.2100.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.5757 - acc: 0.5011 - val_loss: 1.2100 - val_acc: 0.6343\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3228 - acc: 0.5815\n",
      "Epoch 00003: val_loss improved from 1.21004 to 1.11680, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/003-1.1168.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.3227 - acc: 0.5816 - val_loss: 1.1168 - val_acc: 0.6713\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1641 - acc: 0.6312\n",
      "Epoch 00004: val_loss improved from 1.11680 to 0.95868, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/004-0.9587.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 1.1642 - acc: 0.6311 - val_loss: 0.9587 - val_acc: 0.7021\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0529 - acc: 0.6723\n",
      "Epoch 00005: val_loss improved from 0.95868 to 0.91839, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/005-0.9184.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.0529 - acc: 0.6723 - val_loss: 0.9184 - val_acc: 0.7307\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9686 - acc: 0.7000\n",
      "Epoch 00006: val_loss improved from 0.91839 to 0.82425, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/006-0.8242.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.9685 - acc: 0.7000 - val_loss: 0.8242 - val_acc: 0.7584\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9016 - acc: 0.7212\n",
      "Epoch 00007: val_loss did not improve from 0.82425\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.9021 - acc: 0.7210 - val_loss: 0.8375 - val_acc: 0.7487\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8514 - acc: 0.7383\n",
      "Epoch 00008: val_loss improved from 0.82425 to 0.77721, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/008-0.7772.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.8514 - acc: 0.7383 - val_loss: 0.7772 - val_acc: 0.7722\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7544\n",
      "Epoch 00009: val_loss improved from 0.77721 to 0.76809, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/009-0.7681.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.8056 - acc: 0.7544 - val_loss: 0.7681 - val_acc: 0.7789\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.7693\n",
      "Epoch 00010: val_loss did not improve from 0.76809\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.7614 - acc: 0.7693 - val_loss: 0.7939 - val_acc: 0.7685\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7776\n",
      "Epoch 00011: val_loss improved from 0.76809 to 0.72483, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/011-0.7248.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.7282 - acc: 0.7776 - val_loss: 0.7248 - val_acc: 0.7952\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.7886\n",
      "Epoch 00012: val_loss improved from 0.72483 to 0.69080, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/012-0.6908.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.6984 - acc: 0.7885 - val_loss: 0.6908 - val_acc: 0.8004\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6721 - acc: 0.7935\n",
      "Epoch 00013: val_loss did not improve from 0.69080\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.6721 - acc: 0.7935 - val_loss: 0.7485 - val_acc: 0.7796\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6464 - acc: 0.8032\n",
      "Epoch 00014: val_loss improved from 0.69080 to 0.65170, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/014-0.6517.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.6465 - acc: 0.8032 - val_loss: 0.6517 - val_acc: 0.8178\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6272 - acc: 0.8093\n",
      "Epoch 00015: val_loss improved from 0.65170 to 0.63028, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/015-0.6303.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.6266 - acc: 0.8095 - val_loss: 0.6303 - val_acc: 0.8204\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8180\n",
      "Epoch 00016: val_loss did not improve from 0.63028\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6002 - acc: 0.8180 - val_loss: 0.7234 - val_acc: 0.7918\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5822 - acc: 0.8235\n",
      "Epoch 00017: val_loss did not improve from 0.63028\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.5822 - acc: 0.8236 - val_loss: 0.6666 - val_acc: 0.8036\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.8320\n",
      "Epoch 00018: val_loss improved from 0.63028 to 0.60073, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/018-0.6007.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.5601 - acc: 0.8320 - val_loss: 0.6007 - val_acc: 0.8288\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8354\n",
      "Epoch 00019: val_loss did not improve from 0.60073\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.5419 - acc: 0.8354 - val_loss: 0.6082 - val_acc: 0.8295\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8394\n",
      "Epoch 00020: val_loss did not improve from 0.60073\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.5270 - acc: 0.8394 - val_loss: 0.8028 - val_acc: 0.7615\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8437\n",
      "Epoch 00021: val_loss did not improve from 0.60073\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.5164 - acc: 0.8437 - val_loss: 0.6588 - val_acc: 0.8178\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8507\n",
      "Epoch 00022: val_loss improved from 0.60073 to 0.59887, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/022-0.5989.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4959 - acc: 0.8506 - val_loss: 0.5989 - val_acc: 0.8362\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8528\n",
      "Epoch 00023: val_loss did not improve from 0.59887\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4852 - acc: 0.8528 - val_loss: 0.6073 - val_acc: 0.8355\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8529\n",
      "Epoch 00024: val_loss improved from 0.59887 to 0.54986, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/024-0.5499.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4772 - acc: 0.8528 - val_loss: 0.5499 - val_acc: 0.8495\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8611\n",
      "Epoch 00025: val_loss did not improve from 0.54986\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4553 - acc: 0.8612 - val_loss: 0.6194 - val_acc: 0.8255\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8631\n",
      "Epoch 00026: val_loss improved from 0.54986 to 0.53489, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/026-0.5349.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4464 - acc: 0.8631 - val_loss: 0.5349 - val_acc: 0.8542\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8662\n",
      "Epoch 00027: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4386 - acc: 0.8662 - val_loss: 0.6572 - val_acc: 0.8085\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8696\n",
      "Epoch 00028: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.4282 - acc: 0.8696 - val_loss: 0.9177 - val_acc: 0.7526\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8726\n",
      "Epoch 00029: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.4176 - acc: 0.8725 - val_loss: 0.8261 - val_acc: 0.7631\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8746\n",
      "Epoch 00030: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4128 - acc: 0.8748 - val_loss: 0.6264 - val_acc: 0.8209\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8779\n",
      "Epoch 00031: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3997 - acc: 0.8777 - val_loss: 0.5944 - val_acc: 0.8383\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8807\n",
      "Epoch 00032: val_loss did not improve from 0.53489\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3912 - acc: 0.8806 - val_loss: 0.5770 - val_acc: 0.8400\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8834\n",
      "Epoch 00033: val_loss improved from 0.53489 to 0.52837, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/033-0.5284.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3816 - acc: 0.8834 - val_loss: 0.5284 - val_acc: 0.8523\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8822\n",
      "Epoch 00034: val_loss did not improve from 0.52837\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3807 - acc: 0.8822 - val_loss: 0.5617 - val_acc: 0.8430\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8830\n",
      "Epoch 00035: val_loss did not improve from 0.52837\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.3735 - acc: 0.8830 - val_loss: 0.8264 - val_acc: 0.7720\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8860\n",
      "Epoch 00036: val_loss improved from 0.52837 to 0.51351, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/036-0.5135.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.3665 - acc: 0.8860 - val_loss: 0.5135 - val_acc: 0.8546\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8916\n",
      "Epoch 00037: val_loss improved from 0.51351 to 0.51017, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/037-0.5102.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3553 - acc: 0.8916 - val_loss: 0.5102 - val_acc: 0.8514\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8910\n",
      "Epoch 00038: val_loss did not improve from 0.51017\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.3533 - acc: 0.8910 - val_loss: 0.5618 - val_acc: 0.8528\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8923\n",
      "Epoch 00039: val_loss did not improve from 0.51017\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.3462 - acc: 0.8922 - val_loss: 0.5128 - val_acc: 0.8637\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8939\n",
      "Epoch 00040: val_loss did not improve from 0.51017\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3464 - acc: 0.8939 - val_loss: 0.5757 - val_acc: 0.8400\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8964\n",
      "Epoch 00041: val_loss improved from 0.51017 to 0.50019, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/041-0.5002.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.3351 - acc: 0.8964 - val_loss: 0.5002 - val_acc: 0.8623\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8977\n",
      "Epoch 00042: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.3255 - acc: 0.8977 - val_loss: 0.5033 - val_acc: 0.8640\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.9007\n",
      "Epoch 00043: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3190 - acc: 0.9007 - val_loss: 0.5040 - val_acc: 0.8612\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8991\n",
      "Epoch 00044: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3201 - acc: 0.8991 - val_loss: 0.5501 - val_acc: 0.8514\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9024\n",
      "Epoch 00045: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3126 - acc: 0.9024 - val_loss: 0.6199 - val_acc: 0.8293\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9034\n",
      "Epoch 00046: val_loss improved from 0.50019 to 0.49738, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/046-0.4974.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3072 - acc: 0.9034 - val_loss: 0.4974 - val_acc: 0.8670\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9060\n",
      "Epoch 00047: val_loss did not improve from 0.49738\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3025 - acc: 0.9060 - val_loss: 0.5211 - val_acc: 0.8600\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9035\n",
      "Epoch 00048: val_loss did not improve from 0.49738\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3022 - acc: 0.9035 - val_loss: 0.5043 - val_acc: 0.8649\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9068\n",
      "Epoch 00049: val_loss did not improve from 0.49738\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.2959 - acc: 0.9068 - val_loss: 0.5973 - val_acc: 0.8393\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9101\n",
      "Epoch 00050: val_loss did not improve from 0.49738\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2872 - acc: 0.9101 - val_loss: 0.5393 - val_acc: 0.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9114\n",
      "Epoch 00051: val_loss did not improve from 0.49738\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.2807 - acc: 0.9114 - val_loss: 0.5221 - val_acc: 0.8579\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9122\n",
      "Epoch 00052: val_loss improved from 0.49738 to 0.48863, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/052-0.4886.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2785 - acc: 0.9121 - val_loss: 0.4886 - val_acc: 0.8672\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9127\n",
      "Epoch 00053: val_loss did not improve from 0.48863\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2788 - acc: 0.9127 - val_loss: 0.5018 - val_acc: 0.8696\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9129\n",
      "Epoch 00054: val_loss improved from 0.48863 to 0.46805, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/054-0.4680.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2751 - acc: 0.9130 - val_loss: 0.4680 - val_acc: 0.8751\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9154\n",
      "Epoch 00055: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.2683 - acc: 0.9153 - val_loss: 0.4957 - val_acc: 0.8658\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9161\n",
      "Epoch 00056: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2642 - acc: 0.9161 - val_loss: 0.6337 - val_acc: 0.8328\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9207\n",
      "Epoch 00057: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2542 - acc: 0.9207 - val_loss: 0.5835 - val_acc: 0.8521\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9176\n",
      "Epoch 00058: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2579 - acc: 0.9175 - val_loss: 0.5613 - val_acc: 0.8539\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9210\n",
      "Epoch 00059: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2512 - acc: 0.9208 - val_loss: 0.5278 - val_acc: 0.8626\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9188\n",
      "Epoch 00060: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2531 - acc: 0.9188 - val_loss: 0.6178 - val_acc: 0.8337\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9193\n",
      "Epoch 00061: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2499 - acc: 0.9193 - val_loss: 0.5625 - val_acc: 0.8549\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9221\n",
      "Epoch 00062: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.2401 - acc: 0.9221 - val_loss: 0.4878 - val_acc: 0.8730\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9247\n",
      "Epoch 00063: val_loss improved from 0.46805 to 0.46546, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/063-0.4655.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2384 - acc: 0.9247 - val_loss: 0.4655 - val_acc: 0.8772\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9236\n",
      "Epoch 00064: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2360 - acc: 0.9236 - val_loss: 0.4694 - val_acc: 0.8763\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9236\n",
      "Epoch 00065: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2393 - acc: 0.9235 - val_loss: 0.5116 - val_acc: 0.8719\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9237\n",
      "Epoch 00066: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2363 - acc: 0.9237 - val_loss: 0.5573 - val_acc: 0.8572\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9268\n",
      "Epoch 00067: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2302 - acc: 0.9268 - val_loss: 0.4723 - val_acc: 0.8733\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9277\n",
      "Epoch 00068: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2247 - acc: 0.9277 - val_loss: 0.5337 - val_acc: 0.8586\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9287\n",
      "Epoch 00069: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2248 - acc: 0.9287 - val_loss: 0.5650 - val_acc: 0.8523\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9292\n",
      "Epoch 00070: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2200 - acc: 0.9292 - val_loss: 0.5409 - val_acc: 0.8523\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9286\n",
      "Epoch 00071: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2213 - acc: 0.9286 - val_loss: 0.4914 - val_acc: 0.8796\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9292\n",
      "Epoch 00072: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2160 - acc: 0.9292 - val_loss: 0.5481 - val_acc: 0.8588\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9328\n",
      "Epoch 00073: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.2116 - acc: 0.9328 - val_loss: 0.5489 - val_acc: 0.8535\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9306\n",
      "Epoch 00074: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2125 - acc: 0.9306 - val_loss: 0.4937 - val_acc: 0.8698\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9328\n",
      "Epoch 00075: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.2099 - acc: 0.9327 - val_loss: 0.5780 - val_acc: 0.8514\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9312\n",
      "Epoch 00076: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2064 - acc: 0.9312 - val_loss: 0.4695 - val_acc: 0.8810\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9333\n",
      "Epoch 00077: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2058 - acc: 0.9333 - val_loss: 0.5215 - val_acc: 0.8658\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9353\n",
      "Epoch 00078: val_loss did not improve from 0.46546\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2029 - acc: 0.9353 - val_loss: 0.5120 - val_acc: 0.8644\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9363\n",
      "Epoch 00079: val_loss improved from 0.46546 to 0.46526, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/079-0.4653.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1988 - acc: 0.9363 - val_loss: 0.4653 - val_acc: 0.8772\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9356\n",
      "Epoch 00080: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1972 - acc: 0.9356 - val_loss: 0.4727 - val_acc: 0.8800\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9376\n",
      "Epoch 00081: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1924 - acc: 0.9376 - val_loss: 0.5372 - val_acc: 0.8628\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9367\n",
      "Epoch 00082: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1979 - acc: 0.9367 - val_loss: 0.4700 - val_acc: 0.8796\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9365\n",
      "Epoch 00083: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1952 - acc: 0.9365 - val_loss: 0.4732 - val_acc: 0.8791\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9371\n",
      "Epoch 00084: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1936 - acc: 0.9371 - val_loss: 0.4887 - val_acc: 0.8749\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9402\n",
      "Epoch 00085: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1869 - acc: 0.9402 - val_loss: 0.5591 - val_acc: 0.8609\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9395\n",
      "Epoch 00086: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1866 - acc: 0.9395 - val_loss: 0.4721 - val_acc: 0.8835\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9402\n",
      "Epoch 00087: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1881 - acc: 0.9402 - val_loss: 0.5137 - val_acc: 0.8740\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9412\n",
      "Epoch 00088: val_loss did not improve from 0.46526\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1817 - acc: 0.9411 - val_loss: 0.4785 - val_acc: 0.8826\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9403\n",
      "Epoch 00089: val_loss improved from 0.46526 to 0.46175, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/089-0.4617.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1832 - acc: 0.9403 - val_loss: 0.4617 - val_acc: 0.8856\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9447\n",
      "Epoch 00090: val_loss improved from 0.46175 to 0.45850, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/090-0.4585.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1723 - acc: 0.9447 - val_loss: 0.4585 - val_acc: 0.8875\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9427\n",
      "Epoch 00091: val_loss did not improve from 0.45850\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1759 - acc: 0.9427 - val_loss: 0.5099 - val_acc: 0.8754\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9421\n",
      "Epoch 00092: val_loss did not improve from 0.45850\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1794 - acc: 0.9421 - val_loss: 0.4810 - val_acc: 0.8800\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9459\n",
      "Epoch 00093: val_loss improved from 0.45850 to 0.44513, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv_checkpoint/093-0.4451.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1675 - acc: 0.9459 - val_loss: 0.4451 - val_acc: 0.8903\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9469\n",
      "Epoch 00094: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1678 - acc: 0.9469 - val_loss: 0.5092 - val_acc: 0.8735\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9464\n",
      "Epoch 00095: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1667 - acc: 0.9464 - val_loss: 0.4930 - val_acc: 0.8835\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9454\n",
      "Epoch 00096: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1657 - acc: 0.9454 - val_loss: 0.4524 - val_acc: 0.8870\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9438\n",
      "Epoch 00097: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1691 - acc: 0.9438 - val_loss: 0.5364 - val_acc: 0.8686\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9437\n",
      "Epoch 00098: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1708 - acc: 0.9437 - val_loss: 0.4955 - val_acc: 0.8779\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9478\n",
      "Epoch 00099: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1644 - acc: 0.9478 - val_loss: 0.4620 - val_acc: 0.8873\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9475\n",
      "Epoch 00100: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1648 - acc: 0.9475 - val_loss: 0.4860 - val_acc: 0.8798\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9480\n",
      "Epoch 00101: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1623 - acc: 0.9480 - val_loss: 0.5005 - val_acc: 0.8763\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9474\n",
      "Epoch 00102: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1608 - acc: 0.9474 - val_loss: 0.4844 - val_acc: 0.8835\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9490\n",
      "Epoch 00103: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1596 - acc: 0.9490 - val_loss: 0.4639 - val_acc: 0.8838\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9506\n",
      "Epoch 00104: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1557 - acc: 0.9506 - val_loss: 0.5063 - val_acc: 0.8789\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9497\n",
      "Epoch 00105: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.1550 - acc: 0.9497 - val_loss: 0.4614 - val_acc: 0.8849\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9487\n",
      "Epoch 00106: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1582 - acc: 0.9487 - val_loss: 0.4828 - val_acc: 0.8814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9514\n",
      "Epoch 00107: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1536 - acc: 0.9513 - val_loss: 0.5262 - val_acc: 0.8733\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9501\n",
      "Epoch 00108: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1525 - acc: 0.9501 - val_loss: 0.4729 - val_acc: 0.8847\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9516\n",
      "Epoch 00109: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1526 - acc: 0.9516 - val_loss: 0.4877 - val_acc: 0.8786\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9519\n",
      "Epoch 00110: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1482 - acc: 0.9519 - val_loss: 0.5030 - val_acc: 0.8810\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9533- ETA: 1s - loss: \n",
      "Epoch 00111: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1468 - acc: 0.9533 - val_loss: 0.4702 - val_acc: 0.8838\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9523\n",
      "Epoch 00112: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1480 - acc: 0.9523 - val_loss: 0.4940 - val_acc: 0.8814\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9519\n",
      "Epoch 00113: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1472 - acc: 0.9519 - val_loss: 0.4689 - val_acc: 0.8894\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9515\n",
      "Epoch 00114: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1474 - acc: 0.9515 - val_loss: 0.4930 - val_acc: 0.8828\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9535\n",
      "Epoch 00115: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1429 - acc: 0.9534 - val_loss: 0.4875 - val_acc: 0.8817\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9535\n",
      "Epoch 00116: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1418 - acc: 0.9535 - val_loss: 0.4596 - val_acc: 0.8947\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9539\n",
      "Epoch 00117: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1406 - acc: 0.9539 - val_loss: 0.4772 - val_acc: 0.8861\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9548\n",
      "Epoch 00118: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1395 - acc: 0.9548 - val_loss: 0.4892 - val_acc: 0.8838\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9552\n",
      "Epoch 00119: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1403 - acc: 0.9551 - val_loss: 0.5203 - val_acc: 0.8793\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9575\n",
      "Epoch 00120: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1357 - acc: 0.9575 - val_loss: 0.4577 - val_acc: 0.8928\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9563\n",
      "Epoch 00121: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1373 - acc: 0.9563 - val_loss: 0.5256 - val_acc: 0.8777\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9563\n",
      "Epoch 00122: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1363 - acc: 0.9563 - val_loss: 0.5479 - val_acc: 0.8717\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9557\n",
      "Epoch 00123: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1372 - acc: 0.9557 - val_loss: 0.5029 - val_acc: 0.8786\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9567\n",
      "Epoch 00124: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1342 - acc: 0.9567 - val_loss: 0.5349 - val_acc: 0.8786\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9573\n",
      "Epoch 00125: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1308 - acc: 0.9573 - val_loss: 0.4737 - val_acc: 0.8910\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9575\n",
      "Epoch 00126: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1301 - acc: 0.9575 - val_loss: 0.5005 - val_acc: 0.8891\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9579\n",
      "Epoch 00127: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1293 - acc: 0.9579 - val_loss: 0.5830 - val_acc: 0.8644\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9584\n",
      "Epoch 00128: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1286 - acc: 0.9584 - val_loss: 0.4924 - val_acc: 0.8882\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9574\n",
      "Epoch 00129: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.1321 - acc: 0.9575 - val_loss: 0.4785 - val_acc: 0.8891\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9583\n",
      "Epoch 00130: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1290 - acc: 0.9583 - val_loss: 0.4870 - val_acc: 0.8835\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9602\n",
      "Epoch 00131: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1243 - acc: 0.9602 - val_loss: 0.4976 - val_acc: 0.8868\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9603\n",
      "Epoch 00132: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1231 - acc: 0.9603 - val_loss: 0.4699 - val_acc: 0.8915\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9588\n",
      "Epoch 00133: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1257 - acc: 0.9588 - val_loss: 0.4845 - val_acc: 0.8875\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9608\n",
      "Epoch 00134: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1238 - acc: 0.9608 - val_loss: 0.5224 - val_acc: 0.8793\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9614\n",
      "Epoch 00135: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1194 - acc: 0.9614 - val_loss: 0.5307 - val_acc: 0.8779\n",
      "Epoch 136/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9604\n",
      "Epoch 00136: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1239 - acc: 0.9603 - val_loss: 0.4652 - val_acc: 0.8949\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9586\n",
      "Epoch 00137: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1271 - acc: 0.9586 - val_loss: 0.5054 - val_acc: 0.8826\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9595\n",
      "Epoch 00138: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1228 - acc: 0.9595 - val_loss: 0.5010 - val_acc: 0.8854\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9604\n",
      "Epoch 00139: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1201 - acc: 0.9604 - val_loss: 0.5820 - val_acc: 0.8654\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9610\n",
      "Epoch 00140: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1173 - acc: 0.9609 - val_loss: 0.5068 - val_acc: 0.8845\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9604\n",
      "Epoch 00141: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1213 - acc: 0.9604 - val_loss: 0.5134 - val_acc: 0.8805\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9621\n",
      "Epoch 00142: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1152 - acc: 0.9621 - val_loss: 0.4836 - val_acc: 0.8889\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9632\n",
      "Epoch 00143: val_loss did not improve from 0.44513\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1141 - acc: 0.9632 - val_loss: 0.5078 - val_acc: 0.8861\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 447us/sample - loss: 0.5140 - acc: 0.8602\n",
      "Loss: 0.5140387244375572 Accuracy: 0.8602285\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4624 - acc: 0.2623\n",
      "Epoch 00001: val_loss improved from inf to 1.67634, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/001-1.6763.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 2.4622 - acc: 0.2624 - val_loss: 1.6763 - val_acc: 0.4549\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6107 - acc: 0.4836\n",
      "Epoch 00002: val_loss improved from 1.67634 to 1.12438, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/002-1.1244.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.6107 - acc: 0.4836 - val_loss: 1.1244 - val_acc: 0.6490\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3094 - acc: 0.5853\n",
      "Epoch 00003: val_loss improved from 1.12438 to 0.96493, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/003-0.9649.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.3095 - acc: 0.5853 - val_loss: 0.9649 - val_acc: 0.7105\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1178 - acc: 0.6486\n",
      "Epoch 00004: val_loss improved from 0.96493 to 0.87819, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/004-0.8782.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.1178 - acc: 0.6487 - val_loss: 0.8782 - val_acc: 0.7484\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.7035\n",
      "Epoch 00005: val_loss improved from 0.87819 to 0.76184, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/005-0.7618.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.9714 - acc: 0.7035 - val_loss: 0.7618 - val_acc: 0.7808\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8626 - acc: 0.7343\n",
      "Epoch 00006: val_loss improved from 0.76184 to 0.66932, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/006-0.6693.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.8625 - acc: 0.7343 - val_loss: 0.6693 - val_acc: 0.8078\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7710 - acc: 0.7642\n",
      "Epoch 00007: val_loss improved from 0.66932 to 0.60677, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/007-0.6068.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.7710 - acc: 0.7641 - val_loss: 0.6068 - val_acc: 0.8323\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.7890\n",
      "Epoch 00008: val_loss improved from 0.60677 to 0.56038, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/008-0.5604.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.7013 - acc: 0.7890 - val_loss: 0.5604 - val_acc: 0.8425\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6472 - acc: 0.8051\n",
      "Epoch 00009: val_loss improved from 0.56038 to 0.53656, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/009-0.5366.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.6474 - acc: 0.8050 - val_loss: 0.5366 - val_acc: 0.8442\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.8182\n",
      "Epoch 00010: val_loss improved from 0.53656 to 0.50715, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/010-0.5072.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.6031 - acc: 0.8182 - val_loss: 0.5072 - val_acc: 0.8523\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.8307\n",
      "Epoch 00011: val_loss improved from 0.50715 to 0.45552, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/011-0.4555.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.5600 - acc: 0.8306 - val_loss: 0.4555 - val_acc: 0.8703\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8436\n",
      "Epoch 00012: val_loss did not improve from 0.45552\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.5251 - acc: 0.8436 - val_loss: 0.5779 - val_acc: 0.8341\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8494\n",
      "Epoch 00013: val_loss improved from 0.45552 to 0.41382, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/013-0.4138.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.4989 - acc: 0.8494 - val_loss: 0.4138 - val_acc: 0.8838\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8540\n",
      "Epoch 00014: val_loss did not improve from 0.41382\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.4776 - acc: 0.8540 - val_loss: 0.4289 - val_acc: 0.8751\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8621\n",
      "Epoch 00015: val_loss improved from 0.41382 to 0.39973, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/015-0.3997.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.4546 - acc: 0.8621 - val_loss: 0.3997 - val_acc: 0.8901\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8717\n",
      "Epoch 00016: val_loss did not improve from 0.39973\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.4291 - acc: 0.8717 - val_loss: 0.4008 - val_acc: 0.8812\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8755\n",
      "Epoch 00017: val_loss did not improve from 0.39973\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.4138 - acc: 0.8755 - val_loss: 0.4220 - val_acc: 0.8763\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8793\n",
      "Epoch 00018: val_loss improved from 0.39973 to 0.37811, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/018-0.3781.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3990 - acc: 0.8793 - val_loss: 0.3781 - val_acc: 0.8901\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8841\n",
      "Epoch 00019: val_loss did not improve from 0.37811\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.3844 - acc: 0.8840 - val_loss: 0.3915 - val_acc: 0.8861\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8853\n",
      "Epoch 00020: val_loss did not improve from 0.37811\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3761 - acc: 0.8853 - val_loss: 0.5160 - val_acc: 0.8495\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8907\n",
      "Epoch 00021: val_loss improved from 0.37811 to 0.35219, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/021-0.3522.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.3574 - acc: 0.8907 - val_loss: 0.3522 - val_acc: 0.8926\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8950\n",
      "Epoch 00022: val_loss improved from 0.35219 to 0.34485, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/022-0.3449.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.3470 - acc: 0.8950 - val_loss: 0.3449 - val_acc: 0.9078\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8979\n",
      "Epoch 00023: val_loss improved from 0.34485 to 0.30885, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/023-0.3088.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3388 - acc: 0.8979 - val_loss: 0.3088 - val_acc: 0.9133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.9000\n",
      "Epoch 00024: val_loss did not improve from 0.30885\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3260 - acc: 0.8999 - val_loss: 0.3769 - val_acc: 0.8910\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8985\n",
      "Epoch 00025: val_loss did not improve from 0.30885\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3279 - acc: 0.8984 - val_loss: 0.3354 - val_acc: 0.9038\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9059\n",
      "Epoch 00026: val_loss improved from 0.30885 to 0.29065, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/026-0.2907.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3067 - acc: 0.9059 - val_loss: 0.2907 - val_acc: 0.9187\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9082\n",
      "Epoch 00027: val_loss did not improve from 0.29065\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2983 - acc: 0.9081 - val_loss: 0.3096 - val_acc: 0.9103\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9114\n",
      "Epoch 00028: val_loss did not improve from 0.29065\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2891 - acc: 0.9114 - val_loss: 0.2907 - val_acc: 0.9199\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9118\n",
      "Epoch 00029: val_loss improved from 0.29065 to 0.28045, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/029-0.2805.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.2816 - acc: 0.9119 - val_loss: 0.2805 - val_acc: 0.9234\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9162\n",
      "Epoch 00030: val_loss did not improve from 0.28045\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.2739 - acc: 0.9162 - val_loss: 0.3274 - val_acc: 0.9089\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9165\n",
      "Epoch 00031: val_loss did not improve from 0.28045\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.2710 - acc: 0.9165 - val_loss: 0.2917 - val_acc: 0.9164\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9180\n",
      "Epoch 00032: val_loss improved from 0.28045 to 0.26826, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/032-0.2683.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2647 - acc: 0.9180 - val_loss: 0.2683 - val_acc: 0.9278\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9193\n",
      "Epoch 00033: val_loss did not improve from 0.26826\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2585 - acc: 0.9193 - val_loss: 0.2865 - val_acc: 0.9171\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9233\n",
      "Epoch 00034: val_loss did not improve from 0.26826\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2472 - acc: 0.9233 - val_loss: 0.3514 - val_acc: 0.8968\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9241\n",
      "Epoch 00035: val_loss did not improve from 0.26826\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2462 - acc: 0.9241 - val_loss: 0.2790 - val_acc: 0.9217\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9246\n",
      "Epoch 00036: val_loss did not improve from 0.26826\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2390 - acc: 0.9245 - val_loss: 0.3031 - val_acc: 0.9133\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9274\n",
      "Epoch 00037: val_loss improved from 0.26826 to 0.26343, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/037-0.2634.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2333 - acc: 0.9274 - val_loss: 0.2634 - val_acc: 0.9245\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9300\n",
      "Epoch 00038: val_loss did not improve from 0.26343\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2261 - acc: 0.9300 - val_loss: 0.2751 - val_acc: 0.9255\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9282\n",
      "Epoch 00039: val_loss did not improve from 0.26343\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2311 - acc: 0.9282 - val_loss: 0.2741 - val_acc: 0.9259\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9305\n",
      "Epoch 00040: val_loss did not improve from 0.26343\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.2217 - acc: 0.9305 - val_loss: 0.2784 - val_acc: 0.9252\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9312\n",
      "Epoch 00041: val_loss improved from 0.26343 to 0.25852, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/041-0.2585.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2173 - acc: 0.9312 - val_loss: 0.2585 - val_acc: 0.9311\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9340\n",
      "Epoch 00042: val_loss did not improve from 0.25852\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2110 - acc: 0.9339 - val_loss: 0.2767 - val_acc: 0.9250\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9354\n",
      "Epoch 00043: val_loss improved from 0.25852 to 0.24865, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/043-0.2487.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2045 - acc: 0.9354 - val_loss: 0.2487 - val_acc: 0.9322\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9342\n",
      "Epoch 00044: val_loss did not improve from 0.24865\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.2066 - acc: 0.9341 - val_loss: 0.3215 - val_acc: 0.9106\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9359\n",
      "Epoch 00045: val_loss improved from 0.24865 to 0.24598, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/045-0.2460.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2035 - acc: 0.9359 - val_loss: 0.2460 - val_acc: 0.9329\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9371\n",
      "Epoch 00046: val_loss improved from 0.24598 to 0.24318, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/046-0.2432.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1965 - acc: 0.9370 - val_loss: 0.2432 - val_acc: 0.9387\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9378\n",
      "Epoch 00047: val_loss did not improve from 0.24318\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1999 - acc: 0.9378 - val_loss: 0.6759 - val_acc: 0.8164\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9397\n",
      "Epoch 00048: val_loss did not improve from 0.24318\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1896 - acc: 0.9397 - val_loss: 0.2570 - val_acc: 0.9315\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9400\n",
      "Epoch 00049: val_loss did not improve from 0.24318\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1882 - acc: 0.9400 - val_loss: 0.2555 - val_acc: 0.9331\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9430\n",
      "Epoch 00050: val_loss did not improve from 0.24318\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1805 - acc: 0.9430 - val_loss: 0.2888 - val_acc: 0.9236\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9446\n",
      "Epoch 00051: val_loss improved from 0.24318 to 0.24123, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/051-0.2412.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1747 - acc: 0.9446 - val_loss: 0.2412 - val_acc: 0.9327\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9441\n",
      "Epoch 00052: val_loss did not improve from 0.24123\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1732 - acc: 0.9441 - val_loss: 0.2558 - val_acc: 0.9285\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9452\n",
      "Epoch 00053: val_loss did not improve from 0.24123\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1728 - acc: 0.9451 - val_loss: 0.2591 - val_acc: 0.9336\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9460\n",
      "Epoch 00054: val_loss improved from 0.24123 to 0.22980, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/054-0.2298.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1695 - acc: 0.9460 - val_loss: 0.2298 - val_acc: 0.9383\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9467\n",
      "Epoch 00055: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1698 - acc: 0.9467 - val_loss: 0.2821 - val_acc: 0.9278\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9440\n",
      "Epoch 00056: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1711 - acc: 0.9440 - val_loss: 0.2676 - val_acc: 0.9276\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9485\n",
      "Epoch 00057: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1639 - acc: 0.9485 - val_loss: 0.3506 - val_acc: 0.9057\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9509\n",
      "Epoch 00058: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1534 - acc: 0.9509 - val_loss: 0.2625 - val_acc: 0.9299\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9490\n",
      "Epoch 00059: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1587 - acc: 0.9489 - val_loss: 0.2371 - val_acc: 0.9369\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9496\n",
      "Epoch 00060: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1561 - acc: 0.9496 - val_loss: 0.2503 - val_acc: 0.9369\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9529\n",
      "Epoch 00061: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1489 - acc: 0.9529 - val_loss: 0.2390 - val_acc: 0.9338\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9498\n",
      "Epoch 00062: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1547 - acc: 0.9498 - val_loss: 0.2475 - val_acc: 0.9352\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9531\n",
      "Epoch 00063: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1463 - acc: 0.9530 - val_loss: 0.2558 - val_acc: 0.9357\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9527\n",
      "Epoch 00064: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1459 - acc: 0.9526 - val_loss: 0.2618 - val_acc: 0.9317\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9541\n",
      "Epoch 00065: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1421 - acc: 0.9541 - val_loss: 0.2761 - val_acc: 0.9182\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9538\n",
      "Epoch 00066: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1414 - acc: 0.9538 - val_loss: 0.2889 - val_acc: 0.9280\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9555\n",
      "Epoch 00067: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1398 - acc: 0.9554 - val_loss: 0.2372 - val_acc: 0.9399\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9548\n",
      "Epoch 00068: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1407 - acc: 0.9547 - val_loss: 0.2460 - val_acc: 0.9378\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9555\n",
      "Epoch 00069: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1331 - acc: 0.9554 - val_loss: 0.3269 - val_acc: 0.9064\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9552\n",
      "Epoch 00070: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1341 - acc: 0.9552 - val_loss: 0.2546 - val_acc: 0.9338\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9567\n",
      "Epoch 00071: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1326 - acc: 0.9567 - val_loss: 0.2337 - val_acc: 0.9394\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9584\n",
      "Epoch 00072: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1285 - acc: 0.9584 - val_loss: 0.2622 - val_acc: 0.9283\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9588\n",
      "Epoch 00073: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1293 - acc: 0.9587 - val_loss: 0.3144 - val_acc: 0.9145\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9580\n",
      "Epoch 00074: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1285 - acc: 0.9580 - val_loss: 0.2467 - val_acc: 0.9378\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9565\n",
      "Epoch 00075: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1287 - acc: 0.9565 - val_loss: 0.2672 - val_acc: 0.9306\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9595\n",
      "Epoch 00076: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1247 - acc: 0.9595 - val_loss: 0.2685 - val_acc: 0.9320\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9599\n",
      "Epoch 00077: val_loss did not improve from 0.22980\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1197 - acc: 0.9599 - val_loss: 0.2315 - val_acc: 0.9371\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9606\n",
      "Epoch 00078: val_loss improved from 0.22980 to 0.22915, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/078-0.2292.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1204 - acc: 0.9606 - val_loss: 0.2292 - val_acc: 0.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9611\n",
      "Epoch 00079: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1189 - acc: 0.9611 - val_loss: 0.3122 - val_acc: 0.9182\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9619\n",
      "Epoch 00080: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1181 - acc: 0.9619 - val_loss: 0.2574 - val_acc: 0.9276\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9634\n",
      "Epoch 00081: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1131 - acc: 0.9634 - val_loss: 0.2333 - val_acc: 0.9378\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9620\n",
      "Epoch 00082: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1152 - acc: 0.9620 - val_loss: 0.3133 - val_acc: 0.9124\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9614\n",
      "Epoch 00083: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1148 - acc: 0.9614 - val_loss: 0.2620 - val_acc: 0.9299\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9639\n",
      "Epoch 00084: val_loss did not improve from 0.22915\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1116 - acc: 0.9639 - val_loss: 0.2361 - val_acc: 0.9369\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9623\n",
      "Epoch 00085: val_loss improved from 0.22915 to 0.22214, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv_checkpoint/085-0.2221.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1149 - acc: 0.9623 - val_loss: 0.2221 - val_acc: 0.9453\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9654\n",
      "Epoch 00086: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1083 - acc: 0.9654 - val_loss: 0.2664 - val_acc: 0.9252\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9649\n",
      "Epoch 00087: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1085 - acc: 0.9648 - val_loss: 0.2839 - val_acc: 0.9294\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9640\n",
      "Epoch 00088: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1078 - acc: 0.9640 - val_loss: 0.2379 - val_acc: 0.9390\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9660\n",
      "Epoch 00089: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1076 - acc: 0.9660 - val_loss: 0.2375 - val_acc: 0.9387\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9665\n",
      "Epoch 00090: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1031 - acc: 0.9666 - val_loss: 0.2608 - val_acc: 0.9345\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9665\n",
      "Epoch 00091: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1023 - acc: 0.9665 - val_loss: 0.2309 - val_acc: 0.9394\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9674- ETA: 1s - l\n",
      "Epoch 00092: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0993 - acc: 0.9674 - val_loss: 0.2641 - val_acc: 0.9359\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9657\n",
      "Epoch 00093: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1032 - acc: 0.9657 - val_loss: 0.2424 - val_acc: 0.9392\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9678\n",
      "Epoch 00094: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0996 - acc: 0.9677 - val_loss: 0.2429 - val_acc: 0.9366\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9680\n",
      "Epoch 00095: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0959 - acc: 0.9680 - val_loss: 0.2374 - val_acc: 0.9415\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9694\n",
      "Epoch 00096: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0944 - acc: 0.9693 - val_loss: 0.2763 - val_acc: 0.9320\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9686\n",
      "Epoch 00097: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0981 - acc: 0.9686 - val_loss: 0.2315 - val_acc: 0.9415\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9688\n",
      "Epoch 00098: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0944 - acc: 0.9688 - val_loss: 0.2678 - val_acc: 0.9331\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9683\n",
      "Epoch 00099: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0961 - acc: 0.9683 - val_loss: 0.3167 - val_acc: 0.9175\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9681\n",
      "Epoch 00100: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0944 - acc: 0.9681 - val_loss: 0.2722 - val_acc: 0.9324\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9698\n",
      "Epoch 00101: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0906 - acc: 0.9698 - val_loss: 0.2625 - val_acc: 0.9366\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9699\n",
      "Epoch 00102: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0930 - acc: 0.9699 - val_loss: 0.2600 - val_acc: 0.9304\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9691\n",
      "Epoch 00103: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0942 - acc: 0.9691 - val_loss: 0.2661 - val_acc: 0.9315\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9692\n",
      "Epoch 00104: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0921 - acc: 0.9692 - val_loss: 0.2342 - val_acc: 0.9429\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9700\n",
      "Epoch 00105: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0882 - acc: 0.9700 - val_loss: 0.2636 - val_acc: 0.9348\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9729\n",
      "Epoch 00106: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0838 - acc: 0.9728 - val_loss: 0.2636 - val_acc: 0.9355\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9700\n",
      "Epoch 00107: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0929 - acc: 0.9699 - val_loss: 0.2680 - val_acc: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9704\n",
      "Epoch 00108: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0897 - acc: 0.9704 - val_loss: 0.2402 - val_acc: 0.9408\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9721\n",
      "Epoch 00109: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0877 - acc: 0.9721 - val_loss: 0.2512 - val_acc: 0.9373\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9734\n",
      "Epoch 00110: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0825 - acc: 0.9735 - val_loss: 0.2349 - val_acc: 0.9425\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9721\n",
      "Epoch 00111: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0827 - acc: 0.9721 - val_loss: 0.2348 - val_acc: 0.9420\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9712\n",
      "Epoch 00112: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0856 - acc: 0.9712 - val_loss: 0.3215 - val_acc: 0.9129\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9736\n",
      "Epoch 00113: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0808 - acc: 0.9736 - val_loss: 0.2424 - val_acc: 0.9383\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9708\n",
      "Epoch 00114: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0883 - acc: 0.9708 - val_loss: 0.2557 - val_acc: 0.9380\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9735\n",
      "Epoch 00115: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0808 - acc: 0.9735 - val_loss: 0.2705 - val_acc: 0.9364\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9741\n",
      "Epoch 00116: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0798 - acc: 0.9741 - val_loss: 0.2565 - val_acc: 0.9394\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9739\n",
      "Epoch 00117: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0794 - acc: 0.9739 - val_loss: 0.2641 - val_acc: 0.9343\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9743\n",
      "Epoch 00118: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0774 - acc: 0.9743 - val_loss: 0.2647 - val_acc: 0.9327\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9754\n",
      "Epoch 00119: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0755 - acc: 0.9754 - val_loss: 0.2518 - val_acc: 0.9378\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9738\n",
      "Epoch 00120: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0790 - acc: 0.9737 - val_loss: 0.2619 - val_acc: 0.9380\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9730\n",
      "Epoch 00121: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0822 - acc: 0.9730 - val_loss: 0.3226 - val_acc: 0.9173\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9750\n",
      "Epoch 00122: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0768 - acc: 0.9750 - val_loss: 0.2583 - val_acc: 0.9373\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9782\n",
      "Epoch 00123: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0714 - acc: 0.9782 - val_loss: 0.2380 - val_acc: 0.9415\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9760\n",
      "Epoch 00124: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.0760 - acc: 0.9760 - val_loss: 0.2611 - val_acc: 0.9352\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9757\n",
      "Epoch 00125: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0734 - acc: 0.9757 - val_loss: 0.3085 - val_acc: 0.9234\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9766\n",
      "Epoch 00126: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0724 - acc: 0.9766 - val_loss: 0.2462 - val_acc: 0.9411\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9758\n",
      "Epoch 00127: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0723 - acc: 0.9758 - val_loss: 0.2944 - val_acc: 0.9257\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9754\n",
      "Epoch 00128: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0749 - acc: 0.9754 - val_loss: 0.3077 - val_acc: 0.9215\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9773\n",
      "Epoch 00129: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0686 - acc: 0.9773 - val_loss: 0.2504 - val_acc: 0.9420\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9752\n",
      "Epoch 00130: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0735 - acc: 0.9752 - val_loss: 0.2389 - val_acc: 0.9397\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9777\n",
      "Epoch 00131: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.0707 - acc: 0.9777 - val_loss: 0.2440 - val_acc: 0.9427\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9778\n",
      "Epoch 00132: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0667 - acc: 0.9778 - val_loss: 0.2427 - val_acc: 0.9397\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9786\n",
      "Epoch 00133: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0660 - acc: 0.9786 - val_loss: 0.2447 - val_acc: 0.9422\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9781\n",
      "Epoch 00134: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0677 - acc: 0.9781 - val_loss: 0.2599 - val_acc: 0.9401\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9782\n",
      "Epoch 00135: val_loss did not improve from 0.22214\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0688 - acc: 0.9782 - val_loss: 0.2516 - val_acc: 0.9399\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 449us/sample - loss: 0.2756 - acc: 0.9238\n",
      "Loss: 0.27560059349608695 Accuracy: 0.92377985\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6621 - acc: 0.2207\n",
      "Epoch 00001: val_loss improved from inf to 1.84711, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/001-1.8471.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 2.6620 - acc: 0.2207 - val_loss: 1.8471 - val_acc: 0.4284\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6897 - acc: 0.4552\n",
      "Epoch 00002: val_loss improved from 1.84711 to 1.16154, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/002-1.1615.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 1.6896 - acc: 0.4552 - val_loss: 1.1615 - val_acc: 0.6497\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2941 - acc: 0.5799\n",
      "Epoch 00003: val_loss improved from 1.16154 to 0.90096, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/003-0.9010.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 1.2942 - acc: 0.5798 - val_loss: 0.9010 - val_acc: 0.7421\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0538 - acc: 0.6676\n",
      "Epoch 00004: val_loss improved from 0.90096 to 0.71901, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/004-0.7190.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 1.0538 - acc: 0.6675 - val_loss: 0.7190 - val_acc: 0.8039\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8744 - acc: 0.7283\n",
      "Epoch 00005: val_loss improved from 0.71901 to 0.60295, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/005-0.6029.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.8743 - acc: 0.7283 - val_loss: 0.6029 - val_acc: 0.8318\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7710\n",
      "Epoch 00006: val_loss improved from 0.60295 to 0.59027, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/006-0.5903.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.7496 - acc: 0.7710 - val_loss: 0.5903 - val_acc: 0.8274\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.7997\n",
      "Epoch 00007: val_loss improved from 0.59027 to 0.49764, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/007-0.4976.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.6579 - acc: 0.7997 - val_loss: 0.4976 - val_acc: 0.8584\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.8235\n",
      "Epoch 00008: val_loss improved from 0.49764 to 0.45903, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/008-0.4590.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.5723 - acc: 0.8235 - val_loss: 0.4590 - val_acc: 0.8672\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8414\n",
      "Epoch 00009: val_loss did not improve from 0.45903\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.5211 - acc: 0.8414 - val_loss: 0.4690 - val_acc: 0.8593\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8557\n",
      "Epoch 00010: val_loss improved from 0.45903 to 0.37277, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/010-0.3728.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.4785 - acc: 0.8557 - val_loss: 0.3728 - val_acc: 0.8961\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8682\n",
      "Epoch 00011: val_loss improved from 0.37277 to 0.32636, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/011-0.3264.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.4348 - acc: 0.8682 - val_loss: 0.3264 - val_acc: 0.9057\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8789\n",
      "Epoch 00012: val_loss improved from 0.32636 to 0.31077, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/012-0.3108.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.4032 - acc: 0.8788 - val_loss: 0.3108 - val_acc: 0.9101\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8857\n",
      "Epoch 00013: val_loss improved from 0.31077 to 0.27909, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/013-0.2791.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.3766 - acc: 0.8857 - val_loss: 0.2791 - val_acc: 0.9217\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8940\n",
      "Epoch 00014: val_loss improved from 0.27909 to 0.26184, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/014-0.2618.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.3465 - acc: 0.8940 - val_loss: 0.2618 - val_acc: 0.9269\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.9004\n",
      "Epoch 00015: val_loss did not improve from 0.26184\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.3302 - acc: 0.9004 - val_loss: 0.2969 - val_acc: 0.9164\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9040\n",
      "Epoch 00016: val_loss improved from 0.26184 to 0.25100, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/016-0.2510.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.3174 - acc: 0.9039 - val_loss: 0.2510 - val_acc: 0.9285\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9091\n",
      "Epoch 00017: val_loss did not improve from 0.25100\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.3017 - acc: 0.9091 - val_loss: 0.2533 - val_acc: 0.9257\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9123\n",
      "Epoch 00018: val_loss did not improve from 0.25100\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2858 - acc: 0.9123 - val_loss: 0.2624 - val_acc: 0.9196\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9186\n",
      "Epoch 00019: val_loss improved from 0.25100 to 0.23589, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/019-0.2359.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.2673 - acc: 0.9186 - val_loss: 0.2359 - val_acc: 0.9311\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9210\n",
      "Epoch 00020: val_loss did not improve from 0.23589\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.2589 - acc: 0.9210 - val_loss: 0.2696 - val_acc: 0.9185\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9236\n",
      "Epoch 00021: val_loss did not improve from 0.23589\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2487 - acc: 0.9236 - val_loss: 0.3104 - val_acc: 0.9064\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9264\n",
      "Epoch 00022: val_loss improved from 0.23589 to 0.21571, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/022-0.2157.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.2414 - acc: 0.9263 - val_loss: 0.2157 - val_acc: 0.9364\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9274\n",
      "Epoch 00023: val_loss improved from 0.21571 to 0.21395, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/023-0.2139.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.2329 - acc: 0.9274 - val_loss: 0.2139 - val_acc: 0.9352\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9323\n",
      "Epoch 00024: val_loss improved from 0.21395 to 0.19751, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/024-0.1975.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.2204 - acc: 0.9323 - val_loss: 0.1975 - val_acc: 0.9434\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9349\n",
      "Epoch 00025: val_loss did not improve from 0.19751\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2136 - acc: 0.9349 - val_loss: 0.2160 - val_acc: 0.9355\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9345\n",
      "Epoch 00026: val_loss did not improve from 0.19751\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.2077 - acc: 0.9345 - val_loss: 0.2068 - val_acc: 0.9406\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9355\n",
      "Epoch 00027: val_loss did not improve from 0.19751\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.2040 - acc: 0.9355 - val_loss: 0.2105 - val_acc: 0.9399\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9401\n",
      "Epoch 00028: val_loss improved from 0.19751 to 0.19594, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/028-0.1959.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.1923 - acc: 0.9401 - val_loss: 0.1959 - val_acc: 0.9429\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9417\n",
      "Epoch 00029: val_loss improved from 0.19594 to 0.17848, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/029-0.1785.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1899 - acc: 0.9417 - val_loss: 0.1785 - val_acc: 0.9485\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9432\n",
      "Epoch 00030: val_loss improved from 0.17848 to 0.16709, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/030-0.1671.hdf5\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.1835 - acc: 0.9432 - val_loss: 0.1671 - val_acc: 0.9513\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9453\n",
      "Epoch 00031: val_loss did not improve from 0.16709\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.1754 - acc: 0.9453 - val_loss: 0.1876 - val_acc: 0.9446\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9470\n",
      "Epoch 00032: val_loss did not improve from 0.16709\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1706 - acc: 0.9470 - val_loss: 0.1840 - val_acc: 0.9441\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9472\n",
      "Epoch 00033: val_loss did not improve from 0.16709\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1694 - acc: 0.9472 - val_loss: 0.1913 - val_acc: 0.9446\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9468\n",
      "Epoch 00034: val_loss did not improve from 0.16709\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1659 - acc: 0.9468 - val_loss: 0.1826 - val_acc: 0.9476\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9491\n",
      "Epoch 00035: val_loss did not improve from 0.16709\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1591 - acc: 0.9491 - val_loss: 0.1792 - val_acc: 0.9527\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9511\n",
      "Epoch 00036: val_loss improved from 0.16709 to 0.16577, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/036-0.1658.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1562 - acc: 0.9510 - val_loss: 0.1658 - val_acc: 0.9513\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9498\n",
      "Epoch 00037: val_loss did not improve from 0.16577\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.1555 - acc: 0.9498 - val_loss: 0.1719 - val_acc: 0.9485\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9539\n",
      "Epoch 00038: val_loss did not improve from 0.16577\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1478 - acc: 0.9539 - val_loss: 0.1696 - val_acc: 0.9513\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9538\n",
      "Epoch 00039: val_loss did not improve from 0.16577\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1445 - acc: 0.9538 - val_loss: 0.2284 - val_acc: 0.9331\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9562\n",
      "Epoch 00040: val_loss improved from 0.16577 to 0.16095, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/040-0.1609.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1389 - acc: 0.9562 - val_loss: 0.1609 - val_acc: 0.9522\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9571\n",
      "Epoch 00041: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.1343 - acc: 0.9570 - val_loss: 0.1724 - val_acc: 0.9546\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9546\n",
      "Epoch 00042: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1416 - acc: 0.9545 - val_loss: 0.1766 - val_acc: 0.9462\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9587\n",
      "Epoch 00043: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1334 - acc: 0.9586 - val_loss: 0.1673 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9582\n",
      "Epoch 00044: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1315 - acc: 0.9581 - val_loss: 0.1875 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9601\n",
      "Epoch 00045: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1245 - acc: 0.9601 - val_loss: 0.1653 - val_acc: 0.9550\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9599\n",
      "Epoch 00046: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1248 - acc: 0.9599 - val_loss: 0.1713 - val_acc: 0.9536\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9596\n",
      "Epoch 00047: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1251 - acc: 0.9597 - val_loss: 0.1692 - val_acc: 0.9515\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9623\n",
      "Epoch 00048: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1192 - acc: 0.9623 - val_loss: 0.1678 - val_acc: 0.9502\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9626\n",
      "Epoch 00049: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1177 - acc: 0.9626 - val_loss: 0.1797 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9625\n",
      "Epoch 00050: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.1178 - acc: 0.9625 - val_loss: 0.1702 - val_acc: 0.9490\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9659\n",
      "Epoch 00051: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.1079 - acc: 0.9659 - val_loss: 0.1812 - val_acc: 0.9448\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9640\n",
      "Epoch 00052: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.1115 - acc: 0.9640 - val_loss: 0.1934 - val_acc: 0.9478\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9667\n",
      "Epoch 00053: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1042 - acc: 0.9667 - val_loss: 0.1663 - val_acc: 0.9532\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9659\n",
      "Epoch 00054: val_loss did not improve from 0.16095\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.1064 - acc: 0.9659 - val_loss: 0.1980 - val_acc: 0.9401\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9684\n",
      "Epoch 00055: val_loss improved from 0.16095 to 0.15989, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/055-0.1599.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1009 - acc: 0.9684 - val_loss: 0.1599 - val_acc: 0.9534\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9673\n",
      "Epoch 00056: val_loss did not improve from 0.15989\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1010 - acc: 0.9673 - val_loss: 0.2250 - val_acc: 0.9364\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9670\n",
      "Epoch 00057: val_loss improved from 0.15989 to 0.15487, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/057-0.1549.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1021 - acc: 0.9670 - val_loss: 0.1549 - val_acc: 0.9543\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9688\n",
      "Epoch 00058: val_loss did not improve from 0.15487\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0953 - acc: 0.9688 - val_loss: 0.1872 - val_acc: 0.9504\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9695\n",
      "Epoch 00059: val_loss improved from 0.15487 to 0.15337, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/059-0.1534.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0953 - acc: 0.9695 - val_loss: 0.1534 - val_acc: 0.9555\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9683\n",
      "Epoch 00060: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0972 - acc: 0.9683 - val_loss: 0.1822 - val_acc: 0.9455\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9702\n",
      "Epoch 00061: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0914 - acc: 0.9702 - val_loss: 0.1707 - val_acc: 0.9518\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9709\n",
      "Epoch 00062: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0909 - acc: 0.9709 - val_loss: 0.1639 - val_acc: 0.9518\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9721\n",
      "Epoch 00063: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0866 - acc: 0.9721 - val_loss: 0.1657 - val_acc: 0.9553\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9705\n",
      "Epoch 00064: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0921 - acc: 0.9705 - val_loss: 0.1652 - val_acc: 0.9539\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9724\n",
      "Epoch 00065: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0848 - acc: 0.9724 - val_loss: 0.1744 - val_acc: 0.9488\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9728\n",
      "Epoch 00066: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0849 - acc: 0.9728 - val_loss: 0.1599 - val_acc: 0.9541\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9742\n",
      "Epoch 00067: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0810 - acc: 0.9742 - val_loss: 0.2587 - val_acc: 0.9285\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9725\n",
      "Epoch 00068: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0855 - acc: 0.9724 - val_loss: 0.1713 - val_acc: 0.9515\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9729\n",
      "Epoch 00069: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0824 - acc: 0.9729 - val_loss: 0.1789 - val_acc: 0.9529\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9749\n",
      "Epoch 00070: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0768 - acc: 0.9749 - val_loss: 0.1774 - val_acc: 0.9518\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9745\n",
      "Epoch 00071: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0784 - acc: 0.9745 - val_loss: 0.1853 - val_acc: 0.9488\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9740\n",
      "Epoch 00072: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0774 - acc: 0.9739 - val_loss: 0.1717 - val_acc: 0.9485\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9740\n",
      "Epoch 00073: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0788 - acc: 0.9740 - val_loss: 0.1781 - val_acc: 0.9525\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9758\n",
      "Epoch 00074: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0732 - acc: 0.9758 - val_loss: 0.1975 - val_acc: 0.9446\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9750\n",
      "Epoch 00075: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0750 - acc: 0.9750 - val_loss: 0.1728 - val_acc: 0.9527\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9749\n",
      "Epoch 00076: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0764 - acc: 0.9749 - val_loss: 0.1665 - val_acc: 0.9543\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9783\n",
      "Epoch 00077: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0680 - acc: 0.9783 - val_loss: 0.1852 - val_acc: 0.9497\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9780\n",
      "Epoch 00078: val_loss did not improve from 0.15337\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0692 - acc: 0.9780 - val_loss: 0.2009 - val_acc: 0.9446\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9756\n",
      "Epoch 00079: val_loss improved from 0.15337 to 0.14819, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv_checkpoint/079-0.1482.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0744 - acc: 0.9756 - val_loss: 0.1482 - val_acc: 0.9562\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9786\n",
      "Epoch 00080: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0656 - acc: 0.9786 - val_loss: 0.1633 - val_acc: 0.9585\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9783\n",
      "Epoch 00081: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0680 - acc: 0.9783 - val_loss: 0.2005 - val_acc: 0.9439\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9793\n",
      "Epoch 00082: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0639 - acc: 0.9793 - val_loss: 0.1928 - val_acc: 0.9488\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9795\n",
      "Epoch 00083: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0626 - acc: 0.9795 - val_loss: 0.1843 - val_acc: 0.9509\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9783\n",
      "Epoch 00084: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0674 - acc: 0.9783 - val_loss: 0.1944 - val_acc: 0.9490\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9792\n",
      "Epoch 00085: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0641 - acc: 0.9792 - val_loss: 0.1715 - val_acc: 0.9527\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9807\n",
      "Epoch 00086: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0594 - acc: 0.9807 - val_loss: 0.1850 - val_acc: 0.9527\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9795\n",
      "Epoch 00087: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0621 - acc: 0.9795 - val_loss: 0.1705 - val_acc: 0.9560\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9790\n",
      "Epoch 00088: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0635 - acc: 0.9790 - val_loss: 0.1663 - val_acc: 0.9518\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9819\n",
      "Epoch 00089: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0570 - acc: 0.9819 - val_loss: 0.1991 - val_acc: 0.9448\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9793\n",
      "Epoch 00090: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0638 - acc: 0.9793 - val_loss: 0.1738 - val_acc: 0.9548\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9826\n",
      "Epoch 00091: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0546 - acc: 0.9826 - val_loss: 0.1845 - val_acc: 0.9450\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9808\n",
      "Epoch 00092: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0581 - acc: 0.9808 - val_loss: 0.1627 - val_acc: 0.9569\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9818\n",
      "Epoch 00093: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0564 - acc: 0.9818 - val_loss: 0.2405 - val_acc: 0.9373\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9804\n",
      "Epoch 00094: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0586 - acc: 0.9804 - val_loss: 0.2082 - val_acc: 0.9441\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9827\n",
      "Epoch 00095: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0537 - acc: 0.9827 - val_loss: 0.1727 - val_acc: 0.9527\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9830\n",
      "Epoch 00096: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0521 - acc: 0.9830 - val_loss: 0.1663 - val_acc: 0.9546\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9821\n",
      "Epoch 00097: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0551 - acc: 0.9821 - val_loss: 0.1879 - val_acc: 0.9502\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9836\n",
      "Epoch 00098: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0508 - acc: 0.9836 - val_loss: 0.1771 - val_acc: 0.9534\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9826\n",
      "Epoch 00099: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0534 - acc: 0.9826 - val_loss: 0.1784 - val_acc: 0.9534\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9839\n",
      "Epoch 00100: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0496 - acc: 0.9839 - val_loss: 0.1766 - val_acc: 0.9529\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9840\n",
      "Epoch 00101: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0495 - acc: 0.9840 - val_loss: 0.1815 - val_acc: 0.9543\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9840\n",
      "Epoch 00102: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0506 - acc: 0.9841 - val_loss: 0.1853 - val_acc: 0.9527\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9831- ETA: 1s\n",
      "Epoch 00103: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0515 - acc: 0.9831 - val_loss: 0.1665 - val_acc: 0.9569\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9837\n",
      "Epoch 00104: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0488 - acc: 0.9837 - val_loss: 0.1736 - val_acc: 0.9567\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9845\n",
      "Epoch 00105: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0467 - acc: 0.9845 - val_loss: 0.1781 - val_acc: 0.9527\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9841\n",
      "Epoch 00106: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0479 - acc: 0.9841 - val_loss: 0.2614 - val_acc: 0.9336\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9860\n",
      "Epoch 00107: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.0436 - acc: 0.9860 - val_loss: 0.1790 - val_acc: 0.9522\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9826\n",
      "Epoch 00108: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0513 - acc: 0.9826 - val_loss: 0.1739 - val_acc: 0.9553\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9855\n",
      "Epoch 00109: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0462 - acc: 0.9855 - val_loss: 0.1759 - val_acc: 0.9529\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9825\n",
      "Epoch 00110: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0541 - acc: 0.9825 - val_loss: 0.1793 - val_acc: 0.9527\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9851\n",
      "Epoch 00111: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0473 - acc: 0.9851 - val_loss: 0.1855 - val_acc: 0.9509\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9849\n",
      "Epoch 00112: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0466 - acc: 0.9849 - val_loss: 0.1701 - val_acc: 0.9541\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9847\n",
      "Epoch 00113: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0487 - acc: 0.9846 - val_loss: 0.1907 - val_acc: 0.9515\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9857\n",
      "Epoch 00114: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0451 - acc: 0.9857 - val_loss: 0.1945 - val_acc: 0.9553\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9831\n",
      "Epoch 00115: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0507 - acc: 0.9831 - val_loss: 0.1785 - val_acc: 0.9504\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9860\n",
      "Epoch 00116: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0420 - acc: 0.9860 - val_loss: 0.2146 - val_acc: 0.9453\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9860\n",
      "Epoch 00117: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0431 - acc: 0.9860 - val_loss: 0.1864 - val_acc: 0.9536\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9875\n",
      "Epoch 00118: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0409 - acc: 0.9875 - val_loss: 0.1799 - val_acc: 0.9541\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9855\n",
      "Epoch 00119: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0450 - acc: 0.9855 - val_loss: 0.1711 - val_acc: 0.9529\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9867\n",
      "Epoch 00120: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0422 - acc: 0.9867 - val_loss: 0.1769 - val_acc: 0.9560\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9874\n",
      "Epoch 00121: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0419 - acc: 0.9874 - val_loss: 0.2118 - val_acc: 0.9495\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9868\n",
      "Epoch 00122: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0407 - acc: 0.9868 - val_loss: 0.2001 - val_acc: 0.9529\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9868\n",
      "Epoch 00123: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0413 - acc: 0.9867 - val_loss: 0.2699 - val_acc: 0.9341\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9851\n",
      "Epoch 00124: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0458 - acc: 0.9851 - val_loss: 0.2009 - val_acc: 0.9529\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9882\n",
      "Epoch 00125: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0372 - acc: 0.9882 - val_loss: 0.2185 - val_acc: 0.9455\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9878\n",
      "Epoch 00126: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0381 - acc: 0.9878 - val_loss: 0.1821 - val_acc: 0.9557\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9859\n",
      "Epoch 00127: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0425 - acc: 0.9859 - val_loss: 0.2128 - val_acc: 0.9460\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9886\n",
      "Epoch 00128: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.0354 - acc: 0.9886 - val_loss: 0.1836 - val_acc: 0.9522\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9884\n",
      "Epoch 00129: val_loss did not improve from 0.14819\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0379 - acc: 0.9884 - val_loss: 0.2364 - val_acc: 0.9511\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 518us/sample - loss: 0.2017 - acc: 0.9379\n",
      "Loss: 0.20172141901181617 Accuracy: 0.9379024\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7000 - acc: 0.2161\n",
      "Epoch 00001: val_loss improved from inf to 1.82984, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/001-1.8298.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.7001 - acc: 0.2161 - val_loss: 1.8298 - val_acc: 0.4430\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7009 - acc: 0.4497\n",
      "Epoch 00002: val_loss improved from 1.82984 to 1.07609, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/002-1.0761.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 1.7009 - acc: 0.4497 - val_loss: 1.0761 - val_acc: 0.6746\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2488 - acc: 0.5972\n",
      "Epoch 00003: val_loss improved from 1.07609 to 0.78230, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/003-0.7823.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 1.2489 - acc: 0.5972 - val_loss: 0.7823 - val_acc: 0.7736\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9523 - acc: 0.6965\n",
      "Epoch 00004: val_loss improved from 0.78230 to 0.60819, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/004-0.6082.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.9523 - acc: 0.6965 - val_loss: 0.6082 - val_acc: 0.8262\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7584\n",
      "Epoch 00005: val_loss improved from 0.60819 to 0.49841, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/005-0.4984.hdf5\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.7725 - acc: 0.7584 - val_loss: 0.4984 - val_acc: 0.8567\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6553 - acc: 0.7973\n",
      "Epoch 00006: val_loss improved from 0.49841 to 0.42771, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/006-0.4277.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.6553 - acc: 0.7973 - val_loss: 0.4277 - val_acc: 0.8796\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.8259\n",
      "Epoch 00007: val_loss did not improve from 0.42771\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.5660 - acc: 0.8259 - val_loss: 0.4756 - val_acc: 0.8546\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8468\n",
      "Epoch 00008: val_loss improved from 0.42771 to 0.42137, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/008-0.4214.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.5045 - acc: 0.8468 - val_loss: 0.4214 - val_acc: 0.8679\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8620\n",
      "Epoch 00009: val_loss improved from 0.42137 to 0.40407, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/009-0.4041.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4546 - acc: 0.8620 - val_loss: 0.4041 - val_acc: 0.8798\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8733\n",
      "Epoch 00010: val_loss improved from 0.40407 to 0.32997, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/010-0.3300.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.4145 - acc: 0.8733 - val_loss: 0.3300 - val_acc: 0.9057\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8818\n",
      "Epoch 00011: val_loss did not improve from 0.32997\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.3822 - acc: 0.8818 - val_loss: 0.3541 - val_acc: 0.8952\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8916\n",
      "Epoch 00012: val_loss improved from 0.32997 to 0.24686, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/012-0.2469.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.3497 - acc: 0.8916 - val_loss: 0.2469 - val_acc: 0.9297\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8995\n",
      "Epoch 00013: val_loss did not improve from 0.24686\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3281 - acc: 0.8995 - val_loss: 0.2653 - val_acc: 0.9192\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9054\n",
      "Epoch 00014: val_loss did not improve from 0.24686\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.3057 - acc: 0.9054 - val_loss: 0.2692 - val_acc: 0.9192\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9113\n",
      "Epoch 00015: val_loss did not improve from 0.24686\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.2859 - acc: 0.9113 - val_loss: 0.3275 - val_acc: 0.8994\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9154\n",
      "Epoch 00016: val_loss did not improve from 0.24686\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.2717 - acc: 0.9154 - val_loss: 0.2618 - val_acc: 0.9231\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9210\n",
      "Epoch 00017: val_loss did not improve from 0.24686\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.2560 - acc: 0.9209 - val_loss: 0.2798 - val_acc: 0.9171\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9241\n",
      "Epoch 00018: val_loss improved from 0.24686 to 0.22217, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/018-0.2222.hdf5\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.2482 - acc: 0.9241 - val_loss: 0.2222 - val_acc: 0.9329\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9267\n",
      "Epoch 00019: val_loss did not improve from 0.22217\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.2349 - acc: 0.9267 - val_loss: 0.2300 - val_acc: 0.9283\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9297\n",
      "Epoch 00020: val_loss did not improve from 0.22217\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.2256 - acc: 0.9297 - val_loss: 0.2238 - val_acc: 0.9348\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9324\n",
      "Epoch 00021: val_loss did not improve from 0.22217\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2144 - acc: 0.9323 - val_loss: 0.2396 - val_acc: 0.9266\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9349\n",
      "Epoch 00022: val_loss improved from 0.22217 to 0.21247, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/022-0.2125.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.2082 - acc: 0.9349 - val_loss: 0.2125 - val_acc: 0.9406\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9384\n",
      "Epoch 00023: val_loss improved from 0.21247 to 0.19432, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/023-0.1943.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.1944 - acc: 0.9384 - val_loss: 0.1943 - val_acc: 0.9397\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9393\n",
      "Epoch 00024: val_loss did not improve from 0.19432\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1926 - acc: 0.9393 - val_loss: 0.2178 - val_acc: 0.9327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9427\n",
      "Epoch 00025: val_loss improved from 0.19432 to 0.19025, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/025-0.1902.hdf5\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.1848 - acc: 0.9426 - val_loss: 0.1902 - val_acc: 0.9413\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9446\n",
      "Epoch 00026: val_loss did not improve from 0.19025\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1758 - acc: 0.9445 - val_loss: 0.1992 - val_acc: 0.9408\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9467\n",
      "Epoch 00027: val_loss improved from 0.19025 to 0.18549, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/027-0.1855.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.1702 - acc: 0.9467 - val_loss: 0.1855 - val_acc: 0.9439\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9470\n",
      "Epoch 00028: val_loss did not improve from 0.18549\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.1648 - acc: 0.9470 - val_loss: 0.1887 - val_acc: 0.9408\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9504\n",
      "Epoch 00029: val_loss did not improve from 0.18549\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1568 - acc: 0.9503 - val_loss: 0.2065 - val_acc: 0.9383\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9525\n",
      "Epoch 00030: val_loss improved from 0.18549 to 0.18031, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/030-0.1803.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1525 - acc: 0.9525 - val_loss: 0.1803 - val_acc: 0.9490\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9539\n",
      "Epoch 00031: val_loss did not improve from 0.18031\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1477 - acc: 0.9539 - val_loss: 0.2083 - val_acc: 0.9348\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9554\n",
      "Epoch 00032: val_loss improved from 0.18031 to 0.16837, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/032-0.1684.hdf5\n",
      "36805/36805 [==============================] - 34s 928us/sample - loss: 0.1430 - acc: 0.9554 - val_loss: 0.1684 - val_acc: 0.9497\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9558\n",
      "Epoch 00033: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1395 - acc: 0.9558 - val_loss: 0.2105 - val_acc: 0.9446\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9585\n",
      "Epoch 00034: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1328 - acc: 0.9585 - val_loss: 0.1797 - val_acc: 0.9448\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9592\n",
      "Epoch 00035: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1295 - acc: 0.9591 - val_loss: 0.1827 - val_acc: 0.9439\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9594\n",
      "Epoch 00036: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1285 - acc: 0.9593 - val_loss: 0.1774 - val_acc: 0.9432\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9614\n",
      "Epoch 00037: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1238 - acc: 0.9614 - val_loss: 0.1830 - val_acc: 0.9448\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9628\n",
      "Epoch 00038: val_loss did not improve from 0.16837\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1168 - acc: 0.9627 - val_loss: 0.1696 - val_acc: 0.9478\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9606\n",
      "Epoch 00039: val_loss improved from 0.16837 to 0.16499, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/039-0.1650.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.1238 - acc: 0.9606 - val_loss: 0.1650 - val_acc: 0.9499\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9634\n",
      "Epoch 00040: val_loss improved from 0.16499 to 0.15973, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv_checkpoint/040-0.1597.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.1162 - acc: 0.9634 - val_loss: 0.1597 - val_acc: 0.9518\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9644\n",
      "Epoch 00041: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1099 - acc: 0.9644 - val_loss: 0.1690 - val_acc: 0.9492\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9661\n",
      "Epoch 00042: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.1064 - acc: 0.9661 - val_loss: 0.1699 - val_acc: 0.9499\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9685\n",
      "Epoch 00043: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1021 - acc: 0.9685 - val_loss: 0.1661 - val_acc: 0.9495\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9684\n",
      "Epoch 00044: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1002 - acc: 0.9684 - val_loss: 0.1853 - val_acc: 0.9464\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9695\n",
      "Epoch 00045: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0962 - acc: 0.9695 - val_loss: 0.1783 - val_acc: 0.9462\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9692\n",
      "Epoch 00046: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0955 - acc: 0.9692 - val_loss: 0.1858 - val_acc: 0.9462\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9688\n",
      "Epoch 00047: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0967 - acc: 0.9688 - val_loss: 0.1821 - val_acc: 0.9434\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9710\n",
      "Epoch 00048: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0921 - acc: 0.9710 - val_loss: 0.1628 - val_acc: 0.9485\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9717\n",
      "Epoch 00049: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0881 - acc: 0.9717 - val_loss: 0.1678 - val_acc: 0.9490\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9721\n",
      "Epoch 00050: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0870 - acc: 0.9721 - val_loss: 0.1767 - val_acc: 0.9485\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9721\n",
      "Epoch 00051: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0854 - acc: 0.9721 - val_loss: 0.1833 - val_acc: 0.9469\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9722\n",
      "Epoch 00052: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0846 - acc: 0.9722 - val_loss: 0.1789 - val_acc: 0.9474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9737\n",
      "Epoch 00053: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0801 - acc: 0.9738 - val_loss: 0.1791 - val_acc: 0.9490\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9752\n",
      "Epoch 00054: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0760 - acc: 0.9752 - val_loss: 0.2223 - val_acc: 0.9359\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9757\n",
      "Epoch 00055: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0750 - acc: 0.9757 - val_loss: 0.1706 - val_acc: 0.9525\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9750\n",
      "Epoch 00056: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0773 - acc: 0.9750 - val_loss: 0.1661 - val_acc: 0.9499\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9775\n",
      "Epoch 00057: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0712 - acc: 0.9774 - val_loss: 0.1829 - val_acc: 0.9476\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9750\n",
      "Epoch 00058: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0777 - acc: 0.9749 - val_loss: 0.1931 - val_acc: 0.9481\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9764\n",
      "Epoch 00059: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0752 - acc: 0.9764 - val_loss: 0.1717 - val_acc: 0.9502\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9800\n",
      "Epoch 00060: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0646 - acc: 0.9800 - val_loss: 0.1838 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9805\n",
      "Epoch 00061: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0630 - acc: 0.9805 - val_loss: 0.1707 - val_acc: 0.9506\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9789\n",
      "Epoch 00062: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0660 - acc: 0.9789 - val_loss: 0.1740 - val_acc: 0.9504\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9803\n",
      "Epoch 00063: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0634 - acc: 0.9802 - val_loss: 0.1910 - val_acc: 0.9448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9766\n",
      "Epoch 00064: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0708 - acc: 0.9766 - val_loss: 0.1733 - val_acc: 0.9504\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9795\n",
      "Epoch 00065: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0634 - acc: 0.9795 - val_loss: 0.1692 - val_acc: 0.9490\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9814\n",
      "Epoch 00066: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0578 - acc: 0.9814 - val_loss: 0.1858 - val_acc: 0.9460\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9818\n",
      "Epoch 00067: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0578 - acc: 0.9819 - val_loss: 0.1870 - val_acc: 0.9485\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9836\n",
      "Epoch 00068: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0530 - acc: 0.9836 - val_loss: 0.2122 - val_acc: 0.9425\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9812\n",
      "Epoch 00069: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0595 - acc: 0.9811 - val_loss: 0.1844 - val_acc: 0.9504\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9817\n",
      "Epoch 00070: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0584 - acc: 0.9816 - val_loss: 0.1762 - val_acc: 0.9502\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9806\n",
      "Epoch 00071: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0578 - acc: 0.9806 - val_loss: 0.1871 - val_acc: 0.9499\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9841\n",
      "Epoch 00072: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0515 - acc: 0.9841 - val_loss: 0.1850 - val_acc: 0.9490\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9814\n",
      "Epoch 00073: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0575 - acc: 0.9814 - val_loss: 0.1702 - val_acc: 0.9548\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9836\n",
      "Epoch 00074: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0509 - acc: 0.9836 - val_loss: 0.1967 - val_acc: 0.9497\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9851\n",
      "Epoch 00075: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0478 - acc: 0.9850 - val_loss: 0.1940 - val_acc: 0.9474\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9818\n",
      "Epoch 00076: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0602 - acc: 0.9818 - val_loss: 0.1916 - val_acc: 0.9511\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9848\n",
      "Epoch 00077: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0493 - acc: 0.9848 - val_loss: 0.1598 - val_acc: 0.9557\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9853\n",
      "Epoch 00078: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0448 - acc: 0.9853 - val_loss: 0.1796 - val_acc: 0.9522\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9857\n",
      "Epoch 00079: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0452 - acc: 0.9857 - val_loss: 0.1843 - val_acc: 0.9515\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9858\n",
      "Epoch 00080: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0466 - acc: 0.9858 - val_loss: 0.2093 - val_acc: 0.9457\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9843\n",
      "Epoch 00081: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0492 - acc: 0.9843 - val_loss: 0.1770 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9845\n",
      "Epoch 00082: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0492 - acc: 0.9844 - val_loss: 0.1714 - val_acc: 0.9569\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9842\n",
      "Epoch 00083: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0485 - acc: 0.9842 - val_loss: 0.1901 - val_acc: 0.9513\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9873\n",
      "Epoch 00084: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0412 - acc: 0.9873 - val_loss: 0.1859 - val_acc: 0.9546\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9850\n",
      "Epoch 00085: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0469 - acc: 0.9850 - val_loss: 0.1761 - val_acc: 0.9543\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9887\n",
      "Epoch 00086: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0369 - acc: 0.9887 - val_loss: 0.1943 - val_acc: 0.9506\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9856\n",
      "Epoch 00087: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0444 - acc: 0.9856 - val_loss: 0.1953 - val_acc: 0.9481\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9844\n",
      "Epoch 00088: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0501 - acc: 0.9844 - val_loss: 0.1699 - val_acc: 0.9583\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9850\n",
      "Epoch 00089: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0460 - acc: 0.9850 - val_loss: 0.1922 - val_acc: 0.9513\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9891\n",
      "Epoch 00090: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0347 - acc: 0.9891 - val_loss: 0.1833 - val_acc: 0.9536\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 578us/sample - loss: 0.2084 - acc: 0.9394\n",
      "Loss: 0.20844632921003478 Accuracy: 0.9393562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 75808)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,223,824\n",
      "Trainable params: 1,223,632\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 501us/sample - loss: 1.3305 - acc: 0.6191\n",
      "Loss: 1.3304850657285807 Accuracy: 0.61910695\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 25248)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,144\n",
      "Trainable params: 419,888\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 559us/sample - loss: 0.9603 - acc: 0.7400\n",
      "Loss: 0.9603418133340521 Accuracy: 0.7399792\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 10464)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 194,160\n",
      "Trainable params: 193,776\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 542us/sample - loss: 0.5140 - acc: 0.8602\n",
      "Loss: 0.5140387244375572 Accuracy: 0.8602285\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 5504)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 135,600\n",
      "Trainable params: 135,088\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 571us/sample - loss: 0.2756 - acc: 0.9238\n",
      "Loss: 0.27560059349608695 Accuracy: 0.92377985\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1792)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 97,008\n",
      "Trainable params: 96,368\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 604us/sample - loss: 0.2017 - acc: 0.9379\n",
      "Loss: 0.20172141901181617 Accuracy: 0.9379024\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 576)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 98,352\n",
      "Trainable params: 97,584\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 651us/sample - loss: 0.2084 - acc: 0.9394\n",
      "Loss: 0.20844632921003478 Accuracy: 0.9393562\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 75808)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,223,824\n",
      "Trainable params: 1,223,632\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 643us/sample - loss: 2.2485 - acc: 0.6098\n",
      "Loss: 2.2484614069216717 Accuracy: 0.6097612\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 25248)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,144\n",
      "Trainable params: 419,888\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 628us/sample - loss: 1.1917 - acc: 0.7333\n",
      "Loss: 1.1917428730432862 Accuracy: 0.73333335\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 10464)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 194,160\n",
      "Trainable params: 193,776\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 715us/sample - loss: 0.5749 - acc: 0.8579\n",
      "Loss: 0.5748625411546614 Accuracy: 0.85794395\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 5504)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 135,600\n",
      "Trainable params: 135,088\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 745us/sample - loss: 0.2780 - acc: 0.9265\n",
      "Loss: 0.27802235705706435 Accuracy: 0.92647976\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1792)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 97,008\n",
      "Trainable params: 96,368\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 699us/sample - loss: 0.2886 - acc: 0.9298\n",
      "Loss: 0.2885684156343573 Accuracy: 0.9298027\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 576)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 98,352\n",
      "Trainable params: 97,584\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 732us/sample - loss: 0.2371 - acc: 0.9410\n",
      "Loss: 0.2370868181451954 Accuracy: 0.9410176\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
