{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_BN(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 4096000)           16384000  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 81,922,576\n",
      "Trainable params: 73,730,064\n",
      "Non-trainable params: 8,192,512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 1365248)           5460992   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 27,636,496\n",
      "Trainable params: 24,904,976\n",
      "Non-trainable params: 2,731,520\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 454912)            1819648   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 9,758,736\n",
      "Trainable params: 8,847,376\n",
      "Non-trainable params: 911,360\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 151552)            606208    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 4,020,496\n",
      "Trainable params: 3,715,344\n",
      "Non-trainable params: 305,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,658,256\n",
      "Trainable params: 1,605,520\n",
      "Non-trainable params: 52,736\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,402,896\n",
      "Trainable params: 1,383,696\n",
      "Non-trainable params: 19,200\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,372,816\n",
      "Trainable params: 1,364,624\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,419,536\n",
      "Trainable params: 1,414,672\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 7, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,445,456\n",
      "Trainable params: 1,442,000\n",
      "Non-trainable params: 3,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5479 - acc: 0.5283\n",
      "Epoch 00001: val_loss improved from inf to 1.30350, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/001-1.3035.hdf5\n",
      "36805/36805 [==============================] - 430s 12ms/sample - loss: 1.5484 - acc: 0.5282 - val_loss: 1.3035 - val_acc: 0.5959\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.7198\n",
      "Epoch 00002: val_loss improved from 1.30350 to 0.87471, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/002-0.8747.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.9509 - acc: 0.7198 - val_loss: 0.8747 - val_acc: 0.7424\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7794\n",
      "Epoch 00003: val_loss improved from 0.87471 to 0.72946, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/003-0.7295.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.7593 - acc: 0.7793 - val_loss: 0.7295 - val_acc: 0.7850\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6422 - acc: 0.8127\n",
      "Epoch 00004: val_loss did not improve from 0.72946\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.6423 - acc: 0.8126 - val_loss: 0.7671 - val_acc: 0.7801\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8399\n",
      "Epoch 00005: val_loss did not improve from 0.72946\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.5488 - acc: 0.8399 - val_loss: 0.7495 - val_acc: 0.7785\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4810 - acc: 0.8618\n",
      "Epoch 00006: val_loss improved from 0.72946 to 0.66195, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/006-0.6619.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.4814 - acc: 0.8618 - val_loss: 0.6619 - val_acc: 0.8074\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8806\n",
      "Epoch 00007: val_loss improved from 0.66195 to 0.57923, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/007-0.5792.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.4139 - acc: 0.8806 - val_loss: 0.5792 - val_acc: 0.8300\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8984\n",
      "Epoch 00008: val_loss did not improve from 0.57923\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.3520 - acc: 0.8984 - val_loss: 0.6228 - val_acc: 0.8288\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9077\n",
      "Epoch 00009: val_loss did not improve from 0.57923\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.3197 - acc: 0.9077 - val_loss: 0.6501 - val_acc: 0.8120\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9139\n",
      "Epoch 00010: val_loss improved from 0.57923 to 0.53152, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/010-0.5315.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.2942 - acc: 0.9138 - val_loss: 0.5315 - val_acc: 0.8491\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9308\n",
      "Epoch 00011: val_loss did not improve from 0.53152\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.2429 - acc: 0.9308 - val_loss: 0.7553 - val_acc: 0.7955\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9374\n",
      "Epoch 00012: val_loss improved from 0.53152 to 0.52891, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/012-0.5289.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2188 - acc: 0.9373 - val_loss: 0.5289 - val_acc: 0.8493\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9427\n",
      "Epoch 00013: val_loss did not improve from 0.52891\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.1978 - acc: 0.9427 - val_loss: 0.6235 - val_acc: 0.8328\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9514\n",
      "Epoch 00014: val_loss did not improve from 0.52891\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1738 - acc: 0.9513 - val_loss: 0.5572 - val_acc: 0.8526\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9595\n",
      "Epoch 00015: val_loss did not improve from 0.52891\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1505 - acc: 0.9594 - val_loss: 0.7894 - val_acc: 0.7920\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9540\n",
      "Epoch 00016: val_loss did not improve from 0.52891\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1624 - acc: 0.9540 - val_loss: 0.6568 - val_acc: 0.8318\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9703\n",
      "Epoch 00017: val_loss improved from 0.52891 to 0.51756, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/017-0.5176.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1157 - acc: 0.9703 - val_loss: 0.5176 - val_acc: 0.8607\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9692\n",
      "Epoch 00018: val_loss did not improve from 0.51756\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1169 - acc: 0.9692 - val_loss: 0.5365 - val_acc: 0.8588\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9775\n",
      "Epoch 00019: val_loss did not improve from 0.51756\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0930 - acc: 0.9775 - val_loss: 0.6049 - val_acc: 0.8500\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9729\n",
      "Epoch 00020: val_loss did not improve from 0.51756\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1053 - acc: 0.9729 - val_loss: 0.6455 - val_acc: 0.8418\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9787\n",
      "Epoch 00021: val_loss did not improve from 0.51756\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0886 - acc: 0.9786 - val_loss: 0.5483 - val_acc: 0.8558\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9815\n",
      "Epoch 00022: val_loss improved from 0.51756 to 0.51542, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/022-0.5154.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0788 - acc: 0.9814 - val_loss: 0.5154 - val_acc: 0.8675\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9803\n",
      "Epoch 00023: val_loss improved from 0.51542 to 0.49751, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/023-0.4975.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0819 - acc: 0.9802 - val_loss: 0.4975 - val_acc: 0.8826\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9823\n",
      "Epoch 00024: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0728 - acc: 0.9823 - val_loss: 0.5307 - val_acc: 0.8686\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9839\n",
      "Epoch 00025: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0694 - acc: 0.9839 - val_loss: 0.5852 - val_acc: 0.8551\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9912\n",
      "Epoch 00026: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0479 - acc: 0.9912 - val_loss: 0.5134 - val_acc: 0.8735\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9879\n",
      "Epoch 00027: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0542 - acc: 0.9878 - val_loss: 0.6578 - val_acc: 0.8444\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9832\n",
      "Epoch 00028: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0681 - acc: 0.9832 - val_loss: 0.5068 - val_acc: 0.8796\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9855\n",
      "Epoch 00029: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0617 - acc: 0.9855 - val_loss: 0.5269 - val_acc: 0.8733\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9868\n",
      "Epoch 00030: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0564 - acc: 0.9868 - val_loss: 0.5158 - val_acc: 0.8696\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9942\n",
      "Epoch 00031: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0354 - acc: 0.9941 - val_loss: 0.5698 - val_acc: 0.8682\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9831\n",
      "Epoch 00032: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0684 - acc: 0.9830 - val_loss: 0.5172 - val_acc: 0.8786\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9928\n",
      "Epoch 00033: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0394 - acc: 0.9928 - val_loss: 0.6031 - val_acc: 0.8570\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9941\n",
      "Epoch 00034: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0314 - acc: 0.9941 - val_loss: 0.6219 - val_acc: 0.8654\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9945\n",
      "Epoch 00035: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0324 - acc: 0.9945 - val_loss: 0.5536 - val_acc: 0.8717\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9930\n",
      "Epoch 00036: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0355 - acc: 0.9930 - val_loss: 0.5645 - val_acc: 0.8749\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9933\n",
      "Epoch 00037: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0329 - acc: 0.9932 - val_loss: 0.6540 - val_acc: 0.8614\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9868\n",
      "Epoch 00038: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0527 - acc: 0.9868 - val_loss: 0.5223 - val_acc: 0.8742\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9920\n",
      "Epoch 00039: val_loss improved from 0.49751 to 0.46785, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/039-0.4678.hdf5\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0368 - acc: 0.9920 - val_loss: 0.4678 - val_acc: 0.8915\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9933\n",
      "Epoch 00040: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0320 - acc: 0.9933 - val_loss: 0.7206 - val_acc: 0.8276\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0366 - acc: 0.9918 - val_loss: 0.6548 - val_acc: 0.8481\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9897\n",
      "Epoch 00042: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0434 - acc: 0.9897 - val_loss: 0.5879 - val_acc: 0.8677\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9967\n",
      "Epoch 00043: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0210 - acc: 0.9967 - val_loss: 0.5217 - val_acc: 0.8882\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9959\n",
      "Epoch 00044: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0253 - acc: 0.9959 - val_loss: 0.5752 - val_acc: 0.8735\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9885\n",
      "Epoch 00045: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0448 - acc: 0.9884 - val_loss: 0.5288 - val_acc: 0.8758\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9938\n",
      "Epoch 00046: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0291 - acc: 0.9938 - val_loss: 0.6080 - val_acc: 0.8602\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9967\n",
      "Epoch 00047: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0203 - acc: 0.9967 - val_loss: 0.6258 - val_acc: 0.8612\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9889\n",
      "Epoch 00048: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0432 - acc: 0.9888 - val_loss: 0.5906 - val_acc: 0.8682\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9949\n",
      "Epoch 00049: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0255 - acc: 0.9949 - val_loss: 0.5759 - val_acc: 0.8719\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9971\n",
      "Epoch 00050: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0177 - acc: 0.9971 - val_loss: 0.5444 - val_acc: 0.8807\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0234 - acc: 0.9958 - val_loss: 0.6590 - val_acc: 0.8565\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9923\n",
      "Epoch 00052: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0333 - acc: 0.9922 - val_loss: 0.7112 - val_acc: 0.8493\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9920\n",
      "Epoch 00053: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0359 - acc: 0.9920 - val_loss: 0.5418 - val_acc: 0.8798\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9913\n",
      "Epoch 00054: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0372 - acc: 0.9913 - val_loss: 0.7845 - val_acc: 0.8439\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9954\n",
      "Epoch 00055: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0214 - acc: 0.9954 - val_loss: 0.6230 - val_acc: 0.8712\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9926\n",
      "Epoch 00056: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0324 - acc: 0.9926 - val_loss: 0.5180 - val_acc: 0.8868\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9975\n",
      "Epoch 00057: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0164 - acc: 0.9975 - val_loss: 0.5114 - val_acc: 0.8949\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9979\n",
      "Epoch 00058: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0141 - acc: 0.9979 - val_loss: 0.8127 - val_acc: 0.8442\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9921\n",
      "Epoch 00059: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0314 - acc: 0.9921 - val_loss: 0.6380 - val_acc: 0.8696\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9957\n",
      "Epoch 00060: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0206 - acc: 0.9956 - val_loss: 0.5540 - val_acc: 0.8838\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9928\n",
      "Epoch 00061: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0298 - acc: 0.9927 - val_loss: 0.6335 - val_acc: 0.8712\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9902\n",
      "Epoch 00062: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0394 - acc: 0.9902 - val_loss: 0.5864 - val_acc: 0.8749\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9972\n",
      "Epoch 00063: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0153 - acc: 0.9972 - val_loss: 0.5961 - val_acc: 0.8728\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9939\n",
      "Epoch 00064: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0276 - acc: 0.9939 - val_loss: 0.5851 - val_acc: 0.8821\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9908\n",
      "Epoch 00065: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0345 - acc: 0.9908 - val_loss: 0.6686 - val_acc: 0.8621\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9971\n",
      "Epoch 00066: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0162 - acc: 0.9971 - val_loss: 0.5407 - val_acc: 0.8861\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9950\n",
      "Epoch 00067: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0213 - acc: 0.9950 - val_loss: 0.8035 - val_acc: 0.8486\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9965\n",
      "Epoch 00068: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0182 - acc: 0.9965 - val_loss: 0.5425 - val_acc: 0.8880\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9984\n",
      "Epoch 00069: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0112 - acc: 0.9984 - val_loss: 0.8242 - val_acc: 0.8411\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9971\n",
      "Epoch 00070: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0164 - acc: 0.9971 - val_loss: 0.5967 - val_acc: 0.8838\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9924\n",
      "Epoch 00071: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0307 - acc: 0.9924 - val_loss: 0.5839 - val_acc: 0.8800\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9933\n",
      "Epoch 00072: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0292 - acc: 0.9933 - val_loss: 0.5419 - val_acc: 0.8966\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9982\n",
      "Epoch 00073: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0122 - acc: 0.9982 - val_loss: 0.5927 - val_acc: 0.8831\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9987\n",
      "Epoch 00074: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0102 - acc: 0.9987 - val_loss: 0.5606 - val_acc: 0.8928\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9977\n",
      "Epoch 00075: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0128 - acc: 0.9977 - val_loss: 0.6931 - val_acc: 0.8609\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9935\n",
      "Epoch 00076: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0268 - acc: 0.9935 - val_loss: 0.6446 - val_acc: 0.8691\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0183 - acc: 0.9963 - val_loss: 0.7232 - val_acc: 0.8560\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9941\n",
      "Epoch 00078: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0243 - acc: 0.9940 - val_loss: 0.5755 - val_acc: 0.8803\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9958\n",
      "Epoch 00079: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0188 - acc: 0.9958 - val_loss: 0.5631 - val_acc: 0.8875\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0097 - acc: 0.9987 - val_loss: 0.6072 - val_acc: 0.8807\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9954\n",
      "Epoch 00081: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0215 - acc: 0.9954 - val_loss: 0.6004 - val_acc: 0.8812\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942\n",
      "Epoch 00082: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0258 - acc: 0.9942 - val_loss: 0.5767 - val_acc: 0.8856\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9989\n",
      "Epoch 00083: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0096 - acc: 0.9989 - val_loss: 0.5245 - val_acc: 0.8959\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9974\n",
      "Epoch 00084: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0131 - acc: 0.9974 - val_loss: 0.5799 - val_acc: 0.8859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9983\n",
      "Epoch 00085: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0116 - acc: 0.9982 - val_loss: 0.6337 - val_acc: 0.8754\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9929\n",
      "Epoch 00086: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0287 - acc: 0.9928 - val_loss: 0.5575 - val_acc: 0.8896\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9956\n",
      "Epoch 00087: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0193 - acc: 0.9956 - val_loss: 0.5674 - val_acc: 0.8842\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9981\n",
      "Epoch 00088: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0120 - acc: 0.9981 - val_loss: 0.6930 - val_acc: 0.8605\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9961\n",
      "Epoch 00089: val_loss did not improve from 0.46785\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0171 - acc: 0.9961 - val_loss: 0.5674 - val_acc: 0.8926\n",
      "\n",
      "1D_CNN_custom_2_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4FNXXx783PSGk09IICD9KCAmE3hUFBAEVARGUolhBsaBYXo1iwa4g0kGwgChNehMIKC1AAqETID0hIb1ns+f942Sym+xudhN2CZL7eZ55dndumTuzM/d7z7llBBFBIpFIJBIAsKrrAkgkEonkzkGKgkQikUgqkKIgkUgkkgqkKEgkEomkAikKEolEIqlAioJEIpFIKpCiIJFIJJIKpChIJBKJpAIpChKJRCKpwMZSGQshlgN4CMANIupgIM4AAN8BsAWQTkT9jeXr5eVFAQEBZiypRCKR3P2cOHEinYgaGYtnMVEA8BOAHwCs0hcohHAD8COAIUQUJ4RobEqmAQEBiIiIMFshJRKJpD4ghIg1JZ7F3EdEFA4go5ooTwBYT0Rx5fFvWKosEolEIjGNuuxT+B8AdyHEfiHECSHEU3VYFolEIpHAsu4jU44dCmAgAEcAh4UQR4joUtWIQohnATwLAP7+/re1kBKJRFKfqEtRSABwk4jyAeQLIcIBBAPQEQUiWgxgMQB06dJFZ63v0tJSJCQkoKioyMJFvntxcHCAr68vbG1t67ooEomkDqlLUdgE4AchhA0AOwDdAXxbm4wSEhLQsGFDBAQEQAhhzjLWC4gIN2/eREJCAlq0aFHXxZFIJHWIJYekrgYwAICXECIBwAfgoacgooVEdF4IsQPAaQBqAEuJKLo2xyoqKpKCcAsIIeDp6Ym0tLS6LopEIqljLCYKRDTOhDhfAvjSHMeTgnBryOsnkUiAejSjuaysEMXFiVCrS+u6KBKJRHLHUm9EQa0uQklJMojMLwpZWVn48ccfa5V26NChyMrKMjl+WFgYvvrqq1odSyKRSIxRb0RBCD5VIrXZ865OFFQqVbVpt23bBjc3N7OXSSKRSGpDvREFwLr80/yiMGvWLMTExCAkJAQzZ87E/v370bdvX4wYMQLt27cHADz88MMIDQ1FYGAgFi9eXJE2ICAA6enpuH79Otq1a4epU6ciMDAQgwYNQmFhYbXHjYyMRI8ePdCxY0c88sgjyMzMBADMnTsX7du3R8eOHfH4448DAA4cOICQkBCEhISgU6dOyM3NNft1kEgk/33qckiqRbh8eQby8iL1hKhRVpYPKytH8ChY03F2DkHr1t8ZDJ8zZw6io6MRGcnH3b9/P06ePIno6OiKIZ7Lly+Hh4cHCgsL0bVrV4waNQqenp5Vyn4Zq1evxpIlSzBmzBisW7cOEyZMMHjcp556CvPmzUP//v3x/vvv48MPP8R3332HOXPm4Nq1a7C3t69wTX311VeYP38+evfujby8PDg4ONToGkgkkvpBPbIUFHTmvlmEbt26VRrzP3fuXAQHB6NHjx6Ij4/H5cuXddK0aNECISEhAIDQ0FBcv37dYP7Z2dnIyspC//68sOzEiRMRHh4OAOjYsSPGjx+PX375BTY2LIC9e/fGa6+9hrlz5yIrK6tiv0QikWhz19UMhlr0anUJ8vNPw96+OezsjK4ee8s0aNCg4vv+/fuxZ88eHD58GE5OThgwYIDe2df29vYV362trY26jwyxdetWhIeHY/Pmzfjkk09w5swZzJo1C8OGDcO2bdvQu3dv7Ny5E23btq1V/hKJ5O6l3lgKQih9CmVmz7thw4bV+uizs7Ph7u4OJycnXLhwAUeOHLnlY7q6usLd3R0HDx4EAPz888/o378/1Go14uPjce+99+Lzzz9HdnY28vLyEBMTg6CgILz11lvo2rUrLly4cMtlkEgkdx93naVgGMuNPvL09ETv3r3RoUMHPPjggxg2bFil8CFDhmDhwoVo164d2rRpgx49epjluCtXrsTzzz+PgoICtGzZEitWrEBZWRkmTJiA7OxsEBFefvlluLm54f/+7/+wb98+WFlZITAwEA8++KBZyiCRSO4uBNHt8bGbiy5dulDVl+ycP38e7dq1M5o2N/ckbG0bw8HB11LF+09j6nWUSCT/PYQQJ4ioi7F49cZ9BChzFczvPpJIJJK7hXolCoCVRdxHEolEcrdQr0SBO5ulpSCRSCSGqFeiIC0FiUQiqZ56JQpCSFGQSCSS6qhXosDrH0lRkEgkEkPUK1FgS+HO6FNwdnau0X6JRCK5HVhMFIQQy4UQN4QQ1b5iUwjRVQihEkI8ZqmyaI5lBWkpSCQSiWEsaSn8BGBIdREEDwf6HMAuC5ZDC2uLWAqzZs3C/PnzK34rL8LJy8vDwIED0blzZwQFBWHTpk0m50lEmDlzJjp06ICgoCD8/vvvAIDk5GT069cPISEh6NChAw4ePIiysjJMmjSpIu63335r9nOUSCT1A0u+ozlcCBFgJNp0AOsAdDXbgWfMACL1LZ0N2KmLYUMlIOuGqNEbiUNCgO8ML509duxYzJgxAy+99BIAYO3atdi5cyccHBywYcMGuLi4ID09HT169MCIESNMeh/y+vXrERkZiaioKKSnp6Nr167o168ffvvtNwwePBjvvvsuysrKUFBQgMjISCQmJiI6mo2ymrzJTSKRSLSps7WPhBA+AB4BcC+MiIIQ4lkAzwKAv7//rRz1FtIaplOnTrhx4waSkpKQlpYGd3d3+Pn5obS0FO+88w7Cw8NhZWWFxMREpKamomnTpkbzPHToEMaNGwdra2s0adIE/fv3x/Hjx9G1a1dMmTIFpaWlePjhhxESEoKWLVvi6tWrmD59OoYNG4ZBgwZZ5DwlEsndT10uiPcdgLeISG2s5UxEiwEsBnjto+pzNdyiV5XcQHFxHBo0CIawsq1xgatj9OjR+PPPP5GSkoKxY8cCAH799VekpaXhxIkTsLW1RUBAgN4ls2tCv379EB4ejq1bt2LSpEl47bXX8NRTTyEqKgo7d+7EwoULsXbtWixfvtwcpyWRSOoZdSkKXQCsKRcELwBDhRAqItpouUMqXSjm72weO3Yspk6divT0dBw4cAAAL5nduHFj2NraYt++fYiNjTU5v759+2LRokWYOHEiMjIyEB4eji+//BKxsbHw9fXF1KlTUVxcjJMnT2Lo0KGws7PDqFGj0KZNm2rf1iaRSCTVUWeiQEQVryUTQvwEYItlBUEZfWSZ5bMDAwORm5sLHx8fNGvWDAAwfvx4DB8+HEFBQejSpUuNXmrzyCOP4PDhwwgODoYQAl988QWaNm2KlStX4ssvv4StrS2cnZ2xatUqJCYmYvLkyVCr+bw+++wzs5+fRCKpH1hs6WwhxGoAA8BWQCqADwDYAgARLawS9yewKPxpLN9bWTpbpcpGYeFlODm1hbW1nA9QFbl0tkRy92Lq0tmWHH00rgZxJ1mqHJWxnKUgkUgkdwP1bkYzIEVBIpFIDFGvRIHXPgLk8tkSiUSin3olCtJSkEgkkuqpV6JgySGpEolEcjdQr0RBYylI95FEIpHoo/6IAhFEmRogYXb3UVZWFn788cdapR06dKhcq0gikdwx1B9RyMgAIiNhVWr+5bOrEwWVSlVt2m3btsHNzc2s5ZFIJJLaUn9EwYanZAi1+V+0M2vWLMTExCAkJAQzZ87E/v370bdvX4wYMQLt27cHADz88MMIDQ1FYGAgFi9eXJE2ICAA6enpuH79Otq1a4epU6ciMDAQgwYNQmFhoc6xNm/ejO7du6NTp064//77kZqaCgDIy8vD5MmTERQUhI4dO2LdunUAgB07dqBz584IDg7GwIEDzXreEonk7qMu1z6yCAZXzi5zBgraQG0vABtrWNVADo2snI05c+YgOjoakeUH3r9/P06ePIno6Gi0aMGreSxfvhweHh4oLCxE165dMWrUKHh6elbK5/Lly1i9ejWWLFmCMWPGYN26dTrrGPXp0wdHjhyBEAJLly7FF198ga+//hqzZ8+Gq6srzpw5AwDIzMxEWloapk6divDwcLRo0QIZGRmmn7REIqmX3HWiYJBKC7FaZmkPbbp161YhCAAwd+5cbNiwAQAQHx+Py5cv64hCixYtEBISAgAIDQ3F9evXdfJNSEjA2LFjkZycjJKSkopj7NmzB2vWrKmI5+7ujs2bN6Nfv34VcTw8PMx6jhKJ5O7jrhMFgy36kjLg9EWUNLOHyt0WTk6mL05XGxo0aFDxff/+/dizZw8OHz4MJycnDBgwQO8S2vb29hXfra2t9bqPpk+fjtdeew0jRozA/v37ERYWZpHySySS+kn961MoM//oo4YNGyI3N9dgeHZ2Ntzd3eHk5IQLFy7gyJEjtT5WdnY2fHx8AAArV66s2P/AAw9UeiVoZmYmevTogfDwcFy7dg0ApPtIIpEYpf6IghC8qc0/o9nT0xO9e/dGhw4dMHPmTJ3wIUOGQKVSoV27dpg1axZ69OhR62OFhYVh9OjRCA0NhZeXV8X+9957D5mZmejQoQOCg4Oxb98+NGrUCIsXL8ajjz6K4ODgipf/SCQSiSEstnS2pbiVpbMRGQlVQxsUNSmDs3OwhUr430UunS2R3L2YunR2/bEUAMDGBkJNcu0jiUQiMUD9EgVr6/IFUtX4r1lIEolEcjuwmCgIIZYLIW4IIaINhI8XQpwWQpwRQvwrhLC8P8faGqKMwENSpShIJBJJVSxpKfwEYEg14dcA9CeiIACzASyuJq55sLYGyt9jLF1IEolEoovFRIGIwgEYHANJRP8SUWb5zyMAfC1VlgpsbHhRPADyRTsSiUSiy53Sp/A0gO0WP4q1NVBGAElLQSKRSPRR5zOahRD3gkWhTzVxngXwLAD4+/vX/mDW1hBE5d0JdSsKzs7OyMvLq9MySCQSSVXq1FIQQnQEsBTASCK6aSgeES0moi5E1KVRo0a1P6A1v6NZqOWLdiQSiUQfdSYKQgh/AOsBPElEl27LQSuWujCv+2jWrFmVlpgICwvDV199hby8PAwcOBCdO3dGUFAQNm3aZDQvQ0ts61sC29By2RKJRFJbLOY+EkKsBjAAgJcQIgHABwBsAYCIFgJ4H4AngB+FEACgMmW2nTFm7JiByBR9a2cDUKmAwkKUnQKsbBwhhGmnH9I0BN8NMbx29tixYzFjxgy89NJLAIC1a9di586dcHBwwIYNG+Di4oL09HT06NEDI0aMQPn56kXfEttqtVrvEtj6lsuWSCSSW8FiokBE44yEPwPgGUsdXy/llbEgwJzzFDp16oQbN24gKSkJaWlpcHd3h5+fH0pLS/HOO+8gPDwcVlZWSExMRGpqKpo2bWowL31LbKelpeldAlvfctkSiURyK9R5R7O5qa5Fj8JC4OxZFDYDrBv5wc6uidmOO3r0aPz5559ISUmpWHju119/RVpaGk6cOAFbW1sEBAToXTJbwdQltiUSicRS3ClDUm8PFa/kNP+Q1LFjx2LNmjX4888/MXr0aAC8zHXjxo1ha2uLffv2ITY2tto8DC2xbWgJbH3LZUskEsmtUL9EQRl9VAaYe/JaYGAgcnNz4ePjg2bNmgEAxo8fj4iICAQFBWHVqlVo27b6F/sYWmLb0BLY+pbLlkgkkluhfi2dDQAnTqDEHVB7N4KDwy3MebgLkUtnSyR3L3LpbEPY2ECozf/2NYlEIrkbqH+iYG1tEfeRRCKR3A3cNaJgshvM2toir+T8r/NfcyNKJBLLcFeIgoODA27evGlaxWZtDaEG6nrtozsJIsLNmzfh4OBQ10WRSCR1zF0xT8HX1xcJCQlIS0szHjktDVRciFJVFuzspDAoODg4wNfX8quXSySSO5u7QhRsbW0rZvsaZe5cqNauwMntLRAcfN6yBZNIJJL/GHeF+6hGuLnBOrcUZSq5bLVEIpFUpV6KgihVgwqkKEgkEklV6qUoAIDIzq/jgkgkEsmdR70VBevcUqjVpXVcGIlEIrmzqLeiYJMHlJVJa0EikUi0qeeiIPsVJBKJRJt6LQpqtbQUJBKJRBuLiYIQYrkQ4oYQItpAuBBCzBVCXBFCnBZCdLZUWSpR/nYyaSlIJBKJLpa0FH4CMKSa8AcBtC7fngWwwIJl0eDqCkD2KUgkEok+LCYKRBQOIKOaKCMBrCLmCAA3IUQzS5WnAnt7kKO9tBQkEolED3W5zIUPgHit3wnl+5ItfWBydYFNXpq0FO4QiIDLl4G8PKBTJ0CIyuFFRcC1a0BuLm/5+UBpKaBS8WZrywagiwvg7Mzp1WrefH0BLy/dY5aVafLLywNKSjitiwtv9vaV4yclAX//DRw6xMe2twccHABvb+D++4HgYE25y8qAixeBS5eAhATeMjKAHj2ABx8Eyl/Mh+Ji4NQp3lJTgbQ03oiAhg15c3MDAgKAe+4BWrbksllZ8WK/1tZ87spxMzL4uBcvAllZfBwfH6BRI75+p08DUVF8Lsq1A4D27YHu3XlzdeW4164BKSmc1teXN29vwMNDc7z4eGDvXmD/fi5zQADQvDlvfn68OToC6enAv/8C//zD+Xp68n/i4cHXICcHyM7mMmdm8nlkZ/M1dnbm69C0KdChAxAUBCgvMCws5O36dc25XbzIeSr3h40Nl8HJic+tdWugTRveysr4WiQncxk8PPh8vbz4fsjM5DKlpWn+x5QULn9AAG/NmnEZGzTg/+LKFeDMGd5ycjT3U4MGXKbCQr6fGzUCunThrU0b4OZNLktiIh9D2XJy+B7w9ORt6FBg+PBbedqM859Y+0gI8SzYxQR/fzO8Lc3NtVwUpKVQW3JygH37gF27uGIoLeWHpEULrrwCA/khbtGCK5H8fODGDb75c3K4Ms7M5Ipi924gLo7zDQoCnn8eGD8eiIwEfv4Z+OMPTlNb2rYF+vThyu/CBa6Ez5zhh9MQTk5cOXh5cdkvXuT9bm5cSRUV8Zaby/ubNAH69eOHOjISKCjQ5GVryxXHkiX8u3NnrqgiIrgCU1AqJSsrjWDl5HCFWx12dlz5aR/TEH5+XGnb2fE5qlTApk3A8uXG0yrn0qQJC5LyynEvLz6fxEQWYm3c3LhiVdI2b86/MzI0cW1suMJ2deVr4OEB+PvztcnL43smMhL46SfD5RKCK/x27bgCtrHhTaXiiriggI/555/8qY2VFV+LPAPVgbOzRhhbteLynD8PbN/OeVelZUu+9z09Nf9hXh6LXKNG/JmQAHz/PYtPVRo25GvctCl/ZmWx4N28yb8tLQoWfR2nECIAwBYi6qAnbBGA/US0uvz3RQADiKhaS0Hf6zhrirpnN2SVHkfBxrnw9Z1+S3n9F4iP11Rm2iQmAidPcoWcmMgtlaIifqgaNOCbNzubW3rp6XxzZmfzplRWDRoAAwZw3tev85aSojmGoyN/6nt4AC7XffcBDzzAD+fChVxpC8H5OzsDo0ZxuKsrH8fZWVMRWluzICmtzdxcTmtV7hi9dIlb9//8w+V3c2NrJCSEK0jt/PLyNPlkZmrOWwigf39g4EC2CKy0nK5JSSxqO3dya9jPDwgN5Yo/MJB/e3lxHqdPA9u2cWWiUgE9ewK9egFdu3KL09ZW9/qUlvL/c/Uqb/n53MJVqzmPkhLN5u2taQV7ePD/kJjIYuzvz4JbPs6iEkSc99GjXHm2aKFpBaena1rJycls0aSkcDl69uRrEhjI16S0lOPFxvI9Fx/Px/fzA3r35laxcj+o1RprwNFR1zrUR3o6i/nly/zfOzhwWm9vroQbNDCeh5LPpUt8vX18gMaNOb+SEs1/bmfH18rNTddq1L5ueXl8LfLy+NkJCOD7yRRKSoDoaLYuGjXisnh7m56+ppj6Os66FIVhAKYBGAqgO4C5RNTNWJ7mEAV6cDByr+1C5s7P0Lz5rFvK607h0iVuATk58cORnAz89Re3Ai9d4jgBAfzw2NlxBZCYqElvbc2VgJMT3+T5+XyTu7lxpebpyQ+J0qLz8uKWcc+enJ82eXnA2bN8w589yxVG48bcyvH0ZHNacY8EBPADqUAEHDsGrFvHlfeIEaY/7NWhVrMboHFj0yogieRuw1RRsJj7SAixGsAAAF5CiAQAHwCwBQAiWghgG1gQrgAoADDZUmXRwd0TNmfujo7mtDTgrbeAFSt0w2xtgXvvBV58kSvq6Gjeioq4Qu/WjbeWLbmlYm1tnjI5O2t81DVFiNqnrQ4rKxYliURSPRYTBSIaZyScALxkqeNXh3Bzg22e+E9MXiNit8PmzcCOHWzKdunCLoq0NOC997jCnzmTBaCggFv5zs5s2pePwJVIJBKT+E90NJsdNzdY5xHKVLl1XRK9qFTsB9+wAdi4UdMJ27Urt/K//Zb9twALwfz53MEmkUgkt0q9FQWrMqAkK6GuSwKAW/qRkTwaJSKCOy3T07kjbdAg4P33gWHDeDQCwKMyoqPZIujbV/rIJRKJ+ai3ogAApWmX6+TwOTk85v3AASA8nAVBGZ7n7c0jbR59FBgyRP9IBHt7dh9JJBKJuamfolA+Lq/sZhyIyiCEmXpYjUAE/PIL8NprGkugZ0/g3Xe5wzc0VDOxSSKRSOqC+ikKFS/aUaGoKB6OjgEWP+SVK8ALLwB79rAQ/Pknz3A1NAZaIpFI6oJ6LQo2eUBh4RWLiEJmJs/4Vab3R0TwHIAffwSee67yBCiJRCK5U5CiUHgFwP1myTY3lyeMrVnDncXKGjldugCvvw68/DL3GUgkEsmdSr0WBdsCm3JRuDXUamDBAmDWLB5J5OcHvPIK8Mgj3E8gXUR3HiVlJbAW1rC2uj39SXcCsVmx2HJpC7ycvDC2w9i6Ls4tk16Qjo/DP4ZKrcLrPV9HC/cWFWFEhDM3zqCpc1M0btD4tpbrfNp5rIpahVJ1KcYHjUdI0xAIMwwR3HFlB3r69oSrg2UnH9VPUSif0eVQ5IGsWxSFmBjg6ad5JJEyfLRnT+keulNRkxpLTy7Fm7vfxMTgifj+we9NTpuSl4IP9n2Avs374omgJ2Al7vw/OasoC3OPzsX68+sRlRoFALCxskGodyhaebSqNm1+ST4a2JlhjZFyEnIScCH9Au5vqWuZ/33tb5xOPY0ZPWbohJWUlSC3OBcejh4QQkBNavwU+RPe3P0msouzYSWssOjEIkwKnoRnOj+Dvdf24ufTP+NC+gW08miFI08fgaeTZ43KqiY1zqSewZ6re5Can4refr3Rr3k/uDvqLh6VV5KH82nncTzpOFZFrcLRxKMVDY6vD3+NoMZBmBg8EU8GP1lJoHKKc/DN4W+w7vw6tPZojdBmoQj1DkX/5v3haOtY6RjhseEYsXoEpnSagoUPLazRudQUi659ZAnMsfYRAKBBA6SN9sa1F+3RrZvel8NVCxH3D7z5Jq/d8+23wOTJd8ecgZziHMw5NAe/nvkVj7R9BG/3eRtNnGu+RgQRYe7RuVCpVejj3wedmnWCnbWd0XSJOYnYe20vBt0zCE2dm9bmFPRyMf0int3yLMJjw+Fq74qSshIkvJYAD0cPo2n3X9+PcevGISWPV/vr1LQTvnjgi4oKrlhVjLySvBpXPgoFpQXYfnk7tlzegp6+PfFs6LPVxo9MicSOKzsws9dMvdZOmboMy08tx7t/v4v0gnT08e+DEW1GoKdvTwz6ZRBGthmJ30b9pjfvU8mn8M7f72DHlR24v+X9eKv3WxjYYmC1rd38knxsubQFa86uQXx2PL4d/C36Nu9bEX4y+SSG/joUqfmp2PrEVgxtPbQiLD47HkELgpBdnI29T+3FfS3uqwgjIjzw8wPYe20vXOxd0MqjFcrUZYhKjUIf/z5YMGwB3B3cMefQHCw+uRglZbzsaF//vhh0zyDMDp+NXn69sHPCTpPuvbySPLy+83VsvLgRN/JvAABsrWxRqi6FgEDHJh3hYu+CUnUpVGoVbuTfQFx2XEX69o3aY0rIFEzoOAG21rZYE70GK6NW4ljiMdhY2WBkm5GY0mkKLqRfwKcHP8XNwpvo17wfknKTcCWDG6hBjYOwbfw2+Lr4AgCuZl5FtyXd4OnkiSNPH9ErTKZwRyyIZwnMJgo+Psjp6Y7Il2PQt28+RA1afUlJwJQp3G8wZAgviezre+tFqmtUahWWn1qO/9v3f7iRfwN9/fvi3/h/YW9jj1e6v4I3er2hU4Em5SbhjV1vILRZKF7v9XqlsIURC/HC1hcqfjvaOKJzs85o7dkardxb4R6Pe+Dp6AkXexc0tG+IyzcvY+mppdh2eRvUpIZ3Q2+sH7Me3X11F0IqLC3E8aTjOBR3COfSziE2OxaxWbHILs7GxOCJmNVnFrwbcgdOfHY8vj78NRZGLISjrSO+euArhHqHotOiTvjqga90yq2NmtT4/NDneG/fe2jt0RprR69F9I1ovLP3HcRmx6K5a3NkF2cjq4jXh36w1YP4dvC3aOPVxmCeSssyJjMGVzKuIDIlEtuvbEdBaQEcbRxRqCrEaz1ew5eDvtRrjVy+eRm9lvdCekE6Zt87G+/1e69SeGRKJKZsmoJTKafQx78Pvh/yPTo307zt9t297+LTQ5/i1HOnENI0pGL/1cyreHvv21h7di08HD3weODj2HBhA5LzkhHaLBSdm3WuKHNKXgrcHNzg5eQFNwc3RKVEIb80H82cm8Hexh5x2XH4aMBHeLvv29hzdQ9GrR0FD0cPONs5I70gHaefP40mzk1ARBj8y2D8G/8v3B3d4e7gjpPPnYSNFTsxVpxagSl/TcHUzlNhb22PK5lXkJafhpe6voSJIRMrXZ+EnATsitmFewPurXAl/XL6Fzy54UlMCZmCpSOWQggBIsKlm5fQrGEzuNi7VKRPy0/DsN+G4UTyCTze4XEMajkIA1sOhJeTF44lHsP+6/vxb/y/KCkrgY2VDWytbeHm4Ib2Xu3RvlF7BDYORGuP1nrF83zaeSw7tQwro1YivSAdADDonkH45L5P0MWb6+msoizsjtmNp/96Gi72Ltg+fjv8Xf3Ra3kvJOcm4+gzR9Has7XB+8oYUhSM0b8/igsScPjLq+jRIx4ODqbV6uvWAc8+y0tBf/01r/1/J1gHmYWZyC3JhbuDO5ztnGvkw8wpzsHdo/X9AAAgAElEQVSKUysw79g8xGTGoK9/X3wz+Bt08e6CSzcvIWx/GFZHr4aLvQtmdJ+BGT1mwN3RHWui1+DFrS8isygTAPD9kO/xcveXAQARSRHovbw37mtxH5aNWIbD8YdxKO4QTiSfQExmDJJyk/SWpalzU0wKnoR+zfvhpW0vITE3EQuHLcSkkEk4n34emy5swpbLW3A88ThK1bzWh7+rPwLcAtDctTlUahXWnl0LW2tbPBf6HPJK8rAqahUIhAkdJ+DT+z5Fs4Y8GaTfin5IzE3EpWmX9La2U/NSMWnTJOy4sgOPd3gcix9ajIb2vP54saoYCyIW4GjiUTRyaoTGDRqjpKwE3x/9HgWlBXi528t4vsvzUKlVKFQV4mbBTRyMO4i91/biWOIxqNSqiuP4ufjhof89hMfaP4Y+/n0wc9dMzD02F+M6jMNPD/9UqYV7I/8Gei7riZziHHT36Y7tV7Zj71N7MSBgAADgRNIJ3P/z/XCydcI3g77BmMAxOvdCVlEWWn7fEj18e2Db+G0AgOOJxzHk1yEoVhXj1R6v4o1eb8DVwRXFqmL8fPpnfHP4G6QVpKGVRyu08miFZs7NkFOcg/SCdNwsvIn/efwP44LGoa9/XxSUFuC5Lc9hdfRqdPXuilMpp9C+UXtsH78dmYWZ6LKkCwYEDMDWJ7ZiYcRCvLTtJSwcthCeTp4Y/cdoLBi2AM93eR438m+g7Q9tEdg4EAcmHai1u+7//v4/fHzwY8zoPgOl6lJsvrQZcdlxcLF3wfRu0zGjxwzkFudi8C+DEZ8Tj98f+x0j2oyo1bGMUVJWgu2Xt8PTyRN9/PvojROVEoWhvw1FXkkeAhsF4njSceyasAv3trj3lo5tqiiAiP5TW2hoKJmF6dNJ3cCB9u0FZWTsMxpdpSJ64w0igKhrV6ILF8xTDGMUlhYajRN+PZycPnEihIEQBrL5yIbu+f4een3n63Q4/jCp1Wq96RKyE+jlbS+T86fOhDBQr2W9aMP5DXrjR6VE0ajfRxHCQK6fudJ9K+8jhIF6LO1B526co4fXPEwIA/0c9TNlFGRQwHcB5PeNH6Xnp+s9dn5JPkWnRtPB2IO09dJWWnNmDW2/vJ1Ky0or4qTnp9P9q+4nhIF8v/GtOL+ui7vSW7vfos0XN+vNPyYjhqZsnELWH1qTw8cONG3rNLqeeV0n3u/RvxPCQFsubtEJ23llJzX5sgk5fOxAC44vMHgNq5KSm0LPbHqGRJioKK+yWX1oRd2XdKe397xNG85voDOpZyi/JF8nD7VaTXMOziGEgfqt6Ee/nf6NUvNSKa84j7ou7kqOHzvS4fjDlFucS23mtaGmXzWllNwUOpl0ktznuFPAdwF6z1ebzw99TggDHbh+gPZf20/OnzpTi+9aUExGjEnnaQy1Wk1LTiwhh48d6L6V91FWYVZF2Pxj8wlhoFe2v0JOnzjRkF+GkFqtJrVaTf1W9COvL7woszCTxq8bT7Yf2dK5G+duqSxl6jIavXY0IQzk9IkTjVw9kn489iM9tvYxEmGCnD5xIq8vvMh9jjv9E/fPrZ66WYjLiqPA+YGEMNDiiMVmyRNABJlQx9Z5JV/TzWyisGwZEUBHfgElJi6pNmpGBtHgwXy1XnyRqKSkZodKz0+nN3e9SSm5KTphiTmJ9MymZ+hYwrFK+7MKs2jsH2PJ8WNHuph+0WDeJ5NOkstnLvS/ef+jJSeW0Jf/fElv73mbhv46lGw/siWEgfy+8aNpW6fRzis7qai0iG7k3aDXdrxGDh87kM1HNjRh/QQ6nnjcpHOJTI6kR39/lFw/c6VPwj+pqMQLSwvp3p/uJesPrSl0USjZfmRLR+KP1OAq6ae0rJTe//t9Gv7bcFpwfAEl5iSanDYpJ8mgKBERlahKqNlXzWjIL0Mq7Zu5ayYhDBQ4P5DOpJ6pVbmjUqJo+cnltPrMatp4fiPtvbq3UsVoCisjV5L7HPcKUWn0RSOy+tCKNp7fWOk4Dh87UM+lPcnjcw/y/9afrmVeM5p3fkk+eX/tTa3ntiaHjx2o3Q/tKCE7oaanaZTMwkwqU5dV2qdWq2n4b8MJYSC3OW6Vjnsy6SSJMEH9VvQjhIE+2PeBWcpRrCqm8OvhVFBSUGn/2Rtnafy68dRxQUc6e+OsWY5lLrKLsulg7EGz5SdFwRgREUQARYdZ05UrbxmMdukSUevWRLa2RIsW1fwwarWaRq4eSQgDPbf5OZ3wqX9NJYSBRJigF7e8SJmFmXQ04Si1+K4FWX9oTbYf2dILW17Qm/fF9IvU6ItG5PeNH8VmxeqEZxZm0qrIVTRi9Qhy/NiREAZy/tSZGnzSgKw+tKJJGyfR1YyrNT8pA2QXZVPoolBCGGje0Xlmy9eSfLj/Q0IY6FL6JYrPjqfey3pX/Ff6WvG3G1WZio4lHKNPwz+lQT8PomUnl+nEWXJiSYU1VZOW/qKIRYQwUOiiUErLTzNnsY2Slp9G9/50byWBU3hm0zOEMFCbeW2oqLTotpbrbkaKgjEKC4msrSlhsiedOTNKb5SLF4maNSPy8iI6qEewC0sLKSIxgs7eOEvXM69TZmGmThzFVG41txXZfmRbyay/lnmNbD6yockbJ9PL214mqw+tyOsLL7L5yIb8v/Wnf+L+oac3PU2OHzvqtHjjs+PJ/1t/avRFo2otCYWCkgLafHEzTf1rKk3ZOIXOp503mqY2ZBRk0OaLm012t9Q1ybnJZPuRLQ36eRB5feFFDT5pQL+d/q2ui1Uj1Go1/R79u96GQXWoylS0NnotZRdlW6hktSM1L5Ue+u0hs1iaEg1mFQUArwBwASAALANwEsAgU9KaezObKBARBQZSdv/GdOxYsE7Q5ctE3t5EjRoRRUfrJo3JiKF2P7TT8RuP+WMMJeUkERGb9vaz7Wnor0MpLiuO7Gbb0dS/plbkMfWvqWQ3267CfD6RdIL6rehHj//5OGUUZBARUXRqNCEM9En4JxXpytRlNOCnAeT8qTOdTDppvutRTxn357gKd5GlxFJSA9RqojVriPLr3lK7mzC3KESVfw4GsB5AIICTJqQbAuAi+JWbs/SE+wPYB+AUgNMAhhrL06yi8MQTVNLMmQ4caFCpZXvlCpGvL1sIp0/rJgu/Hk6en3uS+xx3Wn5yOa05s4aWnVxGb+1+i+xn25PLZy4098hcavdDO2r6VVNKzUslIqJpW6eRzUc2dDXjaoWVMG3rNKPFHPzzYGr6VdMKU3rukbmEMNDSE0vNcx3qObFZsfT5oc8przivrosiISI6f56rpuXL67okdxXmFoXT5Z/fA3ik/PspI2msAcQAaAnADkAUgPZV4iwG8EL59/YArhsri1lF4YsviAA6uAlUVJRMRESpqUTNmxN5eBBFRuom+enUT2T7kS39b97/6FL6JZ3wS+mXKkbMiDBBu2N2V4QlZCeQ/Wx7embTMzpWQnXsuLyDEAZaGbmSLqVfIsePHWnor0P/My4ayR1KcjKRnx/RqVN1XZLK7NnDVdP779d1Se4qTBUFU5e5OCGE2AWgBYC3hRANAaiNpOkG4AoRXQUAIcQaACMBnNOKQ2C3FAC4AtA/eN1SBAcDAJxjeGE8K6umGD0aSE0FDh6sCK5g+anlePqvp3Ffi/vw5+g/9c4sbO3ZGrsm7MIf5/5AaVlppSn9Pi4+eDb0WSyIWAAAeD70efi4+Bgt5qB7BqF9o/b45vA3cLZzhr2NPZYMX2KW9VQk9ZizZ4H4eGD/fiAkxGh0s1JWBjz1FPDqq7xipDbJyfwZG3t7yyQBAJg6G+RpALMAdCWiAgC2ACYbSeMDIF7rd0L5Pm3CAEwQQiQA2AZgur6MhBDPCiEihBARaWlpJhbZBKqIwowZ/Ca0Zct079N159Zh6uapGHTPIGx7Ylu1U82FEBgTOAbjO47XCZvVZxashTWshBVm9ZllUjGFEHitx2uISo3CP/H/YN6D8ypm60oktSY1lT8vXrz9x05IAH77Ddi8WTcshZcSqXg5ueS2Yqoo9ARwkYiyhBATALwHINsMxx8H4Cci8gUwFMDPQs96E0S0mIi6EFGXRo0ameGw5TRpAmraFKXXgZeWHcOPWw9g2huZeOKJytF2x+zGE+ufQA/fHlg/Zj3sbWq/7Kl3Q2/8MPQHzHtwnklWgsL4juPRvMgBo22CMT5IV2wk9ZwjR/g9rzXhBq/tgwsXzF8eYyQm8meSHueAYilIUagTTHUfLQAQLIQIBvA6gKUAVgHoX02aRAB+Wr99y/dp8zS4MxpEdFgI4QDAC8ANE8t1y4jgYLzgVYwDZQuAyQvwA4BN3/rB18UXrg6ucLV3xeZLm9HWqy22jNtillUjn+n8TI3TOJA1zn5TBKfuDSHelW4jiRb5+UDfvsB77wEffGB6urq0FBQx0CcKiqUQH8/r0sslh28rpl5tVXlHxUgAPxDRfAANjaQ5DqC1EKKFEMIOwOMA/qoSJw7AQAAQQrQD4ADAjP4h42zq3AAHWmTC/ujrWDtiO+YMnIN+3j3hbNsANwtu4mTySXTx7oKdE3bWenVCs5CUhAYlgDh5iv2xEolCQgKgUgGnT9csnSIKyck1tzJuFUUMEqu2E6GxFEpKNGWU3DZMtRRyhRBvA3gSQN9yF49tdQmISCWEmAZgJ3gk0nIiOiuE+AjcC/4X2OpYIoR4FdzpPKlcfG4LOcU5eM7hAJAShCftXPBYyGCIuHbAyC/4TTnLlwOdOxvP6HagmNL5+dyya9++bssjuXOIL++6O3u2Zum0K9yLF4GuXc1XJmMYsxQaNOB7PS4OaNbs9pVLYrKlMBZAMYApRJQCdgV9aSwREW0jov8R0T1E9En5vvfLBQFEdI6IehNRMBGFENGuWp5HrXjv7/eQqs6A3V/zMKn5PJQUJAITJvB7NG/cALp1A955BygqMu+Bt24FtmypWRpt/6o5VomV3D0kJPDnlStAcbHp6VJTgYAA/n67+xUUCyEtTbfMycmakR5yBNJtxyRRKBeCXwG4CiEeAlBERKssWjILczThKH449gOsTryIySkX4Z+cDtXsN4FDh/jtOWfP8pC5zz4Devc2r8vmgw/4FW01QWkNOjhIUZBURhGFsjLg0iXT06WmAr16AdbWt79fQdtCUPoQAG6AZWVxgwyQnc11gEmiIIQYA+AYgNEAxgA4KoR4zJIFszSv7nwVzuQN9e5P8Wq7HfA6CDh9tQZ44gm2Ftzd2X00fz5w8iSP7jAXsbE1v9nj4gAPDzbxpShItInXGvltqguJiK1hX1/gnnvqRhQalndLavcrKALRti3g4iJFoQ4w1X30LniOwkQiego8Me3/LFcsy1JYWoijiUdRenwyRg5xQZsennBMAUqa2LGVoM348YCtLbBpk3kOnp8PpKcDN2/yd1OJiwP8/dmsPnWKOxYlEoAthfbtucVvqihkZXFHbpMmQJs2t999lJQEhIZqvisootC0KdC8uRSFOsBUUbAiIu1hojdrkPaO42zaWahJjaJrIXjjDQC9eoFsrHDu7TKUOVeZg+DqCgwYAPxVdeBULdG+yWtyw8fFced3ly5sYp87ZzyNxDTUaiAzs65LUXvi44FWrXgz9b5QOpkVUbh8+faNasvL49FOSr+BtigoI4+aNeNG0O3oU0hOBv74w/LH+Y9gasW+QwixUwgxSQgxCcBW8Azk/yRRKVEAgI5NgtG7N4CJE3EzchmyO6iQl3dCN8HIkWxem8PE1r7Ja3LDx8drLAVAupDMycqVLLj6RsL8F0hIYDdQ+/amWwrKxLUmTdhVU1x8+zp1lescFATY2el3HzVtyvf77bAU3n8fGDNGc03qOaZ2NM8EL17XsXxbTERvWbJgluRUchRQ7IxhvVry+5WtrODSeigAIDv7H90EI8rf12oOF1JtLIWcHDb3/f25NejiIkXBnJw9y668pUvruiSGKSnhCWpVy5ifz1aOnx8QGGj6CKSqlgJw+/oVFFHw9QW8vXUtBSsroHFjdh9lZLBlYSlKSvjF60DN53ncpZjsAiKidUT0Wvm2wZKFsjTH4qKAG0Fo3Upz+nZ2jeHo2Fq/KPj5AZ06mUcUYmMBGxv2/5raMlM6Ev38+IEJDZWiYE6UlurixXduX82KFTwybuvWyvuVkUe+viwKZWWmVe7aotC2LX+/Xf0Kigh4e/NW1VJo1IifD39/3mdJa2HPHo3rMCrKcsf5D1GtKAghcoUQOXq2XCHEbZ4CaR6ICOduRgEpwWjVqnKYq2tv5OT8C73z50aOBA4fvnUTMzaWH2BfX9NvdkUUlIekSxe+gUtKbq0sEiYhgYf6JibWfP7I7aCoCJg9m79Xrbi1RUGZ0GhKv0JqKjcwPD0BLy8e2Xa7LQVvb8DHR9dSUCarmVMU4uPZTVT1mVmzBnBzY3G800Th+nXgk0+4z+s2Uq0oEFFDInLRszUkIpfq0t6pxGXHIb8sG0jVFQUXl94oLU1HYaGesd4jR/IwvlutNGJj2Syuib9UiactCiUlQHT0rZVFwiQm8v/r6wssWFDXpdFl0SIuY+/e7B4qLdWEaVuRbdqYPgIpNZXFwNqaf7dte/sshcREnrHcsKGu+yglhfsTAPOKwsqVLKwLF2r2FRUBGzcCo0axJ+BOcx8tWsTrWZ0/f1sP+58dQVRbolK5NWCfFVxx7ym4uvYCAGRn/6ubMDiYb9JbdSEpotC8uenuo7g4btUpLSjZ2Ww+iLiSat4cmDoV2LWLK947hfx84NNPgfvuA557jt1b2uVTLAUfH8DenvucTBWFJk00v9u0ub2Wgo8PIAR/5ubyBlS2FLy9a+ZmrQ7lWfnwQ+6fA4Dt2/m4Y8fy833u3J1lfR89WvnzNlH/RCElCiCBVg2DUPUdNU5ObWFj466/X0EI7nDevRsoKKjdwVUqroD8/XlLSDBtGGB8PD88NuVLVbVowZPrpCjcOunpXBH4+ADPPMOV0OLFdV0qDT/8wC7L2bOBdu14n3aLPj6effAODvy7fXvT3UfaotC2LbfSs82xIr4WmzZVnlwHsCh4l78PRPlMSmI3SWqqxlKwtq6Zm7U6IiJ4HbPMTBZZgF1HjRoB997LolBaWjfLiOujrAw4fpy/S1GwLFGpUbDNuwdtWjjrhAlhBReXXsjJ0SMKALsYCgu5c6o2JCbyja9YCmVlpg2DVCauaQrK1sKdKApEld0bdzpKJ6ePD1dQI0fyTHZzr3dVG7Kzgc8/B4YO5eUolA5hbXeCMhxVwdQRSPosBcC81kJiIvDww5pKWEGfKCQm8oROlaryAnjmGJaanMz5P/kkMHEi8P337HrdvBkYPZobWx07ctw7xYV0/jyPurK2lqJgaaJSoqBK1O1PUHB17Y2CggsoLb2pG9ivH3dKrV9v/EBEuiNZFDNY6VMATLvhq4oCwCOQzpy5MyovbTZsYF/1nTAZ7I8/gH37qo+jLQoA8MILXDn9+adly2YKv/zC1/HDD/m3szMLgDFRMDYCiej2iMLGjfx57FjlYycmasRAue5JSZXnKCiYY1bzifK5R126AB9/zBXtgw9yA+/xxzmsTRt2v90pnc2KEDz2GD/nNVn94BapV6KQV5KHmMwYUFL1ogAY6Fews2MX0l9/Vd8azskB+vcHHnig8n59omDMX6pW84Pv51d5f2goi05NO5sjI/W/AtFc7N3L53/qlOWOYQolJcDTTwP/Z2Q1FkUUlIr1vvvYL79okWXLZwrh4ZpZ7Art2um6j7TvDVNGIOXnc4WoLQr33MMtZnO6T7TH/xcW8vfMTLZiFDFQrIKkpMqzmRVq4mY1REQE98mFhPBx33iD8/Tx4c57gM89MPDOEYVjx7gBOmEC1wEn9EyqtRD1ShTOpJ4BgfSOPFJo2LArhLBFdvZB/RFGjeIb21ALNDeXWyEHDwIHDmg6tQBNi8fPz3RL4cYNruCqWgqdOvFnTSrfrCzgoYe4Y60mSyzXhMhI/jxzxjL5m0p4OP8XJ09WP/cgIYErDKV1amUFPPsszwmo6fsJzAkRl6FPn8r7FVEg0kxc07YUTBmBpD1HQcHWFmjZ0nyWQno63/9BQXz9lftCezgqwCOQGjZkcdZnKfj7c3pFMGpDRARfN+dyl/HMmdwwmzy58lvdgoPvHPfR0aO8Umz37prft4l6JQqRKeU3ZkqIQVGwtnaEq2sfZGTs0B9h0CC+ufS5F/Ly2P979CgwbRo/uNp/Zmwsz9R0dOQ8PDyMi0LV4agKLVvyukwnT1afXpvXX+eHr7CQKxxzo1ZrHqq6frgUa6iwsPohfYmJXDkqnfgAMGkSW4V12eF8/TpXoFVFoW1bvs8SEjQjj7QtBVNGICmi0Lhx5f0hIcA//+gfF//448CLL5pe/k2bOJ/PPuPfigupqigAmrkKSsVfVRSA2ruQiFgUtK2thg15raePPqoct2NHvjaWfNsbETBsGNcPhsjP50ZV9+7cEd6y5d0jCkKIIUKIi0KIK0KIWQbijBFCnBNCnBVC/GbJ8kSlRsGe3GBf7FdhverDw2Mo8vPPoKgoXjfQwYFb2xs3VjZpS0qA4cOBf/8FfvuNO9esrPi3gjIcVUHfsNQFCyr3WWiPQ9dGCH6ITbUUdu7kDtTp07lVuHOnaelqwtWrmiUJLCUKOTl8Hoo7Qh9ELAqKn7y6DvnExMotbYAfxEcfBVatqv44pqBScWexj0/NBigooq3PUgDYWtCeuKaNsTWQ9FkKAHcKp6ToLhOfkAD8/jvfm4cPm1b+dev4BT5Dh/K5K6JQtQ8H0MxqTknhxpKz1iAQ5Xmp7bDUxEQ+X21RAPgZqDr8MDiYPy3ZoNm9G9i2je9hZRhuVU6cYEFV3inRrVvlfhkLYzFREEJYA5gP4EEA7QGME0K0rxKnNYC3AfQmokAAMyxVHoBFwTk/GK3uEdW+C9zT80EAMGwtjBrFb4w6qOVi+vprYP9+4KefeHGthg255VGdKFQdWVFUxK35adM0fRaGLAWAh9hFRRlfmiE7m4dbtmsHfPEF+1EtIQqKi6BvX66UzL3q5sGD/OA+/TQP1TTEuXPAtWvAjBm8TpQytE8fim+5Ks89x+62tWtrX97oaB41NGsWVwDjxmkqcmMcOsSWYGBg5f2KKJw/b1gUOnXilnBGhv68DYnCsGFsIVUdSKGsIOrhwdfU2Azb7GwWwFGjuOLt1k3zHyiWgna/gbalUPXVm0pjqLaWgnJcU141qoxAslS/AhEQFsaiV1hoeM6TYhUorqPu3blxeCsutBpgSUuhG4ArRHSViEoArAEwskqcqQDmE1EmAFRZntusqEmNM6lngGTD/QkKTk7tYW/vj4wMAwvBPvggu4AUF9LVq2yKjhrFw94UevfmVpdKxTdEXJx+S0FZViM8nG+W5GTNzOm4OJ796e6uW45OnVhIjPmBZ87kh27FCrZ0Bg/m1pC5b7LISPZnjxvH5xETY558S0qAt9/mznsrK24JL12quW5VUVxHw4cbXycqMVG/KPTvz5ZGbTucV61i0b52jVvZx4/zfzVmjGkTpA4d0rwVTZvGjbkD8vx5jRVZVRTuvZevzYED+vM25D5ycQHuv59FQfva/v4732vffsst1t+MGPRbtnCjZtQo/t2tm0akkpJYXJR5FYBmVnNyMnRmlDZsyPd+bUUhIqLykNPq8PTke8GQKJSW8lsT58/nhkdNXye/ezdbWl9+yVbUr7/qj3f0KM9FatSIf9/mfgVLioIPAG3/S0L5Pm3+B+B/Qoh/hBBHhBBDLFWYmIwY5JfmI+eycVEQQsDTcygyM/dArdbTIdugAQvD+vXcapo2jW+8776rHK9XL3anREdzx1thoa6lkJurmTC0fTv7hH18NNPxleGoVU1dgCsdoPp+hfPngSVLgNde09xcgwfz5y4zvxI7Kop93orZay4zfPp0YM4cthCioljkLl1iEdXH5s18bXx8uIVoaJ2oggK2BvSJghDc4Xz4cM07zdVq4N132b137hwLQZs2LGSHDwNvGVlg+OZNTlfVdaSUq107jaXg5VW5ggX4+js5AX//rT//1FSumG1tdcMefZSFTKkYr1/nymjsWB4J07UrWz7aQyRv3qzsZlu/nit65X5T7oeIiMpzFBS8vbnCjY7WtRSAms3+r0pEBNChAzfiTKG6zuZ587jxN20aW3BNmwKvvGKayCtWgp8fMGUKN5x279a/ltqxY5prB7Ag29reFaJgCjYAWgMYAGAcgCVCCLeqkYQQzwohIoQQEWlpabU6kLK8RWm8cVEAuF+hrCwP2dkGOmRHjeKWzRtvcGU+e7Zui60XL5uBf//V3NTabqCq/tLt27mF+txzXGHHxOgOOdSmTRuuEKrrV1i6lAXrjTc0+4KDuZVobhdSZCRXhO3bc4veHCOQSku5pfrkkyxuzs484cjFhX9XJS2NK97hw/m3sk6UvrJUHY5alYkTa9fhfPgwV9gvv6xp7QFcsU6fzo2HFSsMtzT/KZ88qU8UAM0IJEP3hp0dpzU0Qq7qHAVtRozg/05xISmuo9Gjef933/F1Cwvjcxg4kM+xWTOuICMi+D5+5BHNyJ7QUBazY8f0W2bK74wMXUsB4Ap4+3auSKtraFy/zm5SxUWlr5PZGMHBLLhVK/qEBLYShg7l53LZMrbI5s7lBqKxmeCKlfDuu/z/jB/P7tWq7snkZP5ftUXBwYHLdbs6m4nIIhuAngB2av1+G8DbVeIsBDBZ6/de8Gs/DeYbGhpKtSE+O57eXL2MYFNIu3cbj69S5dH+/XZ0+fJr+iNkZxPZ2REBRCEhRKWlunHUaqJmzYjGjyf680+Oe+qUJvzoUd73119EV6/y92+/JUpMJLK2JnrzTaImTYieecZwQbt3J+rfX39YURGRpyfRqFG6YRMmEHl5EZWVGc7bEBcvcp6ZmZp96elc/i+/5N9t2hA9/HDN867K7t2c78aNlfe/8AKRvT1RRtzsoogAACAASURBVEbl/T/9xPEjIvi3cl0XLtTN+++/OWzvXsPHf+IJIldXorw808s8fTqXLTtbN6y4mKh3bz7ugAFEx4/rxpk5k++twkL9+X/xBaf38yMaPlx/nDlzOE5Kim5Ynz58bEMMGEDUvj1/Dw0l6tq1cvjjj3PeANE99xC99x7RuHFEtraa/VWvabt2XFYfH6LJkyuH/fuvJt1nn+mWJz2d6K23iBo25DjDh/O+qjz2GIeHhhLl51f/3xti9WpOExlZef/o0UQODkQxMZX3r1pFZGNDFBREFB+vP0+1mqhnT/6/ios1+zt2JOrRo3LcDRv4+P/+W3n/Sy8ROTsTqVSmn0sVAESQKXW3KZFqs4GtgKsAWgCwAxAFILBKnCEAVpZ/9wK7mzyry7e2okBEtHQpn/G1a6bFj4x8gI4ebWc4wkMPEQnBlbshRo0iatGC6Ouv+eDalVhyMu/74QeiH3/k7xcucNijj3KFDhB99JHh/J9/nsjFRX/lvmYNp9+xQzfs5585TF+lZIxRozjtvHmafXv38r5du/j36NFELVvWPO+qvPQSkZMTUUFB5f2nTvHx5s7VLZu3Nz+IRPzp6Un09NO6eSvXQLnm+ggP5zjLlumGXblCOi0MlYqoaVP+/wxRUsL/eaNGnPcTTxDl5mrCe/Yk6tXLcPrNmzWV6Asv6I+jNDjWrNENa92aaOxYw/nPnctpt2zhz6++qhyekkIUFkZ0+LDmOhMRpaYSff450XPP6TaSJk7k87W2Jnr33cphsbGa81mxwnC5MjL4WbC15YaWNsePc/rBg/mZfOwxzf2vNBBM4dw5TeNGObcdO3jf7Nn60+zaxYLl60t0/bpu+M6d+sXp8895v7bQzJrFIlO1QbBqFcc9c8b0c6lCnYsClwFDAVwCEAPg3fJ9HwEYUf5dAPgGwDkAZwA8bizPWxGFWbP4fjJVbOPivqV9+0AFBVf1R7h4kWjTpuozUcRgzBi+cbQforIyblHOnMmtnxYtNOG7dpn2oCxezHGuXNENu/9+oubN9QtGSgqn+/jj6stfldOnOZ0Q3CKrep6pqfx79mz+rV3ZVcfatVyZaF8ftZpblo88oj9Nly7cQlPSFBdza+rZZyvHGzyYKDhYN73Smq6ujGo1UWCgbmu5rIyPbW1NFB2t2b9vH+f5+++G81TIzuYK0sqKaMQIvjELCvgmffNNw+muXNHcG59+qj9OaSk3FqpeCyLeP3264fzj4jhvHx/+jI01fi7G+OEHTZnnz68cVlysCdPXgKnKBx9w3G3bNPseeIDFPzubRQzgxoGdHVvMplJaylaNYnH89htbQ61bV59PZCSRoyNb4Nqo1WyZ+frqplfEUBGbjAy+z/TVcRcucNylS00/lyrcEaJgie1WROGxx9irYSr5+Rdp3z5QQsJ845ENceQIX2ZHR6IOHXTDW7ViN0uDBkQvvqjZX1bGN6Mx94bSQvrjj8r7Y2LIqJUREkLUt2/NzkcRN+XBPH2a9z/5JD+EChs3cvjhw8bzLCjg1rXiSlM4doz3rVypP92iRRz+999Ey5cTdetGFS1cbd57jyvv/PzK+6dNY9eQMebNI50W5x9/8D4rKxZfRZief54tm5q4m5QK89VXiQ4c0L0OVVGpuDEBcAvSEMOH8/2lTWGhaY0B5Vr27Gn6eVSH8l/qcwUSaaymqm4bfRQVccXt50eUk0O0Zw+n/fprDler2TIEuOFQUwoL+d5q3VpTZlN8zjNncmPp7FnNvv37Sceq1qZfP6KAALYsFXe0PqEvKyNyc6u+sWAEKQp6CAkhGjbM9PhqtZoOH25JUVE1SFSV4mLNAzx0qG74wIGa8M2bK4cpLR59JqlCYSGbm2+/XXn/O+9whWXIz0nEflobG/2+b32cPcs3/TvvEKWlcYv29dc5LCio8vkp/txFi4zn+913HNfdnf8kpYJ95x2uzPX5j4m4QmjQQPPgtmvHrVBta4NII1BV/bSPPKLxnVdHVhZX9ErfTlkZC3zbtkTffKOp6EpLuXKrzjVjiFde4Xw6deJPQ+esEBTE8fbtMxxHKVtcnGbf9eu8b8mS6vNXrKjvvjP5FKqlqEhT6R07phseEkKVLE1j/PMP34vTp3Pr2te3ssuluJgbMFXdizVBpSJav16/61Af6encYHrsMc2+gQO5X7Cq+1NB8Wk3bkw0YwbRiRO6969CVlbNyl8FKQpVUKvZs/DKKzVLd+nSNDpwwJFKS3NqdVwiYvPRkP938mQOs7PTbV2qVHyTGKNjR6IhQzS/S0u5g9uYAiodrT/8YPwYRNyZ2KABCwIRV6pNmnC5bWy4ElcoK+MHZNq06vMsKOCyDhjAFgFAtG4dh7VvT3TvvdWnnzuXaNIk9v0bepgSEkhv/0O3bux2MIWnn2ZhyMpiVxfAroWSEi5ny5YaX//69ablqY1KxS17wDShGjOG416+bDhOZCTpWFragxuqIzWVr+vNm6aV3xQU6yMhQTds6FBuANRk4MO0aZoGgakVt6V5/30uz8mTmg70qn0y2pSVsQVaUmLxoklRqILiQjdkxRkiK+sw7dsHSkw0ocVriDff5IPPmaMbFhbGYaZWTvqYNIlbGkqlqHSw6TPTtVGp+Li2tkSHDlUf9/x5bpm99ZZm36ZNVOGi0udH79mTzePq+P57qmjxqlTs3+vQQeNDvZWWnjbNmrGLSxt9I2EMERGhKU9gIFslSueUMkLK05OF0NCoIWPk5vJIMkMdmtp8+im7JA21QIm4wvH05E5ehb/+4rJWNzjCUrz8Mt9r+kbqvfGGaWKoTU4Ou5DatdOfZ12QmckW70MPsdB5eprer2ZhpChU4dAhPtvt22uWTq1W07FjQXT8eO3dVhXui9WrdcOWL+ewb76pff5KxRobyxWKtTVXrqY8KDdvst+5cWPDHYpqNY8mcnKqbN6XlLC7RHHhXLxYOd1zz7EfVBGrvDwePaH8VqwE7SG1ypDAnj0152QOhg/nykOhtJTda++9Z3oeXbpozrXqqJ6HH+b9VYXHUhQUVD9qSmHUKCJ/f801X7KEjLokLUVqqmF3V2Fh5SHOppKWZtzVdrv59FOqsGA++aSuS1OBFIUq/PILGbW2DREfP4/27QPl5NRgaJs2RUVsVurz3V+4wL7pW6n8Dh7kk/Pz489x42r2gJ07xy3cTp10O2OJNEMU9XVav/oqhzVooDusS+lAjY9nc7pVK6rw/c+dy52dVf3iir8eIOrc2fRzMMaHH7Klo/hlFZfSggWm56H4f9u31z3XmBg+v4MHzVdmczB/fuUbX7nmtbVmJMbJzeVGlqvrLfcDmBMpCnrIzKzd3I+Skkw6cMCRLlzQM7zvTiAnh336/9/enQfIVdWJHv/+aumtqnpf0lvSiVlIAllIy+qIgL4JiAFGlqCo+HDQUQTG6CAziCMOPlGfyFMUGNFhWFQEHRGRLYAiIpCNrGRfuju9pfeqXqqr6vf+uNWdTtLpdJbq6nT9Pv8k995Tt351+9b91T3n3HNyclQfe+zY9vHMM85Fc/HiA+8Gli937jyWLBm+vvedd/b/sj/YQB//j33MaTMpL3f6fw/ULcPwD9499dThk9CxGugFNtBDZbR160MFg068o+k2OV5s3uz8XQMB1Ysuchpls7OTHdXE99prI/caTAJLCifYpk3X6Z//7D++BudEev31kXsajcb99zsX78JCpzF1xw6nTnTOnJF7KF1xxf6L7VBtbfsv/hdfvL+BWtWpo1+2bPiHcWIxpxpptL2iRmugJ0gotD/xrFp1Yt9jPHrhBaeTw0D/+xN5B2ZOGqNNCuKUPXlUV1friiRMWN/R8QarV5/DzJkPUlb2j2P+/mNmwwZnkpkVK5yhm0WcET5HM2DUcJYtc8Z7+uIXGXG88rHw2mvOPNs/+IETy003OeMAHTxa6ETW1OSMhZWfn+xIzBgTkZWqesSBoCwpjJKqsmLFfFyudBYtGmF8/okgEoHvfc8ZFfLnP3dmm5sozj/fGWr8qqvgxz92hrNOdrIyZgyMNinYt2GURITS0hvo6lpBV9dRTIF5MvJ4nOGR6+omVkIAZ6TL+npn9NiyMksIxhzEvhFHoaTkWlyuTPbuTeLcveb4nHeeMzNcKHT4IbONSWGWFI6C15tLcfHVNDU9TiQSTHY45liIOHcLMPzkOsakOEsKR6m09Aai0S6amn6Z7FDMsbrgAmf2rKuvTnYkxow7nmQHcLLJzj4Ln+9U6usfpKzsM8kOxxwLEacR3RhzCLtTOEr7G5zfpqtrhGkwjTHmJGRJ4Rg4Dc4Z1NcPM0ewMcacxCwpHAOvN4+ioqtobHzUGpyNMRNKQpOCiCwWkc0isk1EvjpCuY+KiIrIER+sGC/KypwG5+bmXyU7FGOMOWESlhRExA3cB1wEzAGuEZE5w5QLADcDbyYqlkTIzj6HrKw57N37QLJDMcaYEyaRdwpnANtUdYeqhoFfApcOU+6bwN1AbwJjOeFEhLKyz9HV9TadnSdVPjPGmMNKZFIoB2qGLNfG1w0SkdOBSlX9QwLjSJhJk67D7c6mtvbeZIdijDEnRNIamkXEBXwfWDaKsjeIyAoRWdHc3Jz44EbJ4wlQWno9zc2/pq+vLtnhGGPMcUtkUqgDKocsV8TXDQgApwKvisgu4Czg6eEam1X1QVWtVtXqoqKiBIZ89MrLb0Q1Sl3dj5MdijHGHLdEJoW3gRkiMlVE0oClwNMDG1W1Q1ULVbVKVauAvwFLVHXsx8U+DpmZ0ygoWMLevQ8QjfYkOxxjjDkuCUsKqhoBbgSeBzYBT6jqBhG5U0SWJOp9k6Gi4hYikRaamh5PdijGGHNcbJKdE8CZgGcBEKO6ei0ikuyQjDHmADbJzhgSESoqbiYUWk9LyzPJDscYY46ZJYUTpLj4Y/h8p/Luu5+ku3tLssMxxphjYknhBHG7Mzj11N8j4mHduo/Q39+W7JCMMeaoWVI4gTIzq5g797f09u5kw4YricX6kx2SMcYcFUsKJ1hu7vuYOfNB2tuXs3XrF1CNJTskY4wZNZt5LQFKS6+jp2cre/Z8C9UIs2b9J874gMYYM75ZUkiQqVP/AxEPu3ffSTQaZPbsR3G50pIdljHGjMiSQoKICFOnfgO3O8COHV8hGg0xd+5TuN0ZyQ7NGGMOy9oUEmzy5C8zc+b9tLY+S23tPckOxxhjRmRJYQyUlX2WgoJLqKn5jnVVNcaMa5YUxsjUqf9BJNJOTc33kh2KMcYcliWFMeL3z6e4eCm1tT8gHG5MdjjGGDMsSwpjqKrqTmKxPnbvvivZoRhjzLAsKYyhrKwZlJb+b/buvZ/e3t3JDscYYw5hSWGMTZlyB+Bi69abbRgMY8y4Y0lhjGVkVDBt2rdoafkd69dfSjQaSnZIxhgzyJJCElRWfomZMx+gtfV51qy5gHC4OdkhGWMMkOCkICKLRWSziGwTka8Os/1LIrJRRNaKyHIRmZLIeMaTsrIbOPXU3xAKrWX16nPp7t6c7JCMMSZxSUGcEeDuAy4C5gDXiMicg4qtBqpVdR7wJPCdRMUzHhUWXsr8+S8RibSxcuV7aW7+TbJDMsakuETeKZwBbFPVHaoaBn4JXDq0gKq+oqrd8cW/ARUJjGdcysk5l0WLVpGVNZsNGz7K9u23EotFkh2WMSZFJTIplAM1Q5Zr4+sO53rgj8NtEJEbRGSFiKxobp549e8ZGZUsXPhnyso+R03Nd1i37sNEIl3JDssYk4LGRUOziFwLVAPfHW67qj6oqtWqWl1UVDS2wY0RlyudmTN/wqxZP6WtbTlr1pxHX19DssMyxqSYRCaFOqByyHJFfN0BROSDwL8BS1S1L4HxnBRKS6/ntNN+T3f3FlavPtsaoI0xYyqRSeFtYIaITBWRNGAp8PTQAiKyEHgAJyE0JTCWk0pBwUUsWPAK0WiIVavOpaPj9WSHZIxJEQlLCqoaAW4Engc2AU+o6gYRuVNElsSLfRfwA78WkTUi8vRhdpdysrPfy+mnv4HXm8+aNRfS1PSrA7YHg+/Q1PTrJEVnjJmoEjrzmqo+Czx70Lo7hvz/g4l8/5NdZuZ7OP30N1i//jI2blxKT89OfL5Tqa29h/b2lwFwu/9AQcHFSY7UGDNRjIuGZnN4Xm8B8+a9SHHxUnbuvI316z9CT88Wpk27m8zMmWzb9iVisXCywzTGTBA2R/NJwO3OYPbsx8jJeT8eTx5FRR/F5fKSlTWH9es/Ql3dfVRW/nOywzTGTACWFE4SIi7Ky//pgHUFBR8mL+/v2bXrG5SUXEta2sTsrmuMGTtWfXQSExGmT/8+0WiQXbvuOPILjDHmCOxO4STn882hvPwL1NX9CI8nF9UI0WiQtLRyJk++FZfLm+wQjTEnEUsKE0BV1ddpbn6SPXu+jcuVhdvto7+/mY6OvzB37pN4PP7Bss64SmrJwhgzLEsKE4DXm8/ZZ+8BwBmcFurrH2Lz5s+yZs15nHbaH3C7M9m79wFqa+8lGg0ybdq3KSv7LCJWg2iM2c+SwgQxkAwGlJZeT1raJDZsuIqVKxcRjXYRjXaRm3shoGzd+nkaGh5m1qwH8PvnJydoY8y4Yz8TJ7CCgg+zYMGruN0+CgouYdGilSxY8BLz57/E7NmP0tu7kxUrFrFu3RKamp4gGu0ZfG0sFiYctpFHjEk1oqrJjuGoVFdX64oVK5IdxoTQ39/Gnj1309j4COHwXtzubLKyZtHXV0s43AAoWVlzKC6+iqKiq/H5Tkl2yMaYYyQiK1W1+ojlLCkY1Shtba/Q2Pgo4XAd6emTyciYjMuVRUvLM3R0vAYoaWmlZGRMJSNjarzX000HNGIful+ls/OviKTh9y/E5RpdbaWq0t7+MoHAmSPu3xgzepYUzAnT17eX5uanCAZX09u7k56enfT17cHnm8vcub8lK2v6Ia9pa3uFnTv/jc7ONwBwuXzk5JxDTs65+P2LCAROJy2tFBE55LU1Nd9n+/Zl5OS8n3nznsPtzkz4ZzRmorOkYBKqtfVFNm5cCsSYPftx8vMX09OzhY6O12lsfJz29uWkpZUzZcrteL15tLe/RkfHa4RC6wDnnPN6S5g8+VYqKm4ZTA6trc+zdu3F+P0LCAZXU1BwCXPnPnXcXWgjkSAdHX8hL++Do75jMWYisaRgEq6nZyfr119OKLQWr7eA/v59AKSlTaKy8lbKyj6H251xwGsikSCh0Dt0da2ipeX3tLW9SFHRVcya9RDhcD2rVp1BevpkFi58ncbGR9i69fOUlHyCU075r2PuPtvVtYaNG6+ip2crPt88Zsz4Ibm57x/Va6PRELFYP15v7ojlVGO0tb1EdvbZeDyBI+yzGxC7AzJjypKCGRPRaDc7d36NSKSV7Oxzyck5l6ysWaO6gKsqNTXfYceOfyUraxaghMPNLFr0NpmZUwHYvfsudu68naKiKykt/Qy5uefhcqUPu79IpJP+/mY8nnw8HuciXld3H9u3L8PrLaSy8kvU1v4/+vr2UFy8lPLyL+LznTZ4EVdVwuEGQqG1tLe/Rnv7K3R1vYWIh1mzfkZJyTXDvm9fXz3vvvtJ2tpeIi1tEtOmfZuSkk8MewyCwXdYt+4SVGPMnPkAhYWXjOYwH/Q5g4TD9fT3NxONdhGJdBGL9ZCXdyHp6WVHvb+j0d/fRnPzk/T0bKGy8l+OON6Wqg5bRWjGniUFc9Joa1vOxo1LiUTamTfvRfLyPjC4TVXZtevfqan5LrFYDy6Xj9zcD+ByZRCL9RCLddPfv4/e3hqi0Y7B17lcGXg8eYTD9eTnf5hTTvkv0tIKiUa72bPnbvbsuZuB2V8zMqaSljaJ7u53iUTa4ntwEwhUk5d3Ph0df6Wj489MmXI7VVXfOOBiv2/fM2ze/Gmi0RBTptzOvn1P09X1JoHAmUyb9i1yc88bfIZk375n2LhxKV5vHh5PLqHQekpKrmX69HvxevMPe3z6+hpoanqcpqZf0N29mWi0a9hyLpePqqqvUVFxCy5XOtFoL83NT9LS8jvy8y9i0qTrDpusVaPU1/+U/v59lJR8koyM/TPpRqM9tLT8gaamx2hpeRZVZ6j2zMwZzJv3HJmZ04bsJ0Zn51u0t79MW9vLdHb+jby8C5k5837S00sP+xlHq6dnFw0Nzl2jx5OD251NdvbZR90zbuC6l+yEFYuFCQbX4vfPw+VKS+h7jYukICKLgXsBN/BTVf32QdvTgf8GFgEtwNWqumukfVpSmJj6+hoIh+sJBBYOuz0a7aG9/RVaWp6hvf1VQHC5MnG7s/B48snIqCQ9vRKvt5hIpJVwuJ5wuIHs7HPjT24f+OUPhxvp7HyLUGgtweA6wuEGsrJm4fOdis83l0CgGo8nG3C+uFu2fJ6GhocoLLycnJz30d29iVBoI52df8Xnm8+cOb/A55uNaozGxkfZseNWwuEGvN4Sior+Aa+3kN2778LvX8hppz09uLxnz7fweHIpLPwoBQUXk5t7ASIuurpW0dX1Jq2tL9LW9iIQIxB4L9nZ55CeXkpaWilebzEeTzZutx/VfnbtupOWlqfJzJxBfv5iGhsfJxJpwePJJRJpJxA4kxkzfkR29oHXhWBwLZs3f4aurrfja1wUFFxMQcGltLe/SkvL7+LjaU2iuPgaSko+TizWx7p1H0HEy7x5z5KVNYfGxkeoqfm/9PQ484r7fPPw+xfS3PwrXK4sZs78McXFV6Oq9PbuIhTagNvtj//tKg57B+j8vZrYvfsu9u79Cc6kjvuvWyIeKiqWUVV1B2531ojnWTTaQ13dfdTUfBe3209h4WUUFl6G3z+fYHA1HR1vEAyuxOstIhB4L4FANVlZs4dthwqFNtLa+gLd3Zvo7t5EX18tkyZ9msrKrxxSbTqcjo7X2bz5Brq7N8aHxL+SkpKPkZPzdwkZaSDpSUGcn0dbgA8BtThzNl+jqhuHlPk8ME9VPyciS4HLVfXqkfZrScEkg6pSW3sv27cvA2J4vYVkZc0mN/d8Jk++7ZCLQDQaoqXlmfgv9WeJxbopLLyc2bMfwe32DZYLBt9h1647aWt7gWg0iIgX1RgQBSAjY1r8QnztqH4Nt7Q8x7ZtN9PTs53CwssoL/8ncnPPp7HxMbZv/wr9/U3k5y8mPX0yaWnFRCKd7N17Hx5PHjNm/JBA4Ezq6/+ThoafEQ434PHkU1T0UYqLlx5w1wMQCm1i7drFRCKtuFw++vsb8fsXUlFxC/n5Fw1WLYVC7/Luu5+iq+stfL559PbuPuCubkBGRhW5ueeTl3ch2dnnEg7vJRhcTVfXSpqaniAW66W09Hqqqu7A6y0hGu2kv7+FPXu+TUPDz8jImMb06T8gEKjG6y3A5UpDVeM/Ehpob3+V3bvvIhyuJy/vQ4h4aGtbPnjnsz+OqfT37xu8I3OS9uUUF19Nbu75tLUtp7b2nniyBo8nH59vDi5XBm1tL5GZOZ3p039IQcFiotFuent30tdXh8uViceTg8uVSW3tPezd+xPS0yczefKtdHT8lX37/odYLITHU0Be3oXk5X0Iv/80QqGNBIOr6OpaTXHxUioqbjzyCTuM8ZAUzgb+XVX/Pr58G4Cq/p8hZZ6Pl3lDRDxAA1CkIwRlScEkU19fPSJe0tIKR/2aaLSb7u4t+P3zDvsLMBYL09HxF1pbn0fEQ3b2mQQCZ5CePumoY4zFIsRivYc84xGJdLJ79zdpafkj/f1N8Y4BSknJJ5k+/ft4vQVD9tFPKLQen2/uiNUafX172bDhSjyebCorvxy/0zm0SiYWi1BT8z1aW/+IzzcHv38BPt9pxGI99PbW0Ne3h2BwDe3trw6pwnN4vYXk5f0vqqq+TlbWzGHjaGt7lS1bPktPz5bBdW63n1isD9X+wXU5Oe9j6tS7BjsaRCKdtLY+R3f3ZgKB08nOPguvtwDVGD09W+nqWkFr6wvs2/c/RKOdiKShGiYtrZTy8huZNOm6A7pWt7a+yNatN9LTswWPp4BIpOUwR85FRcVNVFV9c/DvNPBDoqXlj7S1vUA4XL+/tMuH37+AsrJ/ZNKkTx327zGS8ZAUrgAWq+pn4sufAM5U1RuHlFkfL1MbX94eL7PvcPu1pGDMiaEaJRrtPmJvqbGkGiUYXENn55ukp1fg959Oenr5qOr+o9FeWlufJRxupL+/hUikBZH0eHXbJDIy3kMgsOiY2hFisT5aW5+ntfV5cnLOoajoysMmy1isj7q6HxEKbSIzcyoZGdNITy8nFusjGu0kEunA7z+dQGDBCMdB6e7eSHf3Zny+uWRmzjjuKqXRJoWTosO2iNwA3AAwefLkJEdjzMQg4h5XCQGcmAKBRQQCi476tW53BkVF/5CAqMDlSqewcAmFhUtGVbayctlxvZ+I4PPNxeebe1z7ORaJHBCvDqgcslwRXzdsmXj1UQ5Og/MBVPVBVa1W1eqiIpty0hhjEiWRSeFtYIaITBWRNGAp8PRBZZ4GBirIrgBeHqk9wRhjTGIlrPpIVSMiciPwPE6X1J+p6gYRuRNYoapPAw8Bj4jINqAVJ3EYY4xJkoS2Kajqs8CzB627Y8j/e4ErExmDMcaY0bNJdowxxgyypGCMMWaQJQVjjDGDLCkYY4wZdNKNkioizcDuY3x5IXDYp6VTmB2XQ9kxOZQdk0OdTMdkiqoe8UGvky4pHA8RWTGax7xTjR2XQ9kxOZQdk0NNxGNi1UfGGGMGWVIwxhgzKNWSwoPJDmCcsuNyKDsmh7JjcqgJd0xSqk3BGGPMyFLtTsEYY8wIUiYpiMhiEdksIttE5KvJjicZRKRSRF4RkY0iskFEbo6vzxeRF0Vka/zfvGTHOtZExC0iq0XkmfjyVBF5M36+/Co+0m/KEJFcEXlSRN4VkU0icnaqnyci8s/x7816EfmFiGRMxPMkJZJCfL7o+4CLgDnANSIyJ7lRJUUEWKaqc4CzgC/Ej8NXFIS3NQAABCdJREFUgeWqOgNYHl9ONTcDm4Ys3w3co6rTgTbg+qRElTz3As+p6inAfJxjk7LniYiUAzcB1ap6Ks7Iz0uZgOdJSiQF4Axgm6ruUGeW7l8ClyY5pjGnqvWquir+/y6cL3o5zrF4OF7sYeCy5ESYHCJSAXwY+Gl8WYALgCfjRVLqmIhIDvB+nKHtUdWwqraT4ucJzqjSmfEJwbKAeibgeZIqSaEcqBmyXBtfl7JEpApYCLwJlKjqwCzhDUBJksJKlh8A/wLE4ssFQLuqRuLLqXa+TAWagZ/Hq9R+KiI+Uvg8UdU64HvAHpxk0AGsZAKeJ6mSFMwQIuIHngJuUdXOodviM9+lTJc0EbkEaFLVlcmOZRzxAKcDP1HVhUCIg6qKUvA8ycO5U5oKlAE+YHFSg0qQVEkKo5kvOiWIiBcnITymqr+Jr24UkdL49lKgKVnxJcG5wBIR2YVTrXgBTn16bryaAFLvfKkFalX1zfjykzhJIpXPkw8CO1W1WVX7gd/gnDsT7jxJlaQwmvmiJ7x4XflDwCZV/f6QTUPnyv4U8Luxji1ZVPU2Va1Q1Sqc8+JlVf048ArOvOGQesekAagRkVnxVRcCG0nh8wSn2ugsEcmKf48GjsmEO09S5uE1EbkYp+54YL7ou5Ic0pgTkfcBrwHr2F9//q847QpPAJNxRqC9SlVbkxJkEonIB4Avq+olIjIN584hH1gNXKuqfcmMbyyJyAKchvc0YAfwaZwfkSl7nojIN4CrcXrxrQY+g9OGMKHOk5RJCsYYY44sVaqPjDHGjIIlBWOMMYMsKRhjjBlkScEYY8wgSwrGGGMGWVIwZgyJyAcGRmI1ZjyypGCMMWaQJQVjhiEi14rIWyKyRkQeiM+3EBSRe+Jj6i8XkaJ42QUi8jcRWSsivx2YZ0BEpovISyLyjoisEpH3xHfvHzJXwWPxJ2SNGRcsKRhzEBGZjfPk6rmqugCIAh/HGQRtharOBf4EfD3+kv8GblXVeThPiw+sfwy4T1XnA+fgjK4Jzui0t+DM7TENZwwdY8YFz5GLGJNyLgQWAW/Hf8Rn4gz+FgN+FS/zKPCb+NwDuar6p/j6h4Ffi0gAKFfV3wKoai9AfH9vqWptfHkNUAX8JfEfy5gjs6RgzKEEeFhVbztgpcjXDip3rGPEDB0bJ4p9D804YtVHxhxqOXCFiBTD4BzWU3C+LwMjYn4M+IuqdgBtIvJ38fWfAP4Un9muVkQui+8jXUSyxvRTGHMM7BeKMQdR1Y0icjvwgoi4gH7gCziTzZwR39aE0+4AzpDJ98cv+gMjioKTIB4QkTvj+7hyDD+GMcfERkk1ZpREJKiq/mTHYUwiWfWRMcaYQXanYIwxZpDdKRhjjBlkScEYY8wgSwrGGGMGWVIwxhgzyJKCMcaYQZYUjDHGDPr/BA2VOpHd56cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.5702 - acc: 0.8629\n",
      "Loss: 0.5701692538345591 Accuracy: 0.86292833\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5988 - acc: 0.5150\n",
      "Epoch 00001: val_loss improved from inf to 1.33071, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/001-1.3307.hdf5\n",
      "36805/36805 [==============================] - 434s 12ms/sample - loss: 1.5988 - acc: 0.5150 - val_loss: 1.3307 - val_acc: 0.5889\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8878 - acc: 0.7412\n",
      "Epoch 00002: val_loss improved from 1.33071 to 0.83290, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/002-0.8329.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.8879 - acc: 0.7411 - val_loss: 0.8329 - val_acc: 0.7482\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.8130\n",
      "Epoch 00003: val_loss improved from 0.83290 to 0.64699, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/003-0.6470.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.6505 - acc: 0.8130 - val_loss: 0.6470 - val_acc: 0.8109\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8562\n",
      "Epoch 00004: val_loss improved from 0.64699 to 0.53830, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/004-0.5383.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.5141 - acc: 0.8561 - val_loss: 0.5383 - val_acc: 0.8516\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8782\n",
      "Epoch 00005: val_loss improved from 0.53830 to 0.43348, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/005-0.4335.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.4353 - acc: 0.8782 - val_loss: 0.4335 - val_acc: 0.8717\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8979\n",
      "Epoch 00006: val_loss improved from 0.43348 to 0.41933, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/006-0.4193.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.3643 - acc: 0.8979 - val_loss: 0.4193 - val_acc: 0.8826\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9098\n",
      "Epoch 00007: val_loss did not improve from 0.41933\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.3170 - acc: 0.9097 - val_loss: 0.4221 - val_acc: 0.8807\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9228\n",
      "Epoch 00008: val_loss improved from 0.41933 to 0.40939, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/008-0.4094.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2761 - acc: 0.9228 - val_loss: 0.4094 - val_acc: 0.8793\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9339\n",
      "Epoch 00009: val_loss improved from 0.40939 to 0.30942, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/009-0.3094.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2401 - acc: 0.9339 - val_loss: 0.3094 - val_acc: 0.9143\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9385\n",
      "Epoch 00010: val_loss improved from 0.30942 to 0.29903, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/010-0.2990.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2157 - acc: 0.9384 - val_loss: 0.2990 - val_acc: 0.9201\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9445\n",
      "Epoch 00011: val_loss did not improve from 0.29903\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1975 - acc: 0.9445 - val_loss: 0.3417 - val_acc: 0.9036\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9496\n",
      "Epoch 00012: val_loss did not improve from 0.29903\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1773 - acc: 0.9496 - val_loss: 0.3363 - val_acc: 0.9043\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9563\n",
      "Epoch 00013: val_loss did not improve from 0.29903\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1565 - acc: 0.9562 - val_loss: 0.4204 - val_acc: 0.8896\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9593\n",
      "Epoch 00014: val_loss did not improve from 0.29903\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1467 - acc: 0.9592 - val_loss: 0.3063 - val_acc: 0.9140\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9662\n",
      "Epoch 00015: val_loss improved from 0.29903 to 0.28410, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/015-0.2841.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1256 - acc: 0.9662 - val_loss: 0.2841 - val_acc: 0.9178\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9718\n",
      "Epoch 00016: val_loss did not improve from 0.28410\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1079 - acc: 0.9718 - val_loss: 0.3001 - val_acc: 0.9161\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9725\n",
      "Epoch 00017: val_loss improved from 0.28410 to 0.26391, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/017-0.2639.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1060 - acc: 0.9724 - val_loss: 0.2639 - val_acc: 0.9252\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9764\n",
      "Epoch 00018: val_loss improved from 0.26391 to 0.24281, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/018-0.2428.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0912 - acc: 0.9764 - val_loss: 0.2428 - val_acc: 0.9324\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9801\n",
      "Epoch 00019: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0812 - acc: 0.9801 - val_loss: 0.2639 - val_acc: 0.9243\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9825\n",
      "Epoch 00020: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0710 - acc: 0.9825 - val_loss: 0.3015 - val_acc: 0.9192\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9836\n",
      "Epoch 00021: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0658 - acc: 0.9836 - val_loss: 0.2732 - val_acc: 0.9269\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9868\n",
      "Epoch 00022: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0585 - acc: 0.9867 - val_loss: 0.2922 - val_acc: 0.9217\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9834\n",
      "Epoch 00023: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0653 - acc: 0.9834 - val_loss: 0.3806 - val_acc: 0.9126\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9869\n",
      "Epoch 00024: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0562 - acc: 0.9868 - val_loss: 0.2685 - val_acc: 0.9294\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9818\n",
      "Epoch 00025: val_loss did not improve from 0.24281\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0692 - acc: 0.9819 - val_loss: 0.2496 - val_acc: 0.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9942\n",
      "Epoch 00026: val_loss improved from 0.24281 to 0.23351, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/026-0.2335.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0349 - acc: 0.9942 - val_loss: 0.2335 - val_acc: 0.9385\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9942\n",
      "Epoch 00027: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0322 - acc: 0.9942 - val_loss: 0.2620 - val_acc: 0.9280\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9905\n",
      "Epoch 00028: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0412 - acc: 0.9904 - val_loss: 0.3340 - val_acc: 0.9096\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9844\n",
      "Epoch 00029: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0577 - acc: 0.9844 - val_loss: 0.2617 - val_acc: 0.9348\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9971\n",
      "Epoch 00030: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0225 - acc: 0.9970 - val_loss: 0.2708 - val_acc: 0.9308\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9867\n",
      "Epoch 00031: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0535 - acc: 0.9866 - val_loss: 0.3343 - val_acc: 0.9187\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9920\n",
      "Epoch 00032: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0355 - acc: 0.9919 - val_loss: 0.2571 - val_acc: 0.9313\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9919\n",
      "Epoch 00033: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0359 - acc: 0.9919 - val_loss: 0.2465 - val_acc: 0.9371\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9941\n",
      "Epoch 00034: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0295 - acc: 0.9941 - val_loss: 0.2621 - val_acc: 0.9306\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9967\n",
      "Epoch 00035: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0197 - acc: 0.9966 - val_loss: 0.2653 - val_acc: 0.9308\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9904\n",
      "Epoch 00036: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0383 - acc: 0.9904 - val_loss: 0.3202 - val_acc: 0.9199\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9973\n",
      "Epoch 00037: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0179 - acc: 0.9973 - val_loss: 0.2766 - val_acc: 0.9322\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9946\n",
      "Epoch 00038: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0265 - acc: 0.9946 - val_loss: 0.2641 - val_acc: 0.9348\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9964\n",
      "Epoch 00039: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0208 - acc: 0.9963 - val_loss: 0.2947 - val_acc: 0.9292\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9904\n",
      "Epoch 00040: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0359 - acc: 0.9904 - val_loss: 0.2644 - val_acc: 0.9343\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9923\n",
      "Epoch 00041: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0318 - acc: 0.9923 - val_loss: 0.2866 - val_acc: 0.9276\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9952\n",
      "Epoch 00042: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0240 - acc: 0.9952 - val_loss: 0.2936 - val_acc: 0.9238\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9951\n",
      "Epoch 00043: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0237 - acc: 0.9951 - val_loss: 0.2535 - val_acc: 0.9362\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9956\n",
      "Epoch 00044: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0214 - acc: 0.9956 - val_loss: 0.2703 - val_acc: 0.9324\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9955\n",
      "Epoch 00045: val_loss did not improve from 0.23351\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0210 - acc: 0.9954 - val_loss: 0.2812 - val_acc: 0.9336\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9927\n",
      "Epoch 00046: val_loss improved from 0.23351 to 0.21868, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/046-0.2187.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0294 - acc: 0.9927 - val_loss: 0.2187 - val_acc: 0.9457\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9992\n",
      "Epoch 00047: val_loss did not improve from 0.21868\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0084 - acc: 0.9992 - val_loss: 0.2275 - val_acc: 0.9408\n",
      "Epoch 48/500\n",
      " 9984/36805 [=======>......................] - ETA: 5:01 - loss: 0.0086 - acc: 0.9992"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
