{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_BN(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 4096000)           16384000  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 81,922,576\n",
      "Trainable params: 73,730,064\n",
      "Non-trainable params: 8,192,512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 1365248)           5460992   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 27,636,496\n",
      "Trainable params: 24,904,976\n",
      "Non-trainable params: 2,731,520\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 454912)            1819648   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 9,758,736\n",
      "Trainable params: 8,847,376\n",
      "Non-trainable params: 911,360\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 151552)            606208    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 4,020,496\n",
      "Trainable params: 3,715,344\n",
      "Non-trainable params: 305,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,658,256\n",
      "Trainable params: 1,605,520\n",
      "Non-trainable params: 52,736\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,402,896\n",
      "Trainable params: 1,383,696\n",
      "Non-trainable params: 19,200\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,372,816\n",
      "Trainable params: 1,364,624\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,419,536\n",
      "Trainable params: 1,414,672\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 7, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,445,456\n",
      "Trainable params: 1,442,000\n",
      "Non-trainable params: 3,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6028 - acc: 0.5039\n",
      "Epoch 00001: val_loss improved from inf to 1.27103, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/001-1.2710.hdf5\n",
      "36805/36805 [==============================] - 452s 12ms/sample - loss: 1.6029 - acc: 0.5039 - val_loss: 1.2710 - val_acc: 0.6184\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0298 - acc: 0.6922\n",
      "Epoch 00002: val_loss improved from 1.27103 to 0.98342, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/002-0.9834.hdf5\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 1.0300 - acc: 0.6922 - val_loss: 0.9834 - val_acc: 0.7091\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7539\n",
      "Epoch 00003: val_loss improved from 0.98342 to 0.91975, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/003-0.9197.hdf5\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.8387 - acc: 0.7539 - val_loss: 0.9197 - val_acc: 0.7207\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7908\n",
      "Epoch 00004: val_loss improved from 0.91975 to 0.71005, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/004-0.7100.hdf5\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.7174 - acc: 0.7907 - val_loss: 0.7100 - val_acc: 0.7962\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8235\n",
      "Epoch 00005: val_loss improved from 0.71005 to 0.62962, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/005-0.6296.hdf5\n",
      "36805/36805 [==============================] - 451s 12ms/sample - loss: 0.6150 - acc: 0.8234 - val_loss: 0.6296 - val_acc: 0.8255\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.8380\n",
      "Epoch 00006: val_loss improved from 0.62962 to 0.56930, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/006-0.5693.hdf5\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.5544 - acc: 0.8379 - val_loss: 0.5693 - val_acc: 0.8346\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8567\n",
      "Epoch 00007: val_loss did not improve from 0.56930\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.4959 - acc: 0.8567 - val_loss: 0.5953 - val_acc: 0.8325\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8742\n",
      "Epoch 00008: val_loss improved from 0.56930 to 0.55873, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/008-0.5587.hdf5\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.4346 - acc: 0.8742 - val_loss: 0.5587 - val_acc: 0.8453\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8870\n",
      "Epoch 00009: val_loss improved from 0.55873 to 0.54301, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/009-0.5430.hdf5\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.3911 - acc: 0.8870 - val_loss: 0.5430 - val_acc: 0.8488\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8969\n",
      "Epoch 00010: val_loss improved from 0.54301 to 0.47203, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/010-0.4720.hdf5\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.3559 - acc: 0.8969 - val_loss: 0.4720 - val_acc: 0.8656\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9050\n",
      "Epoch 00011: val_loss did not improve from 0.47203\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.3299 - acc: 0.9050 - val_loss: 0.5259 - val_acc: 0.8553\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9174\n",
      "Epoch 00012: val_loss did not improve from 0.47203\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.2847 - acc: 0.9174 - val_loss: 0.4864 - val_acc: 0.8644\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9198\n",
      "Epoch 00013: val_loss did not improve from 0.47203\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.2712 - acc: 0.9198 - val_loss: 0.5295 - val_acc: 0.8549\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9297\n",
      "Epoch 00014: val_loss did not improve from 0.47203\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.2438 - acc: 0.9296 - val_loss: 0.5297 - val_acc: 0.8516\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9343\n",
      "Epoch 00015: val_loss did not improve from 0.47203\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.2268 - acc: 0.9342 - val_loss: 0.6377 - val_acc: 0.8288\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9381\n",
      "Epoch 00016: val_loss improved from 0.47203 to 0.45062, saving model to model/checkpoint/1D_CNN_custom_2_BN_6_conv_checkpoint/016-0.4506.hdf5\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.2112 - acc: 0.9381 - val_loss: 0.4506 - val_acc: 0.8814\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9458\n",
      "Epoch 00017: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.1877 - acc: 0.9458 - val_loss: 0.4793 - val_acc: 0.8672\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9557\n",
      "Epoch 00018: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.1594 - acc: 0.9557 - val_loss: 0.6877 - val_acc: 0.8220\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9530\n",
      "Epoch 00019: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.1672 - acc: 0.9530 - val_loss: 0.4591 - val_acc: 0.8768\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9612\n",
      "Epoch 00020: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.1367 - acc: 0.9611 - val_loss: 0.4596 - val_acc: 0.8740\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9618\n",
      "Epoch 00021: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.1392 - acc: 0.9618 - val_loss: 0.4557 - val_acc: 0.8779\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9696\n",
      "Epoch 00022: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.1137 - acc: 0.9696 - val_loss: 0.4667 - val_acc: 0.8763\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9710\n",
      "Epoch 00023: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.1097 - acc: 0.9709 - val_loss: 0.4533 - val_acc: 0.8845\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9699\n",
      "Epoch 00024: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.1100 - acc: 0.9698 - val_loss: 0.5664 - val_acc: 0.8428\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9727\n",
      "Epoch 00025: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.0989 - acc: 0.9727 - val_loss: 0.4863 - val_acc: 0.8758\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9810\n",
      "Epoch 00026: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 451s 12ms/sample - loss: 0.0806 - acc: 0.9809 - val_loss: 0.5519 - val_acc: 0.8689\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9721\n",
      "Epoch 00027: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.1036 - acc: 0.9720 - val_loss: 0.6241 - val_acc: 0.8474\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9761\n",
      "Epoch 00028: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 443s 12ms/sample - loss: 0.0890 - acc: 0.9761 - val_loss: 0.4791 - val_acc: 0.8791\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9830\n",
      "Epoch 00029: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0695 - acc: 0.9830 - val_loss: 0.4749 - val_acc: 0.8814\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9768\n",
      "Epoch 00030: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.0862 - acc: 0.9768 - val_loss: 0.5156 - val_acc: 0.8712\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9861\n",
      "Epoch 00031: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0604 - acc: 0.9861 - val_loss: 0.5023 - val_acc: 0.8733\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9857\n",
      "Epoch 00032: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0618 - acc: 0.9856 - val_loss: 0.6001 - val_acc: 0.8698\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9825\n",
      "Epoch 00033: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.0692 - acc: 0.9824 - val_loss: 0.4932 - val_acc: 0.8768\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9854\n",
      "Epoch 00034: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0600 - acc: 0.9854 - val_loss: 0.5325 - val_acc: 0.8705\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9887\n",
      "Epoch 00035: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0492 - acc: 0.9886 - val_loss: 0.5034 - val_acc: 0.8763\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9802\n",
      "Epoch 00036: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0741 - acc: 0.9801 - val_loss: 0.5874 - val_acc: 0.8621\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9841\n",
      "Epoch 00037: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0633 - acc: 0.9841 - val_loss: 0.5375 - val_acc: 0.8763\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9826\n",
      "Epoch 00038: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0656 - acc: 0.9825 - val_loss: 0.5015 - val_acc: 0.8840\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9865\n",
      "Epoch 00039: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0540 - acc: 0.9865 - val_loss: 0.6669 - val_acc: 0.8362\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9874\n",
      "Epoch 00040: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0521 - acc: 0.9874 - val_loss: 0.5893 - val_acc: 0.8765\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9846\n",
      "Epoch 00041: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0586 - acc: 0.9845 - val_loss: 0.5498 - val_acc: 0.8744\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9867\n",
      "Epoch 00042: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0534 - acc: 0.9867 - val_loss: 0.5702 - val_acc: 0.8737\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9907\n",
      "Epoch 00043: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.0426 - acc: 0.9907 - val_loss: 0.4743 - val_acc: 0.8859\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0549 - acc: 0.9864 - val_loss: 0.5433 - val_acc: 0.8835\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9874\n",
      "Epoch 00045: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 451s 12ms/sample - loss: 0.0499 - acc: 0.9874 - val_loss: 0.5649 - val_acc: 0.8747\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9884\n",
      "Epoch 00046: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0473 - acc: 0.9883 - val_loss: 0.6112 - val_acc: 0.8668\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9847\n",
      "Epoch 00047: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0597 - acc: 0.9846 - val_loss: 0.4744 - val_acc: 0.8931\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9897\n",
      "Epoch 00048: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0445 - acc: 0.9896 - val_loss: 0.4745 - val_acc: 0.8915\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9883\n",
      "Epoch 00049: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0454 - acc: 0.9883 - val_loss: 0.4791 - val_acc: 0.8935\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9919\n",
      "Epoch 00050: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.0352 - acc: 0.9918 - val_loss: 0.5072 - val_acc: 0.8896\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9861\n",
      "Epoch 00051: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.0501 - acc: 0.9861 - val_loss: 0.4809 - val_acc: 0.8912\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9944\n",
      "Epoch 00052: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0269 - acc: 0.9944 - val_loss: 0.5002 - val_acc: 0.8835\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9943\n",
      "Epoch 00053: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.0294 - acc: 0.9942 - val_loss: 0.5511 - val_acc: 0.8726\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9897\n",
      "Epoch 00054: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0437 - acc: 0.9896 - val_loss: 0.6233 - val_acc: 0.8742\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9840\n",
      "Epoch 00055: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.0607 - acc: 0.9840 - val_loss: 0.5753 - val_acc: 0.8777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9906\n",
      "Epoch 00056: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0388 - acc: 0.9905 - val_loss: 0.5722 - val_acc: 0.8833\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9901\n",
      "Epoch 00057: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 451s 12ms/sample - loss: 0.0422 - acc: 0.9901 - val_loss: 0.5143 - val_acc: 0.8875\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9943\n",
      "Epoch 00058: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0284 - acc: 0.9943 - val_loss: 0.4777 - val_acc: 0.8968\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9976\n",
      "Epoch 00059: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0175 - acc: 0.9976 - val_loss: 0.5132 - val_acc: 0.8875\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9956\n",
      "Epoch 00060: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0217 - acc: 0.9956 - val_loss: 0.5375 - val_acc: 0.8845\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9924\n",
      "Epoch 00061: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0333 - acc: 0.9923 - val_loss: 0.6015 - val_acc: 0.8691\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9895\n",
      "Epoch 00062: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0422 - acc: 0.9895 - val_loss: 0.5481 - val_acc: 0.8803\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9935\n",
      "Epoch 00063: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0308 - acc: 0.9934 - val_loss: 0.5334 - val_acc: 0.8861\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9887\n",
      "Epoch 00064: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0433 - acc: 0.9886 - val_loss: 0.5342 - val_acc: 0.8854\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9893\n",
      "Epoch 00065: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 0.0423 - acc: 0.9893 - val_loss: 0.5338 - val_acc: 0.8908\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9917\n",
      "Epoch 00066: val_loss did not improve from 0.45062\n",
      "36805/36805 [==============================] - 449s 12ms/sample - loss: 0.0338 - acc: 0.9916 - val_loss: 0.5733 - val_acc: 0.8798\n",
      "\n",
      "1D_CNN_custom_2_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.5304 - acc: 0.8517\n",
      "Loss: 0.5303669465541344 Accuracy: 0.8517134\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6536 - acc: 0.4929\n",
      "Epoch 00001: val_loss improved from inf to 1.18205, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/001-1.1820.hdf5\n",
      "36805/36805 [==============================] - 450s 12ms/sample - loss: 1.6538 - acc: 0.4929 - val_loss: 1.1820 - val_acc: 0.6464\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9490 - acc: 0.7196\n",
      "Epoch 00002: val_loss improved from 1.18205 to 0.79584, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/002-0.7958.hdf5\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.9491 - acc: 0.7196 - val_loss: 0.7958 - val_acc: 0.7724\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7123 - acc: 0.7968\n",
      "Epoch 00003: val_loss improved from 0.79584 to 0.62716, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/003-0.6272.hdf5\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.7124 - acc: 0.7968 - val_loss: 0.6272 - val_acc: 0.8223\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5794 - acc: 0.8353\n",
      "Epoch 00004: val_loss improved from 0.62716 to 0.51599, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/004-0.5160.hdf5\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.5797 - acc: 0.8352 - val_loss: 0.5160 - val_acc: 0.8542\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8630\n",
      "Epoch 00005: val_loss improved from 0.51599 to 0.47104, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/005-0.4710.hdf5\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.4827 - acc: 0.8631 - val_loss: 0.4710 - val_acc: 0.8705\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8814\n",
      "Epoch 00006: val_loss improved from 0.47104 to 0.41061, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/006-0.4106.hdf5\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.4144 - acc: 0.8813 - val_loss: 0.4106 - val_acc: 0.8866\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8949\n",
      "Epoch 00007: val_loss improved from 0.41061 to 0.35940, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/007-0.3594.hdf5\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.3714 - acc: 0.8949 - val_loss: 0.3594 - val_acc: 0.8980\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.9074\n",
      "Epoch 00008: val_loss improved from 0.35940 to 0.32558, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/008-0.3256.hdf5\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.3286 - acc: 0.9074 - val_loss: 0.3256 - val_acc: 0.9075\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9129\n",
      "Epoch 00009: val_loss did not improve from 0.32558\n",
      "36805/36805 [==============================] - 432s 12ms/sample - loss: 0.3019 - acc: 0.9128 - val_loss: 0.3346 - val_acc: 0.9080\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9208\n",
      "Epoch 00010: val_loss did not improve from 0.32558\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.2787 - acc: 0.9208 - val_loss: 0.3298 - val_acc: 0.9085\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9317\n",
      "Epoch 00011: val_loss improved from 0.32558 to 0.28323, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/011-0.2832.hdf5\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.2433 - acc: 0.9317 - val_loss: 0.2832 - val_acc: 0.9208\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9382\n",
      "Epoch 00012: val_loss did not improve from 0.28323\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.2204 - acc: 0.9381 - val_loss: 0.3328 - val_acc: 0.9061\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9407\n",
      "Epoch 00013: val_loss did not improve from 0.28323\n",
      "36805/36805 [==============================] - 443s 12ms/sample - loss: 0.2058 - acc: 0.9406 - val_loss: 0.3106 - val_acc: 0.9159\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9443\n",
      "Epoch 00014: val_loss improved from 0.28323 to 0.25612, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/014-0.2561.hdf5\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.1935 - acc: 0.9442 - val_loss: 0.2561 - val_acc: 0.9222\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9489\n",
      "Epoch 00015: val_loss did not improve from 0.25612\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.1754 - acc: 0.9489 - val_loss: 0.2987 - val_acc: 0.9157\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9570\n",
      "Epoch 00016: val_loss improved from 0.25612 to 0.25463, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/016-0.2546.hdf5\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.1525 - acc: 0.9570 - val_loss: 0.2546 - val_acc: 0.9294\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9603\n",
      "Epoch 00017: val_loss improved from 0.25463 to 0.24015, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/017-0.2401.hdf5\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.1401 - acc: 0.9603 - val_loss: 0.2401 - val_acc: 0.9338\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9637\n",
      "Epoch 00018: val_loss did not improve from 0.24015\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.1297 - acc: 0.9637 - val_loss: 0.2739 - val_acc: 0.9234\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9631\n",
      "Epoch 00019: val_loss did not improve from 0.24015\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.1299 - acc: 0.9631 - val_loss: 0.2699 - val_acc: 0.9243\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9685\n",
      "Epoch 00020: val_loss did not improve from 0.24015\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.1116 - acc: 0.9684 - val_loss: 0.2430 - val_acc: 0.9334\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9715\n",
      "Epoch 00021: val_loss improved from 0.24015 to 0.23499, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/021-0.2350.hdf5\n",
      "36805/36805 [==============================] - 443s 12ms/sample - loss: 0.1035 - acc: 0.9715 - val_loss: 0.2350 - val_acc: 0.9345\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9763\n",
      "Epoch 00022: val_loss did not improve from 0.23499\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.0885 - acc: 0.9763 - val_loss: 0.2402 - val_acc: 0.9364\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9765\n",
      "Epoch 00023: val_loss improved from 0.23499 to 0.21745, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/023-0.2175.hdf5\n",
      "36805/36805 [==============================] - 443s 12ms/sample - loss: 0.0863 - acc: 0.9766 - val_loss: 0.2175 - val_acc: 0.9427\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9770\n",
      "Epoch 00024: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0843 - acc: 0.9770 - val_loss: 0.2828 - val_acc: 0.9227\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9814\n",
      "Epoch 00025: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.0715 - acc: 0.9814 - val_loss: 0.2907 - val_acc: 0.9248\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9778\n",
      "Epoch 00026: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0811 - acc: 0.9777 - val_loss: 0.2772 - val_acc: 0.9317\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9724\n",
      "Epoch 00027: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0939 - acc: 0.9723 - val_loss: 0.2325 - val_acc: 0.9369\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9825\n",
      "Epoch 00028: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 448s 12ms/sample - loss: 0.0662 - acc: 0.9824 - val_loss: 0.2299 - val_acc: 0.9369\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9829\n",
      "Epoch 00029: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 433s 12ms/sample - loss: 0.0652 - acc: 0.9829 - val_loss: 0.2469 - val_acc: 0.9383\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9891\n",
      "Epoch 00030: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0466 - acc: 0.9890 - val_loss: 0.2648 - val_acc: 0.9280\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9812\n",
      "Epoch 00031: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 431s 12ms/sample - loss: 0.0656 - acc: 0.9812 - val_loss: 0.2199 - val_acc: 0.9413\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9903\n",
      "Epoch 00032: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0417 - acc: 0.9903 - val_loss: 0.3036 - val_acc: 0.9227\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9836\n",
      "Epoch 00033: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0617 - acc: 0.9836 - val_loss: 0.2444 - val_acc: 0.9383\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9924\n",
      "Epoch 00034: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0354 - acc: 0.9924 - val_loss: 0.2579 - val_acc: 0.9341\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9905\n",
      "Epoch 00035: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 438s 12ms/sample - loss: 0.0381 - acc: 0.9905 - val_loss: 0.2649 - val_acc: 0.9311\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9814\n",
      "Epoch 00036: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0664 - acc: 0.9814 - val_loss: 0.2624 - val_acc: 0.9329\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9878\n",
      "Epoch 00037: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0448 - acc: 0.9878 - val_loss: 0.2546 - val_acc: 0.9369\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9844\n",
      "Epoch 00038: val_loss improved from 0.21745 to 0.21276, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/038-0.2128.hdf5\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0593 - acc: 0.9844 - val_loss: 0.2128 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9960\n",
      "Epoch 00039: val_loss did not improve from 0.21276\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0241 - acc: 0.9960 - val_loss: 0.2430 - val_acc: 0.9376\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9892\n",
      "Epoch 00040: val_loss did not improve from 0.21276\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0424 - acc: 0.9892 - val_loss: 0.2174 - val_acc: 0.9429\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9941\n",
      "Epoch 00041: val_loss did not improve from 0.21276\n",
      "36805/36805 [==============================] - 437s 12ms/sample - loss: 0.0265 - acc: 0.9940 - val_loss: 0.2587 - val_acc: 0.9392\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9864\n",
      "Epoch 00042: val_loss improved from 0.21276 to 0.20548, saving model to model/checkpoint/1D_CNN_custom_2_BN_7_conv_checkpoint/042-0.2055.hdf5\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.0507 - acc: 0.9863 - val_loss: 0.2055 - val_acc: 0.9485\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9925\n",
      "Epoch 00043: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 437s 12ms/sample - loss: 0.0317 - acc: 0.9925 - val_loss: 0.2773 - val_acc: 0.9320\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9909\n",
      "Epoch 00044: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.0349 - acc: 0.9908 - val_loss: 0.2180 - val_acc: 0.9462\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9929\n",
      "Epoch 00045: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.0308 - acc: 0.9929 - val_loss: 0.2509 - val_acc: 0.9371\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9916\n",
      "Epoch 00046: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0346 - acc: 0.9915 - val_loss: 0.2710 - val_acc: 0.9341\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9916\n",
      "Epoch 00047: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0334 - acc: 0.9916 - val_loss: 0.2199 - val_acc: 0.9478\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9925\n",
      "Epoch 00048: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0313 - acc: 0.9924 - val_loss: 0.2506 - val_acc: 0.9376\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9900\n",
      "Epoch 00049: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 438s 12ms/sample - loss: 0.0375 - acc: 0.9899 - val_loss: 0.2546 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9947\n",
      "Epoch 00050: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 444s 12ms/sample - loss: 0.0237 - acc: 0.9947 - val_loss: 0.2693 - val_acc: 0.9327\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9929\n",
      "Epoch 00051: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.0304 - acc: 0.9928 - val_loss: 0.2202 - val_acc: 0.9429\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9894\n",
      "Epoch 00052: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0384 - acc: 0.9894 - val_loss: 0.2304 - val_acc: 0.9432\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9935\n",
      "Epoch 00053: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 436s 12ms/sample - loss: 0.0272 - acc: 0.9935 - val_loss: 0.2257 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9978\n",
      "Epoch 00054: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0142 - acc: 0.9977 - val_loss: 0.3025 - val_acc: 0.9271\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9894\n",
      "Epoch 00055: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0383 - acc: 0.9893 - val_loss: 0.2652 - val_acc: 0.9366\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9932\n",
      "Epoch 00056: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0269 - acc: 0.9931 - val_loss: 0.2438 - val_acc: 0.9429\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9934\n",
      "Epoch 00057: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0279 - acc: 0.9934 - val_loss: 0.2436 - val_acc: 0.9446\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9930\n",
      "Epoch 00058: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 446s 12ms/sample - loss: 0.0271 - acc: 0.9930 - val_loss: 0.2368 - val_acc: 0.9436\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 0.0170 - acc: 0.9963 - val_loss: 0.2280 - val_acc: 0.9441\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9928\n",
      "Epoch 00060: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 437s 12ms/sample - loss: 0.0275 - acc: 0.9928 - val_loss: 0.2602 - val_acc: 0.9439\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9944\n",
      "Epoch 00061: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0247 - acc: 0.9943 - val_loss: 0.2560 - val_acc: 0.9404\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9941\n",
      "Epoch 00062: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 447s 12ms/sample - loss: 0.0242 - acc: 0.9940 - val_loss: 0.2948 - val_acc: 0.9299\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9929\n",
      "Epoch 00063: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.0261 - acc: 0.9929 - val_loss: 0.2546 - val_acc: 0.9399\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9954\n",
      "Epoch 00064: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 445s 12ms/sample - loss: 0.0198 - acc: 0.9953 - val_loss: 0.2409 - val_acc: 0.9425\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9928\n",
      "Epoch 00065: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0269 - acc: 0.9927 - val_loss: 0.2662 - val_acc: 0.9394\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9915\n",
      "Epoch 00066: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0308 - acc: 0.9914 - val_loss: 0.2420 - val_acc: 0.9422\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9917\n",
      "Epoch 00067: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 440s 12ms/sample - loss: 0.0308 - acc: 0.9917 - val_loss: 0.2160 - val_acc: 0.9515\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9980\n",
      "Epoch 00068: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 443s 12ms/sample - loss: 0.0120 - acc: 0.9980 - val_loss: 0.2245 - val_acc: 0.9497\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9948\n",
      "Epoch 00069: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 442s 12ms/sample - loss: 0.0215 - acc: 0.9948 - val_loss: 0.2200 - val_acc: 0.9474\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9984\n",
      "Epoch 00070: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 439s 12ms/sample - loss: 0.0096 - acc: 0.9984 - val_loss: 0.2673 - val_acc: 0.9383\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9929\n",
      "Epoch 00071: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 409s 11ms/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 0.2617 - val_acc: 0.9392\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9934\n",
      "Epoch 00072: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0246 - acc: 0.9934 - val_loss: 0.2300 - val_acc: 0.9453\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9973\n",
      "Epoch 00073: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0129 - acc: 0.9972 - val_loss: 0.2428 - val_acc: 0.9436\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9929\n",
      "Epoch 00074: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.2212 - val_acc: 0.9464\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 00075: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0220 - acc: 0.9940 - val_loss: 0.2496 - val_acc: 0.9427\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 00076: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0136 - acc: 0.9968 - val_loss: 0.2329 - val_acc: 0.9492\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9990\n",
      "Epoch 00077: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 377s 10ms/sample - loss: 0.0072 - acc: 0.9989 - val_loss: 0.3167 - val_acc: 0.9336\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9922\n",
      "Epoch 00078: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0281 - acc: 0.9922 - val_loss: 0.2540 - val_acc: 0.9420\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9928\n",
      "Epoch 00079: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0261 - acc: 0.9928 - val_loss: 0.2621 - val_acc: 0.9331\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9949\n",
      "Epoch 00080: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0201 - acc: 0.9949 - val_loss: 0.2186 - val_acc: 0.9471\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9982\n",
      "Epoch 00081: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 377s 10ms/sample - loss: 0.0095 - acc: 0.9982 - val_loss: 0.2189 - val_acc: 0.9525\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9972\n",
      "Epoch 00082: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0114 - acc: 0.9971 - val_loss: 0.2481 - val_acc: 0.9467\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9926\n",
      "Epoch 00083: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0243 - acc: 0.9926 - val_loss: 0.2518 - val_acc: 0.9441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9941\n",
      "Epoch 00084: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0233 - acc: 0.9940 - val_loss: 0.2286 - val_acc: 0.9483\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9959\n",
      "Epoch 00085: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0168 - acc: 0.9959 - val_loss: 0.2380 - val_acc: 0.9471\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9953\n",
      "Epoch 00086: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0181 - acc: 0.9952 - val_loss: 0.2729 - val_acc: 0.9378\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9945\n",
      "Epoch 00087: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0217 - acc: 0.9944 - val_loss: 0.2544 - val_acc: 0.9441\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9934\n",
      "Epoch 00088: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0225 - acc: 0.9933 - val_loss: 0.2272 - val_acc: 0.9455\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9934\n",
      "Epoch 00089: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0230 - acc: 0.9934 - val_loss: 0.2250 - val_acc: 0.9497\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9960\n",
      "Epoch 00090: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0165 - acc: 0.9960 - val_loss: 0.2436 - val_acc: 0.9443\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9984\n",
      "Epoch 00091: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0096 - acc: 0.9983 - val_loss: 0.2522 - val_acc: 0.9390\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9934\n",
      "Epoch 00092: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0240 - acc: 0.9934 - val_loss: 0.2632 - val_acc: 0.9411\n",
      "\n",
      "1D_CNN_custom_2_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.2592 - acc: 0.9315\n",
      "Loss: 0.2591989379496094 Accuracy: 0.9314642\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4926 - acc: 0.5491\n",
      "Epoch 00001: val_loss improved from inf to 0.83688, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/001-0.8369.hdf5\n",
      "36805/36805 [==============================] - 391s 11ms/sample - loss: 1.4925 - acc: 0.5491 - val_loss: 0.8369 - val_acc: 0.7547\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7257 - acc: 0.7922\n",
      "Epoch 00002: val_loss improved from 0.83688 to 0.53370, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/002-0.5337.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.7258 - acc: 0.7921 - val_loss: 0.5337 - val_acc: 0.8423\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8560\n",
      "Epoch 00003: val_loss improved from 0.53370 to 0.50607, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/003-0.5061.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.4993 - acc: 0.8559 - val_loss: 0.5061 - val_acc: 0.8616\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8880\n",
      "Epoch 00004: val_loss improved from 0.50607 to 0.39745, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/004-0.3975.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.3977 - acc: 0.8880 - val_loss: 0.3975 - val_acc: 0.8812\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.9086\n",
      "Epoch 00005: val_loss improved from 0.39745 to 0.26828, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/005-0.2683.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.3168 - acc: 0.9086 - val_loss: 0.2683 - val_acc: 0.9224\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9216\n",
      "Epoch 00006: val_loss improved from 0.26828 to 0.26369, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/006-0.2637.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.2739 - acc: 0.9215 - val_loss: 0.2637 - val_acc: 0.9257\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9262\n",
      "Epoch 00007: val_loss improved from 0.26369 to 0.23531, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/007-0.2353.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.2550 - acc: 0.9262 - val_loss: 0.2353 - val_acc: 0.9290\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9426\n",
      "Epoch 00008: val_loss did not improve from 0.23531\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.2036 - acc: 0.9425 - val_loss: 0.2460 - val_acc: 0.9262\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9388\n",
      "Epoch 00009: val_loss improved from 0.23531 to 0.18612, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/009-0.1861.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.2073 - acc: 0.9388 - val_loss: 0.1861 - val_acc: 0.9425\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9516\n",
      "Epoch 00010: val_loss did not improve from 0.18612\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1671 - acc: 0.9515 - val_loss: 0.2270 - val_acc: 0.9315\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9549\n",
      "Epoch 00011: val_loss improved from 0.18612 to 0.17273, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/011-0.1727.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1601 - acc: 0.9549 - val_loss: 0.1727 - val_acc: 0.9511\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9615\n",
      "Epoch 00012: val_loss did not improve from 0.17273\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1381 - acc: 0.9614 - val_loss: 0.1781 - val_acc: 0.9478\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9588\n",
      "Epoch 00013: val_loss improved from 0.17273 to 0.16513, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/013-0.1651.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1402 - acc: 0.9587 - val_loss: 0.1651 - val_acc: 0.9522\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9643\n",
      "Epoch 00014: val_loss improved from 0.16513 to 0.14186, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/014-0.1419.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1257 - acc: 0.9643 - val_loss: 0.1419 - val_acc: 0.9581\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9684\n",
      "Epoch 00015: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1100 - acc: 0.9684 - val_loss: 0.1496 - val_acc: 0.9529\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9708\n",
      "Epoch 00016: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1016 - acc: 0.9708 - val_loss: 0.1567 - val_acc: 0.9546\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9743\n",
      "Epoch 00017: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0907 - acc: 0.9743 - val_loss: 0.1767 - val_acc: 0.9462\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9780\n",
      "Epoch 00018: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0800 - acc: 0.9779 - val_loss: 0.1542 - val_acc: 0.9518\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9768\n",
      "Epoch 00019: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0833 - acc: 0.9768 - val_loss: 0.1631 - val_acc: 0.9534\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9821\n",
      "Epoch 00020: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0660 - acc: 0.9821 - val_loss: 0.1572 - val_acc: 0.9560\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9824\n",
      "Epoch 00021: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0631 - acc: 0.9824 - val_loss: 0.1702 - val_acc: 0.9499\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9818\n",
      "Epoch 00022: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0643 - acc: 0.9818 - val_loss: 0.1497 - val_acc: 0.9571\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9827\n",
      "Epoch 00023: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0620 - acc: 0.9826 - val_loss: 0.1647 - val_acc: 0.9525\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9812\n",
      "Epoch 00024: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0651 - acc: 0.9812 - val_loss: 0.1605 - val_acc: 0.9543\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9877\n",
      "Epoch 00025: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0461 - acc: 0.9877 - val_loss: 0.1650 - val_acc: 0.9527\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9834\n",
      "Epoch 00026: val_loss did not improve from 0.14186\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0600 - acc: 0.9833 - val_loss: 0.1755 - val_acc: 0.9471\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9856\n",
      "Epoch 00027: val_loss improved from 0.14186 to 0.14149, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/027-0.1415.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0528 - acc: 0.9856 - val_loss: 0.1415 - val_acc: 0.9583\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9879\n",
      "Epoch 00028: val_loss did not improve from 0.14149\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0456 - acc: 0.9878 - val_loss: 0.1466 - val_acc: 0.9571\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9863\n",
      "Epoch 00029: val_loss did not improve from 0.14149\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0514 - acc: 0.9863 - val_loss: 0.1454 - val_acc: 0.9604\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9906\n",
      "Epoch 00030: val_loss improved from 0.14149 to 0.13232, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/030-0.1323.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0374 - acc: 0.9906 - val_loss: 0.1323 - val_acc: 0.9634\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9907\n",
      "Epoch 00031: val_loss did not improve from 0.13232\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0358 - acc: 0.9906 - val_loss: 0.1459 - val_acc: 0.9592\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9896\n",
      "Epoch 00032: val_loss did not improve from 0.13232\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0400 - acc: 0.9896 - val_loss: 0.1451 - val_acc: 0.9592\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9924\n",
      "Epoch 00033: val_loss did not improve from 0.13232\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0314 - acc: 0.9923 - val_loss: 0.1645 - val_acc: 0.9541\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9851\n",
      "Epoch 00034: val_loss improved from 0.13232 to 0.12510, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/034-0.1251.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0518 - acc: 0.9851 - val_loss: 0.1251 - val_acc: 0.9623\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9955\n",
      "Epoch 00035: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0228 - acc: 0.9955 - val_loss: 0.1440 - val_acc: 0.9604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9960\n",
      "Epoch 00036: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0193 - acc: 0.9959 - val_loss: 0.1323 - val_acc: 0.9632\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9948\n",
      "Epoch 00037: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0237 - acc: 0.9947 - val_loss: 0.1635 - val_acc: 0.9567\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9907\n",
      "Epoch 00038: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0358 - acc: 0.9907 - val_loss: 0.1376 - val_acc: 0.9651\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9906\n",
      "Epoch 00039: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0357 - acc: 0.9905 - val_loss: 0.1590 - val_acc: 0.9588\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9933\n",
      "Epoch 00040: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0267 - acc: 0.9932 - val_loss: 0.1458 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9893\n",
      "Epoch 00041: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0382 - acc: 0.9892 - val_loss: 0.1525 - val_acc: 0.9574\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9896\n",
      "Epoch 00042: val_loss did not improve from 0.12510\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0350 - acc: 0.9896 - val_loss: 0.1272 - val_acc: 0.9655\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9966\n",
      "Epoch 00043: val_loss improved from 0.12510 to 0.12470, saving model to model/checkpoint/1D_CNN_custom_2_BN_8_conv_checkpoint/043-0.1247.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0170 - acc: 0.9965 - val_loss: 0.1247 - val_acc: 0.9646\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9957\n",
      "Epoch 00044: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0189 - acc: 0.9957 - val_loss: 0.1481 - val_acc: 0.9630\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9923\n",
      "Epoch 00045: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0285 - acc: 0.9923 - val_loss: 0.1649 - val_acc: 0.9578\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9918\n",
      "Epoch 00046: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0300 - acc: 0.9918 - val_loss: 0.1356 - val_acc: 0.9651\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9971\n",
      "Epoch 00047: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0151 - acc: 0.9971 - val_loss: 0.1263 - val_acc: 0.9665\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9977\n",
      "Epoch 00048: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0116 - acc: 0.9976 - val_loss: 0.1648 - val_acc: 0.9574\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9917\n",
      "Epoch 00049: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0299 - acc: 0.9916 - val_loss: 0.1778 - val_acc: 0.9569\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9923\n",
      "Epoch 00050: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0281 - acc: 0.9923 - val_loss: 0.1498 - val_acc: 0.9574\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9967\n",
      "Epoch 00051: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0162 - acc: 0.9967 - val_loss: 0.1373 - val_acc: 0.9632\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9980\n",
      "Epoch 00052: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0109 - acc: 0.9980 - val_loss: 0.1427 - val_acc: 0.9630\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9943\n",
      "Epoch 00053: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0225 - acc: 0.9942 - val_loss: 0.1373 - val_acc: 0.9648\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9962\n",
      "Epoch 00054: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0162 - acc: 0.9962 - val_loss: 0.1428 - val_acc: 0.9660\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9961\n",
      "Epoch 00055: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0171 - acc: 0.9960 - val_loss: 0.1724 - val_acc: 0.9536\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9943\n",
      "Epoch 00056: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0219 - acc: 0.9942 - val_loss: 0.1368 - val_acc: 0.9632\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0195 - acc: 0.9950 - val_loss: 0.1419 - val_acc: 0.9639\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9959\n",
      "Epoch 00058: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0166 - acc: 0.9959 - val_loss: 0.1426 - val_acc: 0.9634\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9940\n",
      "Epoch 00059: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0215 - acc: 0.9940 - val_loss: 0.1410 - val_acc: 0.9639\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 00060: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.1542 - val_acc: 0.9634\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 00061: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0121 - acc: 0.9974 - val_loss: 0.1594 - val_acc: 0.9609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9980\n",
      "Epoch 00062: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0096 - acc: 0.9980 - val_loss: 0.1635 - val_acc: 0.9599\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9936\n",
      "Epoch 00063: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0223 - acc: 0.9936 - val_loss: 0.1504 - val_acc: 0.9634\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0131 - acc: 0.9969 - val_loss: 0.1687 - val_acc: 0.9578\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9946\n",
      "Epoch 00065: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0200 - acc: 0.9946 - val_loss: 0.1415 - val_acc: 0.9641\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9986\n",
      "Epoch 00066: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.1579 - val_acc: 0.9613\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9954\n",
      "Epoch 00067: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0172 - acc: 0.9953 - val_loss: 0.1399 - val_acc: 0.9618\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9951\n",
      "Epoch 00068: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0192 - acc: 0.9951 - val_loss: 0.1554 - val_acc: 0.9669\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9984\n",
      "Epoch 00069: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0076 - acc: 0.9984 - val_loss: 0.1717 - val_acc: 0.9588\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9984\n",
      "Epoch 00070: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0074 - acc: 0.9984 - val_loss: 0.1608 - val_acc: 0.9623\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9989\n",
      "Epoch 00071: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1824 - val_acc: 0.9585\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0256 - acc: 0.9929 - val_loss: 0.1492 - val_acc: 0.9639\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9956\n",
      "Epoch 00073: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0173 - acc: 0.9956 - val_loss: 0.1375 - val_acc: 0.9641\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9986\n",
      "Epoch 00074: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.1385 - val_acc: 0.9658\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9976\n",
      "Epoch 00075: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0100 - acc: 0.9976 - val_loss: 0.1359 - val_acc: 0.9672\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0101 - acc: 0.9977 - val_loss: 0.1805 - val_acc: 0.9564\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9950\n",
      "Epoch 00077: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0177 - acc: 0.9949 - val_loss: 0.1510 - val_acc: 0.9625\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 00078: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.1513 - val_acc: 0.9641\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9950\n",
      "Epoch 00079: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0182 - acc: 0.9949 - val_loss: 0.1404 - val_acc: 0.9658\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9964\n",
      "Epoch 00080: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0132 - acc: 0.9964 - val_loss: 0.1749 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9952\n",
      "Epoch 00081: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0167 - acc: 0.9952 - val_loss: 0.1428 - val_acc: 0.9653\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9986\n",
      "Epoch 00082: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.1305 - val_acc: 0.9695\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9962\n",
      "Epoch 00083: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0149 - acc: 0.9962 - val_loss: 0.1372 - val_acc: 0.9679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9981\n",
      "Epoch 00084: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0081 - acc: 0.9981 - val_loss: 0.1425 - val_acc: 0.9665\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9992\n",
      "Epoch 00085: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0047 - acc: 0.9992 - val_loss: 0.1350 - val_acc: 0.9686\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9963\n",
      "Epoch 00086: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0136 - acc: 0.9963 - val_loss: 0.1556 - val_acc: 0.9637\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9980\n",
      "Epoch 00087: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0089 - acc: 0.9980 - val_loss: 0.1534 - val_acc: 0.9618\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00088: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.1323 - val_acc: 0.9667\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9986\n",
      "Epoch 00089: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.1510 - val_acc: 0.9632\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968\n",
      "Epoch 00090: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.1355 - val_acc: 0.9655\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9986\n",
      "Epoch 00091: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0066 - acc: 0.9986 - val_loss: 0.1477 - val_acc: 0.9644\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9944\n",
      "Epoch 00092: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0195 - acc: 0.9944 - val_loss: 0.1384 - val_acc: 0.9676\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 00093: val_loss did not improve from 0.12470\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0084 - acc: 0.9979 - val_loss: 0.1565 - val_acc: 0.9672\n",
      "\n",
      "1D_CNN_custom_2_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1773 - acc: 0.9502\n",
      "Loss: 0.17725000670286106 Accuracy: 0.95015574\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2466 - acc: 0.6240\n",
      "Epoch 00001: val_loss improved from inf to 0.61539, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/001-0.6154.hdf5\n",
      "36805/36805 [==============================] - 402s 11ms/sample - loss: 1.2466 - acc: 0.6240 - val_loss: 0.6154 - val_acc: 0.8332\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8423\n",
      "Epoch 00002: val_loss improved from 0.61539 to 0.39369, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/002-0.3937.hdf5\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.5513 - acc: 0.8423 - val_loss: 0.3937 - val_acc: 0.8915\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8916\n",
      "Epoch 00003: val_loss did not improve from 0.39369\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.3838 - acc: 0.8916 - val_loss: 0.6910 - val_acc: 0.7850\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.9054\n",
      "Epoch 00004: val_loss improved from 0.39369 to 0.24163, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/004-0.2416.hdf5\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.3305 - acc: 0.9054 - val_loss: 0.2416 - val_acc: 0.9348\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9288\n",
      "Epoch 00005: val_loss did not improve from 0.24163\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.2555 - acc: 0.9288 - val_loss: 0.2678 - val_acc: 0.9208\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9296\n",
      "Epoch 00006: val_loss improved from 0.24163 to 0.24136, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/006-0.2414.hdf5\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.2485 - acc: 0.9296 - val_loss: 0.2414 - val_acc: 0.9285\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9451\n",
      "Epoch 00007: val_loss improved from 0.24136 to 0.19257, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/007-0.1926.hdf5\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.1955 - acc: 0.9451 - val_loss: 0.1926 - val_acc: 0.9469\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9521\n",
      "Epoch 00008: val_loss did not improve from 0.19257\n",
      "36805/36805 [==============================] - 390s 11ms/sample - loss: 0.1681 - acc: 0.9521 - val_loss: 0.3091 - val_acc: 0.9052\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9540\n",
      "Epoch 00009: val_loss did not improve from 0.19257\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1626 - acc: 0.9539 - val_loss: 0.2841 - val_acc: 0.9182\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9533\n",
      "Epoch 00010: val_loss improved from 0.19257 to 0.15019, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/010-0.1502.hdf5\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.1608 - acc: 0.9533 - val_loss: 0.1502 - val_acc: 0.9564\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9658\n",
      "Epoch 00011: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.1229 - acc: 0.9658 - val_loss: 0.1658 - val_acc: 0.9502\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9699\n",
      "Epoch 00012: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.1072 - acc: 0.9698 - val_loss: 0.1658 - val_acc: 0.9520\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9718\n",
      "Epoch 00013: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 401s 11ms/sample - loss: 0.1019 - acc: 0.9718 - val_loss: 0.1778 - val_acc: 0.9481\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9699\n",
      "Epoch 00014: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1046 - acc: 0.9699 - val_loss: 0.1763 - val_acc: 0.9453\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9793\n",
      "Epoch 00015: val_loss improved from 0.15019 to 0.14333, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/015-0.1433.hdf5\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0781 - acc: 0.9793 - val_loss: 0.1433 - val_acc: 0.9553\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9786\n",
      "Epoch 00016: val_loss did not improve from 0.14333\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0763 - acc: 0.9785 - val_loss: 0.1627 - val_acc: 0.9485\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9733\n",
      "Epoch 00017: val_loss did not improve from 0.14333\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.0919 - acc: 0.9732 - val_loss: 0.1856 - val_acc: 0.9448\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9790\n",
      "Epoch 00018: val_loss improved from 0.14333 to 0.13099, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/018-0.1310.hdf5\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0758 - acc: 0.9790 - val_loss: 0.1310 - val_acc: 0.9576\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9859\n",
      "Epoch 00019: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.0529 - acc: 0.9858 - val_loss: 0.1427 - val_acc: 0.9564\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9805\n",
      "Epoch 00020: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0687 - acc: 0.9804 - val_loss: 0.1541 - val_acc: 0.9525\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9849\n",
      "Epoch 00021: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0563 - acc: 0.9849 - val_loss: 0.1330 - val_acc: 0.9583\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9901\n",
      "Epoch 00022: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0398 - acc: 0.9901 - val_loss: 0.1741 - val_acc: 0.9495\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9896\n",
      "Epoch 00023: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0414 - acc: 0.9896 - val_loss: 0.2009 - val_acc: 0.9406\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9810\n",
      "Epoch 00024: val_loss did not improve from 0.13099\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0632 - acc: 0.9810 - val_loss: 0.1429 - val_acc: 0.9571\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9790\n",
      "Epoch 00025: val_loss improved from 0.13099 to 0.12611, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/025-0.1261.hdf5\n",
      "36805/36805 [==============================] - 386s 11ms/sample - loss: 0.0689 - acc: 0.9790 - val_loss: 0.1261 - val_acc: 0.9625\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9904\n",
      "Epoch 00026: val_loss improved from 0.12611 to 0.11844, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/026-0.1184.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0373 - acc: 0.9904 - val_loss: 0.1184 - val_acc: 0.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9933\n",
      "Epoch 00027: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0302 - acc: 0.9932 - val_loss: 0.1326 - val_acc: 0.9616\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9914\n",
      "Epoch 00028: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0333 - acc: 0.9914 - val_loss: 0.1394 - val_acc: 0.9611\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9916\n",
      "Epoch 00029: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0328 - acc: 0.9916 - val_loss: 0.1242 - val_acc: 0.9644\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9949\n",
      "Epoch 00030: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0226 - acc: 0.9949 - val_loss: 0.1202 - val_acc: 0.9646\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9945\n",
      "Epoch 00031: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 398s 11ms/sample - loss: 0.0221 - acc: 0.9945 - val_loss: 0.1432 - val_acc: 0.9602\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9935\n",
      "Epoch 00032: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0255 - acc: 0.9935 - val_loss: 0.1577 - val_acc: 0.9564\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9895\n",
      "Epoch 00033: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0356 - acc: 0.9895 - val_loss: 0.1459 - val_acc: 0.9583\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9960\n",
      "Epoch 00034: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0182 - acc: 0.9960 - val_loss: 0.1545 - val_acc: 0.9599\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9904\n",
      "Epoch 00035: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 398s 11ms/sample - loss: 0.0336 - acc: 0.9903 - val_loss: 0.1363 - val_acc: 0.9623\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9894\n",
      "Epoch 00036: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0371 - acc: 0.9893 - val_loss: 0.1261 - val_acc: 0.9609\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9929\n",
      "Epoch 00037: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0268 - acc: 0.9929 - val_loss: 0.1275 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9957\n",
      "Epoch 00038: val_loss improved from 0.11844 to 0.11303, saving model to model/checkpoint/1D_CNN_custom_2_BN_9_conv_checkpoint/038-0.1130.hdf5\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 0.1130 - val_acc: 0.9646\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 00039: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0100 - acc: 0.9983 - val_loss: 0.1300 - val_acc: 0.9616\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9973\n",
      "Epoch 00040: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0134 - acc: 0.9973 - val_loss: 0.1649 - val_acc: 0.9550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9944\n",
      "Epoch 00041: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0219 - acc: 0.9944 - val_loss: 0.1349 - val_acc: 0.9627\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9961\n",
      "Epoch 00042: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.1457 - val_acc: 0.9590\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9971\n",
      "Epoch 00043: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0124 - acc: 0.9971 - val_loss: 0.1490 - val_acc: 0.9611\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9938\n",
      "Epoch 00044: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 393s 11ms/sample - loss: 0.0222 - acc: 0.9938 - val_loss: 0.1276 - val_acc: 0.9653\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9942\n",
      "Epoch 00045: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 397s 11ms/sample - loss: 0.0211 - acc: 0.9942 - val_loss: 0.1365 - val_acc: 0.9637\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9963\n",
      "Epoch 00046: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0139 - acc: 0.9963 - val_loss: 0.1400 - val_acc: 0.9632\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9913\n",
      "Epoch 00047: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0296 - acc: 0.9913 - val_loss: 0.1559 - val_acc: 0.9560\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9944\n",
      "Epoch 00048: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0207 - acc: 0.9944 - val_loss: 0.1433 - val_acc: 0.9623\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0171 - acc: 0.9955 - val_loss: 0.1433 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9987\n",
      "Epoch 00050: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 391s 11ms/sample - loss: 0.0073 - acc: 0.9988 - val_loss: 0.1450 - val_acc: 0.9611\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00051: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0082 - acc: 0.9983 - val_loss: 0.1329 - val_acc: 0.9618\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9951\n",
      "Epoch 00052: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.1402 - val_acc: 0.9597\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9967\n",
      "Epoch 00053: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0131 - acc: 0.9966 - val_loss: 0.1532 - val_acc: 0.9602\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9930\n",
      "Epoch 00054: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 0.1308 - val_acc: 0.9665\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
      "Epoch 00055: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 394s 11ms/sample - loss: 0.0079 - acc: 0.9985 - val_loss: 0.1248 - val_acc: 0.9662\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9985\n",
      "Epoch 00056: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.1476 - val_acc: 0.9618\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0197 - acc: 0.9938 - val_loss: 0.1373 - val_acc: 0.9646\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9983\n",
      "Epoch 00058: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1607 - val_acc: 0.9602\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 00059: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0273 - acc: 0.9915 - val_loss: 0.1464 - val_acc: 0.9620\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 00060: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0220 - acc: 0.9940 - val_loss: 0.1250 - val_acc: 0.9669\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9988\n",
      "Epoch 00061: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1447 - val_acc: 0.9632\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9944\n",
      "Epoch 00062: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0192 - acc: 0.9944 - val_loss: 0.1242 - val_acc: 0.9660\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00063: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0063 - acc: 0.9987 - val_loss: 0.1637 - val_acc: 0.9550\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9955\n",
      "Epoch 00064: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0166 - acc: 0.9955 - val_loss: 0.1405 - val_acc: 0.9646\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 00065: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0070 - acc: 0.9983 - val_loss: 0.1373 - val_acc: 0.9660\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9972\n",
      "Epoch 00066: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.1339 - val_acc: 0.9648\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9991\n",
      "Epoch 00067: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 0.1653 - val_acc: 0.9567\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9921\n",
      "Epoch 00068: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0274 - acc: 0.9921 - val_loss: 0.1225 - val_acc: 0.9651\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9983\n",
      "Epoch 00069: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 390s 11ms/sample - loss: 0.0082 - acc: 0.9983 - val_loss: 0.1212 - val_acc: 0.9653\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9974\n",
      "Epoch 00070: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0106 - acc: 0.9974 - val_loss: 0.1219 - val_acc: 0.9676\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0124 - acc: 0.9966 - val_loss: 0.1411 - val_acc: 0.9641\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9910\n",
      "Epoch 00072: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0290 - acc: 0.9910 - val_loss: 0.1431 - val_acc: 0.9623\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00073: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 0.1222 - val_acc: 0.9662\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9983\n",
      "Epoch 00074: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1238 - val_acc: 0.9683\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00075: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1397 - val_acc: 0.9658\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0051 - acc: 0.9990 - val_loss: 0.1453 - val_acc: 0.9655\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0142 - acc: 0.9963 - val_loss: 0.1449 - val_acc: 0.9616\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9964\n",
      "Epoch 00078: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0131 - acc: 0.9964 - val_loss: 0.1464 - val_acc: 0.9609\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9968\n",
      "Epoch 00079: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0123 - acc: 0.9968 - val_loss: 0.1279 - val_acc: 0.9665\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9990\n",
      "Epoch 00080: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1299 - val_acc: 0.9662\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9989\n",
      "Epoch 00081: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.1214 - val_acc: 0.9667\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9934\n",
      "Epoch 00082: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0222 - acc: 0.9934 - val_loss: 0.1328 - val_acc: 0.9669\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979\n",
      "Epoch 00083: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0082 - acc: 0.9979 - val_loss: 0.1350 - val_acc: 0.9658\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n",
      "Epoch 00084: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 398s 11ms/sample - loss: 0.0050 - acc: 0.9988 - val_loss: 0.1554 - val_acc: 0.9623\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 00085: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1365 - val_acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9975\n",
      "Epoch 00086: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.1584 - val_acc: 0.9606\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9952\n",
      "Epoch 00087: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 400s 11ms/sample - loss: 0.0159 - acc: 0.9952 - val_loss: 0.1458 - val_acc: 0.9646\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9935\n",
      "Epoch 00088: val_loss did not improve from 0.11303\n",
      "36805/36805 [==============================] - 399s 11ms/sample - loss: 0.0215 - acc: 0.9935 - val_loss: 0.1412 - val_acc: 0.9637\n",
      "\n",
      "1D_CNN_custom_2_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1639 - acc: 0.9572\n",
      "Loss: 0.16386745620368054 Accuracy: 0.95721704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1402896     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,402,896\n",
      "Trainable params: 1,383,696\n",
      "Non-trainable params: 19,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.5304 - acc: 0.8517\n",
      "Loss: 0.5303669465541344 Accuracy: 0.8517134\n",
      "\n",
      "1D_CNN_custom_2_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1372816     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,372,816\n",
      "Trainable params: 1,364,624\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.2592 - acc: 0.9315\n",
      "Loss: 0.2591989379496094 Accuracy: 0.9314642\n",
      "\n",
      "1D_CNN_custom_2_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1419536     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,419,536\n",
      "Trainable params: 1,414,672\n",
      "Non-trainable params: 4,864\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.1773 - acc: 0.9502\n",
      "Loss: 0.17725000670286106 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_2_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1445456     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,445,456\n",
      "Trainable params: 1,442,000\n",
      "Non-trainable params: 3,456\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1639 - acc: 0.9572\n",
      "Loss: 0.16386745620368054 Accuracy: 0.95721704\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1402896     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,402,896\n",
      "Trainable params: 1,383,696\n",
      "Non-trainable params: 19,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.7286 - acc: 0.8440\n",
      "Loss: 0.7285855315803615 Accuracy: 0.84402907\n",
      "\n",
      "1D_CNN_custom_2_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1372816     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,372,816\n",
      "Trainable params: 1,364,624\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.2847 - acc: 0.9325\n",
      "Loss: 0.28468960595867465 Accuracy: 0.93250257\n",
      "\n",
      "1D_CNN_custom_2_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1419536     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,419,536\n",
      "Trainable params: 1,414,672\n",
      "Non-trainable params: 4,864\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1971 - acc: 0.9506\n",
      "Loss: 0.19712575131240045 Accuracy: 0.9505711\n",
      "\n",
      "1D_CNN_custom_2_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1445456     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,445,456\n",
      "Trainable params: 1,442,000\n",
      "Non-trainable params: 3,456\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1800 - acc: 0.9580\n",
      "Loss: 0.17999305756393189 Accuracy: 0.95804775\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
