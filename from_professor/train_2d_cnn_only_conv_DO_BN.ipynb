{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
    "val_data = np.load(os.path.join(data_dir, 'validation_data.npz'))\n",
    "test_data = np.load(os.path.join(data_dir, 'test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 25443),\n",
       " (36805,),\n",
       " (4293, 25443),\n",
       " (4293,),\n",
       " (4815, 25443),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_data):\n",
    "    x_data = np.reshape(x_data, [x_data.shape[0], 99, 257, 1])\n",
    "    x_data = np.rot90(x_data, 1, (1, 2))\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_2d = preprocess(x_train)\n",
    "mean_vals = np.mean(x_train_2d, axis=0)\n",
    "std_val = np.std(x_train_2d)\n",
    "x_train_2d_norm = (x_train_2d-mean_vals) / std_val\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_2d = preprocess(x_val)\n",
    "x_val_2d_norm = (x_val_2d-mean_vals) / std_val\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_2d = preprocess(x_test)\n",
    "x_test_2d_norm = (x_test_2d-mean_vals) / std_val\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test_2d_norm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_only_conv_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D (kernel_size=5, filters=8, strides=(1,1), padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=(2,2), padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv2D (kernel_size=5, filters=8*(2**(i+1)), strides=(1,1), padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=2, strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                780304    \n",
      "=================================================================\n",
      "Total params: 780,544\n",
      "Trainable params: 780,528\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                349200    \n",
      "=================================================================\n",
      "Total params: 352,720\n",
      "Trainable params: 352,672\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 58, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 58, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                133648    \n",
      "=================================================================\n",
      "Total params: 150,128\n",
      "Trainable params: 150,016\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 58, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 58, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 25, 5, 64)         51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 25, 5, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 25, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                39952     \n",
      "=================================================================\n",
      "Total params: 107,952\n",
      "Trainable params: 107,712\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    model = build_2d_cnn_only_conv_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0446 - acc: 0.4305\n",
      "Epoch 00001: val_loss improved from inf to 1.47300, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/01-1.4730.hdf5\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 2.0439 - acc: 0.4307 - val_loss: 1.4730 - val_acc: 0.5865\n",
      "Epoch 2/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6387 - acc: 0.5329\n",
      "Epoch 00002: val_loss improved from 1.47300 to 1.44872, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/02-1.4487.hdf5\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 1.6381 - acc: 0.5331 - val_loss: 1.4487 - val_acc: 0.5879\n",
      "Epoch 3/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4856 - acc: 0.5735\n",
      "Epoch 00003: val_loss improved from 1.44872 to 1.39063, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/03-1.3906.hdf5\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 1.4852 - acc: 0.5736 - val_loss: 1.3906 - val_acc: 0.6231\n",
      "Epoch 4/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3811 - acc: 0.5980\n",
      "Epoch 00004: val_loss improved from 1.39063 to 1.37783, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/04-1.3778.hdf5\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 1.3814 - acc: 0.5978 - val_loss: 1.3778 - val_acc: 0.6261\n",
      "Epoch 5/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3028 - acc: 0.6149\n",
      "Epoch 00005: val_loss improved from 1.37783 to 1.34582, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/05-1.3458.hdf5\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 1.3027 - acc: 0.6149 - val_loss: 1.3458 - val_acc: 0.6166\n",
      "Epoch 6/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2240 - acc: 0.6354\n",
      "Epoch 00006: val_loss improved from 1.34582 to 1.28330, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/06-1.2833.hdf5\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 1.2242 - acc: 0.6354 - val_loss: 1.2833 - val_acc: 0.6527\n",
      "Epoch 7/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1588 - acc: 0.6498\n",
      "Epoch 00007: val_loss improved from 1.28330 to 1.26921, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/07-1.2692.hdf5\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 1.1589 - acc: 0.6497 - val_loss: 1.2692 - val_acc: 0.6599\n",
      "Epoch 8/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1097 - acc: 0.6644\n",
      "Epoch 00008: val_loss improved from 1.26921 to 1.24650, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/08-1.2465.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 1.1102 - acc: 0.6641 - val_loss: 1.2465 - val_acc: 0.6660\n",
      "Epoch 9/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.6724\n",
      "Epoch 00009: val_loss improved from 1.24650 to 1.23432, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/09-1.2343.hdf5\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 1.0678 - acc: 0.6722 - val_loss: 1.2343 - val_acc: 0.6669\n",
      "Epoch 10/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0336 - acc: 0.6851\n",
      "Epoch 00010: val_loss improved from 1.23432 to 1.18306, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/10-1.1831.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 1.0336 - acc: 0.6851 - val_loss: 1.1831 - val_acc: 0.6872\n",
      "Epoch 11/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9742 - acc: 0.7005\n",
      "Epoch 00011: val_loss did not improve from 1.18306\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.9748 - acc: 0.7005 - val_loss: 1.1904 - val_acc: 0.6776\n",
      "Epoch 12/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9195 - acc: 0.7153\n",
      "Epoch 00012: val_loss improved from 1.18306 to 1.17062, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/12-1.1706.hdf5\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.9197 - acc: 0.7153 - val_loss: 1.1706 - val_acc: 0.6946\n",
      "Epoch 13/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8966 - acc: 0.7193\n",
      "Epoch 00013: val_loss improved from 1.17062 to 1.15599, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/13-1.1560.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.8965 - acc: 0.7192 - val_loss: 1.1560 - val_acc: 0.7004\n",
      "Epoch 14/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8433 - acc: 0.7395\n",
      "Epoch 00014: val_loss improved from 1.15599 to 1.15001, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/14-1.1500.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.8436 - acc: 0.7395 - val_loss: 1.1500 - val_acc: 0.7025\n",
      "Epoch 15/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8151 - acc: 0.7443\n",
      "Epoch 00015: val_loss improved from 1.15001 to 1.12856, saving model to model/checkpoint/2D_CNN_1_only_conv_DO_BN_checkpoint/15-1.1286.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.8146 - acc: 0.7443 - val_loss: 1.1286 - val_acc: 0.7044\n",
      "Epoch 16/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.7588\n",
      "Epoch 00016: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.7768 - acc: 0.7589 - val_loss: 1.1424 - val_acc: 0.7004\n",
      "Epoch 17/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7644\n",
      "Epoch 00017: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.7530 - acc: 0.7642 - val_loss: 1.1439 - val_acc: 0.7107\n",
      "Epoch 18/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7357 - acc: 0.7714\n",
      "Epoch 00018: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.7357 - acc: 0.7714 - val_loss: 1.1371 - val_acc: 0.7172\n",
      "Epoch 19/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.7760\n",
      "Epoch 00019: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.7111 - acc: 0.7760 - val_loss: 1.1698 - val_acc: 0.7063\n",
      "Epoch 20/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7812\n",
      "Epoch 00020: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6931 - acc: 0.7810 - val_loss: 1.1562 - val_acc: 0.7098\n",
      "Epoch 21/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6718 - acc: 0.7867\n",
      "Epoch 00021: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.6718 - acc: 0.7867 - val_loss: 1.1531 - val_acc: 0.7095\n",
      "Epoch 22/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6558 - acc: 0.7929\n",
      "Epoch 00022: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.6556 - acc: 0.7929 - val_loss: 1.1582 - val_acc: 0.7121\n",
      "Epoch 23/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6450 - acc: 0.7952\n",
      "Epoch 00023: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.6452 - acc: 0.7952 - val_loss: 1.1615 - val_acc: 0.7165\n",
      "Epoch 24/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.8013\n",
      "Epoch 00024: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.6332 - acc: 0.8013 - val_loss: 1.1576 - val_acc: 0.7184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6133 - acc: 0.8081\n",
      "Epoch 00025: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.6132 - acc: 0.8082 - val_loss: 1.1450 - val_acc: 0.7209\n",
      "Epoch 26/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.8060\n",
      "Epoch 00026: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.6111 - acc: 0.8059 - val_loss: 1.1668 - val_acc: 0.7174\n",
      "Epoch 27/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5896 - acc: 0.8119\n",
      "Epoch 00027: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.5896 - acc: 0.8120 - val_loss: 1.1867 - val_acc: 0.7105\n",
      "Epoch 28/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.8130\n",
      "Epoch 00028: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.5895 - acc: 0.8130 - val_loss: 1.1708 - val_acc: 0.7240\n",
      "Epoch 29/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.8140\n",
      "Epoch 00029: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.5797 - acc: 0.8139 - val_loss: 1.1722 - val_acc: 0.7212\n",
      "Epoch 30/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8185\n",
      "Epoch 00030: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.5693 - acc: 0.8185 - val_loss: 1.2240 - val_acc: 0.7205\n",
      "Epoch 31/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8189\n",
      "Epoch 00031: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.5621 - acc: 0.8189 - val_loss: 1.1765 - val_acc: 0.7258\n",
      "Epoch 32/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8206\n",
      "Epoch 00032: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.5555 - acc: 0.8207 - val_loss: 1.1649 - val_acc: 0.7340\n",
      "Epoch 33/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8268\n",
      "Epoch 00033: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.5465 - acc: 0.8268 - val_loss: 1.1813 - val_acc: 0.7209\n",
      "Epoch 34/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.8295\n",
      "Epoch 00034: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.5371 - acc: 0.8295 - val_loss: 1.2052 - val_acc: 0.7205\n",
      "Epoch 35/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8295\n",
      "Epoch 00035: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.5329 - acc: 0.8294 - val_loss: 1.1967 - val_acc: 0.7284\n",
      "Epoch 36/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8315\n",
      "Epoch 00036: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.5249 - acc: 0.8315 - val_loss: 1.1966 - val_acc: 0.7272\n",
      "Epoch 37/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8332\n",
      "Epoch 00037: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.5232 - acc: 0.8333 - val_loss: 1.2005 - val_acc: 0.7263\n",
      "Epoch 38/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8360\n",
      "Epoch 00038: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5070 - acc: 0.8359 - val_loss: 1.2112 - val_acc: 0.7298\n",
      "Epoch 39/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8361\n",
      "Epoch 00039: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.5095 - acc: 0.8361 - val_loss: 1.2146 - val_acc: 0.7242\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8392\n",
      "Epoch 00040: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.5003 - acc: 0.8390 - val_loss: 1.2017 - val_acc: 0.7307\n",
      "Epoch 41/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8390\n",
      "Epoch 00041: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.5012 - acc: 0.8389 - val_loss: 1.1902 - val_acc: 0.7363\n",
      "Epoch 42/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8438\n",
      "Epoch 00042: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.4906 - acc: 0.8439 - val_loss: 1.2059 - val_acc: 0.7324\n",
      "Epoch 43/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8434\n",
      "Epoch 00043: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.4859 - acc: 0.8434 - val_loss: 1.2315 - val_acc: 0.7242\n",
      "Epoch 44/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8442\n",
      "Epoch 00044: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4832 - acc: 0.8442 - val_loss: 1.2393 - val_acc: 0.7249\n",
      "Epoch 45/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8466\n",
      "Epoch 00045: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.4800 - acc: 0.8466 - val_loss: 1.2388 - val_acc: 0.7314\n",
      "Epoch 46/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8441\n",
      "Epoch 00046: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4830 - acc: 0.8441 - val_loss: 1.2363 - val_acc: 0.7237\n",
      "Epoch 47/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8475\n",
      "Epoch 00047: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4699 - acc: 0.8475 - val_loss: 1.2336 - val_acc: 0.7328\n",
      "Epoch 48/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8523\n",
      "Epoch 00048: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.4650 - acc: 0.8522 - val_loss: 1.2164 - val_acc: 0.7303\n",
      "Epoch 49/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8518\n",
      "Epoch 00049: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.4627 - acc: 0.8519 - val_loss: 1.2872 - val_acc: 0.7265\n",
      "Epoch 50/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8530\n",
      "Epoch 00050: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.4602 - acc: 0.8528 - val_loss: 1.2659 - val_acc: 0.7170\n",
      "Epoch 51/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8529\n",
      "Epoch 00051: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.4611 - acc: 0.8527 - val_loss: 1.2579 - val_acc: 0.7293\n",
      "Epoch 52/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.8555\n",
      "Epoch 00052: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4517 - acc: 0.8553 - val_loss: 1.2563 - val_acc: 0.7305\n",
      "Epoch 53/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8564\n",
      "Epoch 00053: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.4504 - acc: 0.8566 - val_loss: 1.2822 - val_acc: 0.7326\n",
      "Epoch 54/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8558\n",
      "Epoch 00054: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.4476 - acc: 0.8558 - val_loss: 1.2642 - val_acc: 0.7338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8595\n",
      "Epoch 00055: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4373 - acc: 0.8594 - val_loss: 1.2795 - val_acc: 0.7305\n",
      "Epoch 56/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8569\n",
      "Epoch 00056: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.4436 - acc: 0.8568 - val_loss: 1.2920 - val_acc: 0.7221\n",
      "Epoch 57/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.8578\n",
      "Epoch 00057: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.4396 - acc: 0.8578 - val_loss: 1.2596 - val_acc: 0.7317\n",
      "Epoch 58/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8610\n",
      "Epoch 00058: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.4339 - acc: 0.8608 - val_loss: 1.2912 - val_acc: 0.7263\n",
      "Epoch 59/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8626\n",
      "Epoch 00059: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.4316 - acc: 0.8626 - val_loss: 1.2910 - val_acc: 0.7261\n",
      "Epoch 60/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8609\n",
      "Epoch 00060: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.4319 - acc: 0.8610 - val_loss: 1.2977 - val_acc: 0.7305\n",
      "Epoch 61/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8631\n",
      "Epoch 00061: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4303 - acc: 0.8631 - val_loss: 1.3162 - val_acc: 0.7235\n",
      "Epoch 62/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8631\n",
      "Epoch 00062: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.4271 - acc: 0.8631 - val_loss: 1.2717 - val_acc: 0.7307\n",
      "Epoch 63/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8659\n",
      "Epoch 00063: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4200 - acc: 0.8658 - val_loss: 1.2912 - val_acc: 0.7284\n",
      "Epoch 64/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4195 - acc: 0.8650\n",
      "Epoch 00064: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.4196 - acc: 0.8650 - val_loss: 1.2986 - val_acc: 0.7319\n",
      "Epoch 65/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8654\n",
      "Epoch 00065: val_loss did not improve from 1.12856\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.4151 - acc: 0.8655 - val_loss: 1.3051 - val_acc: 0.7284\n",
      "\n",
      "1 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 223us/sample - loss: 1.2333 - acc: 0.6741\n",
      "Loss: 1.2332580039310554 Accuracy: 0.6741433\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0388 - acc: 0.3914\n",
      "Epoch 00001: val_loss improved from inf to 1.37255, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/01-1.3725.hdf5\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 2.0379 - acc: 0.3916 - val_loss: 1.3725 - val_acc: 0.5935\n",
      "Epoch 2/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4067 - acc: 0.5727\n",
      "Epoch 00002: val_loss improved from 1.37255 to 1.12099, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/02-1.1210.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 1.4064 - acc: 0.5728 - val_loss: 1.1210 - val_acc: 0.6797\n",
      "Epoch 3/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1592 - acc: 0.6458\n",
      "Epoch 00003: val_loss improved from 1.12099 to 0.98956, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/03-0.9896.hdf5\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 1.1593 - acc: 0.6458 - val_loss: 0.9896 - val_acc: 0.7319\n",
      "Epoch 4/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.6936\n",
      "Epoch 00004: val_loss improved from 0.98956 to 0.91793, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/04-0.9179.hdf5\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 1.0065 - acc: 0.6939 - val_loss: 0.9179 - val_acc: 0.7505\n",
      "Epoch 5/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8990 - acc: 0.7289\n",
      "Epoch 00005: val_loss improved from 0.91793 to 0.84565, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/05-0.8457.hdf5\n",
      "36805/36805 [==============================] - 14s 367us/sample - loss: 0.8990 - acc: 0.7289 - val_loss: 0.8457 - val_acc: 0.7750\n",
      "Epoch 6/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8143 - acc: 0.7526\n",
      "Epoch 00006: val_loss improved from 0.84565 to 0.78913, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/06-0.7891.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.8145 - acc: 0.7525 - val_loss: 0.7891 - val_acc: 0.7901\n",
      "Epoch 7/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7773\n",
      "Epoch 00007: val_loss improved from 0.78913 to 0.74471, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/07-0.7447.hdf5\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.7321 - acc: 0.7772 - val_loss: 0.7447 - val_acc: 0.8022\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.7917\n",
      "Epoch 00008: val_loss improved from 0.74471 to 0.69165, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/08-0.6916.hdf5\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.6838 - acc: 0.7917 - val_loss: 0.6916 - val_acc: 0.8167\n",
      "Epoch 9/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6371 - acc: 0.8058\n",
      "Epoch 00009: val_loss improved from 0.69165 to 0.68742, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/09-0.6874.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.6368 - acc: 0.8058 - val_loss: 0.6874 - val_acc: 0.8237\n",
      "Epoch 10/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8182\n",
      "Epoch 00010: val_loss improved from 0.68742 to 0.65520, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/10-0.6552.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.5971 - acc: 0.8183 - val_loss: 0.6552 - val_acc: 0.8334\n",
      "Epoch 11/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.8270\n",
      "Epoch 00011: val_loss improved from 0.65520 to 0.63309, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/11-0.6331.hdf5\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.5658 - acc: 0.8271 - val_loss: 0.6331 - val_acc: 0.8425\n",
      "Epoch 12/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8330\n",
      "Epoch 00012: val_loss improved from 0.63309 to 0.63212, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/12-0.6321.hdf5\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.5388 - acc: 0.8330 - val_loss: 0.6321 - val_acc: 0.8411\n",
      "Epoch 13/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8431\n",
      "Epoch 00013: val_loss improved from 0.63212 to 0.61387, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/13-0.6139.hdf5\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.5086 - acc: 0.8431 - val_loss: 0.6139 - val_acc: 0.8472\n",
      "Epoch 14/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8470\n",
      "Epoch 00014: val_loss improved from 0.61387 to 0.60416, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/14-0.6042.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.4914 - acc: 0.8471 - val_loss: 0.6042 - val_acc: 0.8479\n",
      "Epoch 15/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8559\n",
      "Epoch 00015: val_loss improved from 0.60416 to 0.58966, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/15-0.5897.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.4707 - acc: 0.8559 - val_loss: 0.5897 - val_acc: 0.8512\n",
      "Epoch 16/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8617\n",
      "Epoch 00016: val_loss improved from 0.58966 to 0.58182, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/16-0.5818.hdf5\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.4461 - acc: 0.8618 - val_loss: 0.5818 - val_acc: 0.8567\n",
      "Epoch 17/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8653\n",
      "Epoch 00017: val_loss improved from 0.58182 to 0.57295, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/17-0.5730.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.4318 - acc: 0.8652 - val_loss: 0.5730 - val_acc: 0.8567\n",
      "Epoch 18/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8706\n",
      "Epoch 00018: val_loss improved from 0.57295 to 0.56171, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/18-0.5617.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.4175 - acc: 0.8706 - val_loss: 0.5617 - val_acc: 0.8595\n",
      "Epoch 19/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8751\n",
      "Epoch 00019: val_loss did not improve from 0.56171\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.4060 - acc: 0.8750 - val_loss: 0.5680 - val_acc: 0.8586\n",
      "Epoch 20/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8746\n",
      "Epoch 00020: val_loss did not improve from 0.56171\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.4014 - acc: 0.8747 - val_loss: 0.5656 - val_acc: 0.8621\n",
      "Epoch 21/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8820\n",
      "Epoch 00021: val_loss did not improve from 0.56171\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.3793 - acc: 0.8819 - val_loss: 0.5640 - val_acc: 0.8649\n",
      "Epoch 22/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8845\n",
      "Epoch 00022: val_loss did not improve from 0.56171\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3708 - acc: 0.8846 - val_loss: 0.5633 - val_acc: 0.8628\n",
      "Epoch 23/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8901\n",
      "Epoch 00023: val_loss improved from 0.56171 to 0.56064, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/23-0.5606.hdf5\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.3539 - acc: 0.8901 - val_loss: 0.5606 - val_acc: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8890\n",
      "Epoch 00024: val_loss improved from 0.56064 to 0.55386, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/24-0.5539.hdf5\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.3563 - acc: 0.8890 - val_loss: 0.5539 - val_acc: 0.8705\n",
      "Epoch 25/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.8946\n",
      "Epoch 00025: val_loss improved from 0.55386 to 0.55059, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/25-0.5506.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.3399 - acc: 0.8945 - val_loss: 0.5506 - val_acc: 0.8677\n",
      "Epoch 26/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8946\n",
      "Epoch 00026: val_loss improved from 0.55059 to 0.54125, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/26-0.5413.hdf5\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.3380 - acc: 0.8946 - val_loss: 0.5413 - val_acc: 0.8696\n",
      "Epoch 27/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8980\n",
      "Epoch 00027: val_loss did not improve from 0.54125\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.3255 - acc: 0.8980 - val_loss: 0.5494 - val_acc: 0.8686\n",
      "Epoch 28/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9008\n",
      "Epoch 00028: val_loss did not improve from 0.54125\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.3185 - acc: 0.9008 - val_loss: 0.5511 - val_acc: 0.8679\n",
      "Epoch 29/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9030\n",
      "Epoch 00029: val_loss did not improve from 0.54125\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.3120 - acc: 0.9031 - val_loss: 0.5502 - val_acc: 0.8696\n",
      "Epoch 30/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.9033\n",
      "Epoch 00030: val_loss improved from 0.54125 to 0.53089, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/30-0.5309.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.3072 - acc: 0.9033 - val_loss: 0.5309 - val_acc: 0.8758\n",
      "Epoch 31/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9060\n",
      "Epoch 00031: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.3007 - acc: 0.9060 - val_loss: 0.5569 - val_acc: 0.8679\n",
      "Epoch 32/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9076\n",
      "Epoch 00032: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2938 - acc: 0.9074 - val_loss: 0.5345 - val_acc: 0.8777\n",
      "Epoch 33/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9123\n",
      "Epoch 00033: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.2833 - acc: 0.9124 - val_loss: 0.5436 - val_acc: 0.8724\n",
      "Epoch 34/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9119- ETA: 1s - lo\n",
      "Epoch 00034: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.2853 - acc: 0.9119 - val_loss: 0.5459 - val_acc: 0.8717\n",
      "Epoch 35/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9134\n",
      "Epoch 00035: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.2781 - acc: 0.9134 - val_loss: 0.5315 - val_acc: 0.8791\n",
      "Epoch 36/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9164\n",
      "Epoch 00036: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.2693 - acc: 0.9165 - val_loss: 0.5350 - val_acc: 0.8782\n",
      "Epoch 37/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9143\n",
      "Epoch 00037: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2721 - acc: 0.9143 - val_loss: 0.5415 - val_acc: 0.8796\n",
      "Epoch 38/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9160\n",
      "Epoch 00038: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2649 - acc: 0.9160 - val_loss: 0.5392 - val_acc: 0.8770\n",
      "Epoch 39/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9198\n",
      "Epoch 00039: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2584 - acc: 0.9197 - val_loss: 0.5393 - val_acc: 0.8740\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9236\n",
      "Epoch 00040: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2492 - acc: 0.9237 - val_loss: 0.5374 - val_acc: 0.8737\n",
      "Epoch 41/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9208\n",
      "Epoch 00041: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2517 - acc: 0.9209 - val_loss: 0.5363 - val_acc: 0.8786\n",
      "Epoch 42/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9242\n",
      "Epoch 00042: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2414 - acc: 0.9242 - val_loss: 0.5410 - val_acc: 0.8742\n",
      "Epoch 43/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9224\n",
      "Epoch 00043: val_loss did not improve from 0.53089\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2497 - acc: 0.9225 - val_loss: 0.5434 - val_acc: 0.8791\n",
      "Epoch 44/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9255\n",
      "Epoch 00044: val_loss improved from 0.53089 to 0.52969, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/44-0.5297.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2401 - acc: 0.9255 - val_loss: 0.5297 - val_acc: 0.8824\n",
      "Epoch 45/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9243\n",
      "Epoch 00045: val_loss did not improve from 0.52969\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2401 - acc: 0.9243 - val_loss: 0.5329 - val_acc: 0.8817\n",
      "Epoch 46/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9278\n",
      "Epoch 00046: val_loss did not improve from 0.52969\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.2325 - acc: 0.9279 - val_loss: 0.5452 - val_acc: 0.8779\n",
      "Epoch 47/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9303\n",
      "Epoch 00047: val_loss did not improve from 0.52969\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2260 - acc: 0.9303 - val_loss: 0.5375 - val_acc: 0.8814\n",
      "Epoch 48/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9309\n",
      "Epoch 00048: val_loss did not improve from 0.52969\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2246 - acc: 0.9309 - val_loss: 0.5341 - val_acc: 0.8849\n",
      "Epoch 49/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9290\n",
      "Epoch 00049: val_loss did not improve from 0.52969\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.2255 - acc: 0.9289 - val_loss: 0.5385 - val_acc: 0.8849\n",
      "Epoch 50/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9322\n",
      "Epoch 00050: val_loss improved from 0.52969 to 0.52379, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/50-0.5238.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.2198 - acc: 0.9320 - val_loss: 0.5238 - val_acc: 0.8838\n",
      "Epoch 51/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9319\n",
      "Epoch 00051: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2180 - acc: 0.9319 - val_loss: 0.5505 - val_acc: 0.8789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9338\n",
      "Epoch 00052: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.2127 - acc: 0.9337 - val_loss: 0.5341 - val_acc: 0.8824\n",
      "Epoch 53/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9345\n",
      "Epoch 00053: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2112 - acc: 0.9345 - val_loss: 0.5412 - val_acc: 0.8840\n",
      "Epoch 54/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9347\n",
      "Epoch 00054: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2106 - acc: 0.9348 - val_loss: 0.5789 - val_acc: 0.8751\n",
      "Epoch 55/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9351\n",
      "Epoch 00055: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2069 - acc: 0.9351 - val_loss: 0.5281 - val_acc: 0.8821\n",
      "Epoch 56/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9351\n",
      "Epoch 00056: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2047 - acc: 0.9351 - val_loss: 0.5341 - val_acc: 0.8838\n",
      "Epoch 57/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9388\n",
      "Epoch 00057: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1989 - acc: 0.9388 - val_loss: 0.5409 - val_acc: 0.8821\n",
      "Epoch 58/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9385\n",
      "Epoch 00058: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.1960 - acc: 0.9384 - val_loss: 0.5299 - val_acc: 0.8831\n",
      "Epoch 59/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9399\n",
      "Epoch 00059: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1955 - acc: 0.9400 - val_loss: 0.5328 - val_acc: 0.8793\n",
      "Epoch 60/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9413\n",
      "Epoch 00060: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1895 - acc: 0.9413 - val_loss: 0.5332 - val_acc: 0.8831\n",
      "Epoch 61/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9396\n",
      "Epoch 00061: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1932 - acc: 0.9396 - val_loss: 0.5385 - val_acc: 0.8831\n",
      "Epoch 62/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9408\n",
      "Epoch 00062: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.1905 - acc: 0.9409 - val_loss: 0.5443 - val_acc: 0.8821\n",
      "Epoch 63/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9401\n",
      "Epoch 00063: val_loss did not improve from 0.52379\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1891 - acc: 0.9401 - val_loss: 0.5371 - val_acc: 0.8803\n",
      "Epoch 64/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9441\n",
      "Epoch 00064: val_loss improved from 0.52379 to 0.52033, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/64-0.5203.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1804 - acc: 0.9441 - val_loss: 0.5203 - val_acc: 0.8854\n",
      "Epoch 65/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9440\n",
      "Epoch 00065: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1826 - acc: 0.9438 - val_loss: 0.5312 - val_acc: 0.8828\n",
      "Epoch 66/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9432\n",
      "Epoch 00066: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1827 - acc: 0.9432 - val_loss: 0.5364 - val_acc: 0.8861\n",
      "Epoch 67/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9448\n",
      "Epoch 00067: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.1798 - acc: 0.9448 - val_loss: 0.5330 - val_acc: 0.8870\n",
      "Epoch 68/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9469\n",
      "Epoch 00068: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1737 - acc: 0.9469 - val_loss: 0.5447 - val_acc: 0.8889\n",
      "Epoch 69/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9449\n",
      "Epoch 00069: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1763 - acc: 0.9449 - val_loss: 0.5324 - val_acc: 0.8852\n",
      "Epoch 70/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9453\n",
      "Epoch 00070: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1736 - acc: 0.9453 - val_loss: 0.5337 - val_acc: 0.8859\n",
      "Epoch 71/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9469\n",
      "Epoch 00071: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1698 - acc: 0.9469 - val_loss: 0.5380 - val_acc: 0.8856\n",
      "Epoch 72/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9474\n",
      "Epoch 00072: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.1696 - acc: 0.9474 - val_loss: 0.5282 - val_acc: 0.8866\n",
      "Epoch 73/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9499\n",
      "Epoch 00073: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1630 - acc: 0.9499 - val_loss: 0.5280 - val_acc: 0.8887\n",
      "Epoch 74/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9484\n",
      "Epoch 00074: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1680 - acc: 0.9483 - val_loss: 0.5350 - val_acc: 0.8859\n",
      "Epoch 75/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9492\n",
      "Epoch 00075: val_loss did not improve from 0.52033\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.1658 - acc: 0.9492 - val_loss: 0.5295 - val_acc: 0.8905\n",
      "Epoch 76/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9496\n",
      "Epoch 00076: val_loss improved from 0.52033 to 0.51450, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/76-0.5145.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1649 - acc: 0.9496 - val_loss: 0.5145 - val_acc: 0.8915\n",
      "Epoch 77/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9481\n",
      "Epoch 00077: val_loss did not improve from 0.51450\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1643 - acc: 0.9481 - val_loss: 0.5339 - val_acc: 0.8889\n",
      "Epoch 78/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9518\n",
      "Epoch 00078: val_loss did not improve from 0.51450\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1585 - acc: 0.9517 - val_loss: 0.5280 - val_acc: 0.8831\n",
      "Epoch 79/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9505\n",
      "Epoch 00079: val_loss improved from 0.51450 to 0.50984, saving model to model/checkpoint/2D_CNN_2_only_conv_DO_BN_checkpoint/79-0.5098.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1568 - acc: 0.9506 - val_loss: 0.5098 - val_acc: 0.8884\n",
      "Epoch 80/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9508\n",
      "Epoch 00080: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1597 - acc: 0.9509 - val_loss: 0.5258 - val_acc: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9518\n",
      "Epoch 00081: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.1580 - acc: 0.9519 - val_loss: 0.5232 - val_acc: 0.8873\n",
      "Epoch 82/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9515\n",
      "Epoch 00082: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1566 - acc: 0.9516 - val_loss: 0.5329 - val_acc: 0.8910\n",
      "Epoch 83/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9532\n",
      "Epoch 00083: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1537 - acc: 0.9532 - val_loss: 0.5383 - val_acc: 0.8847\n",
      "Epoch 84/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9523\n",
      "Epoch 00084: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 12s 335us/sample - loss: 0.1522 - acc: 0.9523 - val_loss: 0.5434 - val_acc: 0.8868\n",
      "Epoch 85/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9549\n",
      "Epoch 00085: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 12s 340us/sample - loss: 0.1479 - acc: 0.9549 - val_loss: 0.5411 - val_acc: 0.8819\n",
      "Epoch 86/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9529\n",
      "Epoch 00086: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.1500 - acc: 0.9529 - val_loss: 0.5627 - val_acc: 0.8838\n",
      "Epoch 87/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9539\n",
      "Epoch 00087: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.1481 - acc: 0.9539 - val_loss: 0.5253 - val_acc: 0.8921\n",
      "Epoch 88/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9549\n",
      "Epoch 00088: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1462 - acc: 0.9549 - val_loss: 0.5358 - val_acc: 0.8877\n",
      "Epoch 89/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9560\n",
      "Epoch 00089: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1449 - acc: 0.9560 - val_loss: 0.5424 - val_acc: 0.8884\n",
      "Epoch 90/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9577\n",
      "Epoch 00090: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.1417 - acc: 0.9576 - val_loss: 0.5410 - val_acc: 0.8889\n",
      "Epoch 91/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9578\n",
      "Epoch 00091: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1392 - acc: 0.9578 - val_loss: 0.5502 - val_acc: 0.8845\n",
      "Epoch 92/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9557\n",
      "Epoch 00092: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1447 - acc: 0.9557 - val_loss: 0.5261 - val_acc: 0.8924\n",
      "Epoch 93/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9547\n",
      "Epoch 00093: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1423 - acc: 0.9547 - val_loss: 0.5297 - val_acc: 0.8875\n",
      "Epoch 94/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9562\n",
      "Epoch 00094: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1397 - acc: 0.9561 - val_loss: 0.5264 - val_acc: 0.8894\n",
      "Epoch 95/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9568\n",
      "Epoch 00095: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1388 - acc: 0.9569 - val_loss: 0.5178 - val_acc: 0.8887\n",
      "Epoch 96/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9574\n",
      "Epoch 00096: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.1386 - acc: 0.9574 - val_loss: 0.5325 - val_acc: 0.8889\n",
      "Epoch 97/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9581\n",
      "Epoch 00097: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1361 - acc: 0.9581 - val_loss: 0.5396 - val_acc: 0.8903\n",
      "Epoch 98/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9576\n",
      "Epoch 00098: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.1350 - acc: 0.9576 - val_loss: 0.5369 - val_acc: 0.8896\n",
      "Epoch 99/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9583\n",
      "Epoch 00099: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1351 - acc: 0.9584 - val_loss: 0.5471 - val_acc: 0.8870\n",
      "Epoch 100/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9596\n",
      "Epoch 00100: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1334 - acc: 0.9596 - val_loss: 0.5238 - val_acc: 0.8949\n",
      "Epoch 101/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9587\n",
      "Epoch 00101: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1331 - acc: 0.9587 - val_loss: 0.5191 - val_acc: 0.8901\n",
      "Epoch 102/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9596\n",
      "Epoch 00102: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.1325 - acc: 0.9595 - val_loss: 0.5276 - val_acc: 0.8901\n",
      "Epoch 103/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9589\n",
      "Epoch 00103: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.1318 - acc: 0.9589 - val_loss: 0.5249 - val_acc: 0.8877\n",
      "Epoch 104/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9612\n",
      "Epoch 00104: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1296 - acc: 0.9612 - val_loss: 0.5330 - val_acc: 0.8896\n",
      "Epoch 105/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9595\n",
      "Epoch 00105: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1291 - acc: 0.9595 - val_loss: 0.5248 - val_acc: 0.8945\n",
      "Epoch 106/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9611\n",
      "Epoch 00106: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1272 - acc: 0.9611 - val_loss: 0.5353 - val_acc: 0.8873\n",
      "Epoch 107/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9618\n",
      "Epoch 00107: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.1289 - acc: 0.9617 - val_loss: 0.5227 - val_acc: 0.8912\n",
      "Epoch 108/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9619\n",
      "Epoch 00108: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1243 - acc: 0.9619 - val_loss: 0.5649 - val_acc: 0.8847\n",
      "Epoch 109/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9611\n",
      "Epoch 00109: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1274 - acc: 0.9611 - val_loss: 0.5468 - val_acc: 0.8884\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9621\n",
      "Epoch 00110: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.1213 - acc: 0.9621 - val_loss: 0.5268 - val_acc: 0.8908\n",
      "Epoch 111/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9624\n",
      "Epoch 00111: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.1236 - acc: 0.9625 - val_loss: 0.5285 - val_acc: 0.8915\n",
      "Epoch 112/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9616\n",
      "Epoch 00112: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1254 - acc: 0.9616 - val_loss: 0.5240 - val_acc: 0.8921\n",
      "Epoch 113/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9619\n",
      "Epoch 00113: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1245 - acc: 0.9618 - val_loss: 0.5391 - val_acc: 0.8905\n",
      "Epoch 114/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9640\n",
      "Epoch 00114: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1173 - acc: 0.9640 - val_loss: 0.5370 - val_acc: 0.8938\n",
      "Epoch 115/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9633\n",
      "Epoch 00115: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1205 - acc: 0.9633 - val_loss: 0.5241 - val_acc: 0.8954\n",
      "Epoch 116/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9631\n",
      "Epoch 00116: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1203 - acc: 0.9631 - val_loss: 0.5291 - val_acc: 0.8924\n",
      "Epoch 117/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9649\n",
      "Epoch 00117: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.1170 - acc: 0.9648 - val_loss: 0.5539 - val_acc: 0.8901\n",
      "Epoch 118/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9636\n",
      "Epoch 00118: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1185 - acc: 0.9636 - val_loss: 0.5395 - val_acc: 0.8882\n",
      "Epoch 119/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9639\n",
      "Epoch 00119: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1156 - acc: 0.9639 - val_loss: 0.5126 - val_acc: 0.8915\n",
      "Epoch 120/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9641\n",
      "Epoch 00120: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.1149 - acc: 0.9641 - val_loss: 0.5312 - val_acc: 0.8926\n",
      "Epoch 121/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9647\n",
      "Epoch 00121: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1163 - acc: 0.9648 - val_loss: 0.5549 - val_acc: 0.8884\n",
      "Epoch 122/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9642\n",
      "Epoch 00122: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1154 - acc: 0.9642 - val_loss: 0.5457 - val_acc: 0.8921\n",
      "Epoch 123/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9650\n",
      "Epoch 00123: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.1133 - acc: 0.9650 - val_loss: 0.5293 - val_acc: 0.8891\n",
      "Epoch 124/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9664\n",
      "Epoch 00124: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1134 - acc: 0.9663 - val_loss: 0.5308 - val_acc: 0.8917\n",
      "Epoch 125/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9659\n",
      "Epoch 00125: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1107 - acc: 0.9658 - val_loss: 0.5578 - val_acc: 0.8889\n",
      "Epoch 126/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9661\n",
      "Epoch 00126: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1103 - acc: 0.9660 - val_loss: 0.5213 - val_acc: 0.8963\n",
      "Epoch 127/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9657\n",
      "Epoch 00127: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.1108 - acc: 0.9657 - val_loss: 0.5332 - val_acc: 0.8910\n",
      "Epoch 128/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9683\n",
      "Epoch 00128: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.1046 - acc: 0.9683 - val_loss: 0.5358 - val_acc: 0.8968\n",
      "Epoch 129/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9678\n",
      "Epoch 00129: val_loss did not improve from 0.50984\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1071 - acc: 0.9677 - val_loss: 0.5475 - val_acc: 0.8935\n",
      "\n",
      "2 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 238us/sample - loss: 0.6056 - acc: 0.8540\n",
      "Loss: 0.6055759042470502 Accuracy: 0.85399795\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1690 - acc: 0.3387\n",
      "Epoch 00001: val_loss improved from inf to 1.36442, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/01-1.3644.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 2.1688 - acc: 0.3389 - val_loss: 1.3644 - val_acc: 0.5945\n",
      "Epoch 2/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3735 - acc: 0.5714\n",
      "Epoch 00002: val_loss improved from 1.36442 to 1.01208, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/02-1.0121.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 1.3732 - acc: 0.5714 - val_loss: 1.0121 - val_acc: 0.7035\n",
      "Epoch 3/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0483 - acc: 0.6787\n",
      "Epoch 00003: val_loss improved from 1.01208 to 0.78520, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/03-0.7852.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 1.0481 - acc: 0.6787 - val_loss: 0.7852 - val_acc: 0.7855\n",
      "Epoch 4/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8482 - acc: 0.7432\n",
      "Epoch 00004: val_loss improved from 0.78520 to 0.65072, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/04-0.6507.hdf5\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.8478 - acc: 0.7433 - val_loss: 0.6507 - val_acc: 0.8265\n",
      "Epoch 5/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7192 - acc: 0.7812\n",
      "Epoch 00005: val_loss improved from 0.65072 to 0.58764, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/05-0.5876.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.7193 - acc: 0.7812 - val_loss: 0.5876 - val_acc: 0.8484\n",
      "Epoch 6/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6433 - acc: 0.8055\n",
      "Epoch 00006: val_loss improved from 0.58764 to 0.51851, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/06-0.5185.hdf5\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.6432 - acc: 0.8056 - val_loss: 0.5185 - val_acc: 0.8663\n",
      "Epoch 7/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.8224\n",
      "Epoch 00007: val_loss improved from 0.51851 to 0.49441, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/07-0.4944.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.5848 - acc: 0.8223 - val_loss: 0.4944 - val_acc: 0.8696\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8389\n",
      "Epoch 00008: val_loss improved from 0.49441 to 0.47382, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/08-0.4738.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.5290 - acc: 0.8389 - val_loss: 0.4738 - val_acc: 0.8763\n",
      "Epoch 9/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4971 - acc: 0.8482\n",
      "Epoch 00009: val_loss improved from 0.47382 to 0.45728, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/09-0.4573.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.4968 - acc: 0.8482 - val_loss: 0.4573 - val_acc: 0.8751\n",
      "Epoch 10/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8577\n",
      "Epoch 00010: val_loss improved from 0.45728 to 0.42702, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/10-0.4270.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.4643 - acc: 0.8577 - val_loss: 0.4270 - val_acc: 0.8910\n",
      "Epoch 11/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8663\n",
      "Epoch 00011: val_loss improved from 0.42702 to 0.40657, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/11-0.4066.hdf5\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.4348 - acc: 0.8664 - val_loss: 0.4066 - val_acc: 0.9001\n",
      "Epoch 12/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8709\n",
      "Epoch 00012: val_loss did not improve from 0.40657\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.4189 - acc: 0.8707 - val_loss: 0.4162 - val_acc: 0.8894\n",
      "Epoch 13/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8783\n",
      "Epoch 00013: val_loss improved from 0.40657 to 0.38628, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/13-0.3863.hdf5\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.3944 - acc: 0.8783 - val_loss: 0.3863 - val_acc: 0.8947\n",
      "Epoch 14/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8822\n",
      "Epoch 00014: val_loss improved from 0.38628 to 0.36760, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/14-0.3676.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.3806 - acc: 0.8822 - val_loss: 0.3676 - val_acc: 0.9036\n",
      "Epoch 15/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8872\n",
      "Epoch 00015: val_loss improved from 0.36760 to 0.35143, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/15-0.3514.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3609 - acc: 0.8872 - val_loss: 0.3514 - val_acc: 0.9096\n",
      "Epoch 16/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8917\n",
      "Epoch 00016: val_loss improved from 0.35143 to 0.33776, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/16-0.3378.hdf5\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.3480 - acc: 0.8917 - val_loss: 0.3378 - val_acc: 0.9143\n",
      "Epoch 17/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8958\n",
      "Epoch 00017: val_loss did not improve from 0.33776\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.3349 - acc: 0.8956 - val_loss: 0.3395 - val_acc: 0.9140\n",
      "Epoch 18/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.9008\n",
      "Epoch 00018: val_loss improved from 0.33776 to 0.32364, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/18-0.3236.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3216 - acc: 0.9009 - val_loss: 0.3236 - val_acc: 0.9171\n",
      "Epoch 19/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9025\n",
      "Epoch 00019: val_loss did not improve from 0.32364\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.3102 - acc: 0.9025 - val_loss: 0.3444 - val_acc: 0.9117\n",
      "Epoch 20/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9067\n",
      "Epoch 00020: val_loss did not improve from 0.32364\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.3002 - acc: 0.9066 - val_loss: 0.3395 - val_acc: 0.9154\n",
      "Epoch 21/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9079\n",
      "Epoch 00021: val_loss improved from 0.32364 to 0.32047, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/21-0.3205.hdf5\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.2939 - acc: 0.9078 - val_loss: 0.3205 - val_acc: 0.9182\n",
      "Epoch 22/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9106\n",
      "Epoch 00022: val_loss did not improve from 0.32047\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.2857 - acc: 0.9107 - val_loss: 0.3331 - val_acc: 0.9119\n",
      "Epoch 23/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9155\n",
      "Epoch 00023: val_loss did not improve from 0.32047\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2742 - acc: 0.9155 - val_loss: 0.3224 - val_acc: 0.9166\n",
      "Epoch 24/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9149\n",
      "Epoch 00024: val_loss did not improve from 0.32047\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2721 - acc: 0.9149 - val_loss: 0.3462 - val_acc: 0.9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9170\n",
      "Epoch 00025: val_loss improved from 0.32047 to 0.31430, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/25-0.3143.hdf5\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.2654 - acc: 0.9170 - val_loss: 0.3143 - val_acc: 0.9140\n",
      "Epoch 26/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9188\n",
      "Epoch 00026: val_loss did not improve from 0.31430\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2564 - acc: 0.9188 - val_loss: 0.3635 - val_acc: 0.9001\n",
      "Epoch 27/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9212\n",
      "Epoch 00027: val_loss did not improve from 0.31430\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.2495 - acc: 0.9213 - val_loss: 0.3158 - val_acc: 0.9164\n",
      "Epoch 28/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9222\n",
      "Epoch 00028: val_loss improved from 0.31430 to 0.31149, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/28-0.3115.hdf5\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2471 - acc: 0.9222 - val_loss: 0.3115 - val_acc: 0.9262\n",
      "Epoch 29/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9236\n",
      "Epoch 00029: val_loss improved from 0.31149 to 0.29984, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/29-0.2998.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2410 - acc: 0.9236 - val_loss: 0.2998 - val_acc: 0.9227\n",
      "Epoch 30/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9263\n",
      "Epoch 00030: val_loss improved from 0.29984 to 0.29775, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/30-0.2977.hdf5\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2355 - acc: 0.9262 - val_loss: 0.2977 - val_acc: 0.9241\n",
      "Epoch 31/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9277\n",
      "Epoch 00031: val_loss improved from 0.29775 to 0.27317, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/31-0.2732.hdf5\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.2304 - acc: 0.9277 - val_loss: 0.2732 - val_acc: 0.9317\n",
      "Epoch 32/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9298\n",
      "Epoch 00032: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2195 - acc: 0.9298 - val_loss: 0.2904 - val_acc: 0.9271\n",
      "Epoch 33/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9287\n",
      "Epoch 00033: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2214 - acc: 0.9287 - val_loss: 0.3145 - val_acc: 0.9199\n",
      "Epoch 34/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9314\n",
      "Epoch 00034: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.2151 - acc: 0.9314 - val_loss: 0.2787 - val_acc: 0.9304\n",
      "Epoch 35/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9335\n",
      "Epoch 00035: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2133 - acc: 0.9334 - val_loss: 0.2840 - val_acc: 0.9243\n",
      "Epoch 36/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9344\n",
      "Epoch 00036: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2086 - acc: 0.9345 - val_loss: 0.2744 - val_acc: 0.9311\n",
      "Epoch 37/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9352\n",
      "Epoch 00037: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2058 - acc: 0.9353 - val_loss: 0.2865 - val_acc: 0.9264\n",
      "Epoch 38/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9369\n",
      "Epoch 00038: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.2005 - acc: 0.9369 - val_loss: 0.2799 - val_acc: 0.9304\n",
      "Epoch 39/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9358\n",
      "Epoch 00039: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1981 - acc: 0.9358 - val_loss: 0.2819 - val_acc: 0.9322\n",
      "Epoch 40/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9370\n",
      "Epoch 00040: val_loss did not improve from 0.27317\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1968 - acc: 0.9368 - val_loss: 0.2861 - val_acc: 0.9322\n",
      "Epoch 41/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9395\n",
      "Epoch 00041: val_loss improved from 0.27317 to 0.25746, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/41-0.2575.hdf5\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.1908 - acc: 0.9395 - val_loss: 0.2575 - val_acc: 0.9369\n",
      "Epoch 42/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9417\n",
      "Epoch 00042: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1873 - acc: 0.9417 - val_loss: 0.2826 - val_acc: 0.9320\n",
      "Epoch 43/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9408\n",
      "Epoch 00043: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1865 - acc: 0.9408 - val_loss: 0.2813 - val_acc: 0.9364\n",
      "Epoch 44/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9415\n",
      "Epoch 00044: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1829 - acc: 0.9413 - val_loss: 0.2684 - val_acc: 0.9371\n",
      "Epoch 45/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9440\n",
      "Epoch 00045: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1770 - acc: 0.9441 - val_loss: 0.2597 - val_acc: 0.9352\n",
      "Epoch 46/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9432\n",
      "Epoch 00046: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.1780 - acc: 0.9431 - val_loss: 0.2938 - val_acc: 0.9345\n",
      "Epoch 47/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9450\n",
      "Epoch 00047: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.1740 - acc: 0.9450 - val_loss: 0.2675 - val_acc: 0.9352\n",
      "Epoch 48/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9422\n",
      "Epoch 00048: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.1775 - acc: 0.9421 - val_loss: 0.2821 - val_acc: 0.9311\n",
      "Epoch 49/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9455\n",
      "Epoch 00049: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.1696 - acc: 0.9455 - val_loss: 0.2665 - val_acc: 0.9397\n",
      "Epoch 50/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9452\n",
      "Epoch 00050: val_loss did not improve from 0.25746\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1710 - acc: 0.9451 - val_loss: 0.2993 - val_acc: 0.9217\n",
      "Epoch 51/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9450\n",
      "Epoch 00051: val_loss improved from 0.25746 to 0.25354, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/51-0.2535.hdf5\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1685 - acc: 0.9450 - val_loss: 0.2535 - val_acc: 0.9376\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9467\n",
      "Epoch 00052: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1651 - acc: 0.9467 - val_loss: 0.2739 - val_acc: 0.9301\n",
      "Epoch 53/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9471\n",
      "Epoch 00053: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1644 - acc: 0.9470 - val_loss: 0.2663 - val_acc: 0.9341\n",
      "Epoch 54/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9495\n",
      "Epoch 00054: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1580 - acc: 0.9495 - val_loss: 0.2653 - val_acc: 0.9378\n",
      "Epoch 55/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9489\n",
      "Epoch 00055: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1571 - acc: 0.9489 - val_loss: 0.2829 - val_acc: 0.9294\n",
      "Epoch 56/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9511\n",
      "Epoch 00056: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.1562 - acc: 0.9511 - val_loss: 0.2651 - val_acc: 0.9378\n",
      "Epoch 57/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9504\n",
      "Epoch 00057: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1556 - acc: 0.9504 - val_loss: 0.2956 - val_acc: 0.9299\n",
      "Epoch 58/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9501\n",
      "Epoch 00058: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1541 - acc: 0.9501 - val_loss: 0.2800 - val_acc: 0.9355\n",
      "Epoch 59/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9524\n",
      "Epoch 00059: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1516 - acc: 0.9523 - val_loss: 0.3249 - val_acc: 0.9168\n",
      "Epoch 60/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9528\n",
      "Epoch 00060: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.1480 - acc: 0.9528 - val_loss: 0.2923 - val_acc: 0.9276\n",
      "Epoch 61/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9515\n",
      "Epoch 00061: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1503 - acc: 0.9514 - val_loss: 0.2741 - val_acc: 0.9362\n",
      "Epoch 62/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9532\n",
      "Epoch 00062: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1471 - acc: 0.9532 - val_loss: 0.2872 - val_acc: 0.9357\n",
      "Epoch 63/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9535\n",
      "Epoch 00063: val_loss did not improve from 0.25354\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1458 - acc: 0.9534 - val_loss: 0.2812 - val_acc: 0.9350\n",
      "Epoch 64/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9539\n",
      "Epoch 00064: val_loss improved from 0.25354 to 0.25324, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/64-0.2532.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.1442 - acc: 0.9540 - val_loss: 0.2532 - val_acc: 0.9385\n",
      "Epoch 65/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9551\n",
      "Epoch 00065: val_loss did not improve from 0.25324\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1401 - acc: 0.9551 - val_loss: 0.2909 - val_acc: 0.9350\n",
      "Epoch 66/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9559\n",
      "Epoch 00066: val_loss did not improve from 0.25324\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1370 - acc: 0.9559 - val_loss: 0.2690 - val_acc: 0.9359\n",
      "Epoch 67/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9555\n",
      "Epoch 00067: val_loss improved from 0.25324 to 0.25260, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/67-0.2526.hdf5\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1400 - acc: 0.9554 - val_loss: 0.2526 - val_acc: 0.9369\n",
      "Epoch 68/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9549\n",
      "Epoch 00068: val_loss did not improve from 0.25260\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1382 - acc: 0.9549 - val_loss: 0.2951 - val_acc: 0.9366\n",
      "Epoch 69/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9560\n",
      "Epoch 00069: val_loss did not improve from 0.25260\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1376 - acc: 0.9560 - val_loss: 0.2646 - val_acc: 0.9371\n",
      "Epoch 70/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9557\n",
      "Epoch 00070: val_loss did not improve from 0.25260\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1350 - acc: 0.9556 - val_loss: 0.2873 - val_acc: 0.9248\n",
      "Epoch 71/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9579\n",
      "Epoch 00071: val_loss did not improve from 0.25260\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1323 - acc: 0.9579 - val_loss: 0.2705 - val_acc: 0.9350\n",
      "Epoch 72/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9569\n",
      "Epoch 00072: val_loss did not improve from 0.25260\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1353 - acc: 0.9569 - val_loss: 0.2739 - val_acc: 0.9338\n",
      "Epoch 73/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9582\n",
      "Epoch 00073: val_loss improved from 0.25260 to 0.25063, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/73-0.2506.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1319 - acc: 0.9581 - val_loss: 0.2506 - val_acc: 0.9418\n",
      "Epoch 74/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9587\n",
      "Epoch 00074: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1275 - acc: 0.9587 - val_loss: 0.2649 - val_acc: 0.9411\n",
      "Epoch 75/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9586\n",
      "Epoch 00075: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1288 - acc: 0.9586 - val_loss: 0.2583 - val_acc: 0.9432\n",
      "Epoch 76/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9590\n",
      "Epoch 00076: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1283 - acc: 0.9591 - val_loss: 0.2605 - val_acc: 0.9397\n",
      "Epoch 77/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9574\n",
      "Epoch 00077: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1314 - acc: 0.9574 - val_loss: 0.2988 - val_acc: 0.9236\n",
      "Epoch 78/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9606\n",
      "Epoch 00078: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1241 - acc: 0.9606 - val_loss: 0.2553 - val_acc: 0.9420\n",
      "Epoch 79/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9595\n",
      "Epoch 00079: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1239 - acc: 0.9595 - val_loss: 0.2719 - val_acc: 0.9350\n",
      "Epoch 80/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9595\n",
      "Epoch 00080: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1260 - acc: 0.9595 - val_loss: 0.2665 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9619\n",
      "Epoch 00081: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1206 - acc: 0.9619 - val_loss: 0.2829 - val_acc: 0.9355\n",
      "Epoch 82/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9607\n",
      "Epoch 00082: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1201 - acc: 0.9608 - val_loss: 0.2559 - val_acc: 0.9411\n",
      "Epoch 83/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9605\n",
      "Epoch 00083: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1223 - acc: 0.9605 - val_loss: 0.2679 - val_acc: 0.9392\n",
      "Epoch 84/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9631\n",
      "Epoch 00084: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1158 - acc: 0.9631 - val_loss: 0.2810 - val_acc: 0.9378\n",
      "Epoch 85/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9631\n",
      "Epoch 00085: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1147 - acc: 0.9631 - val_loss: 0.3120 - val_acc: 0.9250\n",
      "Epoch 86/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9626\n",
      "Epoch 00086: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.1152 - acc: 0.9626 - val_loss: 0.2858 - val_acc: 0.9304\n",
      "Epoch 87/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9637\n",
      "Epoch 00087: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1165 - acc: 0.9636 - val_loss: 0.2751 - val_acc: 0.9366\n",
      "Epoch 88/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9626\n",
      "Epoch 00088: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1166 - acc: 0.9626 - val_loss: 0.2544 - val_acc: 0.9413\n",
      "Epoch 89/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9633\n",
      "Epoch 00089: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1152 - acc: 0.9632 - val_loss: 0.2636 - val_acc: 0.9420\n",
      "Epoch 90/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9635\n",
      "Epoch 00090: val_loss did not improve from 0.25063\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1142 - acc: 0.9635 - val_loss: 0.2629 - val_acc: 0.9436\n",
      "Epoch 91/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9645\n",
      "Epoch 00091: val_loss improved from 0.25063 to 0.24639, saving model to model/checkpoint/2D_CNN_3_only_conv_DO_BN_checkpoint/91-0.2464.hdf5\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1117 - acc: 0.9645 - val_loss: 0.2464 - val_acc: 0.9448\n",
      "Epoch 92/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9657\n",
      "Epoch 00092: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1084 - acc: 0.9657 - val_loss: 0.2827 - val_acc: 0.9357\n",
      "Epoch 93/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9623\n",
      "Epoch 00093: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1112 - acc: 0.9623 - val_loss: 0.2753 - val_acc: 0.9411\n",
      "Epoch 94/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9665\n",
      "Epoch 00094: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1073 - acc: 0.9664 - val_loss: 0.2679 - val_acc: 0.9415\n",
      "Epoch 95/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9669\n",
      "Epoch 00095: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1063 - acc: 0.9669 - val_loss: 0.2506 - val_acc: 0.9443\n",
      "Epoch 96/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9646\n",
      "Epoch 00096: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1077 - acc: 0.9647 - val_loss: 0.2583 - val_acc: 0.9420\n",
      "Epoch 97/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9658\n",
      "Epoch 00097: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1064 - acc: 0.9657 - val_loss: 0.2519 - val_acc: 0.9427\n",
      "Epoch 98/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9650\n",
      "Epoch 00098: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1083 - acc: 0.9650 - val_loss: 0.2955 - val_acc: 0.9359\n",
      "Epoch 99/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9659\n",
      "Epoch 00099: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1071 - acc: 0.9658 - val_loss: 0.2505 - val_acc: 0.9448\n",
      "Epoch 100/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9636\n",
      "Epoch 00100: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1106 - acc: 0.9636 - val_loss: 0.2487 - val_acc: 0.9418\n",
      "Epoch 101/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9669\n",
      "Epoch 00101: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1052 - acc: 0.9669 - val_loss: 0.2688 - val_acc: 0.9364\n",
      "Epoch 102/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9650\n",
      "Epoch 00102: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1066 - acc: 0.9650 - val_loss: 0.2541 - val_acc: 0.9418\n",
      "Epoch 103/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9667\n",
      "Epoch 00103: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1019 - acc: 0.9666 - val_loss: 0.2710 - val_acc: 0.9369\n",
      "Epoch 104/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9677\n",
      "Epoch 00104: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1020 - acc: 0.9677 - val_loss: 0.2550 - val_acc: 0.9429\n",
      "Epoch 105/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9672\n",
      "Epoch 00105: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1014 - acc: 0.9672 - val_loss: 0.2568 - val_acc: 0.9432\n",
      "Epoch 106/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9674\n",
      "Epoch 00106: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1008 - acc: 0.9674 - val_loss: 0.2881 - val_acc: 0.9317\n",
      "Epoch 107/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9678\n",
      "Epoch 00107: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0971 - acc: 0.9678 - val_loss: 0.2644 - val_acc: 0.9376\n",
      "Epoch 108/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9677\n",
      "Epoch 00108: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0999 - acc: 0.9677 - val_loss: 0.2689 - val_acc: 0.9415\n",
      "Epoch 109/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9683\n",
      "Epoch 00109: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0994 - acc: 0.9683 - val_loss: 0.2621 - val_acc: 0.9413\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9693\n",
      "Epoch 00110: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0988 - acc: 0.9693 - val_loss: 0.2788 - val_acc: 0.9390\n",
      "Epoch 111/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9694\n",
      "Epoch 00111: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0962 - acc: 0.9694 - val_loss: 0.2620 - val_acc: 0.9432\n",
      "Epoch 112/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9701\n",
      "Epoch 00112: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0939 - acc: 0.9702 - val_loss: 0.2573 - val_acc: 0.9429\n",
      "Epoch 113/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9697\n",
      "Epoch 00113: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0928 - acc: 0.9697 - val_loss: 0.2578 - val_acc: 0.9425\n",
      "Epoch 114/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9684\n",
      "Epoch 00114: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0975 - acc: 0.9684 - val_loss: 0.2620 - val_acc: 0.9446\n",
      "Epoch 115/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9693\n",
      "Epoch 00115: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0957 - acc: 0.9694 - val_loss: 0.2746 - val_acc: 0.9348\n",
      "Epoch 116/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9697\n",
      "Epoch 00116: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0934 - acc: 0.9697 - val_loss: 0.3313 - val_acc: 0.9238\n",
      "Epoch 117/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9692\n",
      "Epoch 00117: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0956 - acc: 0.9692 - val_loss: 0.2603 - val_acc: 0.9413\n",
      "Epoch 118/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9702\n",
      "Epoch 00118: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0933 - acc: 0.9702 - val_loss: 0.2621 - val_acc: 0.9432\n",
      "Epoch 119/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9698\n",
      "Epoch 00119: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0949 - acc: 0.9699 - val_loss: 0.2520 - val_acc: 0.9441\n",
      "Epoch 120/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9703\n",
      "Epoch 00120: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0923 - acc: 0.9702 - val_loss: 0.2691 - val_acc: 0.9434\n",
      "Epoch 121/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9707\n",
      "Epoch 00121: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0920 - acc: 0.9707 - val_loss: 0.2523 - val_acc: 0.9436\n",
      "Epoch 122/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9710\n",
      "Epoch 00122: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0910 - acc: 0.9710 - val_loss: 0.3107 - val_acc: 0.9236\n",
      "Epoch 123/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9707\n",
      "Epoch 00123: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0902 - acc: 0.9706 - val_loss: 0.2685 - val_acc: 0.9394\n",
      "Epoch 124/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9703\n",
      "Epoch 00124: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0901 - acc: 0.9703 - val_loss: 0.2803 - val_acc: 0.9422\n",
      "Epoch 125/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9712\n",
      "Epoch 00125: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0908 - acc: 0.9712 - val_loss: 0.2712 - val_acc: 0.9413\n",
      "Epoch 126/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9717\n",
      "Epoch 00126: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0890 - acc: 0.9718 - val_loss: 0.2724 - val_acc: 0.9415\n",
      "Epoch 127/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9731\n",
      "Epoch 00127: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0843 - acc: 0.9731 - val_loss: 0.2753 - val_acc: 0.9392\n",
      "Epoch 128/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9706\n",
      "Epoch 00128: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0892 - acc: 0.9706 - val_loss: 0.2574 - val_acc: 0.9457\n",
      "Epoch 129/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9723\n",
      "Epoch 00129: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.0881 - acc: 0.9723 - val_loss: 0.2574 - val_acc: 0.9406\n",
      "Epoch 130/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9717\n",
      "Epoch 00130: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0873 - acc: 0.9717 - val_loss: 0.2782 - val_acc: 0.9448\n",
      "Epoch 131/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9726\n",
      "Epoch 00131: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0866 - acc: 0.9726 - val_loss: 0.2520 - val_acc: 0.9443\n",
      "Epoch 132/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9736\n",
      "Epoch 00132: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0838 - acc: 0.9736 - val_loss: 0.2734 - val_acc: 0.9387\n",
      "Epoch 133/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9731\n",
      "Epoch 00133: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0831 - acc: 0.9731 - val_loss: 0.2637 - val_acc: 0.9450\n",
      "Epoch 134/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9729\n",
      "Epoch 00134: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0862 - acc: 0.9728 - val_loss: 0.2755 - val_acc: 0.9436\n",
      "Epoch 135/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9734\n",
      "Epoch 00135: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0822 - acc: 0.9733 - val_loss: 0.3044 - val_acc: 0.9343\n",
      "Epoch 136/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9734\n",
      "Epoch 00136: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0845 - acc: 0.9734 - val_loss: 0.2621 - val_acc: 0.9420\n",
      "Epoch 137/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9719\n",
      "Epoch 00137: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0861 - acc: 0.9720 - val_loss: 0.2776 - val_acc: 0.9413\n",
      "Epoch 138/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9725\n",
      "Epoch 00138: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0869 - acc: 0.9726 - val_loss: 0.3076 - val_acc: 0.9348\n",
      "Epoch 139/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9734\n",
      "Epoch 00139: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0821 - acc: 0.9734 - val_loss: 0.2658 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9742\n",
      "Epoch 00140: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0807 - acc: 0.9742 - val_loss: 0.2852 - val_acc: 0.9394\n",
      "Epoch 141/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9746\n",
      "Epoch 00141: val_loss did not improve from 0.24639\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0790 - acc: 0.9746 - val_loss: 0.2848 - val_acc: 0.9415\n",
      "\n",
      "3 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 267us/sample - loss: 0.3214 - acc: 0.9126\n",
      "Loss: 0.3213669447388852 Accuracy: 0.9125649\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3097 - acc: 0.3023\n",
      "Epoch 00001: val_loss improved from inf to 1.44584, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/01-1.4458.hdf5\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 2.3096 - acc: 0.3023 - val_loss: 1.4458 - val_acc: 0.5460\n",
      "Epoch 2/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4696 - acc: 0.5284\n",
      "Epoch 00002: val_loss improved from 1.44584 to 0.98767, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/02-0.9877.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.4686 - acc: 0.5285 - val_loss: 0.9877 - val_acc: 0.7107\n",
      "Epoch 3/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0490 - acc: 0.6668\n",
      "Epoch 00003: val_loss improved from 0.98767 to 0.69226, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/03-0.6923.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0488 - acc: 0.6669 - val_loss: 0.6923 - val_acc: 0.8076\n",
      "Epoch 4/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8280 - acc: 0.7391\n",
      "Epoch 00004: val_loss improved from 0.69226 to 0.57068, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/04-0.5707.hdf5\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.8278 - acc: 0.7392 - val_loss: 0.5707 - val_acc: 0.8374\n",
      "Epoch 5/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.7803\n",
      "Epoch 00005: val_loss improved from 0.57068 to 0.48609, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/05-0.4861.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.6983 - acc: 0.7803 - val_loss: 0.4861 - val_acc: 0.8686\n",
      "Epoch 6/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6109 - acc: 0.8086\n",
      "Epoch 00006: val_loss improved from 0.48609 to 0.44765, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/06-0.4476.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.6110 - acc: 0.8086 - val_loss: 0.4476 - val_acc: 0.8712\n",
      "Epoch 7/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8280\n",
      "Epoch 00007: val_loss improved from 0.44765 to 0.41554, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/07-0.4155.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.5475 - acc: 0.8280 - val_loss: 0.4155 - val_acc: 0.8831\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8433\n",
      "Epoch 00008: val_loss improved from 0.41554 to 0.39985, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/08-0.3999.hdf5\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.4954 - acc: 0.8434 - val_loss: 0.3999 - val_acc: 0.8905\n",
      "Epoch 9/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8563\n",
      "Epoch 00009: val_loss improved from 0.39985 to 0.34526, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/09-0.3453.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.4576 - acc: 0.8563 - val_loss: 0.3453 - val_acc: 0.9050\n",
      "Epoch 10/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8687\n",
      "Epoch 00010: val_loss improved from 0.34526 to 0.33422, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/10-0.3342.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.4202 - acc: 0.8686 - val_loss: 0.3342 - val_acc: 0.9080\n",
      "Epoch 11/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8787\n",
      "Epoch 00011: val_loss improved from 0.33422 to 0.30012, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/11-0.3001.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.3885 - acc: 0.8787 - val_loss: 0.3001 - val_acc: 0.9178\n",
      "Epoch 12/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8873\n",
      "Epoch 00012: val_loss did not improve from 0.30012\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.3624 - acc: 0.8873 - val_loss: 0.3084 - val_acc: 0.9157\n",
      "Epoch 13/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8919\n",
      "Epoch 00013: val_loss improved from 0.30012 to 0.29402, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/13-0.2940.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.3429 - acc: 0.8919 - val_loss: 0.2940 - val_acc: 0.9166\n",
      "Epoch 14/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.8974\n",
      "Epoch 00014: val_loss improved from 0.29402 to 0.26282, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/14-0.2628.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.3260 - acc: 0.8974 - val_loss: 0.2628 - val_acc: 0.9285\n",
      "Epoch 15/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9038\n",
      "Epoch 00015: val_loss did not improve from 0.26282\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.3067 - acc: 0.9038 - val_loss: 0.2789 - val_acc: 0.9222\n",
      "Epoch 16/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9067\n",
      "Epoch 00016: val_loss did not improve from 0.26282\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.2985 - acc: 0.9067 - val_loss: 0.2746 - val_acc: 0.9252\n",
      "Epoch 17/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9116\n",
      "Epoch 00017: val_loss did not improve from 0.26282\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.2825 - acc: 0.9116 - val_loss: 0.2692 - val_acc: 0.9252\n",
      "Epoch 18/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9150\n",
      "Epoch 00018: val_loss improved from 0.26282 to 0.25043, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/18-0.2504.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.2733 - acc: 0.9151 - val_loss: 0.2504 - val_acc: 0.9313\n",
      "Epoch 19/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9174\n",
      "Epoch 00019: val_loss improved from 0.25043 to 0.24152, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/19-0.2415.hdf5\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2630 - acc: 0.9175 - val_loss: 0.2415 - val_acc: 0.9331\n",
      "Epoch 20/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9222\n",
      "Epoch 00020: val_loss improved from 0.24152 to 0.24001, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/20-0.2400.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.2506 - acc: 0.9223 - val_loss: 0.2400 - val_acc: 0.9315\n",
      "Epoch 21/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9237\n",
      "Epoch 00021: val_loss improved from 0.24001 to 0.22608, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/21-0.2261.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.2424 - acc: 0.9237 - val_loss: 0.2261 - val_acc: 0.9401\n",
      "Epoch 22/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9270\n",
      "Epoch 00022: val_loss improved from 0.22608 to 0.21506, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/22-0.2151.hdf5\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.2324 - acc: 0.9270 - val_loss: 0.2151 - val_acc: 0.9408\n",
      "Epoch 23/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9295\n",
      "Epoch 00023: val_loss did not improve from 0.21506\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2252 - acc: 0.9295 - val_loss: 0.2186 - val_acc: 0.9390\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9305\n",
      "Epoch 00024: val_loss improved from 0.21506 to 0.21397, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/24-0.2140.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.2179 - acc: 0.9306 - val_loss: 0.2140 - val_acc: 0.9401\n",
      "Epoch 25/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9328\n",
      "Epoch 00025: val_loss did not improve from 0.21397\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.2137 - acc: 0.9328 - val_loss: 0.2293 - val_acc: 0.9341\n",
      "Epoch 26/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9335\n",
      "Epoch 00026: val_loss improved from 0.21397 to 0.21142, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/26-0.2114.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.2079 - acc: 0.9335 - val_loss: 0.2114 - val_acc: 0.9399\n",
      "Epoch 27/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9371\n",
      "Epoch 00027: val_loss did not improve from 0.21142\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.2019 - acc: 0.9371 - val_loss: 0.2354 - val_acc: 0.9327\n",
      "Epoch 28/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9389\n",
      "Epoch 00028: val_loss did not improve from 0.21142\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1945 - acc: 0.9389 - val_loss: 0.2136 - val_acc: 0.9401\n",
      "Epoch 29/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9407\n",
      "Epoch 00029: val_loss improved from 0.21142 to 0.20554, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/29-0.2055.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.1899 - acc: 0.9407 - val_loss: 0.2055 - val_acc: 0.9448\n",
      "Epoch 30/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9414\n",
      "Epoch 00030: val_loss improved from 0.20554 to 0.20161, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/30-0.2016.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1885 - acc: 0.9414 - val_loss: 0.2016 - val_acc: 0.9481\n",
      "Epoch 31/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9431\n",
      "Epoch 00031: val_loss did not improve from 0.20161\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1808 - acc: 0.9431 - val_loss: 0.2031 - val_acc: 0.9446\n",
      "Epoch 32/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9442\n",
      "Epoch 00032: val_loss did not improve from 0.20161\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1760 - acc: 0.9442 - val_loss: 0.2053 - val_acc: 0.9453\n",
      "Epoch 33/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9444\n",
      "Epoch 00033: val_loss did not improve from 0.20161\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1733 - acc: 0.9444 - val_loss: 0.2109 - val_acc: 0.9415\n",
      "Epoch 34/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9463\n",
      "Epoch 00034: val_loss improved from 0.20161 to 0.20033, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/34-0.2003.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1677 - acc: 0.9463 - val_loss: 0.2003 - val_acc: 0.9474\n",
      "Epoch 35/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9479\n",
      "Epoch 00035: val_loss improved from 0.20033 to 0.19318, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/35-0.1932.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.1657 - acc: 0.9479 - val_loss: 0.1932 - val_acc: 0.9439\n",
      "Epoch 36/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9491\n",
      "Epoch 00036: val_loss did not improve from 0.19318\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1630 - acc: 0.9491 - val_loss: 0.1973 - val_acc: 0.9485\n",
      "Epoch 37/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9500\n",
      "Epoch 00037: val_loss did not improve from 0.19318\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.1554 - acc: 0.9501 - val_loss: 0.1993 - val_acc: 0.9443\n",
      "Epoch 38/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9508\n",
      "Epoch 00038: val_loss improved from 0.19318 to 0.19241, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/38-0.1924.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1543 - acc: 0.9506 - val_loss: 0.1924 - val_acc: 0.9495\n",
      "Epoch 39/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9512\n",
      "Epoch 00039: val_loss did not improve from 0.19241\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.1516 - acc: 0.9511 - val_loss: 0.2115 - val_acc: 0.9457\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9521\n",
      "Epoch 00040: val_loss did not improve from 0.19241\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1484 - acc: 0.9522 - val_loss: 0.2069 - val_acc: 0.9469\n",
      "Epoch 41/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9530\n",
      "Epoch 00041: val_loss did not improve from 0.19241\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.1473 - acc: 0.9531 - val_loss: 0.2020 - val_acc: 0.9443\n",
      "Epoch 42/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9541\n",
      "Epoch 00042: val_loss improved from 0.19241 to 0.18818, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/42-0.1882.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.1431 - acc: 0.9540 - val_loss: 0.1882 - val_acc: 0.9490\n",
      "Epoch 43/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9534\n",
      "Epoch 00043: val_loss improved from 0.18818 to 0.18573, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/43-0.1857.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1416 - acc: 0.9534 - val_loss: 0.1857 - val_acc: 0.9529\n",
      "Epoch 44/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9555\n",
      "Epoch 00044: val_loss did not improve from 0.18573\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.1383 - acc: 0.9555 - val_loss: 0.1894 - val_acc: 0.9518\n",
      "Epoch 45/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9569\n",
      "Epoch 00045: val_loss did not improve from 0.18573\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.1342 - acc: 0.9569 - val_loss: 0.1965 - val_acc: 0.9492\n",
      "Epoch 46/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9571\n",
      "Epoch 00046: val_loss improved from 0.18573 to 0.18209, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/46-0.1821.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1341 - acc: 0.9572 - val_loss: 0.1821 - val_acc: 0.9504\n",
      "Epoch 47/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9575\n",
      "Epoch 00047: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1324 - acc: 0.9575 - val_loss: 0.1855 - val_acc: 0.9492\n",
      "Epoch 48/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9598\n",
      "Epoch 00048: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1241 - acc: 0.9598 - val_loss: 0.1887 - val_acc: 0.9490\n",
      "Epoch 49/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9594\n",
      "Epoch 00049: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.1261 - acc: 0.9594 - val_loss: 0.1971 - val_acc: 0.9488\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9601\n",
      "Epoch 00050: val_loss improved from 0.18209 to 0.18163, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/50-0.1816.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1236 - acc: 0.9601 - val_loss: 0.1816 - val_acc: 0.9511\n",
      "Epoch 51/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9606\n",
      "Epoch 00051: val_loss did not improve from 0.18163\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1231 - acc: 0.9607 - val_loss: 0.1874 - val_acc: 0.9525\n",
      "Epoch 52/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9620\n",
      "Epoch 00052: val_loss improved from 0.18163 to 0.17298, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/52-0.1730.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1184 - acc: 0.9620 - val_loss: 0.1730 - val_acc: 0.9539\n",
      "Epoch 53/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9613\n",
      "Epoch 00053: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.1198 - acc: 0.9614 - val_loss: 0.1866 - val_acc: 0.9511\n",
      "Epoch 54/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9627\n",
      "Epoch 00054: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.1170 - acc: 0.9627 - val_loss: 0.1899 - val_acc: 0.9495\n",
      "Epoch 55/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9641\n",
      "Epoch 00055: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1113 - acc: 0.9641 - val_loss: 0.1798 - val_acc: 0.9529\n",
      "Epoch 56/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9637\n",
      "Epoch 00056: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1130 - acc: 0.9637 - val_loss: 0.1740 - val_acc: 0.9534\n",
      "Epoch 57/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9638\n",
      "Epoch 00057: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.1136 - acc: 0.9637 - val_loss: 0.1763 - val_acc: 0.9557\n",
      "Epoch 58/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9652\n",
      "Epoch 00058: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1093 - acc: 0.9652 - val_loss: 0.1817 - val_acc: 0.9560\n",
      "Epoch 59/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9667\n",
      "Epoch 00059: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1054 - acc: 0.9667 - val_loss: 0.2014 - val_acc: 0.9476\n",
      "Epoch 60/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9665\n",
      "Epoch 00060: val_loss improved from 0.17298 to 0.17216, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/60-0.1722.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1045 - acc: 0.9665 - val_loss: 0.1722 - val_acc: 0.9555\n",
      "Epoch 61/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9672\n",
      "Epoch 00061: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1037 - acc: 0.9672 - val_loss: 0.1895 - val_acc: 0.9485\n",
      "Epoch 62/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9654\n",
      "Epoch 00062: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1060 - acc: 0.9654 - val_loss: 0.1815 - val_acc: 0.9553\n",
      "Epoch 63/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9683\n",
      "Epoch 00063: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.1020 - acc: 0.9683 - val_loss: 0.2037 - val_acc: 0.9511\n",
      "Epoch 64/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9673\n",
      "Epoch 00064: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.1004 - acc: 0.9674 - val_loss: 0.1784 - val_acc: 0.9520\n",
      "Epoch 65/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9687\n",
      "Epoch 00065: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0964 - acc: 0.9687 - val_loss: 0.1813 - val_acc: 0.9529\n",
      "Epoch 66/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9680\n",
      "Epoch 00066: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0970 - acc: 0.9680 - val_loss: 0.1800 - val_acc: 0.9527\n",
      "Epoch 67/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9694\n",
      "Epoch 00067: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0962 - acc: 0.9693 - val_loss: 0.1888 - val_acc: 0.9515\n",
      "Epoch 68/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9696\n",
      "Epoch 00068: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0930 - acc: 0.9696 - val_loss: 0.2097 - val_acc: 0.9481\n",
      "Epoch 69/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9693\n",
      "Epoch 00069: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0959 - acc: 0.9693 - val_loss: 0.1869 - val_acc: 0.9557\n",
      "Epoch 70/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9720\n",
      "Epoch 00070: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0867 - acc: 0.9720 - val_loss: 0.1878 - val_acc: 0.9506\n",
      "Epoch 71/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9696\n",
      "Epoch 00071: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0940 - acc: 0.9697 - val_loss: 0.1857 - val_acc: 0.9562\n",
      "Epoch 72/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9724\n",
      "Epoch 00072: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0878 - acc: 0.9724 - val_loss: 0.1877 - val_acc: 0.9534\n",
      "Epoch 73/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9712\n",
      "Epoch 00073: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0883 - acc: 0.9712 - val_loss: 0.1794 - val_acc: 0.9550\n",
      "Epoch 74/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9705\n",
      "Epoch 00074: val_loss did not improve from 0.17216\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0917 - acc: 0.9705 - val_loss: 0.1770 - val_acc: 0.9581\n",
      "Epoch 75/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9715\n",
      "Epoch 00075: val_loss improved from 0.17216 to 0.16764, saving model to model/checkpoint/2D_CNN_4_only_conv_DO_BN_checkpoint/75-0.1676.hdf5\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0864 - acc: 0.9715 - val_loss: 0.1676 - val_acc: 0.9543\n",
      "Epoch 76/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9705\n",
      "Epoch 00076: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0902 - acc: 0.9705 - val_loss: 0.1868 - val_acc: 0.9522\n",
      "Epoch 77/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9730\n",
      "Epoch 00077: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0830 - acc: 0.9730 - val_loss: 0.1940 - val_acc: 0.9511\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9733\n",
      "Epoch 00078: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0830 - acc: 0.9733 - val_loss: 0.1751 - val_acc: 0.9574\n",
      "Epoch 79/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9722\n",
      "Epoch 00079: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0869 - acc: 0.9722 - val_loss: 0.1835 - val_acc: 0.9564\n",
      "Epoch 80/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9730\n",
      "Epoch 00080: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0851 - acc: 0.9729 - val_loss: 0.1811 - val_acc: 0.9560\n",
      "Epoch 81/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9748\n",
      "Epoch 00081: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0794 - acc: 0.9748 - val_loss: 0.2007 - val_acc: 0.9502\n",
      "Epoch 82/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9738\n",
      "Epoch 00082: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0821 - acc: 0.9738 - val_loss: 0.1753 - val_acc: 0.9574\n",
      "Epoch 83/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9739\n",
      "Epoch 00083: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0785 - acc: 0.9740 - val_loss: 0.1816 - val_acc: 0.9534\n",
      "Epoch 84/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9752\n",
      "Epoch 00084: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0785 - acc: 0.9752 - val_loss: 0.1899 - val_acc: 0.9534\n",
      "Epoch 85/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9737\n",
      "Epoch 00085: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0801 - acc: 0.9737 - val_loss: 0.1931 - val_acc: 0.9560\n",
      "Epoch 86/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9757\n",
      "Epoch 00086: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0755 - acc: 0.9757 - val_loss: 0.1791 - val_acc: 0.9567\n",
      "Epoch 87/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9759\n",
      "Epoch 00087: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0770 - acc: 0.9758 - val_loss: 0.1844 - val_acc: 0.9550\n",
      "Epoch 88/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9753\n",
      "Epoch 00088: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0754 - acc: 0.9753 - val_loss: 0.1720 - val_acc: 0.9571\n",
      "Epoch 89/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9758\n",
      "Epoch 00089: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0740 - acc: 0.9758 - val_loss: 0.1853 - val_acc: 0.9553\n",
      "Epoch 90/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9754\n",
      "Epoch 00090: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0730 - acc: 0.9754 - val_loss: 0.2051 - val_acc: 0.9522\n",
      "Epoch 91/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9762\n",
      "Epoch 00091: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0741 - acc: 0.9761 - val_loss: 0.1930 - val_acc: 0.9546\n",
      "Epoch 92/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9765\n",
      "Epoch 00092: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0749 - acc: 0.9764 - val_loss: 0.2067 - val_acc: 0.9532\n",
      "Epoch 93/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9760\n",
      "Epoch 00093: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0746 - acc: 0.9760 - val_loss: 0.1954 - val_acc: 0.9543\n",
      "Epoch 94/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9778\n",
      "Epoch 00094: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0703 - acc: 0.9778 - val_loss: 0.1863 - val_acc: 0.9548\n",
      "Epoch 95/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9772\n",
      "Epoch 00095: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0721 - acc: 0.9772 - val_loss: 0.1938 - val_acc: 0.9553\n",
      "Epoch 96/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9774\n",
      "Epoch 00096: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0710 - acc: 0.9774 - val_loss: 0.1788 - val_acc: 0.9588\n",
      "Epoch 97/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9765\n",
      "Epoch 00097: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0732 - acc: 0.9765 - val_loss: 0.1866 - val_acc: 0.9583\n",
      "Epoch 98/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9777\n",
      "Epoch 00098: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0695 - acc: 0.9777 - val_loss: 0.1742 - val_acc: 0.9581\n",
      "Epoch 99/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9783\n",
      "Epoch 00099: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0646 - acc: 0.9783 - val_loss: 0.1984 - val_acc: 0.9553\n",
      "Epoch 100/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9772\n",
      "Epoch 00100: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0707 - acc: 0.9772 - val_loss: 0.2030 - val_acc: 0.9532\n",
      "Epoch 101/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9790\n",
      "Epoch 00101: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0677 - acc: 0.9790 - val_loss: 0.1819 - val_acc: 0.9595\n",
      "Epoch 102/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9780\n",
      "Epoch 00102: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0697 - acc: 0.9780 - val_loss: 0.1841 - val_acc: 0.9592\n",
      "Epoch 103/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9798\n",
      "Epoch 00103: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0655 - acc: 0.9798 - val_loss: 0.1882 - val_acc: 0.9590\n",
      "Epoch 104/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9785\n",
      "Epoch 00104: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0656 - acc: 0.9785 - val_loss: 0.1997 - val_acc: 0.9562\n",
      "Epoch 105/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9783\n",
      "Epoch 00105: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0682 - acc: 0.9783 - val_loss: 0.2095 - val_acc: 0.9525\n",
      "Epoch 106/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9788\n",
      "Epoch 00106: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0652 - acc: 0.9788 - val_loss: 0.1914 - val_acc: 0.9583\n",
      "Epoch 107/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9789\n",
      "Epoch 00107: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0651 - acc: 0.9789 - val_loss: 0.1978 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9787\n",
      "Epoch 00108: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0657 - acc: 0.9786 - val_loss: 0.1943 - val_acc: 0.9581\n",
      "Epoch 109/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9798\n",
      "Epoch 00109: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0649 - acc: 0.9798 - val_loss: 0.1948 - val_acc: 0.9581\n",
      "Epoch 110/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9809\n",
      "Epoch 00110: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0611 - acc: 0.9809 - val_loss: 0.1887 - val_acc: 0.9585\n",
      "Epoch 111/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9808\n",
      "Epoch 00111: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0618 - acc: 0.9809 - val_loss: 0.1961 - val_acc: 0.9569\n",
      "Epoch 112/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9804\n",
      "Epoch 00112: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0605 - acc: 0.9804 - val_loss: 0.1910 - val_acc: 0.9567\n",
      "Epoch 113/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9797\n",
      "Epoch 00113: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0637 - acc: 0.9797 - val_loss: 0.1890 - val_acc: 0.9578\n",
      "Epoch 114/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9806\n",
      "Epoch 00114: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0608 - acc: 0.9806 - val_loss: 0.2061 - val_acc: 0.9560\n",
      "Epoch 115/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9804\n",
      "Epoch 00115: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0603 - acc: 0.9804 - val_loss: 0.1932 - val_acc: 0.9578\n",
      "Epoch 116/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9815\n",
      "Epoch 00116: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0590 - acc: 0.9814 - val_loss: 0.1978 - val_acc: 0.9569\n",
      "Epoch 117/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9807\n",
      "Epoch 00117: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0608 - acc: 0.9807 - val_loss: 0.2069 - val_acc: 0.9553\n",
      "Epoch 118/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9810\n",
      "Epoch 00118: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0575 - acc: 0.9810 - val_loss: 0.1888 - val_acc: 0.9571\n",
      "Epoch 119/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9824\n",
      "Epoch 00119: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0551 - acc: 0.9824 - val_loss: 0.1849 - val_acc: 0.9588\n",
      "Epoch 120/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9802\n",
      "Epoch 00120: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0614 - acc: 0.9802 - val_loss: 0.1793 - val_acc: 0.9595\n",
      "Epoch 121/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9819\n",
      "Epoch 00121: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0599 - acc: 0.9819 - val_loss: 0.2014 - val_acc: 0.9564\n",
      "Epoch 122/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9825\n",
      "Epoch 00122: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0552 - acc: 0.9825 - val_loss: 0.1965 - val_acc: 0.9578\n",
      "Epoch 123/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9823\n",
      "Epoch 00123: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0572 - acc: 0.9823 - val_loss: 0.2408 - val_acc: 0.9492\n",
      "Epoch 124/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9825\n",
      "Epoch 00124: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0557 - acc: 0.9825 - val_loss: 0.1983 - val_acc: 0.9555\n",
      "Epoch 125/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9820\n",
      "Epoch 00125: val_loss did not improve from 0.16764\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0558 - acc: 0.9820 - val_loss: 0.1854 - val_acc: 0.9576\n",
      "\n",
      "4 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 295us/sample - loss: 0.2459 - acc: 0.9371\n",
      "Loss: 0.24588433228672973 Accuracy: 0.9370716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    model_name = '2D_CNN_{}_only_conv_DO_BN'.format(i)\n",
    "    model = build_2d_cnn_only_conv_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_2d_norm, y_train_onehot, batch_size=64, epochs=200, \n",
    "                     validation_data=[x_val_2d_norm, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print('{} Only Conv Model'.format(i))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_2d_norm, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2D_CNN_1_only_conv_DO_BN Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                780304    \n",
      "=================================================================\n",
      "Total params: 780,544\n",
      "Trainable params: 780,528\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 244us/sample - loss: 1.2333 - acc: 0.6741\n",
      "Loss: 1.2332580039310554 Accuracy: 0.6741433\n",
      "\n",
      "2D_CNN_2_only_conv_DO_BN Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                349200    \n",
      "=================================================================\n",
      "Total params: 352,720\n",
      "Trainable params: 352,672\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 253us/sample - loss: 0.6056 - acc: 0.8540\n",
      "Loss: 0.6055759042470502 Accuracy: 0.85399795\n",
      "\n",
      "2D_CNN_3_only_conv_DO_BN Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 58, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 58, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                133648    \n",
      "=================================================================\n",
      "Total params: 150,128\n",
      "Trainable params: 150,016\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 289us/sample - loss: 0.3214 - acc: 0.9126\n",
      "Loss: 0.3213669447388852 Accuracy: 0.9125649\n",
      "\n",
      "2D_CNN_4_only_conv_DO_BN Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 253, 95, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 253, 95, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 123, 44, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 123, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 58, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 58, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 25, 5, 64)         51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 25, 5, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 25, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                39952     \n",
      "=================================================================\n",
      "Total params: 107,952\n",
      "Trainable params: 107,712\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 324us/sample - loss: 0.2459 - acc: 0.9371\n",
      "Loss: 0.24588433228672973 Accuracy: 0.9370716\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    model_name = '2D_CNN_{}_only_conv_DO_BN'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_2d_norm, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
