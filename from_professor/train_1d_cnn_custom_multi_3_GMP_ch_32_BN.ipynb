{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 32)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 96)           384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1552        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 96)           384         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1552        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 32)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 128)          512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2064        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 32)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 160)          0           global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 160)          640         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2576        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 192)          768         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           3088        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 192)          768         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           3088        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4342 - acc: 0.2335\n",
      "Epoch 00001: val_loss improved from inf to 2.26445, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/001-2.2645.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 2.4341 - acc: 0.2334 - val_loss: 2.2645 - val_acc: 0.2809\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8841 - acc: 0.3973\n",
      "Epoch 00002: val_loss improved from 2.26445 to 1.77986, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/002-1.7799.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 1.8840 - acc: 0.3972 - val_loss: 1.7799 - val_acc: 0.4361\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6526 - acc: 0.4773\n",
      "Epoch 00003: val_loss improved from 1.77986 to 1.56133, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/003-1.5613.hdf5\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 1.6526 - acc: 0.4773 - val_loss: 1.5613 - val_acc: 0.5055\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4723 - acc: 0.5382\n",
      "Epoch 00004: val_loss improved from 1.56133 to 1.38867, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/004-1.3887.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 1.4723 - acc: 0.5381 - val_loss: 1.3887 - val_acc: 0.5784\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3361 - acc: 0.5823\n",
      "Epoch 00005: val_loss improved from 1.38867 to 1.28276, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/005-1.2828.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 1.3362 - acc: 0.5823 - val_loss: 1.2828 - val_acc: 0.6068\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2326 - acc: 0.6169\n",
      "Epoch 00006: val_loss improved from 1.28276 to 1.15552, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/006-1.1555.hdf5\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 1.2326 - acc: 0.6169 - val_loss: 1.1555 - val_acc: 0.6441\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1510 - acc: 0.6413\n",
      "Epoch 00007: val_loss improved from 1.15552 to 1.07284, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/007-1.0728.hdf5\n",
      "36805/36805 [==============================] - 35s 939us/sample - loss: 1.1509 - acc: 0.6413 - val_loss: 1.0728 - val_acc: 0.6709\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0899 - acc: 0.6612\n",
      "Epoch 00008: val_loss improved from 1.07284 to 1.03001, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/008-1.0300.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 1.0899 - acc: 0.6612 - val_loss: 1.0300 - val_acc: 0.6660\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0363 - acc: 0.6751\n",
      "Epoch 00009: val_loss improved from 1.03001 to 1.00391, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/009-1.0039.hdf5\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 1.0362 - acc: 0.6751 - val_loss: 1.0039 - val_acc: 0.6997\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9904 - acc: 0.6918\n",
      "Epoch 00010: val_loss improved from 1.00391 to 0.96858, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/010-0.9686.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.9905 - acc: 0.6917 - val_loss: 0.9686 - val_acc: 0.6890\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9497 - acc: 0.7037\n",
      "Epoch 00011: val_loss improved from 0.96858 to 0.89035, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/011-0.8904.hdf5\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.9503 - acc: 0.7034 - val_loss: 0.8904 - val_acc: 0.7242\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9206 - acc: 0.7131\n",
      "Epoch 00012: val_loss did not improve from 0.89035\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.9206 - acc: 0.7131 - val_loss: 0.8984 - val_acc: 0.7137\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8910 - acc: 0.7224\n",
      "Epoch 00013: val_loss improved from 0.89035 to 0.86731, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/013-0.8673.hdf5\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.8911 - acc: 0.7224 - val_loss: 0.8673 - val_acc: 0.7205\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8648 - acc: 0.7302\n",
      "Epoch 00014: val_loss improved from 0.86731 to 0.82611, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/014-0.8261.hdf5\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.8649 - acc: 0.7301 - val_loss: 0.8261 - val_acc: 0.7370\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8421 - acc: 0.7370\n",
      "Epoch 00015: val_loss did not improve from 0.82611\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.8422 - acc: 0.7370 - val_loss: 0.8351 - val_acc: 0.7417\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8214 - acc: 0.7441\n",
      "Epoch 00016: val_loss did not improve from 0.82611\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.8216 - acc: 0.7441 - val_loss: 0.8859 - val_acc: 0.7014\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8015 - acc: 0.7499\n",
      "Epoch 00017: val_loss improved from 0.82611 to 0.78570, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/017-0.7857.hdf5\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.8016 - acc: 0.7499 - val_loss: 0.7857 - val_acc: 0.7584\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7816 - acc: 0.7580\n",
      "Epoch 00018: val_loss did not improve from 0.78570\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.7816 - acc: 0.7580 - val_loss: 0.8008 - val_acc: 0.7435\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7666 - acc: 0.7620\n",
      "Epoch 00019: val_loss improved from 0.78570 to 0.76346, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/019-0.7635.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.7666 - acc: 0.7620 - val_loss: 0.7635 - val_acc: 0.7659\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7642\n",
      "Epoch 00020: val_loss did not improve from 0.76346\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.7577 - acc: 0.7642 - val_loss: 0.7771 - val_acc: 0.7622\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7714\n",
      "Epoch 00021: val_loss did not improve from 0.76346\n",
      "36805/36805 [==============================] - 35s 939us/sample - loss: 0.7397 - acc: 0.7714 - val_loss: 0.7788 - val_acc: 0.7543\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7306 - acc: 0.7727\n",
      "Epoch 00022: val_loss improved from 0.76346 to 0.73209, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/022-0.7321.hdf5\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.7305 - acc: 0.7727 - val_loss: 0.7321 - val_acc: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7185 - acc: 0.7757\n",
      "Epoch 00023: val_loss improved from 0.73209 to 0.71452, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/023-0.7145.hdf5\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.7185 - acc: 0.7757 - val_loss: 0.7145 - val_acc: 0.7824\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7034 - acc: 0.7802\n",
      "Epoch 00024: val_loss did not improve from 0.71452\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.7034 - acc: 0.7801 - val_loss: 0.7241 - val_acc: 0.7741\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7839\n",
      "Epoch 00025: val_loss did not improve from 0.71452\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.6974 - acc: 0.7839 - val_loss: 0.7274 - val_acc: 0.7785\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.7867\n",
      "Epoch 00026: val_loss improved from 0.71452 to 0.69344, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/026-0.6934.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.6861 - acc: 0.7867 - val_loss: 0.6934 - val_acc: 0.7831\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.7884\n",
      "Epoch 00027: val_loss did not improve from 0.69344\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.6774 - acc: 0.7883 - val_loss: 0.7626 - val_acc: 0.7554\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6704 - acc: 0.7930\n",
      "Epoch 00028: val_loss did not improve from 0.69344\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.6704 - acc: 0.7930 - val_loss: 0.7610 - val_acc: 0.7570\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6583 - acc: 0.7931\n",
      "Epoch 00029: val_loss improved from 0.69344 to 0.66918, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/029-0.6692.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.6583 - acc: 0.7931 - val_loss: 0.6692 - val_acc: 0.7957\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6543 - acc: 0.7955\n",
      "Epoch 00030: val_loss did not improve from 0.66918\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.6544 - acc: 0.7954 - val_loss: 0.6792 - val_acc: 0.7850\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.7983\n",
      "Epoch 00031: val_loss did not improve from 0.66918\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.6478 - acc: 0.7983 - val_loss: 0.6696 - val_acc: 0.7929\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.8015\n",
      "Epoch 00032: val_loss did not improve from 0.66918\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.6384 - acc: 0.8015 - val_loss: 0.6734 - val_acc: 0.7955\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.8021\n",
      "Epoch 00033: val_loss did not improve from 0.66918\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.6319 - acc: 0.8021 - val_loss: 0.7080 - val_acc: 0.7741\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6255 - acc: 0.8050\n",
      "Epoch 00034: val_loss did not improve from 0.66918\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.6255 - acc: 0.8050 - val_loss: 0.7126 - val_acc: 0.7694\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6182 - acc: 0.8073\n",
      "Epoch 00035: val_loss improved from 0.66918 to 0.65669, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/035-0.6567.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.6183 - acc: 0.8073 - val_loss: 0.6567 - val_acc: 0.8013\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.8093\n",
      "Epoch 00036: val_loss did not improve from 0.65669\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.6145 - acc: 0.8092 - val_loss: 0.6583 - val_acc: 0.7985\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.8078\n",
      "Epoch 00037: val_loss improved from 0.65669 to 0.64778, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/037-0.6478.hdf5\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.6094 - acc: 0.8078 - val_loss: 0.6478 - val_acc: 0.7966\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.8123\n",
      "Epoch 00038: val_loss improved from 0.64778 to 0.63979, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/038-0.6398.hdf5\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.6022 - acc: 0.8123 - val_loss: 0.6398 - val_acc: 0.8055\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8140\n",
      "Epoch 00039: val_loss did not improve from 0.63979\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.5973 - acc: 0.8139 - val_loss: 0.7430 - val_acc: 0.7750\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8180\n",
      "Epoch 00040: val_loss did not improve from 0.63979\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.5899 - acc: 0.8179 - val_loss: 0.6646 - val_acc: 0.7925\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8162\n",
      "Epoch 00041: val_loss did not improve from 0.63979\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.5868 - acc: 0.8161 - val_loss: 0.6680 - val_acc: 0.7890\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8181\n",
      "Epoch 00042: val_loss improved from 0.63979 to 0.62633, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/042-0.6263.hdf5\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.5842 - acc: 0.8181 - val_loss: 0.6263 - val_acc: 0.8057\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5750 - acc: 0.8208\n",
      "Epoch 00043: val_loss did not improve from 0.62633\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.5750 - acc: 0.8208 - val_loss: 0.6664 - val_acc: 0.7971\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8207\n",
      "Epoch 00044: val_loss did not improve from 0.62633\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.5718 - acc: 0.8206 - val_loss: 0.6394 - val_acc: 0.8011\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.8234\n",
      "Epoch 00045: val_loss did not improve from 0.62633\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.5674 - acc: 0.8234 - val_loss: 0.6905 - val_acc: 0.7864\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.8235\n",
      "Epoch 00046: val_loss did not improve from 0.62633\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.5603 - acc: 0.8234 - val_loss: 0.6404 - val_acc: 0.8015\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.8247\n",
      "Epoch 00047: val_loss improved from 0.62633 to 0.61378, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/047-0.6138.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.5608 - acc: 0.8246 - val_loss: 0.6138 - val_acc: 0.8111\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8260\n",
      "Epoch 00048: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.5522 - acc: 0.8259 - val_loss: 0.6610 - val_acc: 0.7911\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8285\n",
      "Epoch 00049: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.5505 - acc: 0.8285 - val_loss: 0.6256 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8286\n",
      "Epoch 00050: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.5483 - acc: 0.8286 - val_loss: 0.6338 - val_acc: 0.8078\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8327\n",
      "Epoch 00051: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.5407 - acc: 0.8326 - val_loss: 0.6378 - val_acc: 0.7985\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8299\n",
      "Epoch 00052: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.5407 - acc: 0.8299 - val_loss: 0.6231 - val_acc: 0.8057\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8324\n",
      "Epoch 00053: val_loss did not improve from 0.61378\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.5331 - acc: 0.8324 - val_loss: 0.6358 - val_acc: 0.8046\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8336\n",
      "Epoch 00054: val_loss improved from 0.61378 to 0.61235, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/054-0.6123.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.5281 - acc: 0.8336 - val_loss: 0.6123 - val_acc: 0.8088\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.8351\n",
      "Epoch 00055: val_loss did not improve from 0.61235\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.5287 - acc: 0.8351 - val_loss: 0.6322 - val_acc: 0.8029\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8358\n",
      "Epoch 00056: val_loss did not improve from 0.61235\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.5241 - acc: 0.8358 - val_loss: 0.6286 - val_acc: 0.8088\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.8377\n",
      "Epoch 00057: val_loss did not improve from 0.61235\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.5178 - acc: 0.8375 - val_loss: 0.6338 - val_acc: 0.8102\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8390\n",
      "Epoch 00058: val_loss improved from 0.61235 to 0.60804, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/058-0.6080.hdf5\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.5146 - acc: 0.8390 - val_loss: 0.6080 - val_acc: 0.8125\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8390\n",
      "Epoch 00059: val_loss did not improve from 0.60804\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.5136 - acc: 0.8390 - val_loss: 0.6237 - val_acc: 0.8090\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8393\n",
      "Epoch 00060: val_loss did not improve from 0.60804\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.5098 - acc: 0.8393 - val_loss: 0.6291 - val_acc: 0.8090\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8403\n",
      "Epoch 00061: val_loss improved from 0.60804 to 0.59921, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/061-0.5992.hdf5\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.5083 - acc: 0.8402 - val_loss: 0.5992 - val_acc: 0.8134\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8427\n",
      "Epoch 00062: val_loss improved from 0.59921 to 0.59880, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/062-0.5988.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.5023 - acc: 0.8426 - val_loss: 0.5988 - val_acc: 0.8167\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8431\n",
      "Epoch 00063: val_loss improved from 0.59880 to 0.58441, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/063-0.5844.hdf5\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.5017 - acc: 0.8431 - val_loss: 0.5844 - val_acc: 0.8237\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8423\n",
      "Epoch 00064: val_loss did not improve from 0.58441\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.4980 - acc: 0.8422 - val_loss: 0.6443 - val_acc: 0.8039\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4933 - acc: 0.8451\n",
      "Epoch 00065: val_loss did not improve from 0.58441\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.4935 - acc: 0.8450 - val_loss: 0.6122 - val_acc: 0.8132\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8452\n",
      "Epoch 00066: val_loss did not improve from 0.58441\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.4921 - acc: 0.8452 - val_loss: 0.6479 - val_acc: 0.8034\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8469\n",
      "Epoch 00067: val_loss did not improve from 0.58441\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.4905 - acc: 0.8469 - val_loss: 0.6351 - val_acc: 0.8034\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.8467\n",
      "Epoch 00068: val_loss improved from 0.58441 to 0.57555, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/068-0.5755.hdf5\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.4873 - acc: 0.8467 - val_loss: 0.5755 - val_acc: 0.8218\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8486\n",
      "Epoch 00069: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.4804 - acc: 0.8486 - val_loss: 0.6129 - val_acc: 0.8092\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.8471\n",
      "Epoch 00070: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.4807 - acc: 0.8471 - val_loss: 0.6546 - val_acc: 0.7950\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8498\n",
      "Epoch 00071: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.4771 - acc: 0.8498 - val_loss: 0.5897 - val_acc: 0.8174\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8494\n",
      "Epoch 00072: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.4785 - acc: 0.8493 - val_loss: 0.6086 - val_acc: 0.8143\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4752 - acc: 0.8517\n",
      "Epoch 00073: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.4754 - acc: 0.8517 - val_loss: 0.6241 - val_acc: 0.8104\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8526\n",
      "Epoch 00074: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.4715 - acc: 0.8525 - val_loss: 0.6037 - val_acc: 0.8123\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8508\n",
      "Epoch 00075: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.4693 - acc: 0.8508 - val_loss: 0.6199 - val_acc: 0.8055\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.8522\n",
      "Epoch 00076: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4652 - acc: 0.8522 - val_loss: 0.6143 - val_acc: 0.8123\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.8528\n",
      "Epoch 00077: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.4668 - acc: 0.8528 - val_loss: 0.6229 - val_acc: 0.8057\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.8537\n",
      "Epoch 00078: val_loss did not improve from 0.57555\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4641 - acc: 0.8537 - val_loss: 0.6156 - val_acc: 0.8097\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8532\n",
      "Epoch 00079: val_loss improved from 0.57555 to 0.56696, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/079-0.5670.hdf5\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.4620 - acc: 0.8532 - val_loss: 0.5670 - val_acc: 0.8255\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8550\n",
      "Epoch 00080: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.4596 - acc: 0.8550 - val_loss: 0.5924 - val_acc: 0.8171\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8571\n",
      "Epoch 00081: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.4551 - acc: 0.8570 - val_loss: 0.6524 - val_acc: 0.7966\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8563\n",
      "Epoch 00082: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 942us/sample - loss: 0.4546 - acc: 0.8563 - val_loss: 0.5919 - val_acc: 0.8153\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8556\n",
      "Epoch 00083: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.4545 - acc: 0.8556 - val_loss: 0.6237 - val_acc: 0.8141\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8589\n",
      "Epoch 00084: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.4526 - acc: 0.8589 - val_loss: 0.6324 - val_acc: 0.8099\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8599\n",
      "Epoch 00085: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.4514 - acc: 0.8598 - val_loss: 0.5784 - val_acc: 0.8213\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8596\n",
      "Epoch 00086: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.4492 - acc: 0.8596 - val_loss: 0.6151 - val_acc: 0.8109\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8602\n",
      "Epoch 00087: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.4458 - acc: 0.8603 - val_loss: 0.6129 - val_acc: 0.8134\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8597\n",
      "Epoch 00088: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.4436 - acc: 0.8597 - val_loss: 0.6506 - val_acc: 0.8081\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8595\n",
      "Epoch 00089: val_loss did not improve from 0.56696\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.4430 - acc: 0.8595 - val_loss: 0.6590 - val_acc: 0.7969\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8619\n",
      "Epoch 00090: val_loss improved from 0.56696 to 0.55416, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv_checkpoint/090-0.5542.hdf5\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.4387 - acc: 0.8618 - val_loss: 0.5542 - val_acc: 0.8276\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8600\n",
      "Epoch 00091: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.4353 - acc: 0.8599 - val_loss: 0.5920 - val_acc: 0.8190\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8611\n",
      "Epoch 00092: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4369 - acc: 0.8611 - val_loss: 0.6208 - val_acc: 0.8018\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4325 - acc: 0.8640\n",
      "Epoch 00093: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4326 - acc: 0.8639 - val_loss: 0.5877 - val_acc: 0.8209\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8645\n",
      "Epoch 00094: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.4319 - acc: 0.8645 - val_loss: 0.5779 - val_acc: 0.8267\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.8643\n",
      "Epoch 00095: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.4302 - acc: 0.8643 - val_loss: 0.5778 - val_acc: 0.8213\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8657\n",
      "Epoch 00096: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4310 - acc: 0.8656 - val_loss: 0.5786 - val_acc: 0.8293\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8667\n",
      "Epoch 00097: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.4284 - acc: 0.8667 - val_loss: 0.5827 - val_acc: 0.8188\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8659\n",
      "Epoch 00098: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.4265 - acc: 0.8658 - val_loss: 0.5806 - val_acc: 0.8204\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8647\n",
      "Epoch 00099: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.4274 - acc: 0.8647 - val_loss: 0.5993 - val_acc: 0.8171\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8683\n",
      "Epoch 00100: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.4225 - acc: 0.8683 - val_loss: 0.6708 - val_acc: 0.7950\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.8672\n",
      "Epoch 00101: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4210 - acc: 0.8672 - val_loss: 0.5752 - val_acc: 0.8260\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8679\n",
      "Epoch 00102: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.4178 - acc: 0.8679 - val_loss: 0.5791 - val_acc: 0.8244\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8665\n",
      "Epoch 00103: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.4198 - acc: 0.8665 - val_loss: 0.5864 - val_acc: 0.8218\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8682\n",
      "Epoch 00104: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.4172 - acc: 0.8682 - val_loss: 0.5923 - val_acc: 0.8120\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8694\n",
      "Epoch 00105: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.4124 - acc: 0.8694 - val_loss: 0.6311 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8699\n",
      "Epoch 00106: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.4139 - acc: 0.8699 - val_loss: 0.6168 - val_acc: 0.8109\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8690\n",
      "Epoch 00107: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.4131 - acc: 0.8690 - val_loss: 0.6262 - val_acc: 0.8029\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8676\n",
      "Epoch 00108: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.4157 - acc: 0.8676 - val_loss: 0.5667 - val_acc: 0.8248\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8701\n",
      "Epoch 00109: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.4097 - acc: 0.8702 - val_loss: 0.6062 - val_acc: 0.8202\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8707\n",
      "Epoch 00110: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4088 - acc: 0.8706 - val_loss: 0.6175 - val_acc: 0.8076\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8696\n",
      "Epoch 00111: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.4096 - acc: 0.8696 - val_loss: 0.6126 - val_acc: 0.8176\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8709\n",
      "Epoch 00112: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.4077 - acc: 0.8709 - val_loss: 0.5718 - val_acc: 0.8283\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8731\n",
      "Epoch 00113: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.4019 - acc: 0.8731 - val_loss: 0.5663 - val_acc: 0.8260\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8720\n",
      "Epoch 00114: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.4029 - acc: 0.8720 - val_loss: 0.6409 - val_acc: 0.8162\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8734\n",
      "Epoch 00115: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.3995 - acc: 0.8734 - val_loss: 0.6437 - val_acc: 0.8041\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8722\n",
      "Epoch 00116: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.4051 - acc: 0.8722 - val_loss: 0.6155 - val_acc: 0.8157\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8719\n",
      "Epoch 00117: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.4033 - acc: 0.8719 - val_loss: 0.6437 - val_acc: 0.8078\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8724\n",
      "Epoch 00118: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.3999 - acc: 0.8724 - val_loss: 0.6420 - val_acc: 0.8027\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8738\n",
      "Epoch 00119: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.3999 - acc: 0.8738 - val_loss: 0.5997 - val_acc: 0.8120\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8723\n",
      "Epoch 00120: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3971 - acc: 0.8723 - val_loss: 0.5828 - val_acc: 0.8202\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8720\n",
      "Epoch 00121: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.3948 - acc: 0.8720 - val_loss: 0.5804 - val_acc: 0.8234\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8742\n",
      "Epoch 00122: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.3919 - acc: 0.8739 - val_loss: 0.6410 - val_acc: 0.8046\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8754\n",
      "Epoch 00123: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.3914 - acc: 0.8754 - val_loss: 0.5854 - val_acc: 0.8248\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8764\n",
      "Epoch 00124: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.3915 - acc: 0.8764 - val_loss: 0.5870 - val_acc: 0.8246\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8740\n",
      "Epoch 00125: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.3926 - acc: 0.8740 - val_loss: 0.6639 - val_acc: 0.8055\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8741\n",
      "Epoch 00126: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.3914 - acc: 0.8741 - val_loss: 0.5831 - val_acc: 0.8258\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8763\n",
      "Epoch 00127: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.3878 - acc: 0.8763 - val_loss: 0.5947 - val_acc: 0.8202\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8783\n",
      "Epoch 00128: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.3858 - acc: 0.8782 - val_loss: 0.6053 - val_acc: 0.8102\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8764\n",
      "Epoch 00129: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.3876 - acc: 0.8764 - val_loss: 0.5978 - val_acc: 0.8246\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8761\n",
      "Epoch 00130: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3872 - acc: 0.8761 - val_loss: 0.5813 - val_acc: 0.8283\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8775\n",
      "Epoch 00131: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.3855 - acc: 0.8775 - val_loss: 0.6163 - val_acc: 0.8162\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8758\n",
      "Epoch 00132: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.3862 - acc: 0.8758 - val_loss: 0.6234 - val_acc: 0.8171\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8775\n",
      "Epoch 00133: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3841 - acc: 0.8775 - val_loss: 0.5730 - val_acc: 0.8281\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8774\n",
      "Epoch 00134: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.3842 - acc: 0.8774 - val_loss: 0.5901 - val_acc: 0.8209\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8791\n",
      "Epoch 00135: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.3820 - acc: 0.8791 - val_loss: 0.5632 - val_acc: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8802\n",
      "Epoch 00136: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.3786 - acc: 0.8802 - val_loss: 0.5922 - val_acc: 0.8227\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8786\n",
      "Epoch 00137: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.3817 - acc: 0.8786 - val_loss: 0.5914 - val_acc: 0.8202\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8784\n",
      "Epoch 00138: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3787 - acc: 0.8784 - val_loss: 0.5682 - val_acc: 0.8297\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8782\n",
      "Epoch 00139: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.3809 - acc: 0.8782 - val_loss: 0.5701 - val_acc: 0.8297\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8795\n",
      "Epoch 00140: val_loss did not improve from 0.55416\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.3803 - acc: 0.8795 - val_loss: 0.5901 - val_acc: 0.8227\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 398us/sample - loss: 0.6380 - acc: 0.8004\n",
      "Loss: 0.6380394508657921 Accuracy: 0.8004154\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4058 - acc: 0.2452\n",
      "Epoch 00001: val_loss improved from inf to 2.40501, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/001-2.4050.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 2.4057 - acc: 0.2453 - val_loss: 2.4050 - val_acc: 0.2385\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7953 - acc: 0.4134\n",
      "Epoch 00002: val_loss improved from 2.40501 to 1.60068, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/002-1.6007.hdf5\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 1.7953 - acc: 0.4134 - val_loss: 1.6007 - val_acc: 0.4906\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5263 - acc: 0.5071\n",
      "Epoch 00003: val_loss improved from 1.60068 to 1.37807, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/003-1.3781.hdf5\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 1.5264 - acc: 0.5071 - val_loss: 1.3781 - val_acc: 0.5628\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3256 - acc: 0.5764\n",
      "Epoch 00004: val_loss improved from 1.37807 to 1.18297, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/004-1.1830.hdf5\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 1.3256 - acc: 0.5764 - val_loss: 1.1830 - val_acc: 0.6338\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1739 - acc: 0.6332\n",
      "Epoch 00005: val_loss improved from 1.18297 to 1.04434, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/005-1.0443.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 1.1740 - acc: 0.6332 - val_loss: 1.0443 - val_acc: 0.6823\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0529 - acc: 0.6746\n",
      "Epoch 00006: val_loss improved from 1.04434 to 0.93197, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/006-0.9320.hdf5\n",
      "36805/36805 [==============================] - 36s 982us/sample - loss: 1.0529 - acc: 0.6747 - val_loss: 0.9320 - val_acc: 0.7126\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9586 - acc: 0.7078\n",
      "Epoch 00007: val_loss improved from 0.93197 to 0.87477, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/007-0.8748.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.9586 - acc: 0.7078 - val_loss: 0.8748 - val_acc: 0.7426\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8823 - acc: 0.7315\n",
      "Epoch 00008: val_loss improved from 0.87477 to 0.81676, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/008-0.8168.hdf5\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.8823 - acc: 0.7315 - val_loss: 0.8168 - val_acc: 0.7577\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8238 - acc: 0.7510\n",
      "Epoch 00009: val_loss improved from 0.81676 to 0.75384, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/009-0.7538.hdf5\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.8238 - acc: 0.7509 - val_loss: 0.7538 - val_acc: 0.7699\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7794 - acc: 0.7649\n",
      "Epoch 00010: val_loss improved from 0.75384 to 0.70171, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/010-0.7017.hdf5\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.7794 - acc: 0.7649 - val_loss: 0.7017 - val_acc: 0.7911\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7391 - acc: 0.7783\n",
      "Epoch 00011: val_loss improved from 0.70171 to 0.68034, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/011-0.6803.hdf5\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.7391 - acc: 0.7783 - val_loss: 0.6803 - val_acc: 0.7915\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7021 - acc: 0.7870\n",
      "Epoch 00012: val_loss improved from 0.68034 to 0.66409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/012-0.6641.hdf5\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.7022 - acc: 0.7869 - val_loss: 0.6641 - val_acc: 0.7964\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7973\n",
      "Epoch 00013: val_loss improved from 0.66409 to 0.64472, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/013-0.6447.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.6728 - acc: 0.7973 - val_loss: 0.6447 - val_acc: 0.7990\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6465 - acc: 0.8041\n",
      "Epoch 00014: val_loss improved from 0.64472 to 0.61843, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/014-0.6184.hdf5\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.6464 - acc: 0.8041 - val_loss: 0.6184 - val_acc: 0.8099\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6245 - acc: 0.8134\n",
      "Epoch 00015: val_loss did not improve from 0.61843\n",
      "36805/36805 [==============================] - 36s 982us/sample - loss: 0.6245 - acc: 0.8134 - val_loss: 0.6774 - val_acc: 0.7773\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6043 - acc: 0.8174\n",
      "Epoch 00016: val_loss improved from 0.61843 to 0.60728, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/016-0.6073.hdf5\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.6044 - acc: 0.8174 - val_loss: 0.6073 - val_acc: 0.8113\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5864 - acc: 0.8232\n",
      "Epoch 00017: val_loss improved from 0.60728 to 0.58103, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/017-0.5810.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.5864 - acc: 0.8231 - val_loss: 0.5810 - val_acc: 0.8190\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8270\n",
      "Epoch 00018: val_loss did not improve from 0.58103\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.5679 - acc: 0.8270 - val_loss: 0.5890 - val_acc: 0.8167\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8317\n",
      "Epoch 00019: val_loss did not improve from 0.58103\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.5525 - acc: 0.8317 - val_loss: 0.5824 - val_acc: 0.8204\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.8352\n",
      "Epoch 00020: val_loss improved from 0.58103 to 0.57709, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/020-0.5771.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.5417 - acc: 0.8352 - val_loss: 0.5771 - val_acc: 0.8227\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.8404\n",
      "Epoch 00021: val_loss did not improve from 0.57709\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.5281 - acc: 0.8404 - val_loss: 0.5802 - val_acc: 0.8181\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8423\n",
      "Epoch 00022: val_loss improved from 0.57709 to 0.55283, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/022-0.5528.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.5164 - acc: 0.8423 - val_loss: 0.5528 - val_acc: 0.8297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8465\n",
      "Epoch 00023: val_loss improved from 0.55283 to 0.51480, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/023-0.5148.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.5040 - acc: 0.8465 - val_loss: 0.5148 - val_acc: 0.8353\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8507\n",
      "Epoch 00024: val_loss improved from 0.51480 to 0.50886, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/024-0.5089.hdf5\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.4908 - acc: 0.8507 - val_loss: 0.5089 - val_acc: 0.8444\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8524\n",
      "Epoch 00025: val_loss did not improve from 0.50886\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.4804 - acc: 0.8524 - val_loss: 0.5212 - val_acc: 0.8355\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8565\n",
      "Epoch 00026: val_loss improved from 0.50886 to 0.50687, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/026-0.5069.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.4749 - acc: 0.8565 - val_loss: 0.5069 - val_acc: 0.8458\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4663 - acc: 0.8593\n",
      "Epoch 00027: val_loss did not improve from 0.50687\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.4663 - acc: 0.8593 - val_loss: 0.5461 - val_acc: 0.8330\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.8610\n",
      "Epoch 00028: val_loss improved from 0.50687 to 0.50294, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/028-0.5029.hdf5\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.4568 - acc: 0.8609 - val_loss: 0.5029 - val_acc: 0.8421\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8639\n",
      "Epoch 00029: val_loss did not improve from 0.50294\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.4500 - acc: 0.8640 - val_loss: 0.5914 - val_acc: 0.8160\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8650\n",
      "Epoch 00030: val_loss improved from 0.50294 to 0.47103, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/030-0.4710.hdf5\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.4421 - acc: 0.8650 - val_loss: 0.4710 - val_acc: 0.8579\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.8678\n",
      "Epoch 00031: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.4331 - acc: 0.8677 - val_loss: 0.4964 - val_acc: 0.8484\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8694\n",
      "Epoch 00032: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.4282 - acc: 0.8694 - val_loss: 0.5129 - val_acc: 0.8428\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8711\n",
      "Epoch 00033: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.4224 - acc: 0.8711 - val_loss: 0.5598 - val_acc: 0.8274\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8714\n",
      "Epoch 00034: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.4193 - acc: 0.8714 - val_loss: 0.5155 - val_acc: 0.8409\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8749\n",
      "Epoch 00035: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 979us/sample - loss: 0.4071 - acc: 0.8750 - val_loss: 0.4789 - val_acc: 0.8563\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8758\n",
      "Epoch 00036: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.4035 - acc: 0.8758 - val_loss: 0.5803 - val_acc: 0.8253\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8783\n",
      "Epoch 00037: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3993 - acc: 0.8783 - val_loss: 0.4747 - val_acc: 0.8572\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8805\n",
      "Epoch 00038: val_loss did not improve from 0.47103\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.3911 - acc: 0.8806 - val_loss: 0.5036 - val_acc: 0.8439\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8801\n",
      "Epoch 00039: val_loss improved from 0.47103 to 0.47082, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/039-0.4708.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.3882 - acc: 0.8801 - val_loss: 0.4708 - val_acc: 0.8558\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8826\n",
      "Epoch 00040: val_loss did not improve from 0.47082\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.3841 - acc: 0.8826 - val_loss: 0.4810 - val_acc: 0.8544\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8846\n",
      "Epoch 00041: val_loss did not improve from 0.47082\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.3790 - acc: 0.8846 - val_loss: 0.4787 - val_acc: 0.8532\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8847\n",
      "Epoch 00042: val_loss did not improve from 0.47082\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.3719 - acc: 0.8847 - val_loss: 0.4817 - val_acc: 0.8588\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8869\n",
      "Epoch 00043: val_loss improved from 0.47082 to 0.45012, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/043-0.4501.hdf5\n",
      "36805/36805 [==============================] - 36s 983us/sample - loss: 0.3684 - acc: 0.8869 - val_loss: 0.4501 - val_acc: 0.8642\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8860\n",
      "Epoch 00044: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.3686 - acc: 0.8860 - val_loss: 0.4654 - val_acc: 0.8591\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8901\n",
      "Epoch 00045: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.3617 - acc: 0.8901 - val_loss: 0.5213 - val_acc: 0.8425\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8918\n",
      "Epoch 00046: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.3542 - acc: 0.8917 - val_loss: 0.4775 - val_acc: 0.8521\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8912\n",
      "Epoch 00047: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.3536 - acc: 0.8912 - val_loss: 0.4683 - val_acc: 0.8556\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8931\n",
      "Epoch 00048: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.3478 - acc: 0.8931 - val_loss: 0.4741 - val_acc: 0.8544\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8947\n",
      "Epoch 00049: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.3411 - acc: 0.8947 - val_loss: 0.5170 - val_acc: 0.8381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8942\n",
      "Epoch 00050: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.3391 - acc: 0.8942 - val_loss: 0.4930 - val_acc: 0.8474\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8968\n",
      "Epoch 00051: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.3366 - acc: 0.8968 - val_loss: 0.4783 - val_acc: 0.8586\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8979\n",
      "Epoch 00052: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3327 - acc: 0.8979 - val_loss: 0.4967 - val_acc: 0.8512\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8973\n",
      "Epoch 00053: val_loss did not improve from 0.45012\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.3307 - acc: 0.8973 - val_loss: 0.4635 - val_acc: 0.8581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8997\n",
      "Epoch 00054: val_loss improved from 0.45012 to 0.43435, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/054-0.4343.hdf5\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.3249 - acc: 0.8997 - val_loss: 0.4343 - val_acc: 0.8665\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9004\n",
      "Epoch 00055: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.3244 - acc: 0.9004 - val_loss: 0.4776 - val_acc: 0.8549\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.9014\n",
      "Epoch 00056: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3184 - acc: 0.9015 - val_loss: 0.4443 - val_acc: 0.8677\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.9035\n",
      "Epoch 00057: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3156 - acc: 0.9034 - val_loss: 0.4948 - val_acc: 0.8495\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9037\n",
      "Epoch 00058: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.3127 - acc: 0.9036 - val_loss: 0.4694 - val_acc: 0.8600\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9040\n",
      "Epoch 00059: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.3101 - acc: 0.9040 - val_loss: 0.4480 - val_acc: 0.8616\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9057\n",
      "Epoch 00060: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 979us/sample - loss: 0.3041 - acc: 0.9057 - val_loss: 0.4733 - val_acc: 0.8544\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9055\n",
      "Epoch 00061: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.3033 - acc: 0.9054 - val_loss: 0.4640 - val_acc: 0.8600\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2991 - acc: 0.9074\n",
      "Epoch 00062: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.2992 - acc: 0.9075 - val_loss: 0.4411 - val_acc: 0.8651\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9072\n",
      "Epoch 00063: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.2985 - acc: 0.9072 - val_loss: 0.4540 - val_acc: 0.8651\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9070\n",
      "Epoch 00064: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.2969 - acc: 0.9070 - val_loss: 0.4586 - val_acc: 0.8605\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9104\n",
      "Epoch 00065: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.2923 - acc: 0.9104 - val_loss: 0.4578 - val_acc: 0.8635\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9108\n",
      "Epoch 00066: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.2897 - acc: 0.9107 - val_loss: 0.4501 - val_acc: 0.8700\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9101\n",
      "Epoch 00067: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.2877 - acc: 0.9101 - val_loss: 0.4527 - val_acc: 0.8661\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9101\n",
      "Epoch 00068: val_loss did not improve from 0.43435\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.2879 - acc: 0.9100 - val_loss: 0.4525 - val_acc: 0.8647\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9129\n",
      "Epoch 00069: val_loss improved from 0.43435 to 0.43149, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/069-0.4315.hdf5\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.2829 - acc: 0.9129 - val_loss: 0.4315 - val_acc: 0.8679\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9135\n",
      "Epoch 00070: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 35s 959us/sample - loss: 0.2794 - acc: 0.9135 - val_loss: 0.5071 - val_acc: 0.8495\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9132\n",
      "Epoch 00071: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.2798 - acc: 0.9132 - val_loss: 0.4789 - val_acc: 0.8546\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9154\n",
      "Epoch 00072: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.2728 - acc: 0.9154 - val_loss: 0.4521 - val_acc: 0.8612\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9143\n",
      "Epoch 00073: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2748 - acc: 0.9142 - val_loss: 0.4643 - val_acc: 0.8661\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9144\n",
      "Epoch 00074: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2746 - acc: 0.9144 - val_loss: 0.4436 - val_acc: 0.8647\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9177\n",
      "Epoch 00075: val_loss did not improve from 0.43149\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.2677 - acc: 0.9177 - val_loss: 0.4969 - val_acc: 0.8512\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9165\n",
      "Epoch 00076: val_loss improved from 0.43149 to 0.42761, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv_checkpoint/076-0.4276.hdf5\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.2678 - acc: 0.9165 - val_loss: 0.4276 - val_acc: 0.8721\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9162\n",
      "Epoch 00077: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.2659 - acc: 0.9162 - val_loss: 0.4477 - val_acc: 0.8679\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9168\n",
      "Epoch 00078: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.2663 - acc: 0.9168 - val_loss: 0.5089 - val_acc: 0.8456\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9179\n",
      "Epoch 00079: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2617 - acc: 0.9179 - val_loss: 0.4494 - val_acc: 0.8647\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9205\n",
      "Epoch 00080: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.2579 - acc: 0.9205 - val_loss: 0.4756 - val_acc: 0.8642\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9199\n",
      "Epoch 00081: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.2575 - acc: 0.9198 - val_loss: 0.4492 - val_acc: 0.8665\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9201\n",
      "Epoch 00082: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.2546 - acc: 0.9200 - val_loss: 0.5085 - val_acc: 0.8523\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9219\n",
      "Epoch 00083: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 958us/sample - loss: 0.2526 - acc: 0.9219 - val_loss: 0.4645 - val_acc: 0.8621\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9222\n",
      "Epoch 00084: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 981us/sample - loss: 0.2518 - acc: 0.9222 - val_loss: 0.4697 - val_acc: 0.8626\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9207\n",
      "Epoch 00085: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.2525 - acc: 0.9207 - val_loss: 0.4503 - val_acc: 0.8658\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9231\n",
      "Epoch 00086: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.2485 - acc: 0.9229 - val_loss: 0.4954 - val_acc: 0.8528\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9218\n",
      "Epoch 00087: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.2479 - acc: 0.9219 - val_loss: 0.5103 - val_acc: 0.8470\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9230\n",
      "Epoch 00088: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.2443 - acc: 0.9231 - val_loss: 0.4531 - val_acc: 0.8698\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9246\n",
      "Epoch 00089: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.2407 - acc: 0.9246 - val_loss: 0.5612 - val_acc: 0.8351\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9223\n",
      "Epoch 00090: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2448 - acc: 0.9222 - val_loss: 0.4830 - val_acc: 0.8572\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9242\n",
      "Epoch 00091: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.2417 - acc: 0.9242 - val_loss: 0.4792 - val_acc: 0.8593\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9245\n",
      "Epoch 00092: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.2395 - acc: 0.9245 - val_loss: 0.4790 - val_acc: 0.8614\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9260\n",
      "Epoch 00093: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.2370 - acc: 0.9259 - val_loss: 0.4715 - val_acc: 0.8619\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9229\n",
      "Epoch 00094: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2389 - acc: 0.9229 - val_loss: 0.4945 - val_acc: 0.8577\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9278\n",
      "Epoch 00095: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.2334 - acc: 0.9278 - val_loss: 0.5071 - val_acc: 0.8484\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9261\n",
      "Epoch 00096: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.2337 - acc: 0.9261 - val_loss: 0.5181 - val_acc: 0.8537\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9268\n",
      "Epoch 00097: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2320 - acc: 0.9267 - val_loss: 0.4699 - val_acc: 0.8663\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9283\n",
      "Epoch 00098: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 979us/sample - loss: 0.2288 - acc: 0.9283 - val_loss: 0.4712 - val_acc: 0.8672\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9298\n",
      "Epoch 00099: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.2276 - acc: 0.9297 - val_loss: 0.5259 - val_acc: 0.8472\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9291\n",
      "Epoch 00100: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.2255 - acc: 0.9291 - val_loss: 0.4795 - val_acc: 0.8591\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9282\n",
      "Epoch 00101: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2250 - acc: 0.9281 - val_loss: 0.4696 - val_acc: 0.8630\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9315\n",
      "Epoch 00102: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2205 - acc: 0.9315 - val_loss: 0.4749 - val_acc: 0.8609\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9309\n",
      "Epoch 00103: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.2210 - acc: 0.9309 - val_loss: 0.5045 - val_acc: 0.8544\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9324\n",
      "Epoch 00104: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2184 - acc: 0.9323 - val_loss: 0.5442 - val_acc: 0.8479\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9295\n",
      "Epoch 00105: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.2224 - acc: 0.9294 - val_loss: 0.4787 - val_acc: 0.8663\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9283\n",
      "Epoch 00106: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.2282 - acc: 0.9283 - val_loss: 0.4744 - val_acc: 0.8644\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9304\n",
      "Epoch 00107: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.2199 - acc: 0.9304 - val_loss: 0.4846 - val_acc: 0.8593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9336\n",
      "Epoch 00108: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.2147 - acc: 0.9336 - val_loss: 0.5100 - val_acc: 0.8519\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9314\n",
      "Epoch 00109: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 982us/sample - loss: 0.2141 - acc: 0.9314 - val_loss: 0.4698 - val_acc: 0.8651\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9321\n",
      "Epoch 00110: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.2135 - acc: 0.9321 - val_loss: 0.4705 - val_acc: 0.8689\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9316\n",
      "Epoch 00111: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.2126 - acc: 0.9316 - val_loss: 0.4836 - val_acc: 0.8637\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9334\n",
      "Epoch 00112: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2084 - acc: 0.9333 - val_loss: 0.4794 - val_acc: 0.8598\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9358\n",
      "Epoch 00113: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.2077 - acc: 0.9358 - val_loss: 0.4979 - val_acc: 0.8591\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9350\n",
      "Epoch 00114: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2066 - acc: 0.9349 - val_loss: 0.4982 - val_acc: 0.8581\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9362\n",
      "Epoch 00115: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.2063 - acc: 0.9361 - val_loss: 0.4720 - val_acc: 0.8663\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9336\n",
      "Epoch 00116: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2084 - acc: 0.9336 - val_loss: 0.4886 - val_acc: 0.8635\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9358\n",
      "Epoch 00117: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 979us/sample - loss: 0.2027 - acc: 0.9358 - val_loss: 0.5126 - val_acc: 0.8558\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9337\n",
      "Epoch 00118: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2093 - acc: 0.9337 - val_loss: 0.5120 - val_acc: 0.8565\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9351\n",
      "Epoch 00119: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.2034 - acc: 0.9351 - val_loss: 0.4898 - val_acc: 0.8614\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9363\n",
      "Epoch 00120: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.2009 - acc: 0.9363 - val_loss: 0.4989 - val_acc: 0.8619\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9359\n",
      "Epoch 00121: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.2025 - acc: 0.9359 - val_loss: 0.5147 - val_acc: 0.8537\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9354\n",
      "Epoch 00122: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.2032 - acc: 0.9354 - val_loss: 0.5741 - val_acc: 0.8360\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9377\n",
      "Epoch 00123: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.1991 - acc: 0.9377 - val_loss: 0.4815 - val_acc: 0.8707\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9356\n",
      "Epoch 00124: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.2005 - acc: 0.9356 - val_loss: 0.5617 - val_acc: 0.8516\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9367\n",
      "Epoch 00125: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.1986 - acc: 0.9367 - val_loss: 0.4995 - val_acc: 0.8623\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9382\n",
      "Epoch 00126: val_loss did not improve from 0.42761\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.1951 - acc: 0.9381 - val_loss: 0.5010 - val_acc: 0.8591\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 449us/sample - loss: 0.5293 - acc: 0.8430\n",
      "Loss: 0.5293050099818011 Accuracy: 0.84299064\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0981 - acc: 0.3399\n",
      "Epoch 00001: val_loss improved from inf to 1.83603, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/001-1.8360.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 2.0981 - acc: 0.3400 - val_loss: 1.8360 - val_acc: 0.4484\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3554 - acc: 0.5842\n",
      "Epoch 00002: val_loss improved from 1.83603 to 1.17594, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/002-1.1759.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 1.3553 - acc: 0.5842 - val_loss: 1.1759 - val_acc: 0.6555\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0273 - acc: 0.6968\n",
      "Epoch 00003: val_loss improved from 1.17594 to 0.89742, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/003-0.8974.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 1.0273 - acc: 0.6969 - val_loss: 0.8974 - val_acc: 0.7498\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8108 - acc: 0.7651\n",
      "Epoch 00004: val_loss improved from 0.89742 to 0.71243, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/004-0.7124.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.8108 - acc: 0.7651 - val_loss: 0.7124 - val_acc: 0.8032\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.8070\n",
      "Epoch 00005: val_loss improved from 0.71243 to 0.59819, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/005-0.5982.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.6736 - acc: 0.8070 - val_loss: 0.5982 - val_acc: 0.8295\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.8349\n",
      "Epoch 00006: val_loss improved from 0.59819 to 0.52580, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/006-0.5258.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.5835 - acc: 0.8349 - val_loss: 0.5258 - val_acc: 0.8535\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8525\n",
      "Epoch 00007: val_loss improved from 0.52580 to 0.46013, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/007-0.4601.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.5194 - acc: 0.8524 - val_loss: 0.4601 - val_acc: 0.8712\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8664\n",
      "Epoch 00008: val_loss improved from 0.46013 to 0.44543, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/008-0.4454.hdf5\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.4705 - acc: 0.8664 - val_loss: 0.4454 - val_acc: 0.8805\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8758\n",
      "Epoch 00009: val_loss improved from 0.44543 to 0.42038, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/009-0.4204.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4331 - acc: 0.8758 - val_loss: 0.4204 - val_acc: 0.8775\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8863\n",
      "Epoch 00010: val_loss improved from 0.42038 to 0.37856, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/010-0.3786.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4014 - acc: 0.8863 - val_loss: 0.3786 - val_acc: 0.8961\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8915\n",
      "Epoch 00011: val_loss did not improve from 0.37856\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.3754 - acc: 0.8915 - val_loss: 0.4038 - val_acc: 0.8807\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8995\n",
      "Epoch 00012: val_loss did not improve from 0.37856\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.3533 - acc: 0.8995 - val_loss: 0.4020 - val_acc: 0.8838\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.9047\n",
      "Epoch 00013: val_loss improved from 0.37856 to 0.33883, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/013-0.3388.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3341 - acc: 0.9047 - val_loss: 0.3388 - val_acc: 0.9033\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.9095\n",
      "Epoch 00014: val_loss improved from 0.33883 to 0.32497, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/014-0.3250.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3155 - acc: 0.9095 - val_loss: 0.3250 - val_acc: 0.9061\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9137\n",
      "Epoch 00015: val_loss did not improve from 0.32497\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.3033 - acc: 0.9137 - val_loss: 0.4098 - val_acc: 0.8726\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9173\n",
      "Epoch 00016: val_loss did not improve from 0.32497\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2899 - acc: 0.9173 - val_loss: 0.3252 - val_acc: 0.9050\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9214\n",
      "Epoch 00017: val_loss improved from 0.32497 to 0.31289, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/017-0.3129.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2776 - acc: 0.9214 - val_loss: 0.3129 - val_acc: 0.9087\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9249\n",
      "Epoch 00018: val_loss improved from 0.31289 to 0.30916, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/018-0.3092.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2655 - acc: 0.9249 - val_loss: 0.3092 - val_acc: 0.9089\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9279\n",
      "Epoch 00019: val_loss improved from 0.30916 to 0.29757, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/019-0.2976.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2544 - acc: 0.9278 - val_loss: 0.2976 - val_acc: 0.9133\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9292\n",
      "Epoch 00020: val_loss did not improve from 0.29757\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2491 - acc: 0.9291 - val_loss: 0.3135 - val_acc: 0.9071\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9312\n",
      "Epoch 00021: val_loss did not improve from 0.29757\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2415 - acc: 0.9311 - val_loss: 0.2996 - val_acc: 0.9089\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9331\n",
      "Epoch 00022: val_loss improved from 0.29757 to 0.29740, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/022-0.2974.hdf5\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.2321 - acc: 0.9331 - val_loss: 0.2974 - val_acc: 0.9124\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9374\n",
      "Epoch 00023: val_loss improved from 0.29740 to 0.28725, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/023-0.2872.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2221 - acc: 0.9374 - val_loss: 0.2872 - val_acc: 0.9117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9384\n",
      "Epoch 00024: val_loss did not improve from 0.28725\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.2150 - acc: 0.9384 - val_loss: 0.2990 - val_acc: 0.9080\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9415\n",
      "Epoch 00025: val_loss did not improve from 0.28725\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.2067 - acc: 0.9415 - val_loss: 0.3291 - val_acc: 0.9031\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9420\n",
      "Epoch 00026: val_loss improved from 0.28725 to 0.27549, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/026-0.2755.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2042 - acc: 0.9420 - val_loss: 0.2755 - val_acc: 0.9150\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9439\n",
      "Epoch 00027: val_loss improved from 0.27549 to 0.27315, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/027-0.2732.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1957 - acc: 0.9439 - val_loss: 0.2732 - val_acc: 0.9147\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9448\n",
      "Epoch 00028: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1925 - acc: 0.9448 - val_loss: 0.2857 - val_acc: 0.9136\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9456\n",
      "Epoch 00029: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1862 - acc: 0.9456 - val_loss: 0.2858 - val_acc: 0.9147\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9477\n",
      "Epoch 00030: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1817 - acc: 0.9477 - val_loss: 0.2890 - val_acc: 0.9096\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9499\n",
      "Epoch 00031: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.1753 - acc: 0.9498 - val_loss: 0.2874 - val_acc: 0.9164\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9503\n",
      "Epoch 00032: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1731 - acc: 0.9503 - val_loss: 0.3024 - val_acc: 0.9038\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9519\n",
      "Epoch 00033: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 993us/sample - loss: 0.1667 - acc: 0.9519 - val_loss: 0.2860 - val_acc: 0.9152\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9542\n",
      "Epoch 00034: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.1620 - acc: 0.9542 - val_loss: 0.2953 - val_acc: 0.9117\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9548\n",
      "Epoch 00035: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.1576 - acc: 0.9548 - val_loss: 0.2756 - val_acc: 0.9208\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9549\n",
      "Epoch 00036: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1557 - acc: 0.9549 - val_loss: 0.2749 - val_acc: 0.9185\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9587\n",
      "Epoch 00037: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 993us/sample - loss: 0.1467 - acc: 0.9587 - val_loss: 0.2766 - val_acc: 0.9161\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9602\n",
      "Epoch 00038: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1429 - acc: 0.9602 - val_loss: 0.3013 - val_acc: 0.9110\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9607\n",
      "Epoch 00039: val_loss did not improve from 0.27315\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1398 - acc: 0.9607 - val_loss: 0.3053 - val_acc: 0.9106\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9603\n",
      "Epoch 00040: val_loss improved from 0.27315 to 0.27215, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/040-0.2721.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1390 - acc: 0.9603 - val_loss: 0.2721 - val_acc: 0.9175\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9618\n",
      "Epoch 00041: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1353 - acc: 0.9618 - val_loss: 0.2907 - val_acc: 0.9099\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9617\n",
      "Epoch 00042: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1328 - acc: 0.9617 - val_loss: 0.2872 - val_acc: 0.9187\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9644\n",
      "Epoch 00043: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1288 - acc: 0.9644 - val_loss: 0.2902 - val_acc: 0.9119\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9639\n",
      "Epoch 00044: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1265 - acc: 0.9639 - val_loss: 0.2769 - val_acc: 0.9175\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9663\n",
      "Epoch 00045: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1204 - acc: 0.9663 - val_loss: 0.3104 - val_acc: 0.9094\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9673\n",
      "Epoch 00046: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1172 - acc: 0.9673 - val_loss: 0.2756 - val_acc: 0.9187\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9668\n",
      "Epoch 00047: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1176 - acc: 0.9667 - val_loss: 0.2802 - val_acc: 0.9178\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9683\n",
      "Epoch 00048: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1141 - acc: 0.9682 - val_loss: 0.2903 - val_acc: 0.9168\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9678\n",
      "Epoch 00049: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1138 - acc: 0.9678 - val_loss: 0.2846 - val_acc: 0.9168\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9705\n",
      "Epoch 00050: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1074 - acc: 0.9704 - val_loss: 0.3189 - val_acc: 0.9103\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9690\n",
      "Epoch 00051: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1090 - acc: 0.9690 - val_loss: 0.2911 - val_acc: 0.9192\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9721\n",
      "Epoch 00052: val_loss did not improve from 0.27215\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.1025 - acc: 0.9721 - val_loss: 0.2739 - val_acc: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9722\n",
      "Epoch 00053: val_loss improved from 0.27215 to 0.27025, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/053-0.2703.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1007 - acc: 0.9721 - val_loss: 0.2703 - val_acc: 0.9199\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9709\n",
      "Epoch 00054: val_loss improved from 0.27025 to 0.26476, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv_checkpoint/054-0.2648.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1009 - acc: 0.9709 - val_loss: 0.2648 - val_acc: 0.9215\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9732\n",
      "Epoch 00055: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0955 - acc: 0.9732 - val_loss: 0.2803 - val_acc: 0.9201\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9738\n",
      "Epoch 00056: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0947 - acc: 0.9738 - val_loss: 0.2946 - val_acc: 0.9143\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9750\n",
      "Epoch 00057: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0907 - acc: 0.9750 - val_loss: 0.2788 - val_acc: 0.9164\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9773\n",
      "Epoch 00058: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0856 - acc: 0.9773 - val_loss: 0.2823 - val_acc: 0.9178\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9766\n",
      "Epoch 00059: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0865 - acc: 0.9766 - val_loss: 0.2889 - val_acc: 0.9187\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9773\n",
      "Epoch 00060: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0842 - acc: 0.9773 - val_loss: 0.3195 - val_acc: 0.9101\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9767\n",
      "Epoch 00061: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 997us/sample - loss: 0.0841 - acc: 0.9767 - val_loss: 0.3356 - val_acc: 0.9019\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9776\n",
      "Epoch 00062: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0829 - acc: 0.9776 - val_loss: 0.3448 - val_acc: 0.9075\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9801\n",
      "Epoch 00063: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0771 - acc: 0.9801 - val_loss: 0.3056 - val_acc: 0.9110\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9784\n",
      "Epoch 00064: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0777 - acc: 0.9784 - val_loss: 0.2816 - val_acc: 0.9185\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9807\n",
      "Epoch 00065: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0739 - acc: 0.9807 - val_loss: 0.2956 - val_acc: 0.9150\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9820\n",
      "Epoch 00066: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0712 - acc: 0.9820 - val_loss: 0.2988 - val_acc: 0.9129\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9815\n",
      "Epoch 00067: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0708 - acc: 0.9815 - val_loss: 0.3008 - val_acc: 0.9164\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9800\n",
      "Epoch 00068: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0724 - acc: 0.9800 - val_loss: 0.3051 - val_acc: 0.9164\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9813\n",
      "Epoch 00069: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0699 - acc: 0.9813 - val_loss: 0.3443 - val_acc: 0.9038\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9830\n",
      "Epoch 00070: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0659 - acc: 0.9830 - val_loss: 0.2968 - val_acc: 0.9187\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9828\n",
      "Epoch 00071: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0659 - acc: 0.9828 - val_loss: 0.3033 - val_acc: 0.9171\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9827\n",
      "Epoch 00072: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0654 - acc: 0.9827 - val_loss: 0.3008 - val_acc: 0.9182\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9848\n",
      "Epoch 00073: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.0601 - acc: 0.9848 - val_loss: 0.3214 - val_acc: 0.9133\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9847\n",
      "Epoch 00074: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 995us/sample - loss: 0.0606 - acc: 0.9847 - val_loss: 0.3239 - val_acc: 0.9131\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9842\n",
      "Epoch 00075: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0611 - acc: 0.9842 - val_loss: 0.3499 - val_acc: 0.9068\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9866\n",
      "Epoch 00076: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0560 - acc: 0.9866 - val_loss: 0.3127 - val_acc: 0.9185\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9849\n",
      "Epoch 00077: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0573 - acc: 0.9849 - val_loss: 0.2949 - val_acc: 0.9215\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9861\n",
      "Epoch 00078: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0556 - acc: 0.9860 - val_loss: 0.3246 - val_acc: 0.9119\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9860\n",
      "Epoch 00079: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0553 - acc: 0.9860 - val_loss: 0.3173 - val_acc: 0.9159\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9870\n",
      "Epoch 00080: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0520 - acc: 0.9869 - val_loss: 0.3194 - val_acc: 0.9150\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9863\n",
      "Epoch 00081: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0547 - acc: 0.9863 - val_loss: 0.3134 - val_acc: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9874\n",
      "Epoch 00082: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0501 - acc: 0.9874 - val_loss: 0.3138 - val_acc: 0.9187\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9876\n",
      "Epoch 00083: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 36s 990us/sample - loss: 0.0498 - acc: 0.9876 - val_loss: 0.3539 - val_acc: 0.9106\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9865\n",
      "Epoch 00084: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0526 - acc: 0.9866 - val_loss: 0.3421 - val_acc: 0.9136\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9890\n",
      "Epoch 00085: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0469 - acc: 0.9889 - val_loss: 0.3642 - val_acc: 0.9043\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9876\n",
      "Epoch 00086: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0503 - acc: 0.9876 - val_loss: 0.3466 - val_acc: 0.9117\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9866\n",
      "Epoch 00087: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0514 - acc: 0.9866 - val_loss: 0.3342 - val_acc: 0.9145\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9866\n",
      "Epoch 00088: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0520 - acc: 0.9866 - val_loss: 0.3153 - val_acc: 0.9175\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9898\n",
      "Epoch 00089: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0429 - acc: 0.9898 - val_loss: 0.3206 - val_acc: 0.9168\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9892\n",
      "Epoch 00090: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0454 - acc: 0.9892 - val_loss: 0.3106 - val_acc: 0.9189\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9891\n",
      "Epoch 00091: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0453 - acc: 0.9891 - val_loss: 0.3245 - val_acc: 0.9180\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9887\n",
      "Epoch 00092: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0453 - acc: 0.9887 - val_loss: 0.3643 - val_acc: 0.9092\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9886\n",
      "Epoch 00093: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0454 - acc: 0.9886 - val_loss: 0.3472 - val_acc: 0.9108\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9910\n",
      "Epoch 00094: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0402 - acc: 0.9909 - val_loss: 0.3286 - val_acc: 0.9187\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9896\n",
      "Epoch 00095: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0419 - acc: 0.9896 - val_loss: 0.3285 - val_acc: 0.9161\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9890\n",
      "Epoch 00096: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0448 - acc: 0.9889 - val_loss: 0.3196 - val_acc: 0.9175\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9891\n",
      "Epoch 00097: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0417 - acc: 0.9891 - val_loss: 0.3449 - val_acc: 0.9150\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9926\n",
      "Epoch 00098: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0351 - acc: 0.9926 - val_loss: 0.3310 - val_acc: 0.9201\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9893\n",
      "Epoch 00099: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0425 - acc: 0.9893 - val_loss: 0.3455 - val_acc: 0.9133\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9911\n",
      "Epoch 00100: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 999us/sample - loss: 0.0375 - acc: 0.9911 - val_loss: 0.3192 - val_acc: 0.9210\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9918\n",
      "Epoch 00101: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0355 - acc: 0.9918 - val_loss: 0.3454 - val_acc: 0.9143\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9889\n",
      "Epoch 00102: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0433 - acc: 0.9888 - val_loss: 0.3362 - val_acc: 0.9171\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9909\n",
      "Epoch 00103: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 998us/sample - loss: 0.0379 - acc: 0.9909 - val_loss: 0.3707 - val_acc: 0.9082\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9937\n",
      "Epoch 00104: val_loss did not improve from 0.26476\n",
      "36805/36805 [==============================] - 37s 996us/sample - loss: 0.0318 - acc: 0.9938 - val_loss: 0.3330 - val_acc: 0.9189\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 469us/sample - loss: 0.3276 - acc: 0.9022\n",
      "Loss: 0.3276093834038217 Accuracy: 0.9021807\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8979 - acc: 0.4191\n",
      "Epoch 00001: val_loss improved from inf to 1.50367, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/001-1.5037.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.8978 - acc: 0.4192 - val_loss: 1.5037 - val_acc: 0.5674\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9856 - acc: 0.7182\n",
      "Epoch 00002: val_loss improved from 1.50367 to 0.74815, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/002-0.7482.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.9855 - acc: 0.7182 - val_loss: 0.7482 - val_acc: 0.7911\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6628 - acc: 0.8173\n",
      "Epoch 00003: val_loss improved from 0.74815 to 0.53750, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/003-0.5375.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.6628 - acc: 0.8173 - val_loss: 0.5375 - val_acc: 0.8509\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8591\n",
      "Epoch 00004: val_loss improved from 0.53750 to 0.46597, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/004-0.4660.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.5115 - acc: 0.8590 - val_loss: 0.4660 - val_acc: 0.8689\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8819\n",
      "Epoch 00005: val_loss improved from 0.46597 to 0.37332, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/005-0.3733.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.4244 - acc: 0.8819 - val_loss: 0.3733 - val_acc: 0.8952\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8989\n",
      "Epoch 00006: val_loss improved from 0.37332 to 0.34936, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/006-0.3494.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.3646 - acc: 0.8989 - val_loss: 0.3494 - val_acc: 0.9043\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.9098\n",
      "Epoch 00007: val_loss improved from 0.34936 to 0.30086, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/007-0.3009.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.3233 - acc: 0.9097 - val_loss: 0.3009 - val_acc: 0.9131\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9182\n",
      "Epoch 00008: val_loss did not improve from 0.30086\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2943 - acc: 0.9182 - val_loss: 0.3226 - val_acc: 0.9005\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9254\n",
      "Epoch 00009: val_loss improved from 0.30086 to 0.27589, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/009-0.2759.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2671 - acc: 0.9253 - val_loss: 0.2759 - val_acc: 0.9215\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9303\n",
      "Epoch 00010: val_loss improved from 0.27589 to 0.26117, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/010-0.2612.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2472 - acc: 0.9303 - val_loss: 0.2612 - val_acc: 0.9250\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9352\n",
      "Epoch 00011: val_loss improved from 0.26117 to 0.24431, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/011-0.2443.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2306 - acc: 0.9352 - val_loss: 0.2443 - val_acc: 0.9297\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9386\n",
      "Epoch 00012: val_loss improved from 0.24431 to 0.23938, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/012-0.2394.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2141 - acc: 0.9386 - val_loss: 0.2394 - val_acc: 0.9280\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9432\n",
      "Epoch 00013: val_loss improved from 0.23938 to 0.23009, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/013-0.2301.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1999 - acc: 0.9432 - val_loss: 0.2301 - val_acc: 0.9320\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9466\n",
      "Epoch 00014: val_loss improved from 0.23009 to 0.21649, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/014-0.2165.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1872 - acc: 0.9466 - val_loss: 0.2165 - val_acc: 0.9369\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9498\n",
      "Epoch 00015: val_loss did not improve from 0.21649\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1773 - acc: 0.9497 - val_loss: 0.2411 - val_acc: 0.9287\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9520\n",
      "Epoch 00016: val_loss improved from 0.21649 to 0.20662, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/016-0.2066.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1679 - acc: 0.9520 - val_loss: 0.2066 - val_acc: 0.9378\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9545\n",
      "Epoch 00017: val_loss did not improve from 0.20662\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1589 - acc: 0.9545 - val_loss: 0.2119 - val_acc: 0.9355\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9579\n",
      "Epoch 00018: val_loss did not improve from 0.20662\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1492 - acc: 0.9578 - val_loss: 0.2127 - val_acc: 0.9364\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9589\n",
      "Epoch 00019: val_loss improved from 0.20662 to 0.19437, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/019-0.1944.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1453 - acc: 0.9589 - val_loss: 0.1944 - val_acc: 0.9408\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9625\n",
      "Epoch 00020: val_loss improved from 0.19437 to 0.18216, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv_checkpoint/020-0.1822.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1346 - acc: 0.9625 - val_loss: 0.1822 - val_acc: 0.9427\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9646\n",
      "Epoch 00021: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1260 - acc: 0.9646 - val_loss: 0.2189 - val_acc: 0.9317\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9657\n",
      "Epoch 00022: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1216 - acc: 0.9657 - val_loss: 0.1879 - val_acc: 0.9446\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9682\n",
      "Epoch 00023: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1154 - acc: 0.9682 - val_loss: 0.2118 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9698\n",
      "Epoch 00024: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1085 - acc: 0.9698 - val_loss: 0.1951 - val_acc: 0.9427\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9717\n",
      "Epoch 00025: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1034 - acc: 0.9717 - val_loss: 0.2196 - val_acc: 0.9320\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9742\n",
      "Epoch 00026: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0975 - acc: 0.9742 - val_loss: 0.1843 - val_acc: 0.9441\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9756\n",
      "Epoch 00027: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0933 - acc: 0.9756 - val_loss: 0.2255 - val_acc: 0.9327\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9760\n",
      "Epoch 00028: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0896 - acc: 0.9760 - val_loss: 0.1960 - val_acc: 0.9411\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9782\n",
      "Epoch 00029: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0838 - acc: 0.9782 - val_loss: 0.1976 - val_acc: 0.9406\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9778\n",
      "Epoch 00030: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0843 - acc: 0.9778 - val_loss: 0.1844 - val_acc: 0.9436\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9807\n",
      "Epoch 00031: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0759 - acc: 0.9807 - val_loss: 0.1943 - val_acc: 0.9392\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9822\n",
      "Epoch 00032: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0718 - acc: 0.9821 - val_loss: 0.1989 - val_acc: 0.9406\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9815\n",
      "Epoch 00033: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0728 - acc: 0.9815 - val_loss: 0.1936 - val_acc: 0.9422\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9857\n",
      "Epoch 00034: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0629 - acc: 0.9857 - val_loss: 0.1904 - val_acc: 0.9387\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9815\n",
      "Epoch 00035: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0696 - acc: 0.9815 - val_loss: 0.2100 - val_acc: 0.9390\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9845\n",
      "Epoch 00036: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0637 - acc: 0.9845 - val_loss: 0.2064 - val_acc: 0.9357\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9866\n",
      "Epoch 00037: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0561 - acc: 0.9866 - val_loss: 0.2047 - val_acc: 0.9371\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9875\n",
      "Epoch 00038: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0548 - acc: 0.9875 - val_loss: 0.2312 - val_acc: 0.9327\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9873\n",
      "Epoch 00039: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0539 - acc: 0.9873 - val_loss: 0.2034 - val_acc: 0.9399\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9898\n",
      "Epoch 00040: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0487 - acc: 0.9897 - val_loss: 0.2216 - val_acc: 0.9329\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9880\n",
      "Epoch 00041: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0507 - acc: 0.9880 - val_loss: 0.1913 - val_acc: 0.9406\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9908\n",
      "Epoch 00042: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0436 - acc: 0.9908 - val_loss: 0.2117 - val_acc: 0.9369\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9911\n",
      "Epoch 00043: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0423 - acc: 0.9910 - val_loss: 0.1990 - val_acc: 0.9429\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9904\n",
      "Epoch 00044: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0436 - acc: 0.9904 - val_loss: 0.2132 - val_acc: 0.9385\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9921\n",
      "Epoch 00045: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0392 - acc: 0.9921 - val_loss: 0.1957 - val_acc: 0.9425\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9929\n",
      "Epoch 00046: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0366 - acc: 0.9929 - val_loss: 0.2134 - val_acc: 0.9371\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9918\n",
      "Epoch 00047: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0371 - acc: 0.9918 - val_loss: 0.2058 - val_acc: 0.9422\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9916\n",
      "Epoch 00048: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0382 - acc: 0.9916 - val_loss: 0.2213 - val_acc: 0.9366\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9947\n",
      "Epoch 00049: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0302 - acc: 0.9947 - val_loss: 0.2094 - val_acc: 0.9390\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9946\n",
      "Epoch 00050: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0302 - acc: 0.9946 - val_loss: 0.2181 - val_acc: 0.9357\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9955\n",
      "Epoch 00051: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0289 - acc: 0.9955 - val_loss: 0.2164 - val_acc: 0.9404\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9935\n",
      "Epoch 00052: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0315 - acc: 0.9935 - val_loss: 0.2181 - val_acc: 0.9373\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9938\n",
      "Epoch 00053: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0310 - acc: 0.9938 - val_loss: 0.2110 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9934\n",
      "Epoch 00054: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0314 - acc: 0.9934 - val_loss: 0.2207 - val_acc: 0.9385\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9962\n",
      "Epoch 00055: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0231 - acc: 0.9963 - val_loss: 0.2064 - val_acc: 0.9455\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9961\n",
      "Epoch 00056: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0232 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9427\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0223 - acc: 0.9964 - val_loss: 0.2483 - val_acc: 0.9327\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9967\n",
      "Epoch 00058: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0216 - acc: 0.9967 - val_loss: 0.2274 - val_acc: 0.9355\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0224 - acc: 0.9961 - val_loss: 0.2258 - val_acc: 0.9406\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9964\n",
      "Epoch 00060: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0209 - acc: 0.9964 - val_loss: 0.2325 - val_acc: 0.9373\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0236 - acc: 0.9950 - val_loss: 0.2482 - val_acc: 0.9327\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9953\n",
      "Epoch 00062: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0234 - acc: 0.9953 - val_loss: 0.2141 - val_acc: 0.9432\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9976\n",
      "Epoch 00063: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0164 - acc: 0.9976 - val_loss: 0.2374 - val_acc: 0.9362\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9951\n",
      "Epoch 00064: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0231 - acc: 0.9951 - val_loss: 0.2200 - val_acc: 0.9408\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9967\n",
      "Epoch 00065: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0187 - acc: 0.9967 - val_loss: 0.2302 - val_acc: 0.9397\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9983\n",
      "Epoch 00066: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0146 - acc: 0.9983 - val_loss: 0.2603 - val_acc: 0.9308\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9976\n",
      "Epoch 00067: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0167 - acc: 0.9976 - val_loss: 0.2208 - val_acc: 0.9397\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9976\n",
      "Epoch 00068: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0150 - acc: 0.9976 - val_loss: 0.2299 - val_acc: 0.9411\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9969\n",
      "Epoch 00069: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0173 - acc: 0.9969 - val_loss: 0.2244 - val_acc: 0.9408\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9973\n",
      "Epoch 00070: val_loss did not improve from 0.18216\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.0158 - acc: 0.9973 - val_loss: 0.2602 - val_acc: 0.9315\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 535us/sample - loss: 0.2230 - acc: 0.9313\n",
      "Loss: 0.22299909570630588 Accuracy: 0.9312565\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6618 - acc: 0.4873\n",
      "Epoch 00001: val_loss improved from inf to 1.26948, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/001-1.2695.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.6619 - acc: 0.4873 - val_loss: 1.2695 - val_acc: 0.6355\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.7689\n",
      "Epoch 00002: val_loss improved from 1.26948 to 0.56756, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/002-0.5676.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.7948 - acc: 0.7688 - val_loss: 0.5676 - val_acc: 0.8479\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.8474\n",
      "Epoch 00003: val_loss improved from 0.56756 to 0.42823, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/003-0.4282.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.5402 - acc: 0.8474 - val_loss: 0.4282 - val_acc: 0.8845\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8831\n",
      "Epoch 00004: val_loss improved from 0.42823 to 0.36005, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/004-0.3600.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.4140 - acc: 0.8831 - val_loss: 0.3600 - val_acc: 0.8970\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.9034\n",
      "Epoch 00005: val_loss improved from 0.36005 to 0.28555, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/005-0.2856.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.3397 - acc: 0.9034 - val_loss: 0.2856 - val_acc: 0.9196\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9161- ETA: 1s \n",
      "Epoch 00006: val_loss improved from 0.28555 to 0.25585, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/006-0.2558.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.2891 - acc: 0.9162 - val_loss: 0.2558 - val_acc: 0.9283\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9265\n",
      "Epoch 00007: val_loss improved from 0.25585 to 0.24384, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/007-0.2438.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.2513 - acc: 0.9265 - val_loss: 0.2438 - val_acc: 0.9308\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9343\n",
      "Epoch 00008: val_loss improved from 0.24384 to 0.21066, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/008-0.2107.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2248 - acc: 0.9343 - val_loss: 0.2107 - val_acc: 0.9387\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9429\n",
      "Epoch 00009: val_loss did not improve from 0.21066\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1999 - acc: 0.9429 - val_loss: 0.2319 - val_acc: 0.9294\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9466\n",
      "Epoch 00010: val_loss improved from 0.21066 to 0.18779, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/010-0.1878.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1853 - acc: 0.9466 - val_loss: 0.1878 - val_acc: 0.9448\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9518\n",
      "Epoch 00011: val_loss improved from 0.18779 to 0.17749, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/011-0.1775.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1674 - acc: 0.9518 - val_loss: 0.1775 - val_acc: 0.9485\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9573\n",
      "Epoch 00012: val_loss did not improve from 0.17749\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1519 - acc: 0.9572 - val_loss: 0.1850 - val_acc: 0.9469\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9597\n",
      "Epoch 00013: val_loss improved from 0.17749 to 0.16589, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/013-0.1659.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1413 - acc: 0.9597 - val_loss: 0.1659 - val_acc: 0.9515\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9630\n",
      "Epoch 00014: val_loss improved from 0.16589 to 0.15796, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/014-0.1580.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1313 - acc: 0.9630 - val_loss: 0.1580 - val_acc: 0.9525\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9649\n",
      "Epoch 00015: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1234 - acc: 0.9650 - val_loss: 0.1580 - val_acc: 0.9497\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9693\n",
      "Epoch 00016: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1126 - acc: 0.9693 - val_loss: 0.1643 - val_acc: 0.9488\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9720\n",
      "Epoch 00017: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.1044 - acc: 0.9720 - val_loss: 0.1670 - val_acc: 0.9483\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9727\n",
      "Epoch 00018: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0978 - acc: 0.9727 - val_loss: 0.1656 - val_acc: 0.9497\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9746\n",
      "Epoch 00019: val_loss improved from 0.15796 to 0.15391, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/019-0.1539.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0936 - acc: 0.9746 - val_loss: 0.1539 - val_acc: 0.9548\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9775\n",
      "Epoch 00020: val_loss did not improve from 0.15391\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0855 - acc: 0.9775 - val_loss: 0.1644 - val_acc: 0.9499\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9795\n",
      "Epoch 00021: val_loss did not improve from 0.15391\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0788 - acc: 0.9795 - val_loss: 0.1607 - val_acc: 0.9525\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9810\n",
      "Epoch 00022: val_loss improved from 0.15391 to 0.14830, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv_checkpoint/022-0.1483.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0734 - acc: 0.9810 - val_loss: 0.1483 - val_acc: 0.9562\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9833\n",
      "Epoch 00023: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0680 - acc: 0.9833 - val_loss: 0.1608 - val_acc: 0.9532\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9844\n",
      "Epoch 00024: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0639 - acc: 0.9844 - val_loss: 0.1681 - val_acc: 0.9483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9867\n",
      "Epoch 00025: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0573 - acc: 0.9867 - val_loss: 0.1614 - val_acc: 0.9506\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9869\n",
      "Epoch 00026: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0553 - acc: 0.9869 - val_loss: 0.1695 - val_acc: 0.9504\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9873\n",
      "Epoch 00027: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0537 - acc: 0.9873 - val_loss: 0.1853 - val_acc: 0.9439\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9905\n",
      "Epoch 00028: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0456 - acc: 0.9905 - val_loss: 0.1551 - val_acc: 0.9536\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9895\n",
      "Epoch 00029: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0464 - acc: 0.9895 - val_loss: 0.1572 - val_acc: 0.9529\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9918\n",
      "Epoch 00030: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0409 - acc: 0.9918 - val_loss: 0.1668 - val_acc: 0.9515\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9920\n",
      "Epoch 00031: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0375 - acc: 0.9920 - val_loss: 0.1659 - val_acc: 0.9515\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9924\n",
      "Epoch 00032: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0372 - acc: 0.9924 - val_loss: 0.1737 - val_acc: 0.9492\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9941\n",
      "Epoch 00033: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0310 - acc: 0.9941 - val_loss: 0.1604 - val_acc: 0.9541\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9951\n",
      "Epoch 00034: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0294 - acc: 0.9950 - val_loss: 0.2300 - val_acc: 0.9355\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9910\n",
      "Epoch 00035: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0375 - acc: 0.9910 - val_loss: 0.1651 - val_acc: 0.9513\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9952\n",
      "Epoch 00036: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0264 - acc: 0.9952 - val_loss: 0.1537 - val_acc: 0.9567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9961\n",
      "Epoch 00037: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0245 - acc: 0.9961 - val_loss: 0.1911 - val_acc: 0.9460\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9953\n",
      "Epoch 00038: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0261 - acc: 0.9953 - val_loss: 0.1899 - val_acc: 0.9471\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9940\n",
      "Epoch 00039: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0289 - acc: 0.9939 - val_loss: 0.1838 - val_acc: 0.9485\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9959\n",
      "Epoch 00040: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0236 - acc: 0.9959 - val_loss: 0.1792 - val_acc: 0.9481\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9972- ETA: 1s - loss: 0.\n",
      "Epoch 00041: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0192 - acc: 0.9972 - val_loss: 0.1776 - val_acc: 0.9525\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9960\n",
      "Epoch 00042: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0210 - acc: 0.9960 - val_loss: 0.1695 - val_acc: 0.9541\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9970\n",
      "Epoch 00043: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0181 - acc: 0.9970 - val_loss: 0.1853 - val_acc: 0.9483\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9981\n",
      "Epoch 00044: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0154 - acc: 0.9981 - val_loss: 0.1735 - val_acc: 0.9527\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9964\n",
      "Epoch 00045: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0186 - acc: 0.9964 - val_loss: 0.1809 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9960\n",
      "Epoch 00046: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0194 - acc: 0.9960 - val_loss: 0.1733 - val_acc: 0.9543\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0140 - acc: 0.9982 - val_loss: 0.1837 - val_acc: 0.9536\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9979\n",
      "Epoch 00048: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0142 - acc: 0.9979 - val_loss: 0.1819 - val_acc: 0.9529\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9964\n",
      "Epoch 00049: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0186 - acc: 0.9964 - val_loss: 0.1748 - val_acc: 0.9532\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9985\n",
      "Epoch 00050: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0122 - acc: 0.9985 - val_loss: 0.1832 - val_acc: 0.9513\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9983\n",
      "Epoch 00051: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0122 - acc: 0.9983 - val_loss: 0.1783 - val_acc: 0.9534\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9970\n",
      "Epoch 00052: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0162 - acc: 0.9970 - val_loss: 0.1809 - val_acc: 0.9555\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9981\n",
      "Epoch 00053: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0113 - acc: 0.9981 - val_loss: 0.1701 - val_acc: 0.9578\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0151 - acc: 0.9970 - val_loss: 0.1995 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9967\n",
      "Epoch 00055: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0157 - acc: 0.9967 - val_loss: 0.1799 - val_acc: 0.9550\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9985\n",
      "Epoch 00056: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0113 - acc: 0.9984 - val_loss: 0.1798 - val_acc: 0.9574\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9979\n",
      "Epoch 00057: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0122 - acc: 0.9979 - val_loss: 0.1734 - val_acc: 0.9550\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9992\n",
      "Epoch 00058: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0083 - acc: 0.9991 - val_loss: 0.1727 - val_acc: 0.9550\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9917\n",
      "Epoch 00059: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0296 - acc: 0.9917 - val_loss: 0.1986 - val_acc: 0.9534\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9971- ETA: 1s - loss\n",
      "Epoch 00060: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0146 - acc: 0.9971 - val_loss: 0.1785 - val_acc: 0.9567\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9990\n",
      "Epoch 00061: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0078 - acc: 0.9990 - val_loss: 0.1720 - val_acc: 0.9585\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9992\n",
      "Epoch 00062: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0077 - acc: 0.9992 - val_loss: 0.1770 - val_acc: 0.9562\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9994\n",
      "Epoch 00063: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0064 - acc: 0.9994 - val_loss: 0.1880 - val_acc: 0.9534\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9986\n",
      "Epoch 00064: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0086 - acc: 0.9986 - val_loss: 0.2086 - val_acc: 0.9492\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9985\n",
      "Epoch 00065: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0102 - acc: 0.9984 - val_loss: 0.2068 - val_acc: 0.9483\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9956\n",
      "Epoch 00066: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0178 - acc: 0.9956 - val_loss: 0.1872 - val_acc: 0.9595\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9983\n",
      "Epoch 00067: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0102 - acc: 0.9983 - val_loss: 0.1775 - val_acc: 0.9578\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9993\n",
      "Epoch 00068: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0063 - acc: 0.9993 - val_loss: 0.1998 - val_acc: 0.9527\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9961\n",
      "Epoch 00069: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0155 - acc: 0.9961 - val_loss: 0.1904 - val_acc: 0.9539\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00070: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.1780 - val_acc: 0.9585\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 00071: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0058 - acc: 0.9992 - val_loss: 0.1919 - val_acc: 0.9502\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9988\n",
      "Epoch 00072: val_loss did not improve from 0.14830\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.0082 - acc: 0.9988 - val_loss: 0.1991 - val_acc: 0.9548\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 584us/sample - loss: 0.1966 - acc: 0.9379\n",
      "Loss: 0.1966164680903822 Accuracy: 0.9379024\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5946 - acc: 0.5111\n",
      "Epoch 00001: val_loss improved from inf to 1.27154, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/001-1.2715.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.5945 - acc: 0.5111 - val_loss: 1.2715 - val_acc: 0.6401\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7226 - acc: 0.7846\n",
      "Epoch 00002: val_loss improved from 1.27154 to 0.52836, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/002-0.5284.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7226 - acc: 0.7846 - val_loss: 0.5284 - val_acc: 0.8437\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.8551\n",
      "Epoch 00003: val_loss improved from 0.52836 to 0.40125, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/003-0.4012.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4902 - acc: 0.8551 - val_loss: 0.4012 - val_acc: 0.8838\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8854\n",
      "Epoch 00004: val_loss improved from 0.40125 to 0.33773, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/004-0.3377.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3820 - acc: 0.8854 - val_loss: 0.3377 - val_acc: 0.8989\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9061\n",
      "Epoch 00005: val_loss improved from 0.33773 to 0.29361, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/005-0.2936.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3180 - acc: 0.9061 - val_loss: 0.2936 - val_acc: 0.9115\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9181\n",
      "Epoch 00006: val_loss improved from 0.29361 to 0.25499, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/006-0.2550.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2716 - acc: 0.9180 - val_loss: 0.2550 - val_acc: 0.9264\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9266\n",
      "Epoch 00007: val_loss improved from 0.25499 to 0.24192, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/007-0.2419.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2417 - acc: 0.9266 - val_loss: 0.2419 - val_acc: 0.9285\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9370\n",
      "Epoch 00008: val_loss improved from 0.24192 to 0.21312, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/008-0.2131.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2117 - acc: 0.9370 - val_loss: 0.2131 - val_acc: 0.9369\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9411\n",
      "Epoch 00009: val_loss did not improve from 0.21312\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1937 - acc: 0.9410 - val_loss: 0.2162 - val_acc: 0.9336\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9481\n",
      "Epoch 00010: val_loss improved from 0.21312 to 0.19678, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/010-0.1968.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1753 - acc: 0.9481 - val_loss: 0.1968 - val_acc: 0.9387\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9536\n",
      "Epoch 00011: val_loss improved from 0.19678 to 0.19475, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/011-0.1948.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1560 - acc: 0.9536 - val_loss: 0.1948 - val_acc: 0.9394\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9568\n",
      "Epoch 00012: val_loss did not improve from 0.19475\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1451 - acc: 0.9567 - val_loss: 0.2668 - val_acc: 0.9126\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9596\n",
      "Epoch 00013: val_loss improved from 0.19475 to 0.18690, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/013-0.1869.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1380 - acc: 0.9596 - val_loss: 0.1869 - val_acc: 0.9411\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9639\n",
      "Epoch 00014: val_loss improved from 0.18690 to 0.17388, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/014-0.1739.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1219 - acc: 0.9639 - val_loss: 0.1739 - val_acc: 0.9422\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9688\n",
      "Epoch 00015: val_loss improved from 0.17388 to 0.17241, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/015-0.1724.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1087 - acc: 0.9688 - val_loss: 0.1724 - val_acc: 0.9467\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9710\n",
      "Epoch 00016: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1029 - acc: 0.9710 - val_loss: 0.1952 - val_acc: 0.9411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9735\n",
      "Epoch 00017: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0946 - acc: 0.9734 - val_loss: 0.1891 - val_acc: 0.9383\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9773\n",
      "Epoch 00018: val_loss improved from 0.17241 to 0.16686, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/018-0.1669.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0853 - acc: 0.9773 - val_loss: 0.1669 - val_acc: 0.9495\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9798\n",
      "Epoch 00019: val_loss improved from 0.16686 to 0.16227, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv_checkpoint/019-0.1623.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0773 - acc: 0.9798 - val_loss: 0.1623 - val_acc: 0.9476\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9814\n",
      "Epoch 00020: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0694 - acc: 0.9814 - val_loss: 0.1818 - val_acc: 0.9404\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9831\n",
      "Epoch 00021: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0657 - acc: 0.9831 - val_loss: 0.1788 - val_acc: 0.9436\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9842\n",
      "Epoch 00022: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0615 - acc: 0.9841 - val_loss: 0.1685 - val_acc: 0.9460\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9844\n",
      "Epoch 00023: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0611 - acc: 0.9844 - val_loss: 0.1704 - val_acc: 0.9483\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9893\n",
      "Epoch 00024: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0484 - acc: 0.9893 - val_loss: 0.1814 - val_acc: 0.9453\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9897\n",
      "Epoch 00025: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0453 - acc: 0.9897 - val_loss: 0.1729 - val_acc: 0.9492\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9903\n",
      "Epoch 00026: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0420 - acc: 0.9902 - val_loss: 0.1786 - val_acc: 0.9460\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9910\n",
      "Epoch 00027: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0410 - acc: 0.9910 - val_loss: 0.1940 - val_acc: 0.9397\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9898\n",
      "Epoch 00028: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0423 - acc: 0.9898 - val_loss: 0.1882 - val_acc: 0.9429\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9924\n",
      "Epoch 00029: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0353 - acc: 0.9924 - val_loss: 0.1871 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9944\n",
      "Epoch 00030: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0304 - acc: 0.9944 - val_loss: 0.2199 - val_acc: 0.9331\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9915\n",
      "Epoch 00031: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0360 - acc: 0.9915 - val_loss: 0.1842 - val_acc: 0.9457\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9937\n",
      "Epoch 00032: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0307 - acc: 0.9938 - val_loss: 0.1757 - val_acc: 0.9481\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9957\n",
      "Epoch 00033: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0237 - acc: 0.9956 - val_loss: 0.2466 - val_acc: 0.9322\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9937\n",
      "Epoch 00034: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0284 - acc: 0.9937 - val_loss: 0.1851 - val_acc: 0.9462\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9966\n",
      "Epoch 00035: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0208 - acc: 0.9966 - val_loss: 0.1837 - val_acc: 0.9478\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9966\n",
      "Epoch 00036: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0208 - acc: 0.9965 - val_loss: 0.2022 - val_acc: 0.9425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9929\n",
      "Epoch 00037: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0294 - acc: 0.9929 - val_loss: 0.1786 - val_acc: 0.9511\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9937\n",
      "Epoch 00038: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0262 - acc: 0.9937 - val_loss: 0.1856 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9970\n",
      "Epoch 00039: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0167 - acc: 0.9970 - val_loss: 0.1803 - val_acc: 0.9488\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9960\n",
      "Epoch 00040: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0204 - acc: 0.9960 - val_loss: 0.1852 - val_acc: 0.9502\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9972\n",
      "Epoch 00041: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0159 - acc: 0.9971 - val_loss: 0.1908 - val_acc: 0.9483\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9910\n",
      "Epoch 00042: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0322 - acc: 0.9910 - val_loss: 0.1743 - val_acc: 0.9511\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9985\n",
      "Epoch 00043: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0119 - acc: 0.9985 - val_loss: 0.1734 - val_acc: 0.9522\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9976\n",
      "Epoch 00044: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0140 - acc: 0.9976 - val_loss: 0.2007 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9971\n",
      "Epoch 00045: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0158 - acc: 0.9971 - val_loss: 0.1947 - val_acc: 0.9453\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9980\n",
      "Epoch 00046: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0117 - acc: 0.9980 - val_loss: 0.2000 - val_acc: 0.9443\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9977\n",
      "Epoch 00047: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0138 - acc: 0.9977 - val_loss: 0.1853 - val_acc: 0.9509\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9975\n",
      "Epoch 00048: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0134 - acc: 0.9975 - val_loss: 0.1969 - val_acc: 0.9495\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9979\n",
      "Epoch 00049: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0117 - acc: 0.9979 - val_loss: 0.2070 - val_acc: 0.9436\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9982\n",
      "Epoch 00050: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0114 - acc: 0.9982 - val_loss: 0.1869 - val_acc: 0.9504\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9960\n",
      "Epoch 00051: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0159 - acc: 0.9960 - val_loss: 0.1997 - val_acc: 0.9478\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9924\n",
      "Epoch 00052: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0275 - acc: 0.9924 - val_loss: 0.1910 - val_acc: 0.9497\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9990\n",
      "Epoch 00053: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0080 - acc: 0.9990 - val_loss: 0.2056 - val_acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0183 - acc: 0.9955 - val_loss: 0.1833 - val_acc: 0.9506\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9993\n",
      "Epoch 00055: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0069 - acc: 0.9993 - val_loss: 0.1825 - val_acc: 0.9522\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9996\n",
      "Epoch 00056: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0053 - acc: 0.9996 - val_loss: 0.1876 - val_acc: 0.9513\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9982\n",
      "Epoch 00057: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0105 - acc: 0.9982 - val_loss: 0.2821 - val_acc: 0.9292\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9945\n",
      "Epoch 00058: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0220 - acc: 0.9944 - val_loss: 0.2072 - val_acc: 0.9460\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0171 - acc: 0.9960 - val_loss: 0.1835 - val_acc: 0.9527\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9972\n",
      "Epoch 00060: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0121 - acc: 0.9972 - val_loss: 0.1844 - val_acc: 0.9525\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0134 - acc: 0.9969 - val_loss: 0.1767 - val_acc: 0.9532\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9993\n",
      "Epoch 00062: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0057 - acc: 0.9993 - val_loss: 0.1963 - val_acc: 0.9502\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9988\n",
      "Epoch 00063: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0073 - acc: 0.9988 - val_loss: 0.1957 - val_acc: 0.9522\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9992\n",
      "Epoch 00064: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0057 - acc: 0.9991 - val_loss: 0.2005 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9969\n",
      "Epoch 00065: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0136 - acc: 0.9969 - val_loss: 0.2007 - val_acc: 0.9497\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9989\n",
      "Epoch 00066: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0072 - acc: 0.9989 - val_loss: 0.2143 - val_acc: 0.9490\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9986\n",
      "Epoch 00067: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.2217 - val_acc: 0.9446\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 00068: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0162 - acc: 0.9955 - val_loss: 0.1956 - val_acc: 0.9502\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9976\n",
      "Epoch 00069: val_loss did not improve from 0.16227\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0099 - acc: 0.9976 - val_loss: 0.1909 - val_acc: 0.9502\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 652us/sample - loss: 0.2249 - acc: 0.9308\n",
      "Loss: 0.2249257017147504 Accuracy: 0.93084115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GMP_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 96)           384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 585us/sample - loss: 0.6380 - acc: 0.8004\n",
      "Loss: 0.6380394508657921 Accuracy: 0.8004154\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 96)           384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 646us/sample - loss: 0.5293 - acc: 0.8430\n",
      "Loss: 0.5293050099818011 Accuracy: 0.84299064\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 128)          512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 683us/sample - loss: 0.3276 - acc: 0.9022\n",
      "Loss: 0.3276093834038217 Accuracy: 0.9021807\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 160)          640         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 687us/sample - loss: 0.2230 - acc: 0.9313\n",
      "Loss: 0.22299909570630588 Accuracy: 0.9312565\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 192)          768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 784us/sample - loss: 0.1966 - acc: 0.9379\n",
      "Loss: 0.1966164680903822 Accuracy: 0.9379024\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 192)          768         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 796us/sample - loss: 0.2249 - acc: 0.9308\n",
      "Loss: 0.2249257017147504 Accuracy: 0.93084115\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GMP_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 96)           384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 725us/sample - loss: 0.7012 - acc: 0.7913\n",
      "Loss: 0.7011649529015411 Accuracy: 0.7912772\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 96)           384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 766us/sample - loss: 0.6002 - acc: 0.8330\n",
      "Loss: 0.6002456885013006 Accuracy: 0.8330218\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 128)          512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 789us/sample - loss: 0.4176 - acc: 0.8974\n",
      "Loss: 0.4176443649353392 Accuracy: 0.89740396\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 160)          640         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 867us/sample - loss: 0.2955 - acc: 0.9259\n",
      "Loss: 0.2954702365466491 Accuracy: 0.9258567\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 192)          768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 911us/sample - loss: 0.2446 - acc: 0.9398\n",
      "Loss: 0.24457258580567803 Accuracy: 0.93977153\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 192)          768         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 917us/sample - loss: 0.2818 - acc: 0.9331\n",
      "Loss: 0.28179021773246715 Accuracy: 0.9331257\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
