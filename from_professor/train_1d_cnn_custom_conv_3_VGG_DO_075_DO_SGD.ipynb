{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7243 - acc: 0.0843\n",
      "Epoch 00001: val_loss improved from inf to 2.72225, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/001-2.7223.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7243 - acc: 0.0843 - val_loss: 2.7223 - val_acc: 0.0818\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6999 - acc: 0.1013\n",
      "Epoch 00002: val_loss improved from 2.72225 to 2.65173, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/002-2.6517.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 2.6999 - acc: 0.1013 - val_loss: 2.6517 - val_acc: 0.1200\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3222 - acc: 0.2457\n",
      "Epoch 00003: val_loss improved from 2.65173 to 2.12499, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/003-2.1250.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 2.3222 - acc: 0.2456 - val_loss: 2.1250 - val_acc: 0.3205\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9569 - acc: 0.3682\n",
      "Epoch 00004: val_loss improved from 2.12499 to 1.96037, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/004-1.9604.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.9568 - acc: 0.3682 - val_loss: 1.9604 - val_acc: 0.3706\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5388 - acc: 0.5109\n",
      "Epoch 00005: val_loss did not improve from 1.96037\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.5390 - acc: 0.5108 - val_loss: 2.5623 - val_acc: 0.2986\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2022 - acc: 0.6239\n",
      "Epoch 00006: val_loss improved from 1.96037 to 1.23579, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/006-1.2358.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.2022 - acc: 0.6240 - val_loss: 1.2358 - val_acc: 0.6229\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9803 - acc: 0.6942\n",
      "Epoch 00007: val_loss improved from 1.23579 to 1.10654, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv_checkpoint/007-1.1065.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9802 - acc: 0.6942 - val_loss: 1.1065 - val_acc: 0.6727\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8636 - acc: 0.7309\n",
      "Epoch 00008: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8638 - acc: 0.7309 - val_loss: 1.3598 - val_acc: 0.5872\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7640 - acc: 0.7568\n",
      "Epoch 00009: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7640 - acc: 0.7568 - val_loss: 1.1218 - val_acc: 0.6571\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6731 - acc: 0.7865\n",
      "Epoch 00010: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6731 - acc: 0.7866 - val_loss: 1.1123 - val_acc: 0.6774\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8110\n",
      "Epoch 00011: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5966 - acc: 0.8110 - val_loss: 1.1208 - val_acc: 0.6883\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8223\n",
      "Epoch 00012: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5502 - acc: 0.8223 - val_loss: 1.1215 - val_acc: 0.6841\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.8355\n",
      "Epoch 00013: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5100 - acc: 0.8354 - val_loss: 1.1932 - val_acc: 0.6706\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8489\n",
      "Epoch 00014: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4714 - acc: 0.8489 - val_loss: 1.1408 - val_acc: 0.6956\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8559\n",
      "Epoch 00015: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4430 - acc: 0.8559 - val_loss: 1.2869 - val_acc: 0.6734\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8632\n",
      "Epoch 00016: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4199 - acc: 0.8632 - val_loss: 1.2697 - val_acc: 0.6676\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8740\n",
      "Epoch 00017: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3924 - acc: 0.8741 - val_loss: 1.2140 - val_acc: 0.6893\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8771\n",
      "Epoch 00018: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3759 - acc: 0.8771 - val_loss: 1.2461 - val_acc: 0.6855\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8847\n",
      "Epoch 00019: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3550 - acc: 0.8847 - val_loss: 1.2394 - val_acc: 0.6916\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8884\n",
      "Epoch 00020: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3420 - acc: 0.8884 - val_loss: 1.3252 - val_acc: 0.6799\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8925\n",
      "Epoch 00021: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3356 - acc: 0.8925 - val_loss: 1.2580 - val_acc: 0.6953\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.8983\n",
      "Epoch 00022: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3169 - acc: 0.8983 - val_loss: 1.2166 - val_acc: 0.6993\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8990\n",
      "Epoch 00023: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.3116 - acc: 0.8991 - val_loss: 1.2278 - val_acc: 0.7032\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9030\n",
      "Epoch 00024: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2975 - acc: 0.9030 - val_loss: 1.3401 - val_acc: 0.6657\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9078\n",
      "Epoch 00025: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2878 - acc: 0.9078 - val_loss: 1.3893 - val_acc: 0.7002\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9135\n",
      "Epoch 00026: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2688 - acc: 0.9135 - val_loss: 1.3098 - val_acc: 0.7004\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9160\n",
      "Epoch 00027: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2616 - acc: 0.9160 - val_loss: 1.3096 - val_acc: 0.6993\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9148\n",
      "Epoch 00028: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2687 - acc: 0.9147 - val_loss: 1.3252 - val_acc: 0.6562\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9108\n",
      "Epoch 00029: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2759 - acc: 0.9108 - val_loss: 1.2747 - val_acc: 0.7049\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9208\n",
      "Epoch 00030: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2462 - acc: 0.9208 - val_loss: 1.3607 - val_acc: 0.7000\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9226\n",
      "Epoch 00031: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2417 - acc: 0.9226 - val_loss: 1.3884 - val_acc: 0.6946\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9208\n",
      "Epoch 00032: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2457 - acc: 0.9208 - val_loss: 1.2769 - val_acc: 0.7172\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9235\n",
      "Epoch 00033: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2416 - acc: 0.9235 - val_loss: 1.3195 - val_acc: 0.6976\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9281\n",
      "Epoch 00034: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2229 - acc: 0.9281 - val_loss: 1.4361 - val_acc: 0.7116\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9279\n",
      "Epoch 00035: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2265 - acc: 0.9278 - val_loss: 1.4676 - val_acc: 0.6732\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9304\n",
      "Epoch 00036: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2190 - acc: 0.9304 - val_loss: 1.3287 - val_acc: 0.7279\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9315\n",
      "Epoch 00037: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2128 - acc: 0.9314 - val_loss: 1.9974 - val_acc: 0.6010\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9252\n",
      "Epoch 00038: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2382 - acc: 0.9252 - val_loss: 1.2988 - val_acc: 0.7018\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9335\n",
      "Epoch 00039: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2138 - acc: 0.9335 - val_loss: 1.3001 - val_acc: 0.7114\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9314\n",
      "Epoch 00040: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2174 - acc: 0.9314 - val_loss: 1.3185 - val_acc: 0.7254\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9396\n",
      "Epoch 00041: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1911 - acc: 0.9396 - val_loss: 1.3946 - val_acc: 0.7202\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9371\n",
      "Epoch 00042: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1996 - acc: 0.9370 - val_loss: 1.4335 - val_acc: 0.6788\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9336\n",
      "Epoch 00043: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2059 - acc: 0.9336 - val_loss: 1.4332 - val_acc: 0.6942\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9374\n",
      "Epoch 00044: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1945 - acc: 0.9374 - val_loss: 1.5201 - val_acc: 0.6951\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9414\n",
      "Epoch 00045: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1874 - acc: 0.9414 - val_loss: 1.5236 - val_acc: 0.7014\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9376\n",
      "Epoch 00046: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.1929 - acc: 0.9376 - val_loss: 1.3107 - val_acc: 0.7230\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9415\n",
      "Epoch 00047: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1818 - acc: 0.9415 - val_loss: 1.5401 - val_acc: 0.6844\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9410\n",
      "Epoch 00048: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.1894 - acc: 0.9410 - val_loss: 1.3525 - val_acc: 0.7091\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9434\n",
      "Epoch 00049: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1781 - acc: 0.9434 - val_loss: 1.5290 - val_acc: 0.6902\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9381\n",
      "Epoch 00050: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1934 - acc: 0.9381 - val_loss: 1.3687 - val_acc: 0.7142\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9441\n",
      "Epoch 00051: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.1785 - acc: 0.9441 - val_loss: 1.3723 - val_acc: 0.7067\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9459\n",
      "Epoch 00052: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1722 - acc: 0.9459 - val_loss: 1.3720 - val_acc: 0.7135\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9445\n",
      "Epoch 00053: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1775 - acc: 0.9445 - val_loss: 1.6163 - val_acc: 0.6872\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9440\n",
      "Epoch 00054: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1834 - acc: 0.9440 - val_loss: 1.3408 - val_acc: 0.7265\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9487\n",
      "Epoch 00055: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1613 - acc: 0.9487 - val_loss: 1.4494 - val_acc: 0.7228\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9501\n",
      "Epoch 00056: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1562 - acc: 0.9501 - val_loss: 1.3659 - val_acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9503\n",
      "Epoch 00057: val_loss did not improve from 1.10654\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1608 - acc: 0.9503 - val_loss: 1.3546 - val_acc: 0.7191\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 884us/sample - loss: 1.2714 - acc: 0.6195\n",
      "Loss: 1.2713904990833 Accuracy: 0.61952233\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6080 - acc: 0.1365\n",
      "Epoch 00001: val_loss improved from inf to 2.21475, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/001-2.2148.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 2.6079 - acc: 0.1365 - val_loss: 2.2148 - val_acc: 0.3035\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9484 - acc: 0.3576\n",
      "Epoch 00002: val_loss improved from 2.21475 to 2.10194, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/002-2.1019.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.9484 - acc: 0.3576 - val_loss: 2.1019 - val_acc: 0.2718\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4154 - acc: 0.5438\n",
      "Epoch 00003: val_loss improved from 2.10194 to 1.22908, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/003-1.2291.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.4154 - acc: 0.5438 - val_loss: 1.2291 - val_acc: 0.6119\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0250 - acc: 0.6798\n",
      "Epoch 00004: val_loss improved from 1.22908 to 0.94438, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/004-0.9444.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0250 - acc: 0.6798 - val_loss: 0.9444 - val_acc: 0.7119\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8852 - acc: 0.7264\n",
      "Epoch 00005: val_loss improved from 0.94438 to 0.80074, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/005-0.8007.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8851 - acc: 0.7265 - val_loss: 0.8007 - val_acc: 0.7715\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7923 - acc: 0.7539\n",
      "Epoch 00006: val_loss did not improve from 0.80074\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7923 - acc: 0.7539 - val_loss: 0.8636 - val_acc: 0.7496\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7740\n",
      "Epoch 00007: val_loss improved from 0.80074 to 0.72332, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/007-0.7233.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7320 - acc: 0.7741 - val_loss: 0.7233 - val_acc: 0.7990\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.7911\n",
      "Epoch 00008: val_loss improved from 0.72332 to 0.71811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/008-0.7181.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6738 - acc: 0.7911 - val_loss: 0.7181 - val_acc: 0.7897\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.8086\n",
      "Epoch 00009: val_loss did not improve from 0.71811\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6227 - acc: 0.8085 - val_loss: 0.7390 - val_acc: 0.7813\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.8179\n",
      "Epoch 00010: val_loss did not improve from 0.71811\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5905 - acc: 0.8179 - val_loss: 0.7459 - val_acc: 0.7785\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8247\n",
      "Epoch 00011: val_loss improved from 0.71811 to 0.68081, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/011-0.6808.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5601 - acc: 0.8248 - val_loss: 0.6808 - val_acc: 0.8041\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.8297\n",
      "Epoch 00012: val_loss improved from 0.68081 to 0.65980, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/012-0.6598.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5451 - acc: 0.8297 - val_loss: 0.6598 - val_acc: 0.8097\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.8344\n",
      "Epoch 00013: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5224 - acc: 0.8344 - val_loss: 0.7277 - val_acc: 0.7945\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8441\n",
      "Epoch 00014: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4980 - acc: 0.8441 - val_loss: 0.6978 - val_acc: 0.8095\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.8487\n",
      "Epoch 00015: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4779 - acc: 0.8487 - val_loss: 0.6799 - val_acc: 0.8120\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8550\n",
      "Epoch 00016: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4550 - acc: 0.8550 - val_loss: 0.7003 - val_acc: 0.8034\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8561\n",
      "Epoch 00017: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4480 - acc: 0.8561 - val_loss: 0.6760 - val_acc: 0.8141\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8634\n",
      "Epoch 00018: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4273 - acc: 0.8634 - val_loss: 1.0329 - val_acc: 0.7354\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8635\n",
      "Epoch 00019: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4368 - acc: 0.8634 - val_loss: 0.6820 - val_acc: 0.8146\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8665\n",
      "Epoch 00020: val_loss did not improve from 0.65980\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4197 - acc: 0.8665 - val_loss: 0.7064 - val_acc: 0.8046\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8701\n",
      "Epoch 00021: val_loss improved from 0.65980 to 0.63952, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv_checkpoint/021-0.6395.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4027 - acc: 0.8701 - val_loss: 0.6395 - val_acc: 0.8276\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8721\n",
      "Epoch 00022: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4002 - acc: 0.8721 - val_loss: 0.8619 - val_acc: 0.7727\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8714\n",
      "Epoch 00023: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3980 - acc: 0.8714 - val_loss: 0.7491 - val_acc: 0.8081\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8807\n",
      "Epoch 00024: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3727 - acc: 0.8807 - val_loss: 0.6870 - val_acc: 0.8251\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8827\n",
      "Epoch 00025: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3644 - acc: 0.8826 - val_loss: 0.6701 - val_acc: 0.8153\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8798\n",
      "Epoch 00026: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3758 - acc: 0.8798 - val_loss: 0.7147 - val_acc: 0.7971\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8838\n",
      "Epoch 00027: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3601 - acc: 0.8838 - val_loss: 0.6724 - val_acc: 0.8304\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8896\n",
      "Epoch 00028: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3387 - acc: 0.8897 - val_loss: 0.6886 - val_acc: 0.8311\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8865\n",
      "Epoch 00029: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3492 - acc: 0.8865 - val_loss: 1.0367 - val_acc: 0.7209\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8806\n",
      "Epoch 00030: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3638 - acc: 0.8806 - val_loss: 0.7607 - val_acc: 0.8134\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8904\n",
      "Epoch 00031: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3344 - acc: 0.8904 - val_loss: 0.7257 - val_acc: 0.8174\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8920\n",
      "Epoch 00032: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3312 - acc: 0.8919 - val_loss: 0.7145 - val_acc: 0.8246\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8938\n",
      "Epoch 00033: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3242 - acc: 0.8938 - val_loss: 0.7565 - val_acc: 0.8025\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8974\n",
      "Epoch 00034: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3181 - acc: 0.8974 - val_loss: 0.6635 - val_acc: 0.8286\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9017\n",
      "Epoch 00035: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3033 - acc: 0.9017 - val_loss: 0.6744 - val_acc: 0.8288\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9016\n",
      "Epoch 00036: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3021 - acc: 0.9016 - val_loss: 0.6779 - val_acc: 0.8204\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9027\n",
      "Epoch 00037: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2959 - acc: 0.9027 - val_loss: 0.6891 - val_acc: 0.8314\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9003\n",
      "Epoch 00038: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3031 - acc: 0.9003 - val_loss: 0.7363 - val_acc: 0.8130\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9018\n",
      "Epoch 00039: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2963 - acc: 0.9018 - val_loss: 0.7074 - val_acc: 0.8330\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9050\n",
      "Epoch 00040: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2914 - acc: 0.9050 - val_loss: 0.7277 - val_acc: 0.8206\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9068\n",
      "Epoch 00041: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2865 - acc: 0.9068 - val_loss: 0.6950 - val_acc: 0.8316\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9070\n",
      "Epoch 00042: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2871 - acc: 0.9070 - val_loss: 0.6997 - val_acc: 0.8116\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9072\n",
      "Epoch 00043: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2845 - acc: 0.9072 - val_loss: 0.7606 - val_acc: 0.8074\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9071\n",
      "Epoch 00044: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2900 - acc: 0.9071 - val_loss: 0.6677 - val_acc: 0.8293\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9093\n",
      "Epoch 00045: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2785 - acc: 0.9093 - val_loss: 0.6981 - val_acc: 0.8258\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9071\n",
      "Epoch 00046: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2804 - acc: 0.9072 - val_loss: 0.6829 - val_acc: 0.8316\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9122\n",
      "Epoch 00047: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2692 - acc: 0.9122 - val_loss: 0.7289 - val_acc: 0.8332\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9153\n",
      "Epoch 00048: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2637 - acc: 0.9153 - val_loss: 0.7434 - val_acc: 0.8283\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9111\n",
      "Epoch 00049: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2717 - acc: 0.9111 - val_loss: 0.7210 - val_acc: 0.8372\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9142\n",
      "Epoch 00050: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2650 - acc: 0.9142 - val_loss: 0.7196 - val_acc: 0.8283\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9152\n",
      "Epoch 00051: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2612 - acc: 0.9153 - val_loss: 0.7052 - val_acc: 0.8190\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9155\n",
      "Epoch 00052: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2604 - acc: 0.9154 - val_loss: 0.9739 - val_acc: 0.7470\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9039\n",
      "Epoch 00053: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2966 - acc: 0.9039 - val_loss: 0.7980 - val_acc: 0.8195\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9139\n",
      "Epoch 00054: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2674 - acc: 0.9138 - val_loss: 0.6933 - val_acc: 0.8323\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9151\n",
      "Epoch 00055: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2601 - acc: 0.9151 - val_loss: 0.7402 - val_acc: 0.8393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9154\n",
      "Epoch 00056: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2576 - acc: 0.9154 - val_loss: 0.6970 - val_acc: 0.8190\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9161\n",
      "Epoch 00057: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2553 - acc: 0.9161 - val_loss: 0.7211 - val_acc: 0.8269\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9203\n",
      "Epoch 00058: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2468 - acc: 0.9203 - val_loss: 0.8021 - val_acc: 0.8204\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9216\n",
      "Epoch 00059: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2438 - acc: 0.9216 - val_loss: 0.7140 - val_acc: 0.8197\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9208\n",
      "Epoch 00060: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2448 - acc: 0.9208 - val_loss: 0.7014 - val_acc: 0.8339\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9192\n",
      "Epoch 00061: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2407 - acc: 0.9192 - val_loss: 0.6742 - val_acc: 0.8211\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9120\n",
      "Epoch 00062: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2726 - acc: 0.9120 - val_loss: 0.7314 - val_acc: 0.8211\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9192\n",
      "Epoch 00063: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2497 - acc: 0.9192 - val_loss: 0.7931 - val_acc: 0.8255\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9205\n",
      "Epoch 00064: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2455 - acc: 0.9205 - val_loss: 0.7095 - val_acc: 0.8367\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9221\n",
      "Epoch 00065: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2412 - acc: 0.9221 - val_loss: 0.8038 - val_acc: 0.8188\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9187\n",
      "Epoch 00066: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2532 - acc: 0.9187 - val_loss: 0.7308 - val_acc: 0.8334\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9207\n",
      "Epoch 00067: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2427 - acc: 0.9207 - val_loss: 0.7406 - val_acc: 0.8330\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9080\n",
      "Epoch 00068: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2840 - acc: 0.9080 - val_loss: 0.7442 - val_acc: 0.8218\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9173\n",
      "Epoch 00069: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2518 - acc: 0.9173 - val_loss: 0.7699 - val_acc: 0.8241\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9218\n",
      "Epoch 00070: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2465 - acc: 0.9217 - val_loss: 0.7027 - val_acc: 0.8337\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9211\n",
      "Epoch 00071: val_loss did not improve from 0.63952\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2452 - acc: 0.9212 - val_loss: 0.7933 - val_acc: 0.8304\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 924us/sample - loss: 0.7915 - acc: 0.7801\n",
      "Loss: 0.7915258266225164 Accuracy: 0.7800623\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5330 - acc: 0.1623\n",
      "Epoch 00001: val_loss improved from inf to 2.17938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/001-2.1794.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.5328 - acc: 0.1624 - val_loss: 2.1794 - val_acc: 0.3149\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7903 - acc: 0.4087\n",
      "Epoch 00002: val_loss improved from 2.17938 to 1.72044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/002-1.7204.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.7903 - acc: 0.4087 - val_loss: 1.7204 - val_acc: 0.4449\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4760 - acc: 0.5218\n",
      "Epoch 00003: val_loss improved from 1.72044 to 1.56344, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/003-1.5634.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.4762 - acc: 0.5218 - val_loss: 1.5634 - val_acc: 0.4950\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4168 - acc: 0.2243\n",
      "Epoch 00004: val_loss did not improve from 1.56344\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.4166 - acc: 0.2243 - val_loss: 1.6939 - val_acc: 0.4444\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5343 - acc: 0.5035\n",
      "Epoch 00005: val_loss improved from 1.56344 to 1.27286, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/005-1.2729.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.5343 - acc: 0.5035 - val_loss: 1.2729 - val_acc: 0.6042\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2310 - acc: 0.6105\n",
      "Epoch 00006: val_loss improved from 1.27286 to 0.99935, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/006-0.9994.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.2309 - acc: 0.6105 - val_loss: 0.9994 - val_acc: 0.7067\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9972 - acc: 0.6943\n",
      "Epoch 00007: val_loss did not improve from 0.99935\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.9974 - acc: 0.6943 - val_loss: 1.4892 - val_acc: 0.5332\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9046 - acc: 0.7244\n",
      "Epoch 00008: val_loss improved from 0.99935 to 0.96075, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/008-0.9608.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.9046 - acc: 0.7244 - val_loss: 0.9608 - val_acc: 0.7352\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7599 - acc: 0.7700\n",
      "Epoch 00009: val_loss improved from 0.96075 to 0.71632, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/009-0.7163.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7598 - acc: 0.7700 - val_loss: 0.7163 - val_acc: 0.7971\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6976 - acc: 0.7894\n",
      "Epoch 00010: val_loss improved from 0.71632 to 0.60882, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/010-0.6088.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6976 - acc: 0.7893 - val_loss: 0.6088 - val_acc: 0.8251\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8118\n",
      "Epoch 00011: val_loss did not improve from 0.60882\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6310 - acc: 0.8118 - val_loss: 1.6055 - val_acc: 0.5667\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6352 - acc: 0.8080\n",
      "Epoch 00012: val_loss improved from 0.60882 to 0.57585, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/012-0.5758.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6353 - acc: 0.8080 - val_loss: 0.5758 - val_acc: 0.8314\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8281\n",
      "Epoch 00013: val_loss improved from 0.57585 to 0.51892, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/013-0.5189.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5620 - acc: 0.8281 - val_loss: 0.5189 - val_acc: 0.8565\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.8372\n",
      "Epoch 00014: val_loss improved from 0.51892 to 0.49520, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/014-0.4952.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5409 - acc: 0.8373 - val_loss: 0.4952 - val_acc: 0.8605\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8434\n",
      "Epoch 00015: val_loss did not improve from 0.49520\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5108 - acc: 0.8434 - val_loss: 0.5586 - val_acc: 0.8528\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8504\n",
      "Epoch 00016: val_loss did not improve from 0.49520\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4909 - acc: 0.8503 - val_loss: 0.5796 - val_acc: 0.8348\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.8551\n",
      "Epoch 00017: val_loss improved from 0.49520 to 0.44659, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/017-0.4466.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4756 - acc: 0.8551 - val_loss: 0.4466 - val_acc: 0.8763\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8608\n",
      "Epoch 00018: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4543 - acc: 0.8608 - val_loss: 0.4873 - val_acc: 0.8733\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8629\n",
      "Epoch 00019: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4428 - acc: 0.8628 - val_loss: 0.5118 - val_acc: 0.8675\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8663\n",
      "Epoch 00020: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4315 - acc: 0.8663 - val_loss: 0.4920 - val_acc: 0.8602\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8684\n",
      "Epoch 00021: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4231 - acc: 0.8684 - val_loss: 0.4919 - val_acc: 0.8654\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8738\n",
      "Epoch 00022: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4090 - acc: 0.8737 - val_loss: 0.6563 - val_acc: 0.8043\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8773\n",
      "Epoch 00023: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3972 - acc: 0.8773 - val_loss: 0.5353 - val_acc: 0.8516\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8788\n",
      "Epoch 00024: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3869 - acc: 0.8788 - val_loss: 0.5038 - val_acc: 0.8656\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8803\n",
      "Epoch 00025: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3740 - acc: 0.8803 - val_loss: 0.5895 - val_acc: 0.8607\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8809\n",
      "Epoch 00026: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3761 - acc: 0.8809 - val_loss: 0.5388 - val_acc: 0.8498\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8860\n",
      "Epoch 00027: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3621 - acc: 0.8860 - val_loss: 0.5028 - val_acc: 0.8600\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8899\n",
      "Epoch 00028: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3535 - acc: 0.8899 - val_loss: 0.4699 - val_acc: 0.8626\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8932\n",
      "Epoch 00029: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3391 - acc: 0.8932 - val_loss: 0.5223 - val_acc: 0.8663\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8921\n",
      "Epoch 00030: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3382 - acc: 0.8922 - val_loss: 0.4734 - val_acc: 0.8686\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8930\n",
      "Epoch 00031: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3345 - acc: 0.8930 - val_loss: 0.5492 - val_acc: 0.8607\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8927\n",
      "Epoch 00032: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3416 - acc: 0.8927 - val_loss: 0.4574 - val_acc: 0.8733\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8980\n",
      "Epoch 00033: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3161 - acc: 0.8981 - val_loss: 0.4666 - val_acc: 0.8768\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9015\n",
      "Epoch 00034: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3092 - acc: 0.9015 - val_loss: 0.6731 - val_acc: 0.8227\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8945\n",
      "Epoch 00035: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3278 - acc: 0.8945 - val_loss: 0.5014 - val_acc: 0.8693\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8994\n",
      "Epoch 00036: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3151 - acc: 0.8994 - val_loss: 0.5769 - val_acc: 0.8495\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9005\n",
      "Epoch 00037: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3097 - acc: 0.9005 - val_loss: 0.4940 - val_acc: 0.8770\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9052\n",
      "Epoch 00038: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2963 - acc: 0.9052 - val_loss: 0.4760 - val_acc: 0.8726\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9043\n",
      "Epoch 00039: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2955 - acc: 0.9043 - val_loss: 0.6266 - val_acc: 0.8293\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.8999\n",
      "Epoch 00040: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3062 - acc: 0.8999 - val_loss: 0.5408 - val_acc: 0.8607\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8995\n",
      "Epoch 00041: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3118 - acc: 0.8995 - val_loss: 0.6565 - val_acc: 0.8288\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8977\n",
      "Epoch 00042: val_loss did not improve from 0.44659\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3220 - acc: 0.8978 - val_loss: 0.4779 - val_acc: 0.8770\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9058\n",
      "Epoch 00043: val_loss improved from 0.44659 to 0.44654, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/043-0.4465.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2914 - acc: 0.9058 - val_loss: 0.4465 - val_acc: 0.8859\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9078\n",
      "Epoch 00044: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2842 - acc: 0.9078 - val_loss: 0.4580 - val_acc: 0.8831\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9075\n",
      "Epoch 00045: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2873 - acc: 0.9075 - val_loss: 0.4524 - val_acc: 0.8782\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9102\n",
      "Epoch 00046: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2808 - acc: 0.9102 - val_loss: 0.5288 - val_acc: 0.8661\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9114\n",
      "Epoch 00047: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2746 - acc: 0.9115 - val_loss: 0.4801 - val_acc: 0.8761\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9110\n",
      "Epoch 00048: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2756 - acc: 0.9110 - val_loss: 0.5081 - val_acc: 0.8758\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9149\n",
      "Epoch 00049: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2649 - acc: 0.9150 - val_loss: 0.4558 - val_acc: 0.8756\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9126\n",
      "Epoch 00050: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2706 - acc: 0.9126 - val_loss: 0.4580 - val_acc: 0.8856\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9150\n",
      "Epoch 00051: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2661 - acc: 0.9150 - val_loss: 0.4505 - val_acc: 0.8807\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9144\n",
      "Epoch 00052: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2620 - acc: 0.9144 - val_loss: 0.4688 - val_acc: 0.8803\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9137\n",
      "Epoch 00053: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2659 - acc: 0.9137 - val_loss: 0.4703 - val_acc: 0.8765\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9120\n",
      "Epoch 00054: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2708 - acc: 0.9120 - val_loss: 0.5543 - val_acc: 0.8588\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9128\n",
      "Epoch 00055: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2733 - acc: 0.9128 - val_loss: 0.6262 - val_acc: 0.8246\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9104\n",
      "Epoch 00056: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2747 - acc: 0.9104 - val_loss: 0.4982 - val_acc: 0.8810\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9157\n",
      "Epoch 00057: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2626 - acc: 0.9157 - val_loss: 0.5581 - val_acc: 0.8742\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9181\n",
      "Epoch 00058: val_loss did not improve from 0.44654\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2519 - acc: 0.9181 - val_loss: 0.4644 - val_acc: 0.8894\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9129\n",
      "Epoch 00059: val_loss improved from 0.44654 to 0.44184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/059-0.4418.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2696 - acc: 0.9129 - val_loss: 0.4418 - val_acc: 0.8863\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9179\n",
      "Epoch 00060: val_loss did not improve from 0.44184\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2548 - acc: 0.9179 - val_loss: 0.4960 - val_acc: 0.8672\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9217\n",
      "Epoch 00061: val_loss did not improve from 0.44184\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2445 - acc: 0.9217 - val_loss: 0.5061 - val_acc: 0.8803\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9193\n",
      "Epoch 00062: val_loss did not improve from 0.44184\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2512 - acc: 0.9193 - val_loss: 0.4570 - val_acc: 0.8919\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9236\n",
      "Epoch 00063: val_loss improved from 0.44184 to 0.43709, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/063-0.4371.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2369 - acc: 0.9237 - val_loss: 0.4371 - val_acc: 0.8945\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9150\n",
      "Epoch 00064: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2604 - acc: 0.9150 - val_loss: 0.4745 - val_acc: 0.8821\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9199\n",
      "Epoch 00065: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2472 - acc: 0.9200 - val_loss: 0.4463 - val_acc: 0.8840\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9232\n",
      "Epoch 00066: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2343 - acc: 0.9232 - val_loss: 0.4833 - val_acc: 0.8733\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9211\n",
      "Epoch 00067: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2504 - acc: 0.9210 - val_loss: 0.6525 - val_acc: 0.8328\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9064\n",
      "Epoch 00068: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2905 - acc: 0.9064 - val_loss: 0.4785 - val_acc: 0.8838\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9212\n",
      "Epoch 00069: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2406 - acc: 0.9212 - val_loss: 0.4818 - val_acc: 0.8905\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9264\n",
      "Epoch 00070: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2251 - acc: 0.9264 - val_loss: 0.4667 - val_acc: 0.8819\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9262\n",
      "Epoch 00071: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2298 - acc: 0.9262 - val_loss: 0.4553 - val_acc: 0.8866\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9290\n",
      "Epoch 00072: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2215 - acc: 0.9290 - val_loss: 0.5046 - val_acc: 0.8761\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9235\n",
      "Epoch 00073: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2355 - acc: 0.9235 - val_loss: 0.4804 - val_acc: 0.8905\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9274\n",
      "Epoch 00074: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2247 - acc: 0.9274 - val_loss: 0.6441 - val_acc: 0.8337\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9251\n",
      "Epoch 00075: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2335 - acc: 0.9251 - val_loss: 0.5074 - val_acc: 0.8849\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9249\n",
      "Epoch 00076: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2282 - acc: 0.9249 - val_loss: 0.4411 - val_acc: 0.8875\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9307\n",
      "Epoch 00077: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2226 - acc: 0.9307 - val_loss: 0.4577 - val_acc: 0.8866\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9271\n",
      "Epoch 00078: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2218 - acc: 0.9272 - val_loss: 0.5177 - val_acc: 0.8756\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9276\n",
      "Epoch 00079: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2211 - acc: 0.9276 - val_loss: 0.4712 - val_acc: 0.8803\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9326\n",
      "Epoch 00080: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2077 - acc: 0.9326 - val_loss: 0.4636 - val_acc: 0.8905\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9341\n",
      "Epoch 00081: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2059 - acc: 0.9341 - val_loss: 0.4498 - val_acc: 0.8877\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9321\n",
      "Epoch 00082: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2093 - acc: 0.9321 - val_loss: 0.4859 - val_acc: 0.8849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9312\n",
      "Epoch 00083: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2126 - acc: 0.9312 - val_loss: 0.4642 - val_acc: 0.8905\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9300\n",
      "Epoch 00084: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2115 - acc: 0.9300 - val_loss: 0.4520 - val_acc: 0.8901\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9342\n",
      "Epoch 00085: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2080 - acc: 0.9342 - val_loss: 0.4649 - val_acc: 0.8915\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9305\n",
      "Epoch 00086: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2153 - acc: 0.9305 - val_loss: 0.4869 - val_acc: 0.8921\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9308\n",
      "Epoch 00087: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2165 - acc: 0.9308 - val_loss: 0.5103 - val_acc: 0.8758\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9335\n",
      "Epoch 00088: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2067 - acc: 0.9335 - val_loss: 0.4613 - val_acc: 0.8884\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9323\n",
      "Epoch 00089: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2137 - acc: 0.9323 - val_loss: 0.4402 - val_acc: 0.8966\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9299\n",
      "Epoch 00090: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2162 - acc: 0.9299 - val_loss: 0.4484 - val_acc: 0.8875\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9378\n",
      "Epoch 00091: val_loss did not improve from 0.43709\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1903 - acc: 0.9378 - val_loss: 0.5074 - val_acc: 0.8779\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9360\n",
      "Epoch 00092: val_loss improved from 0.43709 to 0.42642, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv_checkpoint/092-0.4264.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2041 - acc: 0.9359 - val_loss: 0.4264 - val_acc: 0.8966\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9370\n",
      "Epoch 00093: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1956 - acc: 0.9370 - val_loss: 0.4483 - val_acc: 0.8887\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9329\n",
      "Epoch 00094: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2081 - acc: 0.9329 - val_loss: 0.4857 - val_acc: 0.8917\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9373\n",
      "Epoch 00095: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1978 - acc: 0.9373 - val_loss: 0.4987 - val_acc: 0.8810\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9339\n",
      "Epoch 00096: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2015 - acc: 0.9338 - val_loss: 0.6202 - val_acc: 0.8686\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9293\n",
      "Epoch 00097: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2199 - acc: 0.9292 - val_loss: 0.4430 - val_acc: 0.8989\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9331\n",
      "Epoch 00098: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2028 - acc: 0.9331 - val_loss: 0.6176 - val_acc: 0.8640\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9290\n",
      "Epoch 00099: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2208 - acc: 0.9290 - val_loss: 0.4467 - val_acc: 0.8894\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9355\n",
      "Epoch 00100: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2021 - acc: 0.9354 - val_loss: 0.5459 - val_acc: 0.8651\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9242\n",
      "Epoch 00101: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2334 - acc: 0.9242 - val_loss: 0.4984 - val_acc: 0.8847\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9269\n",
      "Epoch 00102: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2303 - acc: 0.9269 - val_loss: 0.7486 - val_acc: 0.8190\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9272\n",
      "Epoch 00103: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2289 - acc: 0.9272 - val_loss: 0.4921 - val_acc: 0.8847\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9272\n",
      "Epoch 00104: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2256 - acc: 0.9272 - val_loss: 0.4834 - val_acc: 0.8905\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9193\n",
      "Epoch 00105: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2518 - acc: 0.9193 - val_loss: 0.5464 - val_acc: 0.8805\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9317\n",
      "Epoch 00106: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2154 - acc: 0.9316 - val_loss: 0.4986 - val_acc: 0.8833\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9344\n",
      "Epoch 00107: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2069 - acc: 0.9344 - val_loss: 0.5506 - val_acc: 0.8847\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9326\n",
      "Epoch 00108: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2060 - acc: 0.9326 - val_loss: 0.4313 - val_acc: 0.8982\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9332\n",
      "Epoch 00109: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2097 - acc: 0.9332 - val_loss: 0.4982 - val_acc: 0.8912\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9353\n",
      "Epoch 00110: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2027 - acc: 0.9353 - val_loss: 0.4749 - val_acc: 0.8910\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9361\n",
      "Epoch 00111: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1945 - acc: 0.9361 - val_loss: 0.4626 - val_acc: 0.8891\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9363\n",
      "Epoch 00112: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1968 - acc: 0.9363 - val_loss: 0.4855 - val_acc: 0.8863\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9333\n",
      "Epoch 00113: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2020 - acc: 0.9333 - val_loss: 0.5576 - val_acc: 0.8803\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9276\n",
      "Epoch 00114: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2230 - acc: 0.9276 - val_loss: 0.5035 - val_acc: 0.8668\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9248\n",
      "Epoch 00115: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2354 - acc: 0.9248 - val_loss: 0.5077 - val_acc: 0.8889\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9326\n",
      "Epoch 00116: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2137 - acc: 0.9326 - val_loss: 0.4858 - val_acc: 0.8938\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9257\n",
      "Epoch 00117: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2292 - acc: 0.9257 - val_loss: 0.5001 - val_acc: 0.8859\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9324\n",
      "Epoch 00118: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2083 - acc: 0.9324 - val_loss: 0.4908 - val_acc: 0.8861\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9336\n",
      "Epoch 00119: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2031 - acc: 0.9335 - val_loss: 0.4871 - val_acc: 0.8856\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9330\n",
      "Epoch 00120: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2093 - acc: 0.9330 - val_loss: 0.4414 - val_acc: 0.8905\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9401\n",
      "Epoch 00121: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1863 - acc: 0.9401 - val_loss: 0.4626 - val_acc: 0.8947\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9372\n",
      "Epoch 00122: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1917 - acc: 0.9372 - val_loss: 0.5173 - val_acc: 0.8880\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9073\n",
      "Epoch 00123: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2997 - acc: 0.9073 - val_loss: 0.5645 - val_acc: 0.8670\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9023\n",
      "Epoch 00124: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3152 - acc: 0.9023 - val_loss: 2.5747 - val_acc: 0.5595\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8928\n",
      "Epoch 00125: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3323 - acc: 0.8928 - val_loss: 0.5820 - val_acc: 0.8719\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9199\n",
      "Epoch 00126: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2460 - acc: 0.9200 - val_loss: 0.4890 - val_acc: 0.8919\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9187\n",
      "Epoch 00127: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2498 - acc: 0.9188 - val_loss: 0.5897 - val_acc: 0.8535\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9211\n",
      "Epoch 00128: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2474 - acc: 0.9211 - val_loss: 0.4645 - val_acc: 0.8905\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9252\n",
      "Epoch 00129: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2328 - acc: 0.9252 - val_loss: 0.4742 - val_acc: 0.8856\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9264\n",
      "Epoch 00130: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2282 - acc: 0.9264 - val_loss: 0.5091 - val_acc: 0.8805\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9302\n",
      "Epoch 00131: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2171 - acc: 0.9302 - val_loss: 0.4980 - val_acc: 0.8861\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9299\n",
      "Epoch 00132: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2184 - acc: 0.9300 - val_loss: 0.4848 - val_acc: 0.8870\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9393\n",
      "Epoch 00133: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1844 - acc: 0.9392 - val_loss: 0.4995 - val_acc: 0.8866\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9354\n",
      "Epoch 00134: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2027 - acc: 0.9354 - val_loss: 0.4624 - val_acc: 0.8982\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9401\n",
      "Epoch 00135: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1847 - acc: 0.9401 - val_loss: 0.4669 - val_acc: 0.8933\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9399\n",
      "Epoch 00136: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1890 - acc: 0.9399 - val_loss: 0.4856 - val_acc: 0.8926\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9424\n",
      "Epoch 00137: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1770 - acc: 0.9424 - val_loss: 0.4716 - val_acc: 0.8859\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9382\n",
      "Epoch 00138: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1893 - acc: 0.9382 - val_loss: 0.4845 - val_acc: 0.8889\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9435\n",
      "Epoch 00139: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1755 - acc: 0.9435 - val_loss: 0.4616 - val_acc: 0.9012\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9414\n",
      "Epoch 00140: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1832 - acc: 0.9414 - val_loss: 0.5054 - val_acc: 0.8877\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9353\n",
      "Epoch 00141: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2029 - acc: 0.9353 - val_loss: 0.4627 - val_acc: 0.8959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9429\n",
      "Epoch 00142: val_loss did not improve from 0.42642\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1761 - acc: 0.9429 - val_loss: 0.4880 - val_acc: 0.8889\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 921us/sample - loss: 0.5926 - acc: 0.8594\n",
      "Loss: 0.592554632711262 Accuracy: 0.8593977\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7265 - acc: 0.0793\n",
      "Epoch 00001: val_loss improved from inf to 2.71882, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv_checkpoint/001-2.7188.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.7264 - acc: 0.0793 - val_loss: 2.7188 - val_acc: 0.0818\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7229 - acc: 0.0787\n",
      "Epoch 00002: val_loss did not improve from 2.71882\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7229 - acc: 0.0787 - val_loss: 2.7197 - val_acc: 0.0820\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7219 - acc: 0.0794\n",
      "Epoch 00003: val_loss did not improve from 2.71882\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7219 - acc: 0.0794 - val_loss: 2.7241 - val_acc: 0.0811\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7217 - acc: 0.0796\n",
      "Epoch 00004: val_loss improved from 2.71882 to 2.71839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv_checkpoint/004-2.7184.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7218 - acc: 0.0796 - val_loss: 2.7184 - val_acc: 0.0776\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7210 - acc: 0.0802\n",
      "Epoch 00005: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7211 - acc: 0.0802 - val_loss: 2.7185 - val_acc: 0.0811\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0809\n",
      "Epoch 00006: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0793\n",
      "Epoch 00007: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0815\n",
      "Epoch 00008: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0815 - val_loss: 2.7192 - val_acc: 0.0818\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0791\n",
      "Epoch 00009: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0799\n",
      "Epoch 00010: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0819\n",
      "Epoch 00011: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0819 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00012: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7191 - val_acc: 0.0785\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0795\n",
      "Epoch 00013: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0794 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00014: val_loss did not improve from 2.71839\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0805 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00015: val_loss improved from 2.71839 to 2.71837, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv_checkpoint/015-2.7184.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0783\n",
      "Epoch 00016: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0784 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00017: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0813\n",
      "Epoch 00018: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0813 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.0821\n",
      "Epoch 00019: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7203 - acc: 0.0821 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0801\n",
      "Epoch 00020: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0801 - val_loss: 2.7188 - val_acc: 0.0776\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0793\n",
      "Epoch 00021: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7189 - val_acc: 0.0818\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00022: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0829\n",
      "Epoch 00023: val_loss did not improve from 2.71837\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0829 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0811\n",
      "Epoch 00024: val_loss improved from 2.71837 to 2.71830, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv_checkpoint/024-2.7183.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0811 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0806\n",
      "Epoch 00025: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0806 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00026: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0799\n",
      "Epoch 00027: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0797\n",
      "Epoch 00028: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0797 - val_loss: 2.7184 - val_acc: 0.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0817\n",
      "Epoch 00029: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0817 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0793\n",
      "Epoch 00030: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0796\n",
      "Epoch 00031: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0796 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0804\n",
      "Epoch 00032: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0804 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00033: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00034: val_loss did not improve from 2.71830\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0777\n",
      "Epoch 00035: val_loss improved from 2.71830 to 2.71820, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv_checkpoint/035-2.7182.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0777 - val_loss: 2.7182 - val_acc: 0.0818\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00036: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0803 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00037: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7189 - val_acc: 0.0776\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0804\n",
      "Epoch 00038: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0804 - val_loss: 2.7185 - val_acc: 0.0818\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0786\n",
      "Epoch 00039: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0785 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0795\n",
      "Epoch 00040: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0802\n",
      "Epoch 00041: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0802 - val_loss: 2.7183 - val_acc: 0.0818\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00042: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00043: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0801 - val_loss: 2.7185 - val_acc: 0.0811\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0801\n",
      "Epoch 00044: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0803\n",
      "Epoch 00045: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0803 - val_loss: 2.7192 - val_acc: 0.0776\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0800\n",
      "Epoch 00046: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0800 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0796\n",
      "Epoch 00047: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0796 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00048: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00049: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7192 - val_acc: 0.0776\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00050: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0814\n",
      "Epoch 00051: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0814 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00052: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00053: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0817\n",
      "Epoch 00054: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0817 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00055: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00056: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0801\n",
      "Epoch 00057: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0801 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00058: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0793\n",
      "Epoch 00059: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0794 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0791\n",
      "Epoch 00060: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0819\n",
      "Epoch 00061: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0819 - val_loss: 2.7191 - val_acc: 0.0785\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0794\n",
      "Epoch 00062: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0794 - val_loss: 2.7192 - val_acc: 0.0820\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0802\n",
      "Epoch 00063: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0802 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0826\n",
      "Epoch 00064: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0826 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00065: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0808 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00066: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0809\n",
      "Epoch 00067: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0810\n",
      "Epoch 00068: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0810 - val_loss: 2.7192 - val_acc: 0.0820\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0816\n",
      "Epoch 00069: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0816 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0805\n",
      "Epoch 00070: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7188 - val_acc: 0.0776\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00071: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00072: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00073: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0789\n",
      "Epoch 00074: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0789 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0805\n",
      "Epoch 00075: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0805 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0809\n",
      "Epoch 00076: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00077: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7185 - val_acc: 0.0811\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0804\n",
      "Epoch 00078: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0804 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00079: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0803\n",
      "Epoch 00080: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0803 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00081: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0785\n",
      "Epoch 00082: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0784 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0809\n",
      "Epoch 00083: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0812\n",
      "Epoch 00084: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0812 - val_loss: 2.7182 - val_acc: 0.0820\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00085: val_loss did not improve from 2.71820\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 953us/sample - loss: 2.7130 - acc: 0.0781\n",
      "Loss: 2.7129934609493365 Accuracy: 0.078089304\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7278 - acc: 0.0767\n",
      "Epoch 00001: val_loss improved from inf to 2.71843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/001-2.7184.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.7278 - acc: 0.0766 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7223 - acc: 0.0794\n",
      "Epoch 00002: val_loss did not improve from 2.71843\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7223 - acc: 0.0794 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7217 - acc: 0.0799\n",
      "Epoch 00003: val_loss did not improve from 2.71843\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7217 - acc: 0.0799 - val_loss: 2.7188 - val_acc: 0.0776\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7212 - acc: 0.0805\n",
      "Epoch 00004: val_loss did not improve from 2.71843\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7211 - acc: 0.0806 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7209 - acc: 0.0798\n",
      "Epoch 00005: val_loss did not improve from 2.71843\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7209 - acc: 0.0798 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7201 - acc: 0.0811\n",
      "Epoch 00006: val_loss improved from 2.71843 to 2.71767, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/006-2.7177.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7202 - acc: 0.0810 - val_loss: 2.7177 - val_acc: 0.0787\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5975 - acc: 0.1380\n",
      "Epoch 00007: val_loss did not improve from 2.71767\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.5976 - acc: 0.1380 - val_loss: 3.4180 - val_acc: 0.0850\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3327 - acc: 0.2026\n",
      "Epoch 00008: val_loss improved from 2.71767 to 2.29109, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/008-2.2911.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.3327 - acc: 0.2026 - val_loss: 2.2911 - val_acc: 0.2541\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7639 - acc: 0.4065\n",
      "Epoch 00009: val_loss improved from 2.29109 to 1.40677, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/009-1.4068.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.7637 - acc: 0.4065 - val_loss: 1.4068 - val_acc: 0.5467\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3390 - acc: 0.5586\n",
      "Epoch 00010: val_loss did not improve from 1.40677\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3391 - acc: 0.5586 - val_loss: 2.5335 - val_acc: 0.4239\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.6925\n",
      "Epoch 00011: val_loss improved from 1.40677 to 0.59551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/011-0.5955.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9736 - acc: 0.6925 - val_loss: 0.5955 - val_acc: 0.8199\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6819 - acc: 0.7870\n",
      "Epoch 00012: val_loss did not improve from 0.59551\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6819 - acc: 0.7871 - val_loss: 0.7989 - val_acc: 0.7608\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.8317\n",
      "Epoch 00013: val_loss did not improve from 0.59551\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5535 - acc: 0.8317 - val_loss: 2.0421 - val_acc: 0.5563\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8399\n",
      "Epoch 00014: val_loss improved from 0.59551 to 0.36775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/014-0.3678.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5231 - acc: 0.8399 - val_loss: 0.3678 - val_acc: 0.8921\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8674\n",
      "Epoch 00015: val_loss improved from 0.36775 to 0.33602, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/015-0.3360.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4276 - acc: 0.8674 - val_loss: 0.3360 - val_acc: 0.9038\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8847\n",
      "Epoch 00016: val_loss improved from 0.33602 to 0.31330, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/016-0.3133.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3806 - acc: 0.8847 - val_loss: 0.3133 - val_acc: 0.9075\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8943\n",
      "Epoch 00017: val_loss improved from 0.31330 to 0.28719, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/017-0.2872.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3464 - acc: 0.8943 - val_loss: 0.2872 - val_acc: 0.9152\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9044\n",
      "Epoch 00018: val_loss improved from 0.28719 to 0.26216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/018-0.2622.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3169 - acc: 0.9044 - val_loss: 0.2622 - val_acc: 0.9229\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.9066\n",
      "Epoch 00019: val_loss improved from 0.26216 to 0.20782, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/019-0.2078.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3072 - acc: 0.9066 - val_loss: 0.2078 - val_acc: 0.9390\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9138\n",
      "Epoch 00020: val_loss improved from 0.20782 to 0.20277, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/020-0.2028.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2851 - acc: 0.9138 - val_loss: 0.2028 - val_acc: 0.9439\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9174\n",
      "Epoch 00021: val_loss improved from 0.20277 to 0.19114, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/021-0.1911.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2643 - acc: 0.9174 - val_loss: 0.1911 - val_acc: 0.9425\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9241\n",
      "Epoch 00022: val_loss did not improve from 0.19114\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2479 - acc: 0.9241 - val_loss: 0.1994 - val_acc: 0.9488\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9261\n",
      "Epoch 00023: val_loss did not improve from 0.19114\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2401 - acc: 0.9261 - val_loss: 0.1922 - val_acc: 0.9434\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9287\n",
      "Epoch 00024: val_loss improved from 0.19114 to 0.18163, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/024-0.1816.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2288 - acc: 0.9288 - val_loss: 0.1816 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9318\n",
      "Epoch 00025: val_loss did not improve from 0.18163\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2210 - acc: 0.9317 - val_loss: 0.2374 - val_acc: 0.9366\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9303\n",
      "Epoch 00026: val_loss improved from 0.18163 to 0.17261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/026-0.1726.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2206 - acc: 0.9303 - val_loss: 0.1726 - val_acc: 0.9511\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9355\n",
      "Epoch 00027: val_loss did not improve from 0.17261\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2058 - acc: 0.9355 - val_loss: 0.2740 - val_acc: 0.9297\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9363\n",
      "Epoch 00028: val_loss improved from 0.17261 to 0.15452, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/028-0.1545.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1999 - acc: 0.9363 - val_loss: 0.1545 - val_acc: 0.9560\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9408\n",
      "Epoch 00029: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1907 - acc: 0.9407 - val_loss: 0.1815 - val_acc: 0.9450\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9424\n",
      "Epoch 00030: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1877 - acc: 0.9424 - val_loss: 0.1958 - val_acc: 0.9469\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9456\n",
      "Epoch 00031: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1758 - acc: 0.9456 - val_loss: 0.1623 - val_acc: 0.9511\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9477\n",
      "Epoch 00032: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1675 - acc: 0.9477 - val_loss: 0.1781 - val_acc: 0.9513\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9496\n",
      "Epoch 00033: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1615 - acc: 0.9496 - val_loss: 0.2514 - val_acc: 0.9434\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9475\n",
      "Epoch 00034: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1637 - acc: 0.9475 - val_loss: 0.1835 - val_acc: 0.9497\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9492\n",
      "Epoch 00035: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1582 - acc: 0.9492 - val_loss: 0.1796 - val_acc: 0.9527\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9514\n",
      "Epoch 00036: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1587 - acc: 0.9514 - val_loss: 0.1706 - val_acc: 0.9555\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9527\n",
      "Epoch 00037: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1492 - acc: 0.9527 - val_loss: 0.1736 - val_acc: 0.9562\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9509\n",
      "Epoch 00038: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1526 - acc: 0.9509 - val_loss: 0.1848 - val_acc: 0.9555\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9536\n",
      "Epoch 00039: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1462 - acc: 0.9536 - val_loss: 0.1680 - val_acc: 0.9532\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9558\n",
      "Epoch 00040: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1410 - acc: 0.9558 - val_loss: 0.1751 - val_acc: 0.9567\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9573\n",
      "Epoch 00041: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1366 - acc: 0.9573 - val_loss: 0.1789 - val_acc: 0.9492\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9555\n",
      "Epoch 00042: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1399 - acc: 0.9555 - val_loss: 0.1569 - val_acc: 0.9616\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9561\n",
      "Epoch 00043: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1372 - acc: 0.9561 - val_loss: 0.2094 - val_acc: 0.9478\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9593\n",
      "Epoch 00044: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1304 - acc: 0.9594 - val_loss: 0.1656 - val_acc: 0.9581\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9582\n",
      "Epoch 00045: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1316 - acc: 0.9582 - val_loss: 0.1548 - val_acc: 0.9623\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9585\n",
      "Epoch 00046: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1289 - acc: 0.9585 - val_loss: 0.1711 - val_acc: 0.9592\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9619\n",
      "Epoch 00047: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1223 - acc: 0.9619 - val_loss: 0.1966 - val_acc: 0.9478\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9589\n",
      "Epoch 00048: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1295 - acc: 0.9589 - val_loss: 0.1623 - val_acc: 0.9548\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9610\n",
      "Epoch 00049: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1233 - acc: 0.9609 - val_loss: 0.1734 - val_acc: 0.9574\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9599\n",
      "Epoch 00050: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1292 - acc: 0.9599 - val_loss: 0.1792 - val_acc: 0.9522\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9607\n",
      "Epoch 00051: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1222 - acc: 0.9607 - val_loss: 0.1672 - val_acc: 0.9585\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9635\n",
      "Epoch 00052: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1177 - acc: 0.9634 - val_loss: 0.4133 - val_acc: 0.9117\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9561\n",
      "Epoch 00053: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1413 - acc: 0.9561 - val_loss: 0.1846 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9607\n",
      "Epoch 00054: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1214 - acc: 0.9607 - val_loss: 0.1648 - val_acc: 0.9539\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9618\n",
      "Epoch 00055: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1225 - acc: 0.9619 - val_loss: 0.1825 - val_acc: 0.9560\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9637\n",
      "Epoch 00056: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1136 - acc: 0.9637 - val_loss: 0.1596 - val_acc: 0.9569\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9626\n",
      "Epoch 00057: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1174 - acc: 0.9626 - val_loss: 0.1632 - val_acc: 0.9606\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9633\n",
      "Epoch 00058: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1162 - acc: 0.9633 - val_loss: 0.1650 - val_acc: 0.9599\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9646\n",
      "Epoch 00059: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1085 - acc: 0.9645 - val_loss: 0.1877 - val_acc: 0.9515\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9621\n",
      "Epoch 00060: val_loss did not improve from 0.15452\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1163 - acc: 0.9621 - val_loss: 0.1567 - val_acc: 0.9602\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9661\n",
      "Epoch 00061: val_loss improved from 0.15452 to 0.15132, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/061-0.1513.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1040 - acc: 0.9661 - val_loss: 0.1513 - val_acc: 0.9613\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9629\n",
      "Epoch 00062: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1165 - acc: 0.9629 - val_loss: 0.1963 - val_acc: 0.9469\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9584\n",
      "Epoch 00063: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1308 - acc: 0.9584 - val_loss: 0.1637 - val_acc: 0.9562\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9640\n",
      "Epoch 00064: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1127 - acc: 0.9640 - val_loss: 0.1740 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9667\n",
      "Epoch 00065: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1043 - acc: 0.9667 - val_loss: 0.1833 - val_acc: 0.9522\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9670\n",
      "Epoch 00066: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1037 - acc: 0.9670 - val_loss: 0.1793 - val_acc: 0.9534\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9653\n",
      "Epoch 00067: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1087 - acc: 0.9653 - val_loss: 0.1573 - val_acc: 0.9630\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9681\n",
      "Epoch 00068: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1009 - acc: 0.9681 - val_loss: 0.1692 - val_acc: 0.9611\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9669\n",
      "Epoch 00069: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1022 - acc: 0.9669 - val_loss: 0.1709 - val_acc: 0.9576\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9671\n",
      "Epoch 00070: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1017 - acc: 0.9671 - val_loss: 0.1857 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9682\n",
      "Epoch 00071: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1019 - acc: 0.9682 - val_loss: 0.1650 - val_acc: 0.9564\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9651\n",
      "Epoch 00072: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1095 - acc: 0.9651 - val_loss: 0.1847 - val_acc: 0.9497\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9680\n",
      "Epoch 00073: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1023 - acc: 0.9680 - val_loss: 0.1722 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9669\n",
      "Epoch 00074: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1057 - acc: 0.9669 - val_loss: 0.1938 - val_acc: 0.9536\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9692\n",
      "Epoch 00075: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0978 - acc: 0.9692 - val_loss: 0.1840 - val_acc: 0.9543\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9704\n",
      "Epoch 00076: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0939 - acc: 0.9704 - val_loss: 0.1808 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9667\n",
      "Epoch 00077: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1092 - acc: 0.9667 - val_loss: 0.1773 - val_acc: 0.9599\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9689\n",
      "Epoch 00078: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0999 - acc: 0.9689 - val_loss: 0.1735 - val_acc: 0.9578\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9688\n",
      "Epoch 00079: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0950 - acc: 0.9688 - val_loss: 0.1782 - val_acc: 0.9560\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9675\n",
      "Epoch 00080: val_loss did not improve from 0.15132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1025 - acc: 0.9675 - val_loss: 0.1580 - val_acc: 0.9611\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9702\n",
      "Epoch 00081: val_loss improved from 0.15132 to 0.14770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv_checkpoint/081-0.1477.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0947 - acc: 0.9702 - val_loss: 0.1477 - val_acc: 0.9578\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9657\n",
      "Epoch 00082: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1074 - acc: 0.9657 - val_loss: 0.1651 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9673\n",
      "Epoch 00083: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1004 - acc: 0.9673 - val_loss: 0.1720 - val_acc: 0.9567\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9665\n",
      "Epoch 00084: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1048 - acc: 0.9666 - val_loss: 0.1710 - val_acc: 0.9595\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9689\n",
      "Epoch 00085: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0995 - acc: 0.9689 - val_loss: 0.1632 - val_acc: 0.9581\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9702\n",
      "Epoch 00086: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0952 - acc: 0.9702 - val_loss: 0.1746 - val_acc: 0.9576\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9682\n",
      "Epoch 00087: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1009 - acc: 0.9682 - val_loss: 0.1793 - val_acc: 0.9560\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9673\n",
      "Epoch 00088: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1014 - acc: 0.9673 - val_loss: 0.1846 - val_acc: 0.9546\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9671\n",
      "Epoch 00089: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1040 - acc: 0.9671 - val_loss: 0.1900 - val_acc: 0.9499\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9576\n",
      "Epoch 00090: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1374 - acc: 0.9576 - val_loss: 0.1769 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9671\n",
      "Epoch 00091: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1033 - acc: 0.9671 - val_loss: 0.1735 - val_acc: 0.9602\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9667\n",
      "Epoch 00092: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1026 - acc: 0.9667 - val_loss: 0.1968 - val_acc: 0.9578\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9680\n",
      "Epoch 00093: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1026 - acc: 0.9680 - val_loss: 0.1977 - val_acc: 0.9564\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9696\n",
      "Epoch 00094: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0964 - acc: 0.9696 - val_loss: 0.4659 - val_acc: 0.9019\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9616\n",
      "Epoch 00095: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1218 - acc: 0.9616 - val_loss: 0.1531 - val_acc: 0.9604\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9647\n",
      "Epoch 00096: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1121 - acc: 0.9647 - val_loss: 0.1811 - val_acc: 0.9553\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9695\n",
      "Epoch 00097: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0982 - acc: 0.9695 - val_loss: 0.1838 - val_acc: 0.9564\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9682\n",
      "Epoch 00098: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1013 - acc: 0.9682 - val_loss: 0.1741 - val_acc: 0.9581\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9670\n",
      "Epoch 00099: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1061 - acc: 0.9670 - val_loss: 0.1676 - val_acc: 0.9574\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9670\n",
      "Epoch 00100: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1020 - acc: 0.9670 - val_loss: 0.1872 - val_acc: 0.9550\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9650\n",
      "Epoch 00101: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1096 - acc: 0.9650 - val_loss: 0.1576 - val_acc: 0.9613\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9692\n",
      "Epoch 00102: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0998 - acc: 0.9692 - val_loss: 0.1906 - val_acc: 0.9548\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9705\n",
      "Epoch 00103: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0939 - acc: 0.9705 - val_loss: 0.1907 - val_acc: 0.9557\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9673\n",
      "Epoch 00104: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1009 - acc: 0.9673 - val_loss: 0.2080 - val_acc: 0.9490\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9682\n",
      "Epoch 00105: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1024 - acc: 0.9682 - val_loss: 0.1905 - val_acc: 0.9536\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9669\n",
      "Epoch 00106: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1019 - acc: 0.9669 - val_loss: 0.1751 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9690\n",
      "Epoch 00107: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0949 - acc: 0.9689 - val_loss: 0.2189 - val_acc: 0.9539\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9647\n",
      "Epoch 00108: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1163 - acc: 0.9647 - val_loss: 0.2059 - val_acc: 0.9474\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9699\n",
      "Epoch 00109: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0939 - acc: 0.9699 - val_loss: 0.1930 - val_acc: 0.9564\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9703\n",
      "Epoch 00110: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0933 - acc: 0.9703 - val_loss: 0.2278 - val_acc: 0.9518\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9674\n",
      "Epoch 00111: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1021 - acc: 0.9673 - val_loss: 0.2826 - val_acc: 0.9338\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9575\n",
      "Epoch 00112: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1375 - acc: 0.9575 - val_loss: 0.2048 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9638\n",
      "Epoch 00113: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1116 - acc: 0.9638 - val_loss: 0.1870 - val_acc: 0.9571\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9652\n",
      "Epoch 00114: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1063 - acc: 0.9652 - val_loss: 0.1840 - val_acc: 0.9543\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9652\n",
      "Epoch 00115: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1118 - acc: 0.9652 - val_loss: 0.1774 - val_acc: 0.9562\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9683\n",
      "Epoch 00116: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1027 - acc: 0.9683 - val_loss: 0.1994 - val_acc: 0.9562\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9671\n",
      "Epoch 00117: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1041 - acc: 0.9671 - val_loss: 0.2137 - val_acc: 0.9488\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9667\n",
      "Epoch 00118: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1053 - acc: 0.9667 - val_loss: 0.2065 - val_acc: 0.9495\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9679\n",
      "Epoch 00119: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0976 - acc: 0.9679 - val_loss: 0.1956 - val_acc: 0.9578\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9704\n",
      "Epoch 00120: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0912 - acc: 0.9704 - val_loss: 0.1921 - val_acc: 0.9574\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9690\n",
      "Epoch 00121: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0951 - acc: 0.9690 - val_loss: 0.1975 - val_acc: 0.9581\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9645\n",
      "Epoch 00122: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1155 - acc: 0.9645 - val_loss: 0.1643 - val_acc: 0.9616\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9661\n",
      "Epoch 00123: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1078 - acc: 0.9661 - val_loss: 0.2195 - val_acc: 0.9525\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9660\n",
      "Epoch 00124: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1107 - acc: 0.9660 - val_loss: 0.1707 - val_acc: 0.9557\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9669\n",
      "Epoch 00125: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1042 - acc: 0.9669 - val_loss: 0.1728 - val_acc: 0.9569\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9683\n",
      "Epoch 00126: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1028 - acc: 0.9683 - val_loss: 0.2320 - val_acc: 0.9362\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9488\n",
      "Epoch 00127: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1674 - acc: 0.9488 - val_loss: 0.2404 - val_acc: 0.9401\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9621\n",
      "Epoch 00128: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1169 - acc: 0.9621 - val_loss: 0.1961 - val_acc: 0.9522\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9664\n",
      "Epoch 00129: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1043 - acc: 0.9664 - val_loss: 0.4134 - val_acc: 0.9106\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9536\n",
      "Epoch 00130: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1472 - acc: 0.9536 - val_loss: 0.1898 - val_acc: 0.9529\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9649\n",
      "Epoch 00131: val_loss did not improve from 0.14770\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1110 - acc: 0.9649 - val_loss: 0.1978 - val_acc: 0.9553\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 987us/sample - loss: 0.2081 - acc: 0.9475\n",
      "Loss: 0.2080932543262441 Accuracy: 0.9474559\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7290 - acc: 0.0788\n",
      "Epoch 00001: val_loss improved from inf to 2.71923, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/001-2.7192.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.7290 - acc: 0.0788 - val_loss: 2.7192 - val_acc: 0.0811\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7224 - acc: 0.0800\n",
      "Epoch 00002: val_loss improved from 2.71923 to 2.71905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/002-2.7190.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7224 - acc: 0.0800 - val_loss: 2.7190 - val_acc: 0.0776\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7216 - acc: 0.0793\n",
      "Epoch 00003: val_loss improved from 2.71905 to 2.71872, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/003-2.7187.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7215 - acc: 0.0793 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7207 - acc: 0.0795\n",
      "Epoch 00004: val_loss improved from 2.71872 to 2.71854, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/004-2.7185.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7207 - acc: 0.0795 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7207 - acc: 0.0799\n",
      "Epoch 00005: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7207 - acc: 0.0800 - val_loss: 2.7188 - val_acc: 0.0818\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0822\n",
      "Epoch 00006: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0822 - val_loss: 2.7189 - val_acc: 0.0811\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0784\n",
      "Epoch 00007: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0784 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0796\n",
      "Epoch 00008: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0796 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0797\n",
      "Epoch 00009: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0797 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0804\n",
      "Epoch 00010: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0804 - val_loss: 2.7187 - val_acc: 0.0818\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00011: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0802\n",
      "Epoch 00012: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0819\n",
      "Epoch 00013: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0819 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00014: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0803 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00015: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0799\n",
      "Epoch 00016: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0812\n",
      "Epoch 00017: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0812 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00018: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00019: val_loss did not improve from 2.71854\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0785\n",
      "Epoch 00020: val_loss improved from 2.71854 to 2.71854, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/020-2.7185.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0784 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00021: val_loss improved from 2.71854 to 2.71852, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/021-2.7185.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0808 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0787\n",
      "Epoch 00022: val_loss did not improve from 2.71852\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0787 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00023: val_loss improved from 2.71852 to 2.71851, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/023-2.7185.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0797\n",
      "Epoch 00024: val_loss did not improve from 2.71851\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0797 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0813\n",
      "Epoch 00025: val_loss improved from 2.71851 to 2.71841, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/025-2.7184.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0813 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0784\n",
      "Epoch 00026: val_loss did not improve from 2.71841\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0784 - val_loss: 2.7185 - val_acc: 0.0818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0793\n",
      "Epoch 00027: val_loss did not improve from 2.71841\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00028: val_loss did not improve from 2.71841\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0800\n",
      "Epoch 00029: val_loss did not improve from 2.71841\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7206 - acc: 0.0800 - val_loss: 2.7189 - val_acc: 0.0776\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00030: val_loss did not improve from 2.71841\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0819\n",
      "Epoch 00031: val_loss improved from 2.71841 to 2.71840, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/031-2.7184.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0819 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0790\n",
      "Epoch 00032: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0791 - val_loss: 2.7191 - val_acc: 0.0785\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0805\n",
      "Epoch 00033: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0805 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0805\n",
      "Epoch 00034: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0805 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00035: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0791\n",
      "Epoch 00036: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0808\n",
      "Epoch 00037: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0808 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0817\n",
      "Epoch 00038: val_loss did not improve from 2.71840\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0817 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0816\n",
      "Epoch 00039: val_loss improved from 2.71840 to 2.71830, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/039-2.7183.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0816 - val_loss: 2.7183 - val_acc: 0.0818\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00040: val_loss improved from 2.71830 to 2.71823, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv_checkpoint/040-2.7182.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0808 - val_loss: 2.7182 - val_acc: 0.0818\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0789\n",
      "Epoch 00041: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0788 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00042: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0795\n",
      "Epoch 00043: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0795 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0788\n",
      "Epoch 00044: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0788 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0813\n",
      "Epoch 00045: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0813 - val_loss: 2.7190 - val_acc: 0.0818\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0800\n",
      "Epoch 00046: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0800 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00047: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0814\n",
      "Epoch 00048: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0814 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00049: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0796\n",
      "Epoch 00050: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0796 - val_loss: 2.7188 - val_acc: 0.0818\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00051: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0796\n",
      "Epoch 00052: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0796 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0832\n",
      "Epoch 00053: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0832 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0774\n",
      "Epoch 00054: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0774 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00055: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7184 - val_acc: 0.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0796\n",
      "Epoch 00056: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0796 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0811\n",
      "Epoch 00057: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0811 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0783\n",
      "Epoch 00058: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0783 - val_loss: 2.7187 - val_acc: 0.0736\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00059: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7182 - val_acc: 0.0820\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0783\n",
      "Epoch 00060: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0783 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00061: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0816\n",
      "Epoch 00062: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0816 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00063: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0823\n",
      "Epoch 00064: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0822 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0817\n",
      "Epoch 00065: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0817 - val_loss: 2.7188 - val_acc: 0.0818\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00066: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0821\n",
      "Epoch 00067: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0821 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0820\n",
      "Epoch 00068: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0821 - val_loss: 2.7197 - val_acc: 0.0785\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0816\n",
      "Epoch 00069: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0816 - val_loss: 2.7191 - val_acc: 0.0785\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00070: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0794\n",
      "Epoch 00071: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0794 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0807\n",
      "Epoch 00072: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0806 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00073: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0798 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00074: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0789\n",
      "Epoch 00075: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0788 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0814\n",
      "Epoch 00076: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0814 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00077: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0807\n",
      "Epoch 00078: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0807 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0787\n",
      "Epoch 00079: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0788 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00080: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.7204 - acc: 0.0803 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0799\n",
      "Epoch 00081: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0799 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0791\n",
      "Epoch 00082: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0804\n",
      "Epoch 00083: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0804 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0806\n",
      "Epoch 00084: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0806 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0795\n",
      "Epoch 00085: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0795 - val_loss: 2.7186 - val_acc: 0.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00086: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0799\n",
      "Epoch 00087: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0816\n",
      "Epoch 00088: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0816 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00089: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7185 - val_acc: 0.0776\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00090: val_loss did not improve from 2.71823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7187 - val_acc: 0.0776\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7128 - acc: 0.0781\n",
      "Loss: 2.7127533008500175 Accuracy: 0.078089304\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7284 - acc: 0.0782\n",
      "Epoch 00001: val_loss improved from inf to 2.71909, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/001-2.7191.hdf5\n",
      "36805/36805 [==============================] - 97s 3ms/sample - loss: 2.7284 - acc: 0.0782 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7221 - acc: 0.0821\n",
      "Epoch 00002: val_loss did not improve from 2.71909\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7221 - acc: 0.0821 - val_loss: 2.7192 - val_acc: 0.0785\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7211 - acc: 0.0815\n",
      "Epoch 00003: val_loss improved from 2.71909 to 2.71882, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/003-2.7188.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7211 - acc: 0.0815 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7209 - acc: 0.0810\n",
      "Epoch 00004: val_loss did not improve from 2.71882\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7209 - acc: 0.0810 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0801\n",
      "Epoch 00005: val_loss did not improve from 2.71882\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7204 - acc: 0.0801 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00006: val_loss improved from 2.71882 to 2.71856, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/006-2.7186.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00007: val_loss did not improve from 2.71856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7206 - acc: 0.0801 - val_loss: 2.7186 - val_acc: 0.0776\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0788\n",
      "Epoch 00008: val_loss improved from 2.71856 to 2.71851, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/008-2.7185.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7206 - acc: 0.0789 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00009: val_loss improved from 2.71851 to 2.71832, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/009-2.7183.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0798\n",
      "Epoch 00010: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0798 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00011: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00012: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7191 - val_acc: 0.0785\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0781\n",
      "Epoch 00013: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0781 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0806\n",
      "Epoch 00014: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0806 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00015: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0793\n",
      "Epoch 00016: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00017: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0813\n",
      "Epoch 00018: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0813 - val_loss: 2.7192 - val_acc: 0.0785\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0804\n",
      "Epoch 00019: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0804 - val_loss: 2.7187 - val_acc: 0.0818\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00020: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0804 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0797\n",
      "Epoch 00021: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.0821\n",
      "Epoch 00022: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0821 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0799\n",
      "Epoch 00023: val_loss did not improve from 2.71832\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0799 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0817\n",
      "Epoch 00024: val_loss improved from 2.71832 to 2.71831, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/024-2.7183.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0817 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0828\n",
      "Epoch 00025: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0828 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0801\n",
      "Epoch 00026: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0786\n",
      "Epoch 00027: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0785 - val_loss: 2.7184 - val_acc: 0.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0816\n",
      "Epoch 00028: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0816 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0796\n",
      "Epoch 00029: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0796 - val_loss: 2.7185 - val_acc: 0.0818\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0790\n",
      "Epoch 00030: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0790 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00031: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00032: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0806 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00033: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0802 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00034: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0813\n",
      "Epoch 00035: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0813 - val_loss: 2.7192 - val_acc: 0.0785\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0818\n",
      "Epoch 00036: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0818 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00037: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0803 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00038: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0801\n",
      "Epoch 00039: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0801 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00040: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00041: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0803 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0802\n",
      "Epoch 00042: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0802 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0801\n",
      "Epoch 00043: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0801 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0804\n",
      "Epoch 00044: val_loss did not improve from 2.71831\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0804 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0797\n",
      "Epoch 00045: val_loss improved from 2.71831 to 2.71825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv_checkpoint/045-2.7183.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0796\n",
      "Epoch 00046: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0796 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0813\n",
      "Epoch 00047: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0813 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0800\n",
      "Epoch 00048: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0800 - val_loss: 2.7183 - val_acc: 0.0785\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00049: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0787\n",
      "Epoch 00050: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0788 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0789\n",
      "Epoch 00051: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0789 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0792\n",
      "Epoch 00052: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00053: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0798 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0815\n",
      "Epoch 00054: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0815 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0810\n",
      "Epoch 00055: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0809 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0801\n",
      "Epoch 00056: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00057: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0808 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00058: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0795 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00059: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00060: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0807\n",
      "Epoch 00061: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0807 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0797\n",
      "Epoch 00062: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7190 - val_acc: 0.0820\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0799\n",
      "Epoch 00063: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0799 - val_loss: 2.7193 - val_acc: 0.0820\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00064: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0803\n",
      "Epoch 00065: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0803 - val_loss: 2.7188 - val_acc: 0.0785\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00066: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0807\n",
      "Epoch 00067: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00068: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0798 - val_loss: 2.7195 - val_acc: 0.0820\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0802\n",
      "Epoch 00069: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7186 - val_acc: 0.0811\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0796\n",
      "Epoch 00070: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0796 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0809\n",
      "Epoch 00071: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0812\n",
      "Epoch 00072: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0812\n",
      "Epoch 00073: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0812 - val_loss: 2.7189 - val_acc: 0.0785\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0800\n",
      "Epoch 00074: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0800 - val_loss: 2.7184 - val_acc: 0.0776\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0827\n",
      "Epoch 00075: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7203 - acc: 0.0827 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0796\n",
      "Epoch 00076: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0797 - val_loss: 2.7189 - val_acc: 0.0820\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0816\n",
      "Epoch 00077: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0816 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0820\n",
      "Epoch 00078: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0820 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0791\n",
      "Epoch 00079: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0798\n",
      "Epoch 00080: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0799 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0806\n",
      "Epoch 00081: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0792\n",
      "Epoch 00082: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0791 - val_loss: 2.7186 - val_acc: 0.0811\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0806\n",
      "Epoch 00083: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0803\n",
      "Epoch 00084: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7206 - acc: 0.0803 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0793\n",
      "Epoch 00085: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0793 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00086: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7184 - val_acc: 0.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00087: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0808 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0824\n",
      "Epoch 00088: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0825 - val_loss: 2.7185 - val_acc: 0.0818\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0791\n",
      "Epoch 00089: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0791 - val_loss: 2.7191 - val_acc: 0.0820\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0821\n",
      "Epoch 00090: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7204 - acc: 0.0821 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00091: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0811\n",
      "Epoch 00092: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7183 - val_acc: 0.0818\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0818\n",
      "Epoch 00093: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0818 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0786\n",
      "Epoch 00094: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.7205 - acc: 0.0786 - val_loss: 2.7184 - val_acc: 0.0785\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0794\n",
      "Epoch 00095: val_loss did not improve from 2.71825\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.7205 - acc: 0.0794 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7129 - acc: 0.0847\n",
      "Loss: 2.7128722543409434 Accuracy: 0.0847352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD'\n",
    "\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "    \n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 934us/sample - loss: 1.2714 - acc: 0.6195\n",
      "Loss: 1.2713904990833 Accuracy: 0.61952233\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 968us/sample - loss: 0.7915 - acc: 0.7801\n",
      "Loss: 0.7915258266225164 Accuracy: 0.7800623\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5926 - acc: 0.8594\n",
      "Loss: 0.592554632711262 Accuracy: 0.8593977\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7130 - acc: 0.0781\n",
      "Loss: 2.7129934609493365 Accuracy: 0.078089304\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2081 - acc: 0.9475\n",
      "Loss: 0.2080932543262441 Accuracy: 0.9474559\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7128 - acc: 0.0781\n",
      "Loss: 2.7127533008500175 Accuracy: 0.078089304\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7129 - acc: 0.0847\n",
      "Loss: 2.7128722543409434 Accuracy: 0.0847352\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 989us/sample - loss: 1.5248 - acc: 0.6839\n",
      "Loss: 1.5247803859869145 Accuracy: 0.68390447\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9413 - acc: 0.8004\n",
      "Loss: 0.9412812744902673 Accuracy: 0.8004154\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.6436 - acc: 0.8619\n",
      "Loss: 0.6436304615913027 Accuracy: 0.8618899\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7131 - acc: 0.0800\n",
      "Loss: 2.713131129382679 Accuracy: 0.07995846\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2238 - acc: 0.9456\n",
      "Loss: 0.22379237889792858 Accuracy: 0.9455867\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7131 - acc: 0.0783\n",
      "Loss: 2.7130573417786374 Accuracy: 0.07829699\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_SGD_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7132 - acc: 0.0800\n",
      "Loss: 2.7131908941120373 Accuracy: 0.07995846\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_conv_3_VGG_DO_075_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
