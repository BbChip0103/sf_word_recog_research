{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())   \n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8740 - acc: 0.2379\n",
      "Epoch 00001: val_loss improved from inf to 1.86330, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/001-1.8633.hdf5\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 2.8739 - acc: 0.2379 - val_loss: 1.8633 - val_acc: 0.4309\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0497 - acc: 0.3995\n",
      "Epoch 00002: val_loss improved from 1.86330 to 1.76672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/002-1.7667.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 2.0498 - acc: 0.3995 - val_loss: 1.7667 - val_acc: 0.4559\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6817 - acc: 0.4952\n",
      "Epoch 00003: val_loss did not improve from 1.76672\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.6817 - acc: 0.4952 - val_loss: 3.1383 - val_acc: 0.3629\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4488 - acc: 0.5629\n",
      "Epoch 00004: val_loss did not improve from 1.76672\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.4487 - acc: 0.5629 - val_loss: 2.6788 - val_acc: 0.3955\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2812 - acc: 0.6099\n",
      "Epoch 00005: val_loss improved from 1.76672 to 0.99750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/005-0.9975.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.2813 - acc: 0.6099 - val_loss: 0.9975 - val_acc: 0.7025\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1628 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.99750\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.1629 - acc: 0.6471 - val_loss: 1.3612 - val_acc: 0.6143\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.6786\n",
      "Epoch 00007: val_loss did not improve from 0.99750\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.0673 - acc: 0.6785 - val_loss: 1.2744 - val_acc: 0.6410\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0050 - acc: 0.6953\n",
      "Epoch 00008: val_loss did not improve from 0.99750\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.0050 - acc: 0.6953 - val_loss: 1.3587 - val_acc: 0.6389\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7205\n",
      "Epoch 00009: val_loss did not improve from 0.99750\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.9354 - acc: 0.7205 - val_loss: 1.4793 - val_acc: 0.6110\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7332\n",
      "Epoch 00010: val_loss did not improve from 0.99750\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.8913 - acc: 0.7332 - val_loss: 2.0979 - val_acc: 0.5346\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7409\n",
      "Epoch 00011: val_loss improved from 0.99750 to 0.91097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/011-0.9110.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.8600 - acc: 0.7409 - val_loss: 0.9110 - val_acc: 0.7375\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8238 - acc: 0.7515\n",
      "Epoch 00012: val_loss did not improve from 0.91097\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.8238 - acc: 0.7515 - val_loss: 1.0996 - val_acc: 0.6813\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7874 - acc: 0.7599\n",
      "Epoch 00013: val_loss did not improve from 0.91097\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7875 - acc: 0.7598 - val_loss: 1.0752 - val_acc: 0.7058\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7554 - acc: 0.7718\n",
      "Epoch 00014: val_loss improved from 0.91097 to 0.85448, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/014-0.8545.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7553 - acc: 0.7718 - val_loss: 0.8545 - val_acc: 0.7501\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7365 - acc: 0.7776\n",
      "Epoch 00015: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7365 - acc: 0.7776 - val_loss: 0.9280 - val_acc: 0.7286\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7157 - acc: 0.7859\n",
      "Epoch 00016: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7157 - acc: 0.7859 - val_loss: 1.4227 - val_acc: 0.6198\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7914\n",
      "Epoch 00017: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6973 - acc: 0.7914 - val_loss: 1.5436 - val_acc: 0.6296\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6754 - acc: 0.7971\n",
      "Epoch 00018: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6755 - acc: 0.7970 - val_loss: 1.2209 - val_acc: 0.6718\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.8022\n",
      "Epoch 00019: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6517 - acc: 0.8022 - val_loss: 1.2721 - val_acc: 0.6571\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6385 - acc: 0.8076\n",
      "Epoch 00020: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6389 - acc: 0.8075 - val_loss: 1.7829 - val_acc: 0.5858\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.8066\n",
      "Epoch 00021: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6347 - acc: 0.8066 - val_loss: 2.7918 - val_acc: 0.4941\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6141 - acc: 0.8148\n",
      "Epoch 00022: val_loss did not improve from 0.85448\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6141 - acc: 0.8148 - val_loss: 1.1290 - val_acc: 0.6660\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.8230\n",
      "Epoch 00023: val_loss improved from 0.85448 to 0.72419, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/023-0.7242.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5887 - acc: 0.8230 - val_loss: 0.7242 - val_acc: 0.7941\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5898 - acc: 0.8219\n",
      "Epoch 00024: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5898 - acc: 0.8219 - val_loss: 1.9062 - val_acc: 0.5924\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8274\n",
      "Epoch 00025: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5727 - acc: 0.8274 - val_loss: 1.0675 - val_acc: 0.7137\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.8257\n",
      "Epoch 00026: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5688 - acc: 0.8257 - val_loss: 0.7983 - val_acc: 0.7813\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8344\n",
      "Epoch 00027: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5476 - acc: 0.8344 - val_loss: 1.0956 - val_acc: 0.7177\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.8336\n",
      "Epoch 00028: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5414 - acc: 0.8336 - val_loss: 0.9001 - val_acc: 0.7426\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8396\n",
      "Epoch 00029: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5261 - acc: 0.8396 - val_loss: 1.1787 - val_acc: 0.6865\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8437\n",
      "Epoch 00030: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5190 - acc: 0.8437 - val_loss: 0.8240 - val_acc: 0.7615\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8437\n",
      "Epoch 00031: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5130 - acc: 0.8437 - val_loss: 1.1136 - val_acc: 0.6951\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8450\n",
      "Epoch 00032: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5119 - acc: 0.8450 - val_loss: 2.0410 - val_acc: 0.5777\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8499\n",
      "Epoch 00033: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4953 - acc: 0.8499 - val_loss: 1.4174 - val_acc: 0.6571\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8510\n",
      "Epoch 00034: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4849 - acc: 0.8510 - val_loss: 1.1477 - val_acc: 0.7051\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8519\n",
      "Epoch 00035: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4775 - acc: 0.8519 - val_loss: 0.7585 - val_acc: 0.7890\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8586\n",
      "Epoch 00036: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4655 - acc: 0.8586 - val_loss: 0.9627 - val_acc: 0.7372\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8590\n",
      "Epoch 00037: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4590 - acc: 0.8590 - val_loss: 1.3430 - val_acc: 0.6578\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4579 - acc: 0.8572\n",
      "Epoch 00038: val_loss did not improve from 0.72419\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4578 - acc: 0.8572 - val_loss: 1.1685 - val_acc: 0.7300\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8627\n",
      "Epoch 00039: val_loss improved from 0.72419 to 0.70939, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/039-0.7094.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4411 - acc: 0.8626 - val_loss: 0.7094 - val_acc: 0.7939\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.8639\n",
      "Epoch 00040: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4381 - acc: 0.8638 - val_loss: 2.9484 - val_acc: 0.5064\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8660\n",
      "Epoch 00041: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4296 - acc: 0.8660 - val_loss: 0.8433 - val_acc: 0.7661\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8683\n",
      "Epoch 00042: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4296 - acc: 0.8683 - val_loss: 1.2506 - val_acc: 0.6995\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8702\n",
      "Epoch 00043: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4175 - acc: 0.8701 - val_loss: 1.2897 - val_acc: 0.6969\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8711\n",
      "Epoch 00044: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4134 - acc: 0.8711 - val_loss: 1.4669 - val_acc: 0.6636\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8742\n",
      "Epoch 00045: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4056 - acc: 0.8741 - val_loss: 1.5960 - val_acc: 0.6527\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8718\n",
      "Epoch 00046: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4088 - acc: 0.8718 - val_loss: 0.7985 - val_acc: 0.7880\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8795\n",
      "Epoch 00047: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3861 - acc: 0.8794 - val_loss: 1.6631 - val_acc: 0.6422\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8789\n",
      "Epoch 00048: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3908 - acc: 0.8789 - val_loss: 1.6035 - val_acc: 0.6422\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8810\n",
      "Epoch 00049: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3800 - acc: 0.8810 - val_loss: 2.5196 - val_acc: 0.5660\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8845\n",
      "Epoch 00050: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3761 - acc: 0.8845 - val_loss: 1.2814 - val_acc: 0.6844\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8849\n",
      "Epoch 00051: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3684 - acc: 0.8849 - val_loss: 1.3055 - val_acc: 0.6790\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8852\n",
      "Epoch 00052: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3668 - acc: 0.8852 - val_loss: 0.7872 - val_acc: 0.7897\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8860\n",
      "Epoch 00053: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3619 - acc: 0.8860 - val_loss: 2.2001 - val_acc: 0.6247\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8900\n",
      "Epoch 00054: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3518 - acc: 0.8899 - val_loss: 0.9300 - val_acc: 0.7652\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8913\n",
      "Epoch 00055: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3450 - acc: 0.8913 - val_loss: 1.3018 - val_acc: 0.6997\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8905\n",
      "Epoch 00056: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3472 - acc: 0.8905 - val_loss: 1.2819 - val_acc: 0.7046\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8919\n",
      "Epoch 00057: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3345 - acc: 0.8919 - val_loss: 1.1300 - val_acc: 0.7303\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8938\n",
      "Epoch 00058: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3349 - acc: 0.8937 - val_loss: 1.4805 - val_acc: 0.6434\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8952\n",
      "Epoch 00059: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3289 - acc: 0.8952 - val_loss: 1.8273 - val_acc: 0.6294\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8963\n",
      "Epoch 00060: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3254 - acc: 0.8963 - val_loss: 0.9766 - val_acc: 0.7480\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8973\n",
      "Epoch 00061: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3202 - acc: 0.8973 - val_loss: 1.0986 - val_acc: 0.7216\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8983\n",
      "Epoch 00062: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3165 - acc: 0.8983 - val_loss: 1.2086 - val_acc: 0.7142\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9041\n",
      "Epoch 00063: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3041 - acc: 0.9041 - val_loss: 1.2453 - val_acc: 0.7167\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9023\n",
      "Epoch 00064: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3028 - acc: 0.9022 - val_loss: 1.5267 - val_acc: 0.6762\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.8996\n",
      "Epoch 00065: val_loss did not improve from 0.70939\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3070 - acc: 0.8995 - val_loss: 1.8036 - val_acc: 0.6874\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9039\n",
      "Epoch 00066: val_loss improved from 0.70939 to 0.68543, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv_checkpoint/066-0.6854.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.3035 - acc: 0.9038 - val_loss: 0.6854 - val_acc: 0.8288\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9084\n",
      "Epoch 00067: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2888 - acc: 0.9083 - val_loss: 1.4549 - val_acc: 0.6508\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9042\n",
      "Epoch 00068: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2986 - acc: 0.9042 - val_loss: 0.7436 - val_acc: 0.8106\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9070\n",
      "Epoch 00069: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2874 - acc: 0.9070 - val_loss: 1.5594 - val_acc: 0.6690\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9103\n",
      "Epoch 00070: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2774 - acc: 0.9103 - val_loss: 2.2876 - val_acc: 0.5784\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9105\n",
      "Epoch 00071: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2766 - acc: 0.9105 - val_loss: 1.3868 - val_acc: 0.6895\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9101\n",
      "Epoch 00072: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2825 - acc: 0.9100 - val_loss: 0.9427 - val_acc: 0.7647\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9095\n",
      "Epoch 00073: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2823 - acc: 0.9095 - val_loss: 1.3312 - val_acc: 0.7086\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9130\n",
      "Epoch 00074: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2681 - acc: 0.9130 - val_loss: 0.9056 - val_acc: 0.7757\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9140\n",
      "Epoch 00075: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2692 - acc: 0.9140 - val_loss: 0.8267 - val_acc: 0.7948\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9132\n",
      "Epoch 00076: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2640 - acc: 0.9132 - val_loss: 0.9087 - val_acc: 0.7929\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9161\n",
      "Epoch 00077: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2588 - acc: 0.9161 - val_loss: 0.7362 - val_acc: 0.8160\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9168\n",
      "Epoch 00078: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2549 - acc: 0.9168 - val_loss: 1.0831 - val_acc: 0.7545\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9174\n",
      "Epoch 00079: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2563 - acc: 0.9174 - val_loss: 1.0926 - val_acc: 0.7591\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9218\n",
      "Epoch 00080: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2422 - acc: 0.9218 - val_loss: 1.4100 - val_acc: 0.7095\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9200\n",
      "Epoch 00081: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2440 - acc: 0.9200 - val_loss: 0.9380 - val_acc: 0.7706\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9221\n",
      "Epoch 00082: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2442 - acc: 0.9221 - val_loss: 3.7899 - val_acc: 0.4521\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9214\n",
      "Epoch 00083: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2411 - acc: 0.9214 - val_loss: 1.9660 - val_acc: 0.6217\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9245\n",
      "Epoch 00084: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2337 - acc: 0.9245 - val_loss: 1.1535 - val_acc: 0.7347\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9227\n",
      "Epoch 00085: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2372 - acc: 0.9227 - val_loss: 0.7481 - val_acc: 0.8178\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9254\n",
      "Epoch 00086: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2318 - acc: 0.9254 - val_loss: 1.6539 - val_acc: 0.6925\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9264\n",
      "Epoch 00087: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2275 - acc: 0.9263 - val_loss: 1.6868 - val_acc: 0.6629\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9250\n",
      "Epoch 00088: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2299 - acc: 0.9249 - val_loss: 1.9103 - val_acc: 0.6518\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9214\n",
      "Epoch 00089: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2493 - acc: 0.9214 - val_loss: 0.8204 - val_acc: 0.8036\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9310\n",
      "Epoch 00090: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2135 - acc: 0.9310 - val_loss: 0.8667 - val_acc: 0.7948\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9321\n",
      "Epoch 00091: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2121 - acc: 0.9320 - val_loss: 1.3896 - val_acc: 0.7205\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9329\n",
      "Epoch 00092: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2122 - acc: 0.9329 - val_loss: 0.6855 - val_acc: 0.8265\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9338\n",
      "Epoch 00093: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2072 - acc: 0.9338 - val_loss: 1.1565 - val_acc: 0.7540\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9306\n",
      "Epoch 00094: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2157 - acc: 0.9306 - val_loss: 1.5346 - val_acc: 0.7074\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9339\n",
      "Epoch 00095: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2041 - acc: 0.9339 - val_loss: 0.8888 - val_acc: 0.7857\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9361\n",
      "Epoch 00096: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1955 - acc: 0.9361 - val_loss: 1.2418 - val_acc: 0.7382\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9356\n",
      "Epoch 00097: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1961 - acc: 0.9356 - val_loss: 0.7441 - val_acc: 0.8190\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9316\n",
      "Epoch 00098: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2104 - acc: 0.9316 - val_loss: 0.8773 - val_acc: 0.8071\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9321\n",
      "Epoch 00099: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2047 - acc: 0.9320 - val_loss: 0.7162 - val_acc: 0.8339\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9360\n",
      "Epoch 00100: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1932 - acc: 0.9360 - val_loss: 0.8424 - val_acc: 0.8025\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9382\n",
      "Epoch 00101: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1888 - acc: 0.9382 - val_loss: 1.3097 - val_acc: 0.7358\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9419\n",
      "Epoch 00102: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1827 - acc: 0.9419 - val_loss: 1.1973 - val_acc: 0.7480\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9349\n",
      "Epoch 00103: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1985 - acc: 0.9350 - val_loss: 1.5704 - val_acc: 0.7046\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9386\n",
      "Epoch 00104: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1874 - acc: 0.9386 - val_loss: 1.7643 - val_acc: 0.6909\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9357\n",
      "Epoch 00105: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1943 - acc: 0.9357 - val_loss: 2.3396 - val_acc: 0.6171\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9405\n",
      "Epoch 00106: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1805 - acc: 0.9405 - val_loss: 2.1260 - val_acc: 0.6592\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9385\n",
      "Epoch 00107: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1832 - acc: 0.9385 - val_loss: 1.6223 - val_acc: 0.6986\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9398\n",
      "Epoch 00108: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1836 - acc: 0.9398 - val_loss: 0.9854 - val_acc: 0.7932\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9433\n",
      "Epoch 00109: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1748 - acc: 0.9433 - val_loss: 1.1131 - val_acc: 0.7666\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9402\n",
      "Epoch 00110: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1815 - acc: 0.9402 - val_loss: 1.6022 - val_acc: 0.6958\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9424\n",
      "Epoch 00111: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1781 - acc: 0.9423 - val_loss: 1.1174 - val_acc: 0.7510\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9421\n",
      "Epoch 00112: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1748 - acc: 0.9420 - val_loss: 0.8587 - val_acc: 0.8043\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9408\n",
      "Epoch 00113: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1801 - acc: 0.9408 - val_loss: 1.0205 - val_acc: 0.7764\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9456\n",
      "Epoch 00114: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1722 - acc: 0.9456 - val_loss: 1.0492 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9486\n",
      "Epoch 00115: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1578 - acc: 0.9485 - val_loss: 1.3979 - val_acc: 0.7303\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9443\n",
      "Epoch 00116: val_loss did not improve from 0.68543\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1673 - acc: 0.9443 - val_loss: 2.1741 - val_acc: 0.6455\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.8138 - acc: 0.7896\n",
      "Loss: 0.8137692730251007 Accuracy: 0.7896158\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9432 - acc: 0.2148\n",
      "Epoch 00001: val_loss improved from inf to 1.92318, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/001-1.9232.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 2.9431 - acc: 0.2148 - val_loss: 1.9232 - val_acc: 0.3904\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1049 - acc: 0.3789\n",
      "Epoch 00002: val_loss improved from 1.92318 to 1.67779, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/002-1.6778.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 2.1049 - acc: 0.3789 - val_loss: 1.6778 - val_acc: 0.4675\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7238 - acc: 0.4732\n",
      "Epoch 00003: val_loss did not improve from 1.67779\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.7237 - acc: 0.4733 - val_loss: 2.0145 - val_acc: 0.4358\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4728 - acc: 0.5482\n",
      "Epoch 00004: val_loss improved from 1.67779 to 1.25513, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/004-1.2551.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.4727 - acc: 0.5482 - val_loss: 1.2551 - val_acc: 0.6042\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3014 - acc: 0.5980\n",
      "Epoch 00005: val_loss improved from 1.25513 to 1.10424, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/005-1.1042.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.3013 - acc: 0.5980 - val_loss: 1.1042 - val_acc: 0.6615\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1674 - acc: 0.6388\n",
      "Epoch 00006: val_loss improved from 1.10424 to 0.91993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/006-0.9199.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.1674 - acc: 0.6388 - val_loss: 0.9199 - val_acc: 0.7228\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0587 - acc: 0.6757\n",
      "Epoch 00007: val_loss improved from 0.91993 to 0.91446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/007-0.9145.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.0590 - acc: 0.6756 - val_loss: 0.9145 - val_acc: 0.7305\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9894 - acc: 0.6971\n",
      "Epoch 00008: val_loss did not improve from 0.91446\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.9895 - acc: 0.6971 - val_loss: 1.0538 - val_acc: 0.6981\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9077 - acc: 0.7230\n",
      "Epoch 00009: val_loss improved from 0.91446 to 0.76280, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/009-0.7628.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.9078 - acc: 0.7230 - val_loss: 0.7628 - val_acc: 0.7857\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7413\n",
      "Epoch 00010: val_loss did not improve from 0.76280\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.8564 - acc: 0.7412 - val_loss: 1.4633 - val_acc: 0.5868\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8162 - acc: 0.7569\n",
      "Epoch 00011: val_loss did not improve from 0.76280\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.8165 - acc: 0.7568 - val_loss: 0.7896 - val_acc: 0.7657\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7701 - acc: 0.7689\n",
      "Epoch 00012: val_loss did not improve from 0.76280\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.7702 - acc: 0.7689 - val_loss: 1.2489 - val_acc: 0.6599\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7840\n",
      "Epoch 00013: val_loss did not improve from 0.76280\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.7235 - acc: 0.7840 - val_loss: 0.9656 - val_acc: 0.7349\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.7936\n",
      "Epoch 00014: val_loss did not improve from 0.76280\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6908 - acc: 0.7936 - val_loss: 1.0764 - val_acc: 0.7121\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.8040\n",
      "Epoch 00015: val_loss improved from 0.76280 to 0.68539, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/015-0.6854.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6600 - acc: 0.8039 - val_loss: 0.6854 - val_acc: 0.8064\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6344 - acc: 0.8119\n",
      "Epoch 00016: val_loss improved from 0.68539 to 0.65733, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/016-0.6573.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6345 - acc: 0.8119 - val_loss: 0.6573 - val_acc: 0.8244\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8199\n",
      "Epoch 00017: val_loss improved from 0.65733 to 0.57479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/017-0.5748.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6105 - acc: 0.8199 - val_loss: 0.5748 - val_acc: 0.8407\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8305\n",
      "Epoch 00018: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5849 - acc: 0.8305 - val_loss: 0.7015 - val_acc: 0.7920\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8323\n",
      "Epoch 00019: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5623 - acc: 0.8323 - val_loss: 0.7324 - val_acc: 0.7957\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.8401\n",
      "Epoch 00020: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5391 - acc: 0.8401 - val_loss: 0.6690 - val_acc: 0.8046\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8468\n",
      "Epoch 00021: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5197 - acc: 0.8468 - val_loss: 0.6879 - val_acc: 0.8090\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8527\n",
      "Epoch 00022: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5049 - acc: 0.8527 - val_loss: 0.8332 - val_acc: 0.7552\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8584\n",
      "Epoch 00023: val_loss did not improve from 0.57479\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4853 - acc: 0.8584 - val_loss: 0.6055 - val_acc: 0.8321\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8643\n",
      "Epoch 00024: val_loss improved from 0.57479 to 0.56173, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/024-0.5617.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4621 - acc: 0.8643 - val_loss: 0.5617 - val_acc: 0.8428\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8639\n",
      "Epoch 00025: val_loss did not improve from 0.56173\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4592 - acc: 0.8639 - val_loss: 0.7297 - val_acc: 0.7927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8724\n",
      "Epoch 00026: val_loss improved from 0.56173 to 0.48345, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/026-0.4835.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4317 - acc: 0.8724 - val_loss: 0.4835 - val_acc: 0.8633\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.8744\n",
      "Epoch 00027: val_loss did not improve from 0.48345\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4299 - acc: 0.8743 - val_loss: 0.5587 - val_acc: 0.8341\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8763\n",
      "Epoch 00028: val_loss improved from 0.48345 to 0.44245, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/028-0.4425.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4160 - acc: 0.8762 - val_loss: 0.4425 - val_acc: 0.8747\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8790\n",
      "Epoch 00029: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4150 - acc: 0.8790 - val_loss: 0.6772 - val_acc: 0.8130\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8848\n",
      "Epoch 00030: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3830 - acc: 0.8848 - val_loss: 0.8369 - val_acc: 0.7794\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8861\n",
      "Epoch 00031: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3806 - acc: 0.8861 - val_loss: 1.5823 - val_acc: 0.6420\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8904\n",
      "Epoch 00032: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3717 - acc: 0.8903 - val_loss: 0.9829 - val_acc: 0.7536\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8883\n",
      "Epoch 00033: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3703 - acc: 0.8883 - val_loss: 0.4654 - val_acc: 0.8647\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3488 - acc: 0.8952\n",
      "Epoch 00034: val_loss did not improve from 0.44245\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3488 - acc: 0.8953 - val_loss: 0.6661 - val_acc: 0.8143\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8969\n",
      "Epoch 00035: val_loss improved from 0.44245 to 0.44239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/035-0.4424.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3403 - acc: 0.8968 - val_loss: 0.4424 - val_acc: 0.8800\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8998\n",
      "Epoch 00036: val_loss did not improve from 0.44239\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3349 - acc: 0.8999 - val_loss: 0.4802 - val_acc: 0.8609\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9035\n",
      "Epoch 00037: val_loss improved from 0.44239 to 0.44033, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/037-0.4403.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3249 - acc: 0.9035 - val_loss: 0.4403 - val_acc: 0.8775\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9053\n",
      "Epoch 00038: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3142 - acc: 0.9052 - val_loss: 0.5089 - val_acc: 0.8586\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9068\n",
      "Epoch 00039: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3082 - acc: 0.9068 - val_loss: 0.6642 - val_acc: 0.8220\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9108\n",
      "Epoch 00040: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2993 - acc: 0.9107 - val_loss: 0.6071 - val_acc: 0.8325\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9120\n",
      "Epoch 00041: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2943 - acc: 0.9120 - val_loss: 0.5301 - val_acc: 0.8507\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9149\n",
      "Epoch 00042: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2822 - acc: 0.9149 - val_loss: 0.4497 - val_acc: 0.8696\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9157\n",
      "Epoch 00043: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2842 - acc: 0.9156 - val_loss: 1.3057 - val_acc: 0.7193\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9160\n",
      "Epoch 00044: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2701 - acc: 0.9160 - val_loss: 0.8509 - val_acc: 0.7871\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9174\n",
      "Epoch 00045: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2710 - acc: 0.9174 - val_loss: 0.5253 - val_acc: 0.8446\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9214\n",
      "Epoch 00046: val_loss did not improve from 0.44033\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2569 - acc: 0.9214 - val_loss: 0.7933 - val_acc: 0.7948\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9222\n",
      "Epoch 00047: val_loss improved from 0.44033 to 0.43320, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/047-0.4332.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2514 - acc: 0.9222 - val_loss: 0.4332 - val_acc: 0.8768\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9195\n",
      "Epoch 00048: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2575 - acc: 0.9194 - val_loss: 0.4861 - val_acc: 0.8626\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9257\n",
      "Epoch 00049: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2394 - acc: 0.9256 - val_loss: 1.1260 - val_acc: 0.7561\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9271\n",
      "Epoch 00050: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2358 - acc: 0.9271 - val_loss: 0.6104 - val_acc: 0.8383\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9285\n",
      "Epoch 00051: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2321 - acc: 0.9284 - val_loss: 0.5170 - val_acc: 0.8642\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9287\n",
      "Epoch 00052: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2261 - acc: 0.9287 - val_loss: 0.4638 - val_acc: 0.8691\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9314\n",
      "Epoch 00053: val_loss did not improve from 0.43320\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2235 - acc: 0.9314 - val_loss: 2.0811 - val_acc: 0.6520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9331\n",
      "Epoch 00054: val_loss improved from 0.43320 to 0.41643, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/054-0.4164.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2152 - acc: 0.9331 - val_loss: 0.4164 - val_acc: 0.8873\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9338\n",
      "Epoch 00055: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2106 - acc: 0.9338 - val_loss: 1.0515 - val_acc: 0.7577\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9336\n",
      "Epoch 00056: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2118 - acc: 0.9336 - val_loss: 0.7341 - val_acc: 0.8169\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9268\n",
      "Epoch 00057: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2357 - acc: 0.9268 - val_loss: 1.0549 - val_acc: 0.7792\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9402\n",
      "Epoch 00058: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1932 - acc: 0.9401 - val_loss: 0.4812 - val_acc: 0.8705\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9382\n",
      "Epoch 00059: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1949 - acc: 0.9382 - val_loss: 0.4804 - val_acc: 0.8665\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9400\n",
      "Epoch 00060: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1905 - acc: 0.9400 - val_loss: 0.5785 - val_acc: 0.8430\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9380\n",
      "Epoch 00061: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1992 - acc: 0.9380 - val_loss: 0.4718 - val_acc: 0.8700\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9441\n",
      "Epoch 00062: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1759 - acc: 0.9441 - val_loss: 0.4821 - val_acc: 0.8756\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9459\n",
      "Epoch 00063: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1742 - acc: 0.9459 - val_loss: 0.4906 - val_acc: 0.8691\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9450\n",
      "Epoch 00064: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1751 - acc: 0.9450 - val_loss: 0.5275 - val_acc: 0.8684\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9445\n",
      "Epoch 00065: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1754 - acc: 0.9445 - val_loss: 1.0227 - val_acc: 0.7645\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9471\n",
      "Epoch 00066: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1696 - acc: 0.9471 - val_loss: 0.6077 - val_acc: 0.8470\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9484\n",
      "Epoch 00067: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1664 - acc: 0.9484 - val_loss: 1.0911 - val_acc: 0.7640\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9468\n",
      "Epoch 00068: val_loss did not improve from 0.41643\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1682 - acc: 0.9468 - val_loss: 0.7980 - val_acc: 0.8141\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9530\n",
      "Epoch 00069: val_loss improved from 0.41643 to 0.39652, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/069-0.3965.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1504 - acc: 0.9530 - val_loss: 0.3965 - val_acc: 0.8942\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9501\n",
      "Epoch 00070: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1588 - acc: 0.9500 - val_loss: 0.4792 - val_acc: 0.8805\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9507\n",
      "Epoch 00071: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1584 - acc: 0.9507 - val_loss: 0.5436 - val_acc: 0.8656\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9531\n",
      "Epoch 00072: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1460 - acc: 0.9531 - val_loss: 0.5897 - val_acc: 0.8430\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9558\n",
      "Epoch 00073: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1380 - acc: 0.9558 - val_loss: 0.5558 - val_acc: 0.8619\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9511\n",
      "Epoch 00074: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1525 - acc: 0.9511 - val_loss: 1.3869 - val_acc: 0.7447\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9580\n",
      "Epoch 00075: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1334 - acc: 0.9580 - val_loss: 0.5094 - val_acc: 0.8758\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9543\n",
      "Epoch 00076: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1454 - acc: 0.9543 - val_loss: 0.4679 - val_acc: 0.8796\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9549\n",
      "Epoch 00077: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1411 - acc: 0.9549 - val_loss: 0.4764 - val_acc: 0.8735\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9563\n",
      "Epoch 00078: val_loss did not improve from 0.39652\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1361 - acc: 0.9563 - val_loss: 0.9901 - val_acc: 0.7864\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9583\n",
      "Epoch 00079: val_loss improved from 0.39652 to 0.38980, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/079-0.3898.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1289 - acc: 0.9583 - val_loss: 0.3898 - val_acc: 0.9057\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9591\n",
      "Epoch 00080: val_loss did not improve from 0.38980\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1297 - acc: 0.9591 - val_loss: 0.7099 - val_acc: 0.8227\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9572\n",
      "Epoch 00081: val_loss did not improve from 0.38980\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1347 - acc: 0.9572 - val_loss: 0.6653 - val_acc: 0.8383\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9635\n",
      "Epoch 00082: val_loss did not improve from 0.38980\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1198 - acc: 0.9635 - val_loss: 0.4302 - val_acc: 0.8901\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9642\n",
      "Epoch 00083: val_loss did not improve from 0.38980\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1139 - acc: 0.9642 - val_loss: 0.5043 - val_acc: 0.8770\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9585\n",
      "Epoch 00084: val_loss did not improve from 0.38980\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1273 - acc: 0.9585 - val_loss: 0.4826 - val_acc: 0.8824\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9623\n",
      "Epoch 00085: val_loss improved from 0.38980 to 0.38714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/085-0.3871.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1170 - acc: 0.9623 - val_loss: 0.3871 - val_acc: 0.9036\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9646\n",
      "Epoch 00086: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1113 - acc: 0.9647 - val_loss: 2.3131 - val_acc: 0.6394\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9625\n",
      "Epoch 00087: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1137 - acc: 0.9625 - val_loss: 0.6528 - val_acc: 0.8509\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9594\n",
      "Epoch 00088: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1285 - acc: 0.9594 - val_loss: 0.5288 - val_acc: 0.8838\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9633\n",
      "Epoch 00089: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1143 - acc: 0.9632 - val_loss: 0.6187 - val_acc: 0.8577\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9676\n",
      "Epoch 00090: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1031 - acc: 0.9676 - val_loss: 0.5330 - val_acc: 0.8728\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9659\n",
      "Epoch 00091: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1071 - acc: 0.9659 - val_loss: 0.6515 - val_acc: 0.8528\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9636\n",
      "Epoch 00092: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1120 - acc: 0.9635 - val_loss: 0.4081 - val_acc: 0.8975\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9652\n",
      "Epoch 00093: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1067 - acc: 0.9652 - val_loss: 0.7428 - val_acc: 0.8376\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9673\n",
      "Epoch 00094: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1030 - acc: 0.9672 - val_loss: 0.7209 - val_acc: 0.8407\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9694\n",
      "Epoch 00095: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0997 - acc: 0.9694 - val_loss: 0.4205 - val_acc: 0.8938\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9666\n",
      "Epoch 00096: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1063 - acc: 0.9666 - val_loss: 0.5189 - val_acc: 0.8768\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9715\n",
      "Epoch 00097: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0870 - acc: 0.9715 - val_loss: 0.5246 - val_acc: 0.8779\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9688\n",
      "Epoch 00098: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0954 - acc: 0.9687 - val_loss: 0.4853 - val_acc: 0.8887\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9679\n",
      "Epoch 00099: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1009 - acc: 0.9679 - val_loss: 1.6817 - val_acc: 0.7142\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9673\n",
      "Epoch 00100: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1008 - acc: 0.9672 - val_loss: 0.6466 - val_acc: 0.8493\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9661\n",
      "Epoch 00101: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1083 - acc: 0.9661 - val_loss: 0.5500 - val_acc: 0.8707\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9727\n",
      "Epoch 00102: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0857 - acc: 0.9727 - val_loss: 0.5727 - val_acc: 0.8691\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9708\n",
      "Epoch 00103: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0901 - acc: 0.9708 - val_loss: 0.4167 - val_acc: 0.9017\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9740\n",
      "Epoch 00104: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0846 - acc: 0.9739 - val_loss: 0.7821 - val_acc: 0.8330\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9699\n",
      "Epoch 00105: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0947 - acc: 0.9699 - val_loss: 0.4169 - val_acc: 0.9008\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9717\n",
      "Epoch 00106: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0852 - acc: 0.9717 - val_loss: 1.3324 - val_acc: 0.7575\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9716\n",
      "Epoch 00107: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0897 - acc: 0.9716 - val_loss: 0.4991 - val_acc: 0.8789\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9749\n",
      "Epoch 00108: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0781 - acc: 0.9749 - val_loss: 0.5056 - val_acc: 0.8840\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9730\n",
      "Epoch 00109: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0819 - acc: 0.9730 - val_loss: 0.6624 - val_acc: 0.8458\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9735\n",
      "Epoch 00110: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0822 - acc: 0.9735 - val_loss: 0.8587 - val_acc: 0.8311\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9730\n",
      "Epoch 00111: val_loss did not improve from 0.38714\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0844 - acc: 0.9729 - val_loss: 0.6943 - val_acc: 0.8546\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9643\n",
      "Epoch 00112: val_loss improved from 0.38714 to 0.35231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv_checkpoint/112-0.3523.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1137 - acc: 0.9643 - val_loss: 0.3523 - val_acc: 0.9150\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9764\n",
      "Epoch 00113: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0748 - acc: 0.9764 - val_loss: 0.4592 - val_acc: 0.8903\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9785\n",
      "Epoch 00114: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0689 - acc: 0.9785 - val_loss: 0.8243 - val_acc: 0.8246\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9781\n",
      "Epoch 00115: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0691 - acc: 0.9781 - val_loss: 0.8604 - val_acc: 0.8241\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9730\n",
      "Epoch 00116: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0844 - acc: 0.9730 - val_loss: 1.9677 - val_acc: 0.6797\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9697\n",
      "Epoch 00117: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0927 - acc: 0.9697 - val_loss: 0.4745 - val_acc: 0.8933\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9782\n",
      "Epoch 00118: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0673 - acc: 0.9782 - val_loss: 0.7486 - val_acc: 0.8460\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9763\n",
      "Epoch 00119: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0735 - acc: 0.9763 - val_loss: 0.4881 - val_acc: 0.8840\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9743\n",
      "Epoch 00120: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0802 - acc: 0.9743 - val_loss: 0.4122 - val_acc: 0.9005\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9782\n",
      "Epoch 00121: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0686 - acc: 0.9782 - val_loss: 0.8407 - val_acc: 0.8381\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9776\n",
      "Epoch 00122: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0721 - acc: 0.9776 - val_loss: 0.5128 - val_acc: 0.8810\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9779\n",
      "Epoch 00123: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0669 - acc: 0.9779 - val_loss: 0.7280 - val_acc: 0.8477\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9788\n",
      "Epoch 00124: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0654 - acc: 0.9788 - val_loss: 0.9442 - val_acc: 0.7983\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9753\n",
      "Epoch 00125: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0759 - acc: 0.9753 - val_loss: 0.7544 - val_acc: 0.8502\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9797\n",
      "Epoch 00126: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0638 - acc: 0.9797 - val_loss: 0.5519 - val_acc: 0.8814\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9770\n",
      "Epoch 00127: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0707 - acc: 0.9770 - val_loss: 0.7369 - val_acc: 0.8439\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9805\n",
      "Epoch 00128: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0621 - acc: 0.9805 - val_loss: 0.5035 - val_acc: 0.8812\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9819\n",
      "Epoch 00129: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0584 - acc: 0.9818 - val_loss: 1.0132 - val_acc: 0.7978\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9744\n",
      "Epoch 00130: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0832 - acc: 0.9744 - val_loss: 0.3942 - val_acc: 0.9071\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9759\n",
      "Epoch 00131: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0771 - acc: 0.9759 - val_loss: 0.4575 - val_acc: 0.8982\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9805\n",
      "Epoch 00132: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0606 - acc: 0.9804 - val_loss: 0.4125 - val_acc: 0.9043\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9797\n",
      "Epoch 00133: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0640 - acc: 0.9796 - val_loss: 1.4180 - val_acc: 0.7771\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9768\n",
      "Epoch 00134: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0743 - acc: 0.9767 - val_loss: 0.4329 - val_acc: 0.9024\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9796\n",
      "Epoch 00135: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0669 - acc: 0.9796 - val_loss: 0.6843 - val_acc: 0.8574\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9841\n",
      "Epoch 00136: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0522 - acc: 0.9841 - val_loss: 0.5047 - val_acc: 0.8896\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9814\n",
      "Epoch 00137: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0575 - acc: 0.9814 - val_loss: 0.6725 - val_acc: 0.8658\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9811\n",
      "Epoch 00138: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0592 - acc: 0.9811 - val_loss: 0.4541 - val_acc: 0.8908\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9798\n",
      "Epoch 00139: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0619 - acc: 0.9798 - val_loss: 1.1560 - val_acc: 0.8104\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9821\n",
      "Epoch 00140: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0561 - acc: 0.9821 - val_loss: 0.8400 - val_acc: 0.8332\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9782\n",
      "Epoch 00141: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0679 - acc: 0.9782 - val_loss: 0.6172 - val_acc: 0.8693\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9847\n",
      "Epoch 00142: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0480 - acc: 0.9846 - val_loss: 0.4236 - val_acc: 0.9075\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9759\n",
      "Epoch 00143: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0751 - acc: 0.9759 - val_loss: 0.4668 - val_acc: 0.9012\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9689\n",
      "Epoch 00144: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1044 - acc: 0.9689 - val_loss: 0.4547 - val_acc: 0.8921\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9823\n",
      "Epoch 00145: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0549 - acc: 0.9823 - val_loss: 0.4064 - val_acc: 0.9080\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9861\n",
      "Epoch 00146: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0463 - acc: 0.9861 - val_loss: 0.5048 - val_acc: 0.8898\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9832\n",
      "Epoch 00147: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0525 - acc: 0.9832 - val_loss: 0.4716 - val_acc: 0.8984\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9804\n",
      "Epoch 00148: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0621 - acc: 0.9804 - val_loss: 0.4583 - val_acc: 0.9033\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9799\n",
      "Epoch 00149: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0626 - acc: 0.9799 - val_loss: 0.4867 - val_acc: 0.8996\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9832\n",
      "Epoch 00150: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0530 - acc: 0.9832 - val_loss: 0.6621 - val_acc: 0.8665\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9807\n",
      "Epoch 00151: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0598 - acc: 0.9807 - val_loss: 0.5025 - val_acc: 0.8938\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9819\n",
      "Epoch 00152: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0578 - acc: 0.9819 - val_loss: 0.8944 - val_acc: 0.8283\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9839\n",
      "Epoch 00153: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0489 - acc: 0.9839 - val_loss: 0.5948 - val_acc: 0.8777\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9850\n",
      "Epoch 00154: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0492 - acc: 0.9849 - val_loss: 0.6683 - val_acc: 0.8679\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9780\n",
      "Epoch 00155: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0718 - acc: 0.9780 - val_loss: 0.4495 - val_acc: 0.9031\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9835\n",
      "Epoch 00156: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0502 - acc: 0.9835 - val_loss: 0.4805 - val_acc: 0.8961\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9836\n",
      "Epoch 00157: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0490 - acc: 0.9835 - val_loss: 0.4872 - val_acc: 0.8935\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9789\n",
      "Epoch 00158: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0674 - acc: 0.9789 - val_loss: 0.4741 - val_acc: 0.9066\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
      "Epoch 00159: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0487 - acc: 0.9850 - val_loss: 0.6612 - val_acc: 0.8693\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9849\n",
      "Epoch 00160: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0497 - acc: 0.9849 - val_loss: 1.2752 - val_acc: 0.7706\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9837\n",
      "Epoch 00161: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0516 - acc: 0.9838 - val_loss: 0.4893 - val_acc: 0.9022\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9855\n",
      "Epoch 00162: val_loss did not improve from 0.35231\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0463 - acc: 0.9855 - val_loss: 0.7569 - val_acc: 0.8474\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VFX6xz9nkslMeiOBUBOk9xZEQEFBFFAUENAFK+K6uirqqqwFY1sR6wIiCz9xLawNFFBRFCUg0kGQLiW0ECAJ6X1mzu+Pk5uE9JBM2pzP88wz7dx7z23ne973Pee9QkqJRqPRaDQAprqugEaj0WjqD1oUNBqNRlOAFgWNRqPRFKBFQaPRaDQFaFHQaDQaTQFaFDQajUZTgBYFjUaj0RSgRUGj0Wg0BWhR0Gg0Gk0B7nVdgarSpEkTGR4eXtfV0Gg0mgbFjh07EqSUIRWVa3CiEB4ezvbt2+u6GhqNRtOgEEKcqEw57T7SaDQaTQFOEwUhhFUIsVUIsVsIsU8I8UIpZSxCiM+FEEeEEFuEEOHOqo9Go9FoKsaZlkIOcI2UsifQC7heCDGgWJmpQJKUsh3wNvCaE+uj0Wg0mgpwWkxBqpzc6flfzfmv4nm6bwKi8j8vBeYJIYSsYj7vvLw8Tp8+TXZ2djVq7NpYrVZatmyJ2Wyu66poNJo6xKmBZiGEG7ADaAe8K6XcUqxIC+AUgJTSJoRIAYKBhGLruQ+4D6B169YltnP69Gl8fX0JDw9HCFHj+9HYkVKSmJjI6dOniYiIqOvqaDSaOsSpgWYppV1K2QtoCfQXQnS7xPUslFL2k1L2CwkpOaIqOzub4OBgLQiXiBCC4OBgbWlpNJraGX0kpUwG1gLXF/srFmgFIIRwB/yBxEvZhhaE6qGPn0ajAeeOPgoRQgTkf/YErgUOFiu2Ergz//MtwC9VjSdUFrs9i5ycWByOPGesXqPRaBoFzrQUwoC1Qog/gG3AT1LKb4UQLwohxuSXeR8IFkIcAR4DZjirMg5HNrm5cUhZ86KQnJzM/PnzL2nZUaNGkZycXOnyUVFRvPHGG5e0LY1Go6kIZ44++gPoXcrvM4t8zgYmOKsORSl0j9S8IWKIwgMPPFDiP5vNhrt72Yd51apVNV4fjUajuVRcaEaz2lUpHTW+5hkzZnD06FF69erFE088QXR0NFdeeSVjxoyhS5cuANx888307duXrl27snDhwoJlw8PDSUhI4Pjx43Tu3Jlp06bRtWtXRowYQVZWVrnb3bVrFwMGDKBHjx6MHTuWpKQkAObMmUOXLl3o0aMHt956KwDr1q2jV69e9OrVi969e5OWllbjx0Gj0TR8Glzuo4o4fHg66em7SvwupR2HIxOTyRMV0648Pj69aN/+nTL/nzVrFnv37mXXLrXd6Ohodu7cyd69ewuGeC5evJigoCCysrKIjIxk/PjxBAcHF6v7YT799FMWLVrExIkTWbZsGVOmTClzu3fccQdz585lyJAhzJw5kxdeeIF33nmHWbNmERMTg8ViKXBNvfHGG7z77rsMGjSI9PR0rFZrlY6BRqNxDVzGUqjtwTX9+/e/aMz/nDlz6NmzJwMGDODUqVMcPny4xDIRERH06tULgL59+3L8+PEy15+SkkJycjJDhgwB4M4772T9+vUA9OjRg8mTJ/PJJ58UuK4GDRrEY489xpw5c0hOTi7XpaXRaFyXRtcylNWjt9uzyMzch9XaFrM5yOn18Pb2LvgcHR3NmjVr2LRpE15eXgwdOrTUOQEWi6Xgs5ubW4Xuo7L47rvvWL9+Pd988w2vvPIKe/bsYcaMGYwePZpVq1YxaNAgVq9eTadOnS5p/RqNpvHiQpaC82IKvr6+5froU1JSCAwMxMvLi4MHD7J58+Zqb9Pf35/AwEB+/fVXAD7++GOGDBmCw+Hg1KlTXH311bz22mukpKSQnp7O0aNH6d69O0899RSRkZEcPFh8dLBGo9E0QkuhbAz9q3lRCA4OZtCgQXTr1o2RI0cyevToi/6//vrrWbBgAZ07d6Zjx44MGFA8L+Cl8eGHH3L//feTmZlJ27Zt+eCDD7Db7UyZMoWUlBSklDz88MMEBATw3HPPsXbtWkwmE127dmXkyJE1UgeNRtO4EE6aK+Y0+vXrJ4s/ZOfAgQN07ty53OUcDhsZGbuwWFrh4dHUmVVssFTmOGo0moaJEGKHlLJfReW0+0ij0Wg0BbiMKIAx/EiLgkaj0ZSFy4iCmtFsoqG5yzQajaY2cRlRUAi0paDRaDRl41KioOIKWhQ0Go2mLFxKFJT7SIuCRqPRlIVLiYKKK9SPmIKPj0+VftdoNJrawKVEQVsKGo1GUz4uJwrOiCnMmDGDd999t+C78SCc9PR0hg0bRp8+fejevTsrVqyo9DqllDzxxBN069aN7t278/nnnwMQFxfHVVddRa9evejWrRu//vordrudu+66q6Ds22+/XeP7qNFoXIPGl+Zi+nTYVTJ1NoDVkQVSgptX1dbZqxe8U3bq7EmTJjF9+nQefPBBAL744gtWr16N1Wrl66+/xs/Pj4SEBAYMGMCYMWMq9Tzkr776il27drF7924SEhKIjIzkqquu4n//+x/XXXcdzzzzDHa7nczMTHbt2kVsbCx79+4FqNKT3DQajaYojU8U6oDevXtz/vx5zpw5Q3x8PIGBgbRq1Yq8vDyefvpp1q9fj8lkIjY2lnPnztGsWbMK17lhwwZuu+023NzcaNq0KUOGDGHbtm1ERkZyzz33kJeXx80330yvXr1o27Ytx44d46GHHmL06NGMGDGiFvZao9E0RhqfKJTTo8/NOorDkYW3d7ca3+yECRNYunQpZ8+eZdKkSQAsWbKE+Ph4duzYgdlsJjw8vNSU2VXhqquuYv369Xz33XfcddddPPbYY9xxxx3s3r2b1atXs2DBAr744gsWL15cE7ul0WhcDJeLKTgr0Dxp0iQ+++wzli5dyoQJ6rHTKSkphIaGYjabWbt2LSdOnKj0+q688ko+//xz7HY78fHxrF+/nv79+3PixAmaNm3KtGnTuPfee9m5cycJCQk4HA7Gjx/Pyy+/zM6dO52yjxqNpvHT+CyFclCT15wzJLVr166kpaXRokULwsLCAJg8eTI33ngj3bt3p1+/flV6qM3YsWPZtGkTPXv2RAjB7NmzadasGR9++CGvv/46ZrMZHx8fPvroI2JjY7n77rtxOJTgvfrqq07ZR41G0/hxmdTZANnZJ8nLS8TXt7ezqteg0amzNZrGi06dXSo6zYVGo9GUh0uJguE+amjWkUaj0dQWLiUKhc9U0KKg0Wg0peE0URBCtBJCrBVC7BdC7BNCPFJKmaFCiBQhxK7810xn1UdtTz99TaPRaMrDmaOPbMDjUsqdQghfYIcQ4icp5f5i5X6VUt7gxHoUwdBALQoajUZTGk6zFKSUcVLKnfmf04ADQAtnba8yFKaX0KKg0Wg0pVErMQUhRDjQG9hSyt9XCCF2CyG+F0J0LWP5+4QQ24UQ2+Pj46tRE8N9VLMxheTkZObPn39Jy44aNUrnKtJoNPUGp4uCEMIHWAZMl1KmFvt7J9BGStkTmAssL20dUsqFUsp+Usp+ISEh1aiNc9xH5YmCzWYrd9lVq1YREBBQo/XRaDSaS8WpoiCEMKMEYYmU8qvi/0spU6WU6fmfVwFmIUQT59XHOYHmGTNmcPToUXr16sUTTzxBdHQ0V155JWPGjKFLly4A3HzzzfTt25euXbuycOHCgmXDw8NJSEjg+PHjdO7cmWnTptG1a1dGjBhBVlZWiW198803XH755fTu3Zvhw4dz7tw5ANLT07n77rvp3r07PXr0YNmyZQD88MMP9OnTh549ezJs2LAa3W+NRtP4cFqgWSgH/vvAASnlW2WUaQack1JKIUR/lEglVme75WTORkovHI6OmEyeVCJ7dQEVZM5m1qxZ7N27l135G46Ojmbnzp3s3buXiIgIABYvXkxQUBBZWVlERkYyfvx4goODL1rP4cOH+fTTT1m0aBETJ05k2bJlTJky5aIygwcPZvPmzQgh+L//+z9mz57Nm2++yUsvvYS/vz979uwBICkpifj4eKZNm8b69euJiIjgwoULld9pjUbjkjhz9NEg4HZgjxDCaKafBloDSCkXALcAfxNC2IAs4Fbp1JllVVCCatK/f/8CQQCYM2cOX3/9NQCnTp3i8OHDJUQhIiKCXr16AdC3b1+OHz9eYr2nT59m0qRJxMXFkZubW7CNNWvW8NlnnxWUCwwM5JtvvuGqq64qKBMUFFSj+6jRaBofThMFKeUGKmiFpZTzgHk1ud3yevR2ey6ZmYewWttiNju3gfT29i74HB0dzZo1a9i0aRNeXl4MHTq01BTaFoul4LObm1up7qOHHnqIxx57jDFjxhAdHU1UVJRT6q/RaFwTl5rR7Kwhqb6+vqSlpZX5f0pKCoGBgXh5eXHw4EE2b958ydtKSUmhRQs1svfDDz8s+P3aa6+96JGgSUlJDBgwgPXr1xMTEwOg3UcajaZCXEoUnDUkNTg4mEGDBtGtWzeeeOKJEv9ff/312Gw2OnfuzIwZMxgwYMAlbysqKooJEybQt29fmjQpjMk/++yzJCUl0a1bN3r27MnatWsJCQlh4cKFjBs3jp49exY8/Eej0WjKwqVSZzscNjIydmGxtMLDo6mzqthg0amzNZrGi06dXQqG+0jnPtJoNJrScSlRKNzdhmUdaTQaTW3hUqKgLAWhLQWNRqMpA5cSBYV++pqmjti/H9auretaaDTl4szJa/US4+lrGk2t8+qrsHUrHDpU1zXRaMrEBS0F7T7S1BE5Oeql0dRjXE4UlKVQ96Lg4+NT11XQ1DY2m3ppNPUYlxMFMGlLQVM32GyQl1fXtdBoysUFRUFQ0zGFGTNmXJRiIioqijfeeIP09HSGDRtGnz596N69OytWrKhwXWWl2C4tBXZZ6bI19RS7XVsKmnpPows0T/9hOrvOlpE7G3A4MgEwmbwqvc5ezXrxzvVlZ9qbNGkS06dP58EHHwTgiy++YPXq1VitVr7++mv8/PxISEhgwIABjBkzpkgOppKUlmLb4XCUmgK7tHTZmnqMdh9pGgCNThQqRtR47qPevXtz/vx5zpw5Q3x8PIGBgbRq1Yq8vDyefvpp1q9fj8lkIjY2lnPnztGsWbMy11Vaiu34+PhSU2CXli5bU4/RoqBpADQ6USivRw+QlXUEhyMHb+9SHwd9yUyYMIGlS5dy9uzZgsRzS5YsIT4+nh07dmA2mwkPDy81ZbZBZVNsaxoodruOKWjqPS4YU3BOoHnSpEl89tlnLF26lAkTJgAqzXVoaChms5m1a9dy4sSJctdRVortslJgl5YuW1OP0ZaCpgHgkqLgjCGpXbt2JS0tjRYtWhAWFgbA5MmT2b59O927d+ejjz6iU6dO5a6jrBTbZaXALi1dtqYeY7OBlODQo9809ReXSp0NkJ19kry8RHx9ezujeg0anTrbyURGwvbtkJ0NRZ6yp9HUBjp1dpnU/JBUjaZSGK4j7ULS1GNcThSMGc0NzULSNAK0KGgaAI1GFCrfyOtnKpSGFslaQIuCpgHQKETBarWSmJhYqYZNWQr66WtFkVKSmJiI1Wqt66o0bux29a6HpWrqMY1inkLLli05ffo08fHxFZa129PJy0vEYtmPEOZaqF3DwGq10rJly9rZ2EsvwS23gKsFtbWloGkANApRMJvNBbN9KyIh4Vv27r2RPn224OfXw8k105QgMxNmzgSTCZ55pq5rU7toUdA0ABqF+6gqmM0qRURe3oU6romL4soNo3YfaRoAThMFIUQrIcRaIcR+IcQ+IcQjpZQRQog5QogjQog/hBB9nFUfA7M5GACbLdHZm9KUhiuLgivvu6bB4ExLwQY8LqXsAgwAHhRCdClWZiTQPv91H/CeE+sDgLu7EoW87Hg1iUhTuxi9ZFdsGLUoaBoAThMFKWWclHJn/uc04ADQolixm4CPpGIzECCECHNWnQDc3QMA8FrwHfTQMYVax2gQXdGFYriPtCho6jG1ElMQQoQDvYEtxf5qAZwq8v00JYWjRjGZ3HF3D8D94CnITzCnqUW0peCagqhpMDhdFIQQPsAyYLqUMvUS13GfEGK7EGJ7ZYadVoS7ezAiMVU/HrEucGUXiivvu6bB4FRREGoiwDJgiZTyq1KKxAKtinxvmf/bRUgpF0op+0kp+4WEhFS7XmZzEG6JGepLVla116epAq7cMLqC+2j8ePjoo7quhaYaOHP0kQDeBw5IKd8qo9hK4I78UUgDgBQpZZyz6mRgNgfjlpQfZNaiULsYlpmrWWhSuoYg/vgjbCnuJdY0JJw5eW0QcDuwRwhhPDT5aaA1gJRyAbAKGAUcATKBu51YnwLc3YNwT8pvlDIza2OTGgNXaBhLo+gzFBqzIObkNO79cwGcJgpSyg2oPNXllZHAg86qw0UsXw5Tp8K2bXjk+eGWnZ8nSVsKtYurBpoN1xE03n2XUp1fLQoNmkaR5qJS+PjAhQtw6hRWtyIPONGiULu4qqVQdH8b6767qmuwkeE6aS5a5cezT53CnFxEC7X7qHZx1WGZRYWgse57To56b6yi5yK4jqVQRBQ8RBEt1JZC7aLdR41333Nz1XtjFT0XwXUsBS8vCArKtxSKBP20KNQu2n3UePfdsBS0KDRoXEcUQFkLp04VjjwC7T6qbbQoNN59NyyFxrp/LoJLikLBHAXQlkJt46rByKLuo8a679p91ChwLVFo2VKJQmIGDuOha1oUahdtKTTefdfuo0aBa4lCq1Zw4QLiVBzZofm/afdR7eKqgWZXEAVtKTQKXE8UAPH77+Q2yzcVtKVQu7jqkFRXGH2kh6Q2ClxSFEhKwhZsxeEutCjUNtp91HgFUVsKjQLXFAXAEeyDtLpp91Fto91HjXfftSg0ClxLFFq2LPjoCPbDbkFbCrWNq1oKruQ+0qLQoHEtUbBYIFRFmGVIEA6L1KJQ27jqkFRXch81VtFzEVxLFKDQhRQSgt3iQGr3Ue3iqpaCK7iPtKXQKHA9Uch3IZmatsHuIZGZl/SEUM2lokWh8e67jik0ClxPFPItBfewy3BYwJGRUscVcjFcNdBcWzGFn3+GtDTnrb88tCg0ClxPFNq3B7MZ95bdcFhAalGoXVx1nkJtxBRSUuDaa+Hjj52z/orQ8xQaBa4nCvfdB9u2YQnpmD/6KKOua+RaaPeR8/Y9M1M9/UxbCppq4DrPUzCwWqFnTzwcNhweQKYefVSraPeR8/bdaJSNHnttowPNjQLXsxTyMZnc1TMWsrIrLqypObSl0HhFQVsKjQKXFQUA4e2LyM6t62q4FkXnKUhZt3WpTWojpmCst65FQUpwOMovq6m3uLQomLz8MeXYKy6oqTmKNo6u1HC4kvsItLXQgKmUKAghHhFC+AnF+0KInUKIEc6unLMx+QRiypFIeyMRho0b4cMP67oW5eMK4/VLw9hXi8X5opBbR9Zv0e1qUWiwVNZSuEdKmQqMAAKB24FZTqtVLWHyDgbAnnG+jmtSQyxYADNm1HUtyqdoY+GKomC1Oq/B1JaCpgaorCiI/PdRwMdSyn1FfmuwuPmoPEjZSUfruCY1RGYmpKfXdS3KxxVyAJWGYY3WhqVQ1zEFcC3Bb2RUVhR2CCF+RInCaiGEL1CuQ1gIsVgIcV4IsbeM/4cKIVKEELvyXzOrVvXq4+bXFIC8lJja3rRzyMyEjIz6HcDVloJriIIrCX4jo7LzFKYCvYBjUspMIUQQcHcFy/wXmAd8VE6ZX6WUN1SyDjWOu08YALkpJ+qqCjVLVpYShKwsNdy2PuLqMQVPz8YrCtp91CiorKVwBXBISpkshJgCPAuUmx9CSrkeuFDN+jkVd18lCra0U3VckxrCyPhan11IrioKRd1HjTWmoC2FRkFlReE9IFMI0RN4HDhK+RZAZblCCLFbCPG9EKJrDayvSph8/ADISzlT25t2DsazIeqzKBRtLFyp4ahN91FdjT4qKkauJPiNjMqKgk1KKYGbgHlSyncB32pueyfQRkrZE5gLLC+roBDiPiHEdiHE9vj4+GputgiengDY0+Nqbp11ibYU6i86pqBpIFRWFNKEEP9EDUX9TghhAszV2bCUMlVKmZ7/eRVgFkI0KaPsQillPyllv5CQkOps9mLy/e62tLOVK5+VBYsW1d9ArmEpZNTjJH+uKgqG+8jTU7uPNPWayorCJCAHNV/hLNASeL06GxZCNBNCiPzP/fPrkliddVaZfEvBlhaHw1GJi/j771WW1d27nVyxS6QhWAqu7j5qzENSc3LAlN+kuNK5bWRUShTyhWAJ4C+EuAHIllKWG1MQQnwKbAI6CiFOCyGmCiHuF0Lcn1/kFmCvEGI3MAe4Nd9FVXvki4Ip20FW1pGKyxspiesqNXFFNARRcFVLwRVEITcXvL3VZ1c6t42MSg1JFUJMRFkG0ahJa3OFEE9IKZeWtYyU8rby1imlnIcaslp35LuPTDmQmXkAb+/O5Zc3Gt36+Fxnu72wUajPouCq8xTsdtWL9vBovKKQk6NEIS2t7iyFPXuU8HboUDfbbwRUdp7CM0CklPI8gBAiBFgDlCkKDYJ8S8EtXxQqxBCD+uizzyryXIj6LAo2mwq2Zme7lijYbODmBmZz444pGJZCXYnCffdBaCisWFE3228EVDamYDIEIZ/EKixbf8kXBQ+7PxkZ+ysuX58thaKiUB9Fy8AQBaiffmeHA3btqvn12mzg7q5ezhJD43jWZUK8uhaF1NT6695tIFS2Yf9BCLFaCHGXEOIu4DtglfOqVUt4eIDJhMXRpOFbCkWFqj5bCnl5BWJcLy2F1auhd284dqxm12u3O18U6tpSMNxHUHfnNien7va/kVAp95GU8gkhxHhgUP5PC6WUXzuvWrWEEODpiYfdn8zMA0jpQI22LYOGYinUZ1EoainUR1Ew5sFcuABt29bceg33UW2IQm6uGjYtajlnZW4u+Pioz3VlKWhRqDaVfkazlHIZsMyJdakbPD3xsPnicGSRnX0CT8+IsstqS6H65OUVNhz1URQMcc2u4ce0Gu6j2ogpGJ8tFudspyyKWgpaFBos5bqPhBBpQojUUl5pQojU2qqkU/HywmxTF3KFLiRDDLSlcOnYbIXuo/oYUzDEoOjxrAlq030EVWsYV66EzZurt20p1fnUotDgKVcUpJS+Ukq/Ul6+Ukq/2qqkU/H0xD3XA6DiYLO2FKpPfXcfOdNSqC33EVStYXzqKXjjjept2xCB+hBTqOlz52I0/BFE1cXTE1OOHbM5tGJLoSHEFCwW54jWs8/CTz9Vfz31PdBsNCjOdh85Y55mcfdRZcnKqr5lZIhQXcYUpNSWQg2gRcHLC7Ky8PbuRkbGH+WXbQiWQmiocyyFd96BL7+s/nqKuo/qoyg4O6bgnh/Gc5T7jKpL41Ithezs6ouCse26dB8Z29SiUC20KHh6QmYmvr79SE/fjcNRzgXVECyFkBDniEJ2NqSU+wiNylHf5yk4y1IoGlMA5whiXYqCsT3j4U51cW6NOjQWUfjyS1ha+/ODtSh4ekJWFr6+kUiZR3p6OdaCq1oKeXmqUUtOrpl1uaqlYMQUjO81TXVEobr7a2y7LkeWFRWF+prJuCq8/bay0GsZLQp+fnDhAn5+kQCkpW0ru2xxS+Hrr2HiRCdXsJI401IwGozqWgpS1n/3UW3EFMA5PelLEQXDD98Y3EfGPhsjoRo66ek1PwquEmhR6NIFTpzAkhOA2RxSOVEwLIWff1YmXh2cuBIYdWvSpOYtGaOBrK6lYDxTwBVHH9WW+8iYsFZZUTAa8+rur7G9+iAKxT83VDIytCjUCT17AiD27sXXN5LU1HJEofg8BaORPFvJh/Q4k6ws1QsNCFCfjQa4ptYN1ReFog+vh/rZm3OmpVAb7iPDfVPZRrGm5mUY4uLpqYRJi0L10ZZCHZEvCuzeja9vJJmZB7DZSnG/OByFN5AhDoY7Ja4ePM4zM1MF+YxGoSathZpyHxV9JGXR7/WJ2hp95CxR8PUt/FwZakoEjUbYw0N1TuoyplD8c0NFWwp1RMuWEBgIu3fnxxUcpKfvLFmu6MkpbinUB1HIylK9NEMUajKuULThqM7NZvQe63NMwdnuI2fHFKpqKRjlaspSMERBWwrVw+HQolBnCKGshXxLAcoINhtC4ONTvy0Fw6frDFGA6lkLxd1H9VEUGrL7KC+v0FKoqvvIGGF2qRiiYLGofaxrUWjos5oNMdCiUEf07Al79uDhFozF0obU1FLywBiiEBKiLvi8vPptKdSk+6johVmduILRUHh41J3fuSIai/uoqqJQ/HNVKe4+qmtRqMz+x8XBBx84rz7VwejUVVesLwEtCqBEITMTjh4lIGAoycnRSFlsxmnR0T3G9/poKTjTfQTVEwWjITSbnZsDqDrURkI8cJ77qK5Eoail0FBiCp98AvfcAwkJzqvTpVL0/q1la0GLAlwUbA4MvJq8vAQyMvZeXKaopQDq6U6p+Yli64soODumAJVzHx0+rB5WUxyjoTB86/VRFJw9ec2IKdQXS6Foueo0Pg3RUjDukXPnnFOf6lDU0teiUAd06aJu2N27CQi4GoDk5LUXlykuCkUvpPogCllZ9cdSePNNmDKl5O9GQ2H0mOuj+8jZk9fq6+ij4p8vZdtQf2IKlREF454+f778cnWBthTqGKsVOnWC3buxWltjtV5GUtIvF5cpLgpnzqh3D4/6IQrOtBSKXpSVsRRSU5V4FE81UNvuI4ej6qNQGvrkteq4j6rT+NSH0UdVFbj6LAraUqgHdOkCBw8CEBh4NcnJ65CySIDHOEnFRaFDB/UIR5utToJCBRiWgjH6yBnzFKBylkJGhjoexRum2nYfvf56oWuwMhgpH8D5lkJNN5pGagcjIV1dB5obQkzBEIX66D7SlkI9oEMHiImB3FwCAq7Bbk8hPX1X4f/FA82GKHTurG7Ic+fglltKd5vUBrUVU6isKEBhzMXAaAhry1L48084dKj2G8jScHZMoejILoul9mMKxQPN2n1UPbSlUA/o2FH18o8dK4grXEhcDf/+t2rwy3Ie9MHFAAAgAElEQVQfdeqk3o8dU8HVQ4dqueL5GJaCxaIaH2eIgqdn5dxHxraLi0JRS8HwOzscsGJF4fMF/vwTZs2qmXobda1sT9CZouBs91FR901VRMEZlkJDiyloS+EinCYKQojFQojzQoi9ZfwvhBBzhBBHhBB/CCH6OKsulaJDB/X+559YLM3w87uC1A0LYfp0+PTT8i0FUBlTc3IgMbF2621gWApCKGvBGTGFpk2rZimkpV38e/FAs80GGzfCzTfDunXqv08+gX/+s+Syl4IhCpXNTVX05mtogeaaEIWGHlOoqigY16m2FC7CmZbCf4Hry/l/JNA+/3Uf8J4T61Ixhijk9/TDwu7D/McJ9dvZsxVbCp9/rt4vXKiFyhbDiGcY/uSaFoXsbNXQBARUzlIoy31UNNBs+J2TktRvsbHq3bhBL0UUsrMvfiiJsf3KioLRQPr5OX9Gc003mnVtKeTmqnMqRP2IKVQl0KwthYtwmihIKdcD5bWQNwEfScVmIEAIEeas+lRIYKBq8P/8E4DQ0In4HfZQ/xmi4OamGkYoFIWOHS/+np5etefj1gTGRWOkj/D2rnlR8PRU+16dmEJx95HNVlhPQwyM90up/4oVMGECHMh/1nZVJxcaxzEw0Pm5j5xtKVT2GqzJeQoe+feLMy2Fo0dh/PjS65qTo+5R43NF6JhCqdRlTKEFcKrI99P5v9UdHTsWWApubl4ExgQC4DhzsmRuofPnVUPp5VXoUvLzU+9G77e2MC6aopZCTae5sFrB379mAs1FYwrFTfjqiILhuouPV+9VdR8ZQhAQoD6X9vSu5ORLy//UENxH1bUULBb12Zkxheho+OorJQ7Fycmp2pDchmIp1PLjf91rdWuXiBDiPpSLidatWztvQx06wHffqc82G9ZDqgG0nT6ER9sOqtG1WpWJLGWh1RAWpqbK33gjLFmiXEhNmzqvnsUxLhrDUvDxqRmfvEF2ttrvyriPpCysT/E6lDZPwRAF48asjvvIECFDlKvqPipqKRhDPI3er8Hkyeo4V/XZubXpPvLwKNEoFu3IX7igqhMaCqKIEMjMLJKT4PRpVTYgoLC6QqhLwOh3nD6twkEJCeryMO8YSJgjix5/Qls3C+42GzExsHatOt2BgRAUpHQjJqbwlJhMat12uzr8vr4QHq5O4eHDaptNmqjL4tQpSN0yEPiIQR97EzlRLXPmDOzfDxfW3owltzOBpjiab+tK+jw1/uPYMbWsr6+6LY3xGJa4GUAGCZlNcEyw0baDO9nZsGmTKnPzzTBqFEREwK+/wrx5sG+f2vegIGjeXL1CQwsP+cmT6niEhqr9Tku7+JWZCV27QmQkHD+uRsHb7ao+HTuqPufWrXBi9ys4xPNICY7HfZHPqEvykUdg5syavXSKU5eiEAu0KvK9Zf5vJZBSLgQWAvTr1895D1/t2BEWL1YN34kTiOwcbL4mxNl41Xh5e6sr2MtLfff3V8uFhamrZcyYQlGoTYpbCr6+Nfvgn6q4j7KyCnvY5VkKht+5Ji2FoqKQl1coTlUVhXyxz0vLxhxcTBROngRvb+x2dZOnp6tD4+0Ne/fCjh3qNyGgWzdo317p3emcGzm162pyFgXTjDtp9ntTwnqoS+3PP9Xr6FHVOAQFQXCwMjxTU9XlVPTlcKjGKCMD9uzJ/x4YgT/rsL7aEeuJ5ljjJNY71GHYvFk1QIYWG7vp6wu+jmdIJopcPOAJE7bHyz48JhMMHqy8rMuXF5+Sc5t6dQQTy2jilsz5tpU77JUlMBACZDOyGcYns5vD7ML/hAB/8wBy8gaSJT1hFbBKnZu2baFVK3Vedu1S+5+TAzmZE5BAExJgq2TpcqXb/fopoXnoIfUKDlZGaEgIDBoEw4apS+zMGXXOz59Xx9lshtatlZBt2aIub19fdR79/VWWfg8P2LlT9T1DQ9U1YrGouq1cqa6pyEgYFboDt6R4RGoKpm79EX37YDJBr141e0xLoy5FYSXwdyHEZ8DlQIqUsm6nBhcZgcS+fQDkDOmK13d7sCedxc1odL291R1pWAojR0KzZurqg9oXheKWgq9vQWykRjAsBX9/ddU6HKqFKI2ibqvyAs3FLYXz59WdalgiFYhCUpJqbFNS1I1ssYDHcQsm2pG824ML3llc4FZysOC3rxnx/1GNY1YWtGihdscYLHb2rHolnRlEBL/QdLc3W3iL4038aNZM3bzZ2WrZrNhoMqQXWVW+c5ZANOrFf+Ft1Csfi0X1SPPyVJ0M7RVCXWZBQYUvUL1tqxVuuEE1NGf2ZZN22EZ6jgcJtqZk5VrI/lXpc9++Ku9bZqZaf8uW6vQdOQIZq/cScHovlswk5IBBNBl3FS1bqlOTnKxOtaHx587BN9+ohnD6dGU0hYWpy852z32c2Hae3S8s59gbK4g9LekeNZ6RI9XpNgQtK0tZAs2bFxrcDoc6h1aznZRUwYlTJvz8lKDm5SlvYEhIvmfo8ZeRb73F4flr2NdsGD4+6r+OHcHz7qnw++9kJmYRd91deL/5Ik2bFj6htAQeIWrS6u7d8PkmbP0GIGVh2OfPP2HNGnXdREbC1KmF/a7qkpGh1lW8bgW31nWvqxOwbRuMeBZerL3BmU4TBSHEp8BQoIkQ4jTwPGAGkFIuQGn5KOAIkAnc7ay6VBojaHzokOry+fhgHjYO8e0e7Mf24eabb9gYV4ZhKUyfrt6PHVPvtT0stbil4OdXs+4jI6YQEKDu4tTUQkEsTimiYLern9POm0mnA2kHvElP60tajgfpOzuSwVQ4GoTXwnQupy0mHLy+oDubX1O9Qzc3JQImk9psTIxyLZTkefV6B/XiU/XzYeB+1Xj4+6semeEZCg5WLoWWLaGrTzxHz1vZmNCeSH7h9ocDiU0PICFBNXyenuD56XK8LXZ8H78PHx/lqcvKUrvasSNcfrlquHNzVa80JkY1nK1u7EWrR2/B8vdpnG17BWcff4O4K8bh46OWa9WqMEZqHLP0dLX+or+XyW/7YfAwmPOjmsmdlqb8IBVxx0LI/VW1+AMfhMevKrf4K6+U9U8iwf5H6XM38PNXkL0JHh1f8O9ll1ViH0bdiG/LlrRcuLDgJ6u1MEwAQGoqAugQcJ4OY4stn5MDFgte1gwus8ZCs3K2ZaS/j4hQonDuXIGrzKBDB/V64IFK1L2KGKHJ4hT0tTIy1Mm3Wms90Ow0UZBS3lbB/xJ40FnbvyTatlVnZfVq1R3q3RuPiN4AuB0/B5fni4ZxRos3jEY3rq4tBcPvUE2kzI+rJgeQ7daC7JQIsrmc7B+yyfBVLonjx9XmpVQ9as57sYVVHKY9aYtCSH+/aJxsnHqNh4Ju8gGAOyAJeBhABRA9frUx5Bq1rM2m/MpSqkPbpYvq+bZpo3bV4VCNcM6/3sS+cxcBowcTfMvVBN09BkubMFLPpOOzfyttLxNl9xoBPloHG+6EJ1+C556DBw5CxyLnWEr4+K/gGwgz76vw+A0bVmQ5uRt8bwY/d9oSQ9vWp/OPQ+m4uRX2OSpF8UBzZdNBG1agp2elAs1SSnLtuVjcLSW3X3T00aUE0rdsUe658jCu69IsyXxRqFSg3Whow8PVe30bgZSertoTT8/GIwoNEosF+vdXE6gAHn1UuYUAtywHdgu4QUlLwcDfX93NdR1T8PMrbE2LdX/sdmXIHD+u7r+kJOWCSUlR98Xhw8pXmpGhyqlVf6kW3gJws3If52O1FmpkYiIIEUIXWhHJNvzDgvAZdx2+vqrT47v7V3w+fg/fxXPw+c+b+KbG4hMRgveqLxBILsx+nw1PriCBJkx9NIgWr0+v2nGY+w2wDgIkRHQCDkG3dnAiGpqkgCjDuil+HAPVqLMSjWRWVkEXfm3MWpr7Nqdjk44VVutCejzxwdCxjoekOqSDpKwkgr2CAcix5UBOBhartdI90tm/zeaVX1/hiwlfcH27ItOQKjEkVUpJZl4m3h6ldJON4ElmZvnuyXxRiEk7ySfrXmJ42+F0bNKRbbHbaOmWSNd8UThtTyIwN6P0bUFhT6UUUfh0z6ccTDjIwFYDGdR6ED4ePsQkxTBv6zzGdR7HoNaD1C7bcjiQcIC4tDha+7fmsqDLsLpbyzx2RY/DxlMbaRfUjqY+TQt+W310NWfSznBP73sKLQVPT87lXMA7Nx0fD58K110TaFEozvr1Kp6wdy+MGHHRjZLJSXyhbEtBCNWg1LIoyIxMkgkgLcmXs1vh0MFIMrmP4E+yOXrOh/XrlQjEx6uG2+EouQ4hlCulXTvo0UNdj0FByv8e+OazWJsHYb1hOJbnn8L69iw8B/SkTRulmUbvOy8Pcn9aj/fooeqHy4bDG9cVbmTBPvj4Uxj5Fizfz2lxhBZubRGcBqBl6gZ6GHMYTU9W/UAUDTQbsYmOHVVULy6O948t43TqaR6+/GECPQNLLl90SGrR78XWb8/JZuznY+kT1odf7izMpmt32Fl5aCWDWw8mxFtNcvz1xK9MWjqRzGmQ5OaGqOSQ1M2nN/P4j4+zZNwSwgPCC37/4cgPbDq1iccHPo6fRQ2BdkgHO5L20dECfmWMPrI77Iz7Yhzrjq/j6MNHCfYKZtwX43BEbOH78+0rZSlk5GYwe+NsMvIyuOF/N/DBTR9we8/b1Z9FhqRKszv/6pHC2Pj9dAnpAsChhEM89P1D/HryV3b9dVdJMY2JUe/Z2Wp4T1mjDPPPwVvpPzEvegszowuH4rTrYuXw9oFIiweXt/2ZLp/fzI9TfgTg55ifubzF5fhafEnKSmLtwa8ZC4jAQNWZyx/9tu74OqZ8PQVH/kO2rO5WBrcezPoT68m15/L25re5veftxKbGsu7EOmyOwvPYwrcFv93zG20C2vB/O/+PAGsAt6w9p3xQ115bUO6tTW/xj5/+AUD7oPa0CWjDufRz7Dm/BwA34cadhu/Q05MhId8S8eUEvp/8fbnnp6bQolAcs1mF+I0wf5GbK0PG4JZ5CK+yLAVQLWkNi4KUaj7WqVPKK3Au3saexO14xF/O6VOCbb/eTDy3wXBjievVKz9K07kzdOhkQ4x+lCutVzE0ZAKWZsf4Q37K3T2n0j6sGT4+ZXfOePczaH85jB4Cz/8Abf8GAwqzj6Zkp/C/Pf9jQtcJNLGnFh6HchLinfbMJXzEfv53UDDR+H+PuimwWpHpaeTYskv0vHLtuew9v5c/zv1BC98WDGg5AF9LvtO5qCgYn/PjRFmxJ5i+bTrpuem8vflt/jHwHzxy+SOFy8JFlkKiJyw69D6Hzy4kx55DZPNIxll60QrYGwopOSmsO7GO8xnnCfUOBeD939/nr9/+FU93T8Z2HktcWhzrT6xHCIHNCglu2YTki0JabjojFw8mPCCc98e8j8XdwqGEQ7QNbItEMnXlVPbH7+f1317n3dHvAqpRvmv5XZzLOMeinYuY3H0ymXmZfH/ke2KSY3jiKphdbJ5Cak4qZpOZZ355hpWHVgLw0e6PGNl+JKsOr8Lb14Td4oGb1crLvr+zYlEkBxMOMmvYLB7sf7F394NdH3Ah6wI/TP6BWb/N4r5v72NU+1HK8sjNLUjGeNacy7NXZPL113ew5d4tRB+PZuSSkXiZvZBS8s7md3jvhvf4fO/nRB+PZs7IOZgNUQD4808uhPhw78p7ubvX3dzY8Ub+TPyTR354hEW2C7QE1tqPclXEVfy17185lXKKo0lHWbRzEfHekAyc8cjhzLE1fLb3M2KSY3jml2foHtqd90a/x7RvpnEg4QA/XAbXeXmpoNL588RnxPOXr/5Cu6B2RN8Zzd7ze1l5aCWrj67m1m638syVzzBv6zzmbZ1HpyadeHTAo/Rr3o8Wvi04mnSUh79/mBs/vZFR7Ufx2m+vEWANYOQcM95XXMV7AUfwtfjib/HnyTVPMqbjGAa1GsSW2C3EpcXh7eHN4jGL+eiPj3hg1QP0t0Bnb2/OBbhzyJLGoSM/8NvJ3wqsFGeiRaEiLBbV+09KQnq6cezY03Qry1KAaomC3a7GLW/dCtu3q7ZNStjwm4PTYfPh6AhI7ADDn4XBr+G3+0vCM2/hhq4xdNv8f/i//QJNInzpeHw1PtOnsmDhfbTr3oO7BtzMsv0ruOXLeexjHhf8hrLp0CZy7Dl8dvxt3hv9HhO6TihRn42nNvLAdw8wqts5/mUMSYUSw1Lf2vQWL65/kSfXPMlzfjfyJKjoaloaUkr+veXfLDuwjINJv/N8f/i7uzt/emZhN8Fan3gmhoYq833PHtVjDQ3lefcNvPSKJ2E+YYxqP4o5I+dwJu0M1358LceTj1+0fau7lQBrAGtFNp2gpKUAfHfsB9Jz05lz/Rx+jvmZ59Y+xzub32HuyLnc1v02krOT+Tp7E1NMYPb3593+8HzMIpr5NMMkTCzZs4R/e7XgKPBbfifWIR18deAr7u93P5l5mURFR9GveT96hPZg+aHltAtqxyOXP0I337bc89PfiTVlEOLujl3AbfbP2HT6KL+d+o1zGeewuFn47vB39GvejytaXsH++P30bNqTxbsW8/zQ5wn1DmXe1nmcyzjHu6Pe5cPdHzJn6xy8zF70DeuLLT2V3U0TVafGYiEvL5sZqx/nrc1vFRynh/s/zJbYLfxnx38KjmGGu4PDAXaa5pp5rtkuutu6qwZv9aNc0eoK+jTrDYcOYevQjjc3vcnAVgO5rt11NPdtTo8FPVj8+2KeGPSEEqFg5ZaK8VCDDXbE7eC1315j7ta5qqG9K5qnf36aD3d/yNQ+U7ln5T1k5ik3zvxj7THCPdl/7uemUy+w4eQG1hxbw8apG5n81WT+OPcHHzYNYJo37DMl8K/Lrucv3f8CKIts0c5FbAnMJMlTCWIrv1bc/939pOakMrztcLbGbmXwB4Pxs/jh4+bFZ90ylSiEhuI4d5bbv76dxMxEVv1lFWG+YYT5hnHtZYU9fIA5I+fw+rWvl4ipDGo9iGY+zRi1ZBR7zu9hWMQwfo75mSUtTLTJO8ADq5YVlO3cpDOfjP3k4g5JPte1u46eC3pyz/AENvn4sK2ZGvfrbnLn+ejnWXPHmhLL1DQ6S2plCFPZN7yaRJKQ8BU5bvmNYiUtBVnazFhUZ/bbb+Gpp+Dqq1Wb262bCqJ+tDSRLTuz2LoVWl+1FkY9RPDjQ5n9wyeYrnwdgM5T32DXLsnicd/xGG8z9V7BTTepdExNTbG8c24Wj667m8TMROZtm0cb/za8OPRFfjv5G2M7j2XtnWtpG9iWiUsnsmx/4UWblZfFP378B4MXD2b3ud2sbF1k9BFAcjLbYrdhd9iRUvLF/i/oE9aHQa0G8VTCp5zwV8fMkZrC31f9nUdXP0q2LRu7dPBLBODuzon8G3erb6oaAQLKhRAaivT1YYn3MbqFduPqiKtZ/Ptihvx3CEP+O4S0nDQ+GfsJBx88yOopq3lx6Is80O8Bzqaf5esWasRVQmYCXyVtVOvMF4XP4tfS1LspD0Q+wPJbl7Pl3i10bNKRv3z1F6aumEr397pzj1jJF73cwcuLzS2hm7U1cY/HEftYLAtvWEhMZizbm8OG1tDcM5ROTTrxxb4vAJizZQ5x6XG8NeIt3r/pfRKfTGTLvVt487o36ezfDoBYkQ4mE1FXw3cc5t1R7/L+mPf5JeYXNpzcwKMDHuXohaPM3TqXsZ3G8vktn5Njy2HulrkkZyfz2m+vMbr9aB6IfIAt924h59kckp5KYs0da7jK0oGDTQAPD3Isbgy/4QJvbX6Le3rdw6vDXmX+qPm8dd1b3N/vfg4lHmL+9vl0D+0OwO+B2WwOVTGId65/hx8m/0Codyi3LbuNxJ+/QXbuzMz/TeN48nGeHKjcet2bdmdImyHM3z4f+94/lCmbP2EzxqxEISIggmd+eYakrCQ+v+VzQr1DeXTAo2TZsrj6w6sxCRNTe09lwY4FzI9bqSwNLy/ujn2XDSc38OaIN3E3uRO5KJI/zv1BmE8YX7ROY10bdWqvjri64Jrt27wvbg7Y7JfKpqB0fG0mvpr0Fem56QxuPZhvb/uWjfds5LZut7H+rvWMb3IlX3WGHKs7hIYyO+gAq4+u5p3r36Fns/KfwVEiyJ7PiMtGsGTcEl6++mV+vP1Heof2ZG4/B/+IOELbwLZE3xnNjEEz+Oa2b0oVBIDmvs35R+QjbG4FZ7zsbG2Sg0lC1JAofo75mXXH15VbtxpBStmgXn379pW1zjXXSAnS/vRTcvPmjjJunLeUIOU335QsO2WKlOHhUkopc225cvaG2dL/VX/53qYP5LJlUr77rpT//KeUV1whpZubWo3ZLGVktwz54I3HZdTC7XLowhuk6QWTHLVklJRSyklfTpL+r/rL4NeCJVHI8HfC5au/viqJQm44sUHKqCi1IptN1WHjRrmpJZIo9Rq1ZJQkCvnahtcK6mWQnZct+y/qL/1e9ZMH4w/KlQdXyo5zO0qikPd/c7985PtHpNtMZNbjj6j1CyF/mjlFEoV8MfpFufvsbkkUcv7W+XLPuT2SKOTiXkh5++3yyVEekijkkz8+KR0Ohxz1QgfZ+69ImZMjox7pKYlCus9EZt46XtUfpIyMlPuG9ZBEIRdsWyCllHL5geXS6xUvGfp6qNxzbk+pp6jn/B7ymjuQ0mSSD402SaKQ+8PMUjocMsXXQ1qed5MPrXroomVybbnyb9/+TRKF7DSvk/R53iz/Os5DOvbtk8FPIu95e2hB2cTMROkWZZIzhiFbT0dOXHitfO6X56TpBZN8b9t70u9VPzl6yehS63byyA5JFPI/b9wmpZSy64NCXvtceMH/e87tkYmZiaps8kn51E9Pybi0OCmllGM/GyvdX3SX1petkijk73G/l7qNl9+8SRKFTDt2UK6ecYskCjl3y9wS5TJyM2TArABJFHLN0TXS4zkh//FQR/nM1AjpNhOZlpMmpZRybcxa6f6iu/R/wVNefae6ju5afpe0O+wF6/py35eSKOSKa1pI2bSplGfOSCmlfHHmEEkUcuPJjTJgVoD8z/b/XFSHEbPV+Z2/ea602W1y1JJR0nOmSZ6O7CR/HtZWEoV8IfoFKaU690Qhn/jxCfn2xrckUcjhtyN9nnO/6DqWUso+f/eQw55sJns96SeHPeAjpZRy//n9MiM3o8Rx+GHJC5Io5NcrZ8tfHh4j3WYiJ345UTocjlKP76Xw/k+zC+7BpfuWVnq53fvXqvvo9cny+ulNZI/HPGVmbqYMeyNMzvhpxiXXB9guK9HGakuhMuSPQDL5BtC588fkmvNHLpTmPgoOhgsX2HhqI70X9OXJNU+SmQV/+/oJxk9O5sEH4bXXVLD3qafg55+VxbA1fBJzt/bno6yJ7EvewojLRrDq8Co+3fMpXx/8mjt63sHqKauJbB7Jx2M/5qH+DxFoDeTNTW/CiRNqPKgxoN3Pj+hw9fGmjjex6vAqrO5WpvaeCoDZzVxQXYu7hc9v+RyTMNH53c6M+WwM2bZsfrr9J9674T0GtRqI3QT7PNPV+oODedmmgquv/fYa/978b0zCxPgu4+ka0pVQvPm5LeQ2C2Fh91wmdZnIa9e+hhCCCBlATADg7s5Js/Lf20yws4mNnJBAfmgHjtAQVrZSx/eGDjeofeh0E/se2Mfu+3fTLbRbqadoeNggNrSG9DZhLO2kgoRf9lJZO1dE+pIj7Nza7daLljG7mZk/ej7bp21n5307GZzTlA0tHcTY4kn0gv7ubQrKBnkGMdSzE4t7w8kAGOTdiYldJ+KQDv723d9oG9iWf1//79IvH0swQkIsqUgpifGXdLMHF/zfLbQbQZ5qOHMr/1bMGj6LZj7qmvvXsH8xuftkHur/EN/c9g29mpU+pbUTKv/WocyTbDer4ahTepR84JOX2YvHBjzGNRHXcE3ENXRPMvO7Zwob/VPpmepZMMJlaPhQfv/r7wzNCSM6HP6VPoDFYxZjEoVNxk0db6KF3ZsJA2MJfiiLVw4vBiDGlEpYGlzRvD/xT8RzX9+Lh+/OTezP7B/hr23G42ZyY97Iedil5OkB6fyzbxKtMtx5cpCySG7qdBOnHj3Fa8Nf45a26npYcxlcmRZ40XUMMOCsO5utCezxTOOKeNWb7xzSGS9zyRln17i3p0kGPHP4P1wf9B3tLsCia95BlDtmuWrcFjCYJhkw+ASMa31dxQvk093ahrA0+F4eYqtPCv0TrHiaPdl9/25eHf5qjdWvLLQoVIZ89xFeXvj5ReLXTE3wSZZ7ShRN9WrGzX1bM2jxIPYdTYZPl+OxZC14JjLp3ZeJi4O4Cyk8vOh/nO0/lWdirmDMlyPI27WTTR7nOZZ0jNevfZ3lk5YTERDBncvvJNeey7197qVv875snbaVwa0H4+3hrVwhB5fzc9xG6N69sBK+vkSHQxf35swfPR9vszeTu08uGIpYnPCAcJZOWMpfuv+F5ZOWc/ihwwxvq6LWPYO7ArDbQ7nEfuvszTqPM/w98u/k2nNZvGsx10RcQ6h3KEIIrnG04ZcI+DEoiWRPuKNDYawi3O5Lsick56Zy0pxBm1R1+W31SealoSZGToF3LotnZcgF+l6w0sKvxUV1NBrK0rg2KJJcd5h1lYk4X/C0m/iiow0pJe91z6FNtpUrWl5R6rJ9m/fF0+zJlRlN2Bdk4/vzvwHQv1h+xnGmbpzPHxU42KM9XUO68sLQF/jgpg/YPm07lwWVPkPL7BA0TYdYmUp8ZjyZHhBhq9zwwk5NOvHfm//L7GtnF4hkaXR2qHN7MC2GbW7naJ8IAe6lb+O5Ic/x8x0/I4Sgz3k3dlgvsMUnmYHnL3aLdAvtxvJdnUj7F/xzm6VEg2l2M/PR7+H87WwLgvxC+XyfSh8fI5KJSAJsNtxNJcOWHZJMPLERTPFKvCICwpm+3Z2Pgk+z1SuJF35xYC0S7mzp1xIhBC2lL1fkp9C8OqHkvl0eCxkmG3YhuSKu/HCpOSuXW/bD/jRSG3kAACAASURBVLSjDPLsyG/vg9/Zmk1k6ZmSyeb/gxWfgajCPAiRkcF1R2BF1i4uuOfR/5zaF2NEm7PRolAZmjXjj6Yg8yeH+TVXqh+TNBuHI4eMDPjf/1QCrZA501gx+CCeMWN5kP2sW3gTSQd6c0/vu1kW+2+u/LI9LeY0YfJXk1lxaAUO6eCnYz/x39AzfNIDPN2sjOs8Dou7hVnDZ5HnyKN/i/70aNqjRLWeGvQUXUK6MKHnIY70aFnwe563J7+1gqEigua+zdn/4H7mjJxT7i4OazuMT8Z9wk2dbrqoB3aZZwu8c2G3m8o8+kqvNJrkujNr+Cwe6HoXABPbjCoof012GHG+8LIjmoAsGB7Yt+C/CLtv/nGL4aRbBv3PutE6GdZYYnm3QwpmO8wI3MFmrwuMOV6637YsBls74GGD18Nj8cyD53f6sS8gl5fXv8ymwHSe3RdcYS9wcIqKEf1732KsedDNdrGI3pSrYh/eudDDEYIQgplDZnJXr7twM5Uz7dhmo3kaxDqSOZakZr23zavZMeftbH64OeBAylG2izNExlKpTKG94yTJplwyTXYGnSmlIT1yBO88CtK+FOeaI3beSRnIxC4T2R+/n2xbNsdIIiKZspP+GQkLjWy2CQk8/UseTYQ3nd3DuP13hxpDXZzUVCbmV2PYmZLzAQacKEzGdHlsBT3+zExeiIZFQ95k9cD3CM5CWdw1yYULXJYEQVlULRdZejrXH4FcqYa79i81I5zz0KOPKsF6/2SG/A1Wc5QRgOnygeT178IF85+8+upa5s69nnPn1Jj+a2/6hO/cbHx/z3iGXFV4478y7BXOpJ/Bz+LHrV1vZWT7kQxoOQCB4Iq3uvDSkINkmOHmgMIhlhO6TGDzgM2Mbj+61Hr5WnxZOXAukSeuYWzwGv6QEiEEOzMOk26BIXnNAWjtf+mZZd1ycul+DnZHnGV//H6+D77Ay7ub4O3hzQu2wQStXcRf+oYXlB+W2gS8YEteDHcfBI+MwrHvEXk+4AExyTGcdEvjxkQHMg+WBhwBM/z4Edx7pz8nZRJjqvhUU++MXAaegugIB7f8CXeuT+Gf/WBm9Ey65gZw9+8V51Hsn+SFh11wOPkoA+PA7HPxBLAW6SauiQXfXHBvV4VZpnY7LdLguD2ZmCQ19DIit4xJVZeIR56Dyy5A9OkNnJYp9DuDEoXS8im8+ir88gv89BN9Thc2pANPFStns6nULX5+aiz0+fP509aLkJAATZrQJ6wPdmnn97jfOS1TaJtE2aJgjF4zes8xMfjnwLbOb+PVJAx3x41qFmW7dhcvl5rKA9ugW7IHfSh5PtufzSVQWgmxWwlOrSALbWYmoRlwb99phSlhShOi6lA03U1VRCEjg+HHwIQJizTR9UztPsVOWwqV4Aupnii6D3URfxPsoP3oAP4y7SjPPns97dpls3atmiEcMvA7ArNgkNvFDXEzn2Z8P/l7Pr/lc1665iUGthqISZgQQvByziBO+cMFL5giC91AQgjeuu4thrUdRlm0PZHKi2thry22YJhh9MlfARiSXrq7qEpkZ9PzHOy2n+G9be/hIU3ct1X1YPzPJjNzHXgnF+Y7ikg10SZd9TUm7uOiHEzheaqB2ha7jWxhp3Wy5PL8XtCw3JZcewyWt3iM5/KuoOfxKk7tT03l2vzUUxP3QbM0yZUZav9ft1+DW2LFrgFrZi79UpWQ94+l1Mlr334Kny2lallcbTZapEKsLanAUgjPrnjma5XIzaVzAvx2Wo26ijREoTS2bVMpJaSkx6k83KSgpcOH1vHFZkGfPKmEYeRI9b24teBwqJF2wcH0DlPpYFYeWokDqdxHFVkKRUQBILzTAEK79le/lZbQMTUVDzsMz21Z8vjbbAiHZIbpSh7L61uxlVQ0NUyzZmrCX02LQtFRiFV5ZkN6OsFZMDioF4McLXDPzL8Oly0r02KrSbQoVIBDOlietg2AwwGwYAFM+v/2zjw+qurs498zM8lk3xNCEiCRVUAQEHeUuqJYt7pSt7d1rVpbW2197aJWrdvbvra1FWu1rq1ba32tShX3BRWQTXaBQDBAQvZlMpmZ8/7xzMncmUw2IAnK+X4++Uzmzp17z5x77/M7z3POec7PXqFcf8jQQ5dy992n8+CDFzFzJqBCvNKwkBPXg6eu97mHjl3WyMwvvRQ2wvHb+9iCXLGiw7AuqlwEwDvl77B/jZsh9cFuvthLfD4mb4M63cKfF/+Zs9QE8rfWyQNvVptz5NlRzS3M3p5JYWIOx24gagJbdrubjDbF2+VvAzC8Ho7bAAm4+Vmi9GFMGXYwtyWdjGrz923NgYYGvvMZ3FB0Nt8M25PbGqZxy9G3MCt9qkxM6ym3T2srM5qkw/fgHZ64opCcW0hSgL6LQiPUhJpYWb2SwhY3Kb39aX4/fPBB/AV/YvYbVyshLBeKKZV0bRirqzvyficH4FDXcE4Kjez8e9evl9fTT5fXWINk0qjm5lKWVUamN5MXVsnQ5rI6pI4uuaRzWMZ4CiZ8ZCaulZZGshbGy3ho7iWTN9xJ+LfemHQcV6jpPV/rlhYRAo9HZm2OGNE/ouD1ypT/PnoKAC/O/BPPqnPktwSDcN55kRQ8/YgVhRhCOsSnWz/lvg/vY3X1ahZ+uZCtTWL8nn5nK1ddBcnDVgNw4hXvc84503hs+XPMefZE5m+Yzw5/LbPX0XWm1E2b4Oabo4yK+mwJL+w8lo9eLSKhvCJ6/zfeiKxfHI/lyzkguZQEVwILv1xIIBTgvc3vMbM6bY8kxTOeAkBbsI2rssKTeaqrI+VyJl9rbubeDSNZcsxzJISIKoNqD1DW6GbhlwsBGFEHB26D+nGPMXPIIbJTQUHHzNg+Gd76egqb4J6jbxejDRztHcsvZ/4SFZ5U1eOKeD4fZzQWU5xezMztcdI+NDSI0XK7+7ayXTBIcbgaPtj8Afs19SFh3OOPyyIGc+Z0f06/n/3rpC9ofGKx9AN0JQrm3gxfv/mp3+OPnlNFgJyLJBhRmDFDRtrFioI5Tl4eSikOLDyQdTVizMtqkYUDHnsMXnkl+nvxPIXcXEmHqpSkVDUrq9XVySCKTz6J3EtDh3a+N8xv9XplTk0gED+fi8GspGgoLe0fUcjPl2yOfexTAMjOKSY7PCqNigr5TcX9vzilFQUHIR3iqEeP4uCHD+aG12/g+CeO508L/4QLN2rDCTQmfMFTT0HuGAl4z984n6HFP+TRTW7+tuo/zH56NgrFrPXEn9VcVSX5lO68E/78Z9nW1ATr1pEz6RBK80ZFt6ra2mTpp/vu67rQK1bgHX8Ak4ZMYlHlIhZXLqbJ38TRjbl7Jn12aysHbAeFYkL+BI4oDo/g2bEj4ik4BbC5mZTkDIYUhCekOYUpEKC0ydORL2Z4eNJxcno2zJ4Nl18uKVBNruS+lN+cZ9iwSDImM7mwt9lrW1s5JFRExfUVDA2mxBeFjAwRLWOUulq200nYUwAory+nrCWx916QyRr6zDNy73RFezvjGiUh3UEpo2VbV+s0GxEPi4I3OQ1PcliInUKyfr2EV4qKZLmwWFEwxwmL7pRCCSF5cFPSQORedjZqTOpdiIjCpk2RCYwQLQpLlkgesnffjfYU2tujf59TFMyyoN2FkOKJQj90NJOTI+GpvoSPjPinpkYyH5v6KCras2WMgxUFB6+ue5UPtnzAL4/+JfMvmk9tay1/XfJXQhtmUqKmo7I3cdpZzWyo3UBaYhqLKxfzwuqX2OkPcu4wNyluzdEjZpDXqjobIL9fVkTZskUSZP3+99IqW7pUHpQpU8SFdd6Y69bJzd/Vzer3y9oPEycybeg0Fn65kLc2vgXA0W2Fe8xTSPfDfw+bw/+c8D8os8zojh3xPYWmJrmZzXrVTsMeCFDWLIYrhQQZlQFiZIcNg7lzxaXfFU+hoUGMgTMdhymDyXraC0+BpHCsPympsyg0NsoxU1OlbFqLMbuxh+R94dFHhv1avL33FKqqpKV5882y/mVX3oLfz4TGZHKSc5iVdZBsi2cUTf5xiFw/07qG6Eyp69dLZ69SEVFwCqDDUwA6+hWGJ+Ti1kQEzSkKTU0Rb8QZPooVhU2bZL/wmuls3hztKUB0XfRVFMwqN4bSUjHcezJNtVMUdsFTiBIF47VZT2Fgue+j+yjJKOHmGTczo+QYjqv7O4TcTNQXcPP3RhHUQd7c+CZBHeTCSRei0Vw/73qyk7K596THePLgAL8+MAednd05fPTWW+ICz50rnsLGjbKM1VtixDtEYevWiMFYuVJet8QOCwmzZo3se8ABHFR0EHW+Ov669K+MyxtHYXL+HhMFgNv3v5oTR50YGX1SVRW3T6Fj2dL0mCR1AO3tlLaIKAxX2R25bjqNkNnF8FEnEdgFT6HjIYwnCrGeQl2dPOy/+Y0sytQVjvARQJkvqW+ikJ8fWdWvqxan30+aK4mqG6o4pzA8MCGeUWxoiJzbGGuzngLEFwUQUaipiV53IMZTmDpUVgcrSww3HOKJglOYd+yQEE95eWdRaG+X+950OJeXS9nNykjQO1FYuDD+cxDrKYwYETnPnsKIwpAhfe9T8Holj5X1FAaPRV8u4u1Nb/ODQ35AKJDACSfAv+49hcvrqln08CVMKJSH49/r/g3AhZMuJC0xje3N2zl/4vkMG/ptJo25B1/di/hKk2TigtNQLFggLa4zzoDTTpOb8NJLZTGXww6TFsCIEdI6qgj3Kxh3vStRWCGjopg4kYOKpHW4uno1R484Onr1NdNRtSsYI2FakvnhCTRffBE5fkz4iNTUSF7/mPBRWXjUzXCXI3V1rCjsavhod0WhJ08hVhSMgQyF4Ioruq7jQIAsHyS7xFiVtaX0XRTCs+q7EwUSE2VEm/kN8UTBKeDmPnOKgvnNwaBc49HhUFRJeB6M07iZ6x420uPyxpGSkMKopLDhMga2wtFPZkJHRUWRhoXf31kUQM5vRMF4CsZTg+hGQzxRqKuDww8X0TZ1dN998htbWqLvO7Ouwp7sV4gNH2ktDbmeQofG24ZoT0GpyH3Qj1hRCHPfR/eR4c3gsmmX8aMfwdtvw6OPwtz7s0hMhJHZcqMaUZhYMJGjRsjM5osPvBiAYcN+TFHR91hyw5cE0l2SQ92kg16wQLLdpafLiIcf/UhumhtvFG9Bqc6tFeMpbN8e/wE3y3+OGsWEggkkuqUVPrN0ppynoUFuxPHjZYnG7njySdkvtnVujIS5ObOypPxLlsj7lJT4ngJ0Xha0vZ0ynxxnuCcnsn1PeAoNDRERMKLQ1/BRXzyF5uaIKFx6qTQAnn8+/nEDARRQ7JUwy37+1O4NQ2SpusjcAGMMumpx+v2RBXy6C584BdzpKcSGjyoq5JjGQJvGgAn5mGN5PB317HF5mHfBPH5eIplL4/YpmGswZowYbRMe6koUYsNHpv6ha0/B/JYvvpB6No2n11+HG26AV1+N36cAuy8Kf/6znMOE6Ywo+HwSKRg/Hp54ovtjOJ8hpygUFESucT9iRQFYt3Mdz37+LFdMu4J5L2XwwANw/fUyms5QmFZISkIKFQ0VFKcXk+5N59qDr+XyqZczvWg6IPMKRo26n/SJZ7LwnhqC2oe+4w5pSX78MRx6aOSA11wjD9jdd0ceYnNjOkXBdJqaUI2TzZvlRklOJtGdyOQhkt2xw1NoaBCDvXFj9+ENkAdl1SoZc+vEGEbzoLlcYiCMKEycKOcwKe1iRSHWU/CnkOhOZLRnSGR7f4ePMjKk3N15ClqLYenKUwgExJCYZeSamiKt9iuvlOMbEY8l7EEUJxWQ4EqgmG7W0F67VoT3w3CWV+MpmL6cHjwFIHI/xRuW6RRwZ59CrKdgfsv++8truN8gShSqq8VLcMwUP3L4kRSnhmP+pr4bGiLX0ngKY8bI66cy3DtKFIqL5besXi0Nn5QUOVZlZd88BRNyMcJiPO+NGzuLwtChYnB3VxQefxz++Ed5DtraIuEjgLvuEluwalX3x6ioiDQCnOGjAehPACsKANz1/l0kuhM5d/j1XHaZ2O677oreRynFqBwJIY3LGwfArFGzmPvNuVHpE1wuD+PHP0P2lMvYdlQr+qUXCH22SFpITlEwS505GTZMXsvLpYWzdi0cFO40jBdCKi+PWqHqtLGncWzZsQxNHyoPT3t75OHuKVa6bJm83ndfdFw5VhRAhMh4KZMny3mamiIjccxDa7wVQyBAmvLy6WWfck3GcZHt/R0+crl6XhEv1iOKFQVjgJwdzcZTKC6WcEhX6wuHQ0UT00cyacgk3AdOkQEG8UYHvf661OeSJSImZlhjfn73492dolBaKr958eLO+xlPQanuPQVzP0wMJyDsylMwYuHE2Zo119Kcy+kpgDSWnF4yyJDfsjIZjh0MwlHikbNiRUSUoec+BdM5u26dHMc8C2ZhcacomLkKu9unsHq1HNs0woynAPDii5Hzd8fatR0p3zvux+bmAelPACsKlNeV8/iyx7l06mXcdG0hgYBEUuJ5aSaENDa3+3V5XS4PY8bMxXXehbhaAzRd/035wCkK8UhKkhuovFxu5EAATgxnV4wnCps3R4nCzUfdHFmEwxjIzz6T1+5uRL9fbuYjj5SW6F/+EvnMGAlzc0J0qoNJ4ZxM1dXRQ+lMGWI6mvF4mDRkEilm7VyPJ2LMDHs6fGS2dRc+ihW/WFEwvyNen0JenlyHHkThvonXy/Kdhx0mx166VFqOp5wCf/+77Pv++/K6ZYsYXa3FICckdD/e3SkK2dlyjlfjLN9oRMGMtjG/NdZTWL5c+hFMXWZni+F0ehrGU4jF+fCYRo0RhVhP4eOPxdgZQ24YOTISejVLWe7YEe0p9CQKxlNoa5NnyohCPE/B1IlzBbi+Ul0dqZ93wuse5OZG9wOkpnZ/jtZWuY9M/TifO+spDAz3f3w/CkXhFzfw+uvSUB4ZP9llJ0+hO5RSDD3nUYL56WS8u51gqpv2kQU9fo/SUmklGlfXjE2PFQWtO4lCFKaFZkShqqrr4YyrV4vh+t73ZKLSffdFJv7E8xRMqzEjI9LCiycKeXnR5Q4EIgbDrFWcmhoVfoj6fl88BWf4qKhIzpPt6MzuaUW8WPHrrSjk5spvGT6861ZmOHyUlJgi6yofFp7r8dFH0qL897/h/vvlmr4nKUrYsiXSKjciPGRI78JHIKkpFi3qLCLV1WLcR46MXON4nsLy5RHBB2m95+R09hTiiYLHkVLNiILpbDbCbEY1VVZGwqZOnA+hY33jqD6FnsJHxlMACdl05ymAeCfGA94VVq+O/P/uu/LqDB/l5cG553bfQDNljicK1lMYGD7c8iGHDD2Se382jOOOk0EkXdEXUQDA7cZ9tixsXr9/iM+WHo3P14N7evHF8jD/6ldiLKdNkxhzrCjU1ooR7koUYj0F6NpomRbZpEkSHy8vl9QKEDGMToNjjFRRUSR8sHNnRBTMQ3vssfKQmREkYU8BiBaFWFyuSIjG0Nra9WQsraPDR5deKh37phzQtShUVoqRfvppee/0FJxhtFhRMB3N5oEfPlyuUbxZtGakkfnNJSXy99FH8K9/ybYFC6QfwbSot2yJeCJGhLsb7x4rCieHM9fOmxe9386dUhfmmNC5T6G9XYyoMx27KUdsn0JP4aPp0t8W5SlkZkbmGkB0f4LBiEJurnTOmrrrjadgruHGjRFD+vrrsn96umyPnacAMtKqqiqylGtfMX0FKSmRPqGcHPkNWVnSSTlmjFyD7vqUwHoKg4XWmpVVK2lYP4HGRmmsdZdd+aRRJ3HOhHM4tKSHMJCTc2RZ+uSZF9DWtpVFiw6mvv6Drve//HIZRrd8uTwsKSnS1+Ac1geRUEVPorByZeQh6koUli0TgzJmjAyXTUmJGEkzTNNZMUYUiosjRiGepzA7nN315Zfl1ekpmNd4ogDRs4ZBWr4XXRR/X59Pjm3CRykpMHVq9D5dhY9uuEEM8g03yPuePAUT025vl2ti6mL4cDHM8fLmx4oCiBAZUTBG8cc/ltfx4+X6GgO8K6Jw4IGy/yuvSBjpqqtkH2PInS38WE/BDJvsThS07tpTcIrCqFFiEJ19CllZcq3Mft2Jwtix4qWYIbF98RT8fqmH3Fz45z9l2wknyH3q88UXBYifd6k3rF4t9XjMMZEGRU6ONHJWrIA77uh5lJMRBeNJWU9hYNnauJVGfyMr3hrPZZfJs9gdwzKH8cxZz3S5vmpcZsyAe+4h+bo7mTbtYzyeTJYs+QaVlY/G39/lgocekgdmwoTwiYdJy7GmRpKTbdgQMfDODjonJnzk7Kjr6kZctkxGmSQkiJE+/XR49ll5qJzDNA1OT8EYhXiiMGKEGBanKPTGUzDlN62p7dslRvvvf8cfymlads4+hFjieQrvvgtPPSUG03TyG+OYnNx9+AjkOpi6MNchXr+Cmb9gVsYDEYXycjEW114rN9+CBfK7Z88WI2pCRUYUTPgoXlqNWFFQCmbNEtGZPVtGlS1dGjHkzhZ+bJ+C8RzjiYKJmTc2yvXsyVMoKhKD7hSF7Gwpn/ld3YmCaTGb+s3IEKPvcvXcpwBy7rFjI9fFeFDQWRSMIXaGnfrC6tVyLmfYzcyRMSOqnOuRx2PtWvGizPP7dfMUlFKzlFJrlFLrlVI/jfP5JUqpKqXUkvDfpf1ZnlhWVkmM0dswnltv7aeTuFzSCi0pISVlLFOnfkxW1tGsWfMd1q+/nlAojpGbMEEM4K/DS+8ZUXjiCXnIn3mm954CwBFHdD/cLjZ+PGeOGND//Cd6QpfBPMxFRdLqc7miw0dOQ3/KKRInr6vrffgIoj2F//xHXpuaZIZqLE6D3RU5OWKQTHgnFJJhwSNGSB/Kv/4loQ4z2ibWUzAC5QxfVFVFewoQXxTieQqHHx75/7TT4FvfimwvLZW6Mv1KRngLC0Wk44UeYkXBHLetDb4RXuB++fKIKHTnKSxbJmUdFxMmzcuLWhgnqmxOzO90u+VeKS6ODh+Zvh5Td/FEoaxM9js4nErb1G9GhghKbHixO1Ewv2PIEAnHGmLvPSNEu+oprFoljStzDznDcobeeApGCOHr5SkopdzAA8BJwHjgfKVUvLb4M1rrA8N/D/dXeeKxaLOIwvnHje8IDfc3CQnZHHDAqxQXf5+Kit+yePFhNDev7rzj8cdHPIWSEnkIH3pI3r/3nhgfrzc6NuzEaSDHjOl6uF1NjTywTlE44QR52J9+Or4oOMNHLpfsG89TABGFQEBi2/HCR864v5P09MhD/+qrkXxGb77ZeV8TFupOFLKzI30PAK+9JkbyzjulxThlikwuMukkkpLE0BoRiecpOOuir6IwZYpcv4kT5ZxGFGbMiHgtixZFOrIheq5Ce7uM8Z87NzK8NZ4oLF8uopqSIsbehI+cLfzYPoXly8WQxh4vP19EJRTqlPcoCnNthw6V+6O4OLqj2VzL7kQhKUkMp+nkc4oCRPp1DF2JwrBhkeGd48dHd2rHegrJybL/rohCa6uUd9y4iCjk5HSOR+fny3m7EoV166JFwe2W+nSm9+hn+tNTOBhYr7XeoLX2A38HTuvH8/WZVxeuhOY8rrhwYNY+NbhcHkaPvp8JE17A59vEwoWTWbfuB/j9VfG/YIzEypUSi/3gA3E/hw/vuhPEaSBHjuw6NbCzk9mQkABnny2t5+rqzq2dsWPlz7R28/JkP2ciL8Mhh8jN/MILfQsfpaVJizgYFKP2zW9KfHj+/Oj9gkH42c/EiDh/QyyxqS5+9ztpeZ19dvz9Y1NFxPYpGIxhy8yUz4woOFNexAsfJSbCrbfS4aJOnizj2K+5JnK9ly2LFn3nrOZzzpFW9JVXSpiovr6zEVdKDJTbLQ2MeJ6CUhGjo5SMylm+vHPoCKQsZmGd7jwFIwqmZVtcLEIWCER7Cvn5ch+Y/oJYzKRDiA4fQfeegrMR4/QUJkyQ62TOHysKICGknsJH8UbxrV0rjY7995dnw+OJ3HNOlOp66GttrXhiTlGASKbaHpaT3VP0pygUA84hMxXhbbF8Sym1TCn1vFJqWD+WpxNLv1xJcvP4jgESA01+/plMn76cIUMuZOvW3/PJJ+Ooqnqh847GSHg8cMstYqDeeKPr0BHIDW8eqJEju15EZP58udkOPDB6+5w5YiDeeKOzp5CdLfFT44rn5nYdPnK74TvfkRQQW7f2PXy0cKEc+6STZDTThx9Gjwq64w4p4x/+0HX/CkQe0NpaKfu8eTIEt6u0ASZ+a0YHNTRInbrd8UVBqciw1PnzZZ977xVDEc9TAPjJT+DMMyPvTztNjJa53n5/fFHYsEE6kOfMkd9RUxNJFtcVkyaJZ9HWFt3RbAYRKCX/33mnCFvs/QCRslRXd8p7FEU8UQiFRMycnsKZZ8LVV0eLZVcYb8IY9FhPweeLCFxs+Mh43KbRYLyFeKIwenT3nsKyZXKNYmf+m+GoxsMaO7brln1ZWfxn0Zw3nigMUH8CDH5H8/8BpVrrScDrwGPxdlJKXa6UWqiUWlhV1UVruo+sX69p8K5kUuGEgRLguHi9Qxk37mGmT19GcvJ+fP75WaxZczmhkGP4pTESJ58sCfVAWobdGUGlpOWamys3sZms1NQkIzGamyUE8fDDYnBj19494gg5b3t7Z1GIxXgKZmJSrKH/yU/kIW5piT9PIR7p6ZLa47bb5Lccf7yM6mhrg7/+FW66SR7AX/4SLrhAhKc7jDGpqREB8XplpFdXnHeetMSvuUZGFDmHvDpFwRl3NBPYHnhAynnjjXD++ZGhtLGi0BU5ORHvzCkK5lzPPCPHvOSSyPoc0H1enAMOiPRFODuanQb06qvhwgslf8/VV3c+hjPVhfEU4oWPzO80w06NJ7B2rdwD5lqceSb87/92XWYnxxwj4UwzaCKep2BWOXP+puJiaRS9/bYM94aIwHTlKTjv5VhefFE8v2uuifZa33xTh7LC2gAAG5tJREFUzm0M+oMPSgqbeBhPoaFB7u9vfEM8DNOHGCsKmZndNwD3ML28S3eJrYCz5V8S3taB1tqZX/ph4J54B9JaPwQ8BHDQQQf1vAJ7d4Vq2MoLq16g/LUzIbmWUw7pYcjRAJGaOoEpUz5k48afs2XL3bS0rGH06AdISirFU1oqI4JuuEGEwHQ893SjZGREWmumdXTllTLi5sILRWAqKzu3ekC8jPPPh3vu6Rw+iiUvT0Jajzwi3kPsw5abK8mkbr01YjB6GpI6dapkJHzlFZnVnZcn8XaPR1r4Ho88TNdeC9/9bs+utfEUvvxS8tOcc07X/TEgx3/0UYn9T5smrVwThnCW2SmmI0ZIf8/y5fCDH4iYmgfdHLM3KCXXeO3a6DLm5kqret48uSYzZsj2H/5QDPWpp3Z9TGdoLdZTMPSUNNGZ6mLnTrlHTKvfSVKSfGYaM4cfLq3nRx6R9/G+0xPmfjSkpUUbbiMKIPWslNz/ZhTP0UdH9u3JUwAJIW3aJPV96qkRb+a116Qug0HpB7rtNjHuDz0k/R+mPo88suvfUlYmjbpZs2TE2ZQpcj1efFHKbvq1DE8/PWD9CYCM1e+PP0RwNgBlQCKwFJgQs89Qx/9nAAt6Ou60adP07vDdf31Xcws65bqDNbeg52+Yv1vH6w+2bXtKv/22V7/1Fvqtt9Br1nxPh0LByA5z5kj6uUce6f5As2dr/eMfy//vvWdS1mk9bJi8lpRoXVysdXt7/O8vWSL7zZ7d/Xl++tPIsV97Lf4+9fVaFxZq/YtfyPvFi2V/8z4eoZDWbW1aBx2//cEHtf7tb7Xevr37MsVSWSnnmzVLXufN6933/vhHrSdP1vonP9F65UrZtmVL5PfW10f2vfPOyPalS7WuqtJaKa0zMmRbc3Pvy3vssfKdn/88evvQobL95JN7fyytpSymbO+9J3Xr9WpdWtr7Y1RUyPcffFDryy7TOi+v633nzdO6piby/swztU5IkO8/+WTfyh6PM8/UeuJErefOlbr47ne1LiiIfJ6UJJ/H4/e/l3IsWdL5sxUr5LNrr5VrB1qPGCF1tnOn1i6X3LMbN2p95JGROp0zR+tAoHdlf/75yPcefzyyfcECrV99tbc10GeAhbo3trs3O+3qH3AysBb4Arg5vO024NTw/78GPg8LxlvAuJ6OuTui0NreqjN+naFz78rX3ILmFnRlY+UuH68/aW5eqysrn9CrV1+m33oLvWrVd3Vb23YdDPq1/tOf5NK98UbvD7h5s3xn1Ch5WKdPl/e33NL1d0IhrQ8+WOurrur+2PfeK8c66ij5TlfU1UUEaNky+c499/T+N+wOPp+cTymtc3O19vt3/Vi1tXIsrzf69z75pGyfOjWy7bDDIgagra3357jkEvnO734Xvf3AA+Nv7w1GUFatkvfFxVqPG9f775s6vP12MbgnnND77774YqQeXn65b+WOx0UXaV1UJA0N0Do1VRo7hqwsaQDEY906rU86Seumps6ftbREyllSovXf/ibCOWqU1k88Ids//DCy/zvvaP2b33TdsIrHmjVyH951V++/swforSj0Z/gIrfUrwCsx237h+P8m4Kb+LIOTl9e+TENbAz/Mn8dvP36CYYd/xJDUARqL2kdSUkaTkjKaIUO+TWLiUMrLb2Pbtr8AboZOPYuyG67Ac/h0lNZRWVq7pKREwjff+pbEdJ9+WlJpXHVV199RSiZ49RT2MH0bd9zRfRjHzDiGnvsU9jRer4QLWloklr07eelNmQsKon+vCUuY2DVIP9BHH8n/vQ0fQST0EhviMp3Ns2b1/liGAw6QcKEJRcSkve4Rr1fCMStWyN8FF/T+uyedFBmQ4MxJtaukpkbSyZsRQ87UGfn5kaGosYwaJWHJeJhhqVu2SFh19mwJd510Elx3nZTdOTLlqKMi/Ry9ZcwY6XB3Pg97Ef0qCnsbTy57kqFpQ9n4xrEUf3o8658N9M6gDiJKKcrKbiUz83BaWtbR2rqGysq/UHlyK3w8l8TEQiZNmkdaWjfDMeVA8ItfRN6PGgWPxe3XjyY2e2U8zjhDRk6YGaG9wXTWxhu211/k5IgohFOP7DJmhEts5/xhh8kEQ+cw15NPltX1IDIarDd0JQoTJ4ph70tdG6ZOlZnhxigXF0cv6NMb8vNlGVmQzt/ekpgonfcPPLBrfQqxmPtn//3FwE+YEH2vvvnmrhvds8+WARYmTcusWXIdX3lF7p2+iHtX7KWCAPRv+Kg//nY1fFTdXK0TbkvQ1/37ep2WpvUVV+zSYfYK2tq26YqKP+hNm27XH3xQpD/8sET7fBWDXay+8/rrfQup7C4HHCBx8L64+l2RmyshiJ4IhSRs4/H07fiff6716NFab9sWvT0YlDDOrlBbK3Frw6ZNWq9f37djHHKIhFAyM3sfQzds3Kj197+/Z+r/llukHHPnyvvnn++5j213WL1a67Q0rZ97rv/O0c+wN4SP9iZeXP0i7aF2xvkvoKlJJtp+VUlMHEJxsQwZzMmZzZIlM1i06BDS0iaRkjKWvLzTycg4HJer/5fu2y2OO67nffYkV1whLdY90dIbNqzz0MF4KCUtzmef7dvxx4+PJEdz4nL1znuLR1aWTCY0dDekuSvMENSZM3s3v8BJaalkndwTHHqojPAxISwzI7y/GDtWhjMPwHKYg40SAfnqcNBBB+mF8fLf9EAwFOTDLR/yxqNHcvuvFA0NAxfO7m9qa99my5Z78fu30dKyklBIcvYo5SUv7zTGjn0Ij2cvdle/itTUyPDDeMMaY6mulvCaWUfhq8x//ZfME7n/fvj+9we7NJY+oJRapLU+qKf99hlPwe1yM2PEDO5fIXNZvi6CAJCdPZPs7JkABAJN1NS8RkvLavz+SiorH6KpaTEjR95HdvbxuN29MGKWnulLX0hsrqGvMqaPoy/9CZavFPuMKBg+/zySr+rriMeTRkHBWR3vCwrOZ+XK81ix4nRcrmTS0w8iPX0aeXlnkpl55F7f0W7ZyzjtNJl4ZVJHWL527DPhI5BJj6mpkiHhV7/awwXbiwmF/NTVvcvOnS/T2PgJTU2fEQr5SEraj8LCixgy5AKSk7tYg9RisXwtsOGjOKxZI7PT97VGjsuVSE7OceTkSMduINBEdfU/2LbtMTZtuoVNm24hMbGYzMwjycs7nZycWSQk7IFhgxaL5SvHPiUKZs2SfU0UYvF40igsvIjCwovw+cqprn6JhoYF1NbOp6rqGQCSksrIyppJcfH3SUubTCjUgsuVYsNNFsvXnH1OFNzu3o0k3FdIShpBScm1wLVoHaS+/iPq69+hqWkpO3Y8y7Ztj6JUAlq3k5o6mdLSW0hLm4TWIZKTR1qRsFi+ZuxzojBmzK4P8/66o5SbrKwjycqSDI/t7XVs3/4Yfv82XK4Utm9/nM8/P6Nj/7S0qZSW/qJjVFNbWyWhUIvtn7BYvsLsc6IwefJgl+KrQ0JCFiUl13W8Hz78Jnbu/D+CwUYCgXoqKv6XFStOB1wkJOTT3i4LzRcVXcl++92Nx9PN8pgWi2WvZJ8RhdZWyZk1Z85gl+Sri8vlIT8/4ikUFV1JTc1rNDZ+SlvbFlJTJ9PWVk5Fxf1UVj5CUlIpycn7kZRURmrqBNLTDyElZSweT3qX56iqeoGdO19h9OgHcLt7WNzHYrHscfYZUVi9WvLh7uudzHsSlyuBvLxvkpf3zajtBQVzqKp6Dp9vI62tG2hoWEAgEFkQxe1OIyPjUHJzTyE9fTrJyaPROsi2bX9l40ZJmuv1FlFWtg+NG7ZY9hL2GVEwI4++zhPX9hYyMqaTkRFJL6y1pq1tMw0Nn+DzbaKtbTO1tW+wfv0POn23oOA8wMXmzXeRn38WaWmTo47T0rIKr3c4Hk9ap+9aLJbdZ58RhbPOEkEwq+1ZBg6lFElJI0hKik7A5vOV09y8gtbWL1AqAa+3iNzcbxII1FFb+waLFx+BUm5crkSSk8fS1raZtrYtJCWVccAB/0dqqnX7LJY9zT41o9ny1aG+fgGVlQ/j8aQTDDbT0rIajyeHrKyZbNlyN8FgC3l5p+F2ZwAhtG4nFPKjlJvU1Emkp0+V9a09mQQC9Xg82d32ZVgsX3fsjGbLV5rMzEPJzDw07mf5+d9i7drLqa9/n0CgHqVcKJWIy5VIKORj27ZHO33H48li5Mj/ITPzKGpqXqW9vRqlEsjOPq7TeUKhNlwuO27Zsm9iPQXL1462ti9pbl6Oz7eFYLAetzuD7dufpL7+3bj7Z2QcRkHBHFJTx1NR8Vtqal5j+PD/ZvjwG6mq+gfNzctITh5DauoEUlMn2DTklq8kvfUUrChY9gm0DrFjx99pb99Jbu5skpLKCAab2LbtMb788gFaWlYD4lGkpx9Cbe28jpncSnnQOtBxrOTkUWRlHYPXW0Qo5Asvk7qO3NxTGD78xg7R8Pu309S0hMzMGZ1SlmsdoqbmPyQk5JCRcfDAVYRln8WKgsXSB1pa1tHUtJjs7BNISMgOz5d4lYKCc8jOPhafbzPNzZ/T3LyChoYPqat7h2CwAaU8JCWVkZhYRH39O3g82WRnH4vHk8X27U8SCvlwu9PJzT2VzMzDSEwsxu/fRmXln2lqWgxATs5JDB/+EzIzj0IpRTDoo7V1LW1tFSQllZGcPGqXV9HTOoRSfVgbugfa2ippalpKbu6sPXZMy8BgRcFi6Ue0DgEapSJLUjY2LmbLlt/Q0PAhbW1bGDLkQvLyTqO6+kV27nyF9vYdHft6vSMoK7sVv387mzffQyCwk+TkUWit8fk2AqGOfZXykJw8mpSUcSQljSAYbKW+/n2SkoYzZsyfSEoaQWvrJhoaFtDcvJRgsJVQyEd9/bv4fBspK/s1JSXXdcpTFQy2AiHc7t6tONXWVslnn83A5/uCCRP+ETWR8etKe3sd69Z9j8LCi8nJOXGwi7NbWFGwWAYRrYNRgiFzNbbQ3l5NQkIBXu/Qjs+DwRZ27HiGHTuewePJICVlPKmp++P1ltDaupGWllW0tKykpWUtPl85SrnJyDiEhoaPABdebwktLSsBUCoBlysZpdykp08HNLW1r5OTMwutNcFgPampkwmFmqmq+idaB8jNnU1CQj6trWtJTZ1IUdEVuFwp+HzlaN1GKNROILCTzZvvxefbRFLScPz+7Uyfvhyvt8jxG0NUVT3Pjh3PUF//PoWFl7Dffr/u8FRCoQCBQA2JiQW7Xb+BQD0NDQtISZlAUlLJbh+vq3MsXXoCjY2f4PWWcPDBa77SKxdaUbBYvoaY51UpRWvrRtavv45gsIXc3NlkZc0kNXUCLleiY/8Q5eW3s3XrH/B6h+F2p9PcvBStNQUF5+Jyeamqeo5QqI3k5NE0NS1Ba3/cc7tcqRxwwEt4vcUsXChDfvPzzyQpqYxQyE9l5VyampaQmFhMSso46urmU1BwPsnJY6ivf5+GhgWEQs1kZc0kL+90/P4d4ey7k0hOHonbnU5Dwwfs2PF3vN5hFBdfTSjko7FxEWlpk8nMPAqXKwGfr5xly2Z19AOlpIxn1KjfkJNzIqFQOzU189ix4++43alkZx9Lbu7sXnlDPl85IJmDA4FGli07kcbGhQwbdiObN99BWdntjBhx8+5eQtrbd7J16x8pLLyYpKThHdsDgQba2raSmrr/bp8jHlYULBZLXOSZ11F9DVprlFL4/TvYseNZXK4kkpPLwl6Hh4SEXBITh3a0lKuq/kl5+W00NS3DhLq83hHst9+vKSg4F1CUl9/Opk2/AFxho34ECQl5VFY+SltbOeBGKXcnEUpJGUdbWwXBYFPUdrc7nZSUcfh8mwmFfIwZ80fa26vYuvUPtLauJzGxCL9/OxDE48lF63aCwQYSEwsZPvynKOWhuXkFfv82/P4q2tur0TpAevoUgsEWampeRSk3paW3UVPzCvX1HzFhwnPk55/BihVnUFv7BpMnzyc9fTrt7Tupq3uzwysKhdpISMghP/9bJCWVUlf3HoFALQkJOaSlTSEv7wySkkbQ0rKKFSvOwOfbSEJCHuPGPU5m5hHU17/PmjWX4/dvpbj4WkpLb0PrAMFgI8FgQ4d3uTvsFaKglJoF3A+4gYe11nfFfO4FHgemATuBc7XWm7o7phUFi2XvIRhspr29GnCRmFjYqUO8tXUTCQm5URMHtQ7i928nIaEgvM8afL7NBIMNJCePIi1tKsFgA1VVL5CQkE96+jQaGj6htvZ1WlvXEQr5GT36D6SlSc6aUKiNiorf09y8FK93OBkZh5CTcxKgqK9/l02bfkl9/fsAeDzZJCYWkZiYT0JCPgCNjQsJhdoYOvRSWlpWUlX1POBm/Pi/UVBwNiADERYtmkow2ITHk00gUAtAYmIhOTkn4Xan09r6BbW189A6gNdbQmJiEe3t1fh8G6LqJDFxKKNG/ZZNm35FS8vnHdtTUiaQmXkElZUPxa3rxMRChg27gWHDrt+lazXooqAkYLoWOB6oAD4Fztdar3Ts8z1gktb6SqXUecAZWutzuzuuFQWLxdIXtNY0NS0lISEPr7e424WhtNZUVT2Hx5PZqWPZ769i586Xqa9/n5SUMWRkHEFm5mFRfUft7bUEgw14vcM7zuPzlbNz578JBOpQKpEhQ+bg9RYRCDRRVfUcgUAtbncahYUX43J5qa//gLq6t3G7M3C703G70/D7v6SxcTE5OScyZMj5u1QPe4MoHAbcorU+Mfz+JgCt9a8d+8wL7/ORUsoDbAPydTeFsqJgsVgsfae3orDnBjB3phjY4nhfEd4Wdx8ts4Pqgdx+LJPFYrFYuqE/RWGPoZS6XCm1UCm1sKqqarCLY7FYLF9b+lMUtgLDHO9Lwtvi7hMOH2UiHc5RaK0f0lofpLU+KD8/v5+Ka7FYLJb+FIVPgdFKqTKlVCJwHvBSzD4vAReH/z8LeLO7/gSLxWKx9C/9ljpbax1QSl0DzEOGpD6itf5cKXUbsFBr/RLwF+AJpdR6oAYRDovFYrEMEv26noLW+hXglZhtv3D87wPO7s8yWCwWi6X3fCU6mi0Wi8UyMFhRsFgsFksHX7ncR0qpKqB8F7+eB1TvweLsKWy5+oYtV+/ZG8sEtlx9ZU+Ua4TWusfhm185UdgdlFILezOjb6Cx5eobtly9Z28sE9hy9ZWBLJcNH1ksFoulAysKFovFYulgXxOF+DlpBx9brr5hy9V79sYygS1XXxmwcu1TfQoWi8Vi6Z59zVOwWCwWSzfsM6KglJqllFqjlFqvlPrpIJZjmFLqLaXUSqXU50qp68Lbc5RSryul1oVfswehbG6l1GdKqZfD78uUUh+H6+yZcA6rgS5TllLqeaXUaqXUKqXUYXtJXf0wfP1WKKX+ppRKGoz6Uko9opTaoZRa4dgWt36U8Ltw+ZYppaYOcLnuDV/HZUqpfyqlshyf3RQu1xql1Inxj9o/5XJ89iOllFZK5YXfD2p9hbdfG66zz5VS9zi29199aa2/9n9I7qUvgP2ARGApMH6QyjIUmBr+Px1ZnW48cA/w0/D2nwJ3D0LZrgeeBl4Ov38WOC/8/4PAVYNQpseAS8P/JwJZg11XyDogG4FkRz1dMhj1BRwFTAVWOLbFrR/gZOBVQAGHAh8PcLlOADzh/+92lGt8+Jn0AmXhZ9U9UOUKbx+G5GkrB/L2kvr6BvAG4A2/LxiI+urXG3Zv+QMOA+Y53t8E3DTY5QqX5V/IkqVrgKHhbUOBNQNcjhJgPnAM8HL4Qah2PMRRdThAZcoMG18Vs32w68osDpWD5A97GThxsOoLKI0xJnHrB5iLLInbab+BKFfMZ2cAT4X/j3oew8b5sIEsF/A8MBnY5BCFQa0vpJFxXJz9+rW+9pXwUW9WgRtwlFKlwBTgY2CI1roy/NE2YMgAF+d/gRuBUPh9LlCnZUU8GJw6KwOqgEfDYa2HlVKpDHJdaa23AvcBm4FKZMXARQx+fRm6qp+96Tn4DtIKh0Eul1LqNGCr1nppzEeDXV9jgBnhkOQ7SqnpA1GufUUU9jqUUmnAC8APtNYNzs+0yP+ADQtTSp0C7NBaLxqoc/YSD+JS/0lrPQVoRsIhHQx0XQGEY/SnIaJVBKQCswayDL1lMOqnJ5RSNwMB4Km9oCwpwH8Dv+hp30HAg3ijhwI3AM8qpVR/n3RfEYXerAI3YCilEhBBeEpr/Y/w5u1KqaHhz4cCOwawSEcApyqlNgF/R0JI9wNZSlbEg8GpswqgQmv9cfj984hIDGZdARwHbNRaV2mt24F/IHU42PVl6Kp+Bv05UEpdApwCfDssWINdrpGIuC8N3/8lwGKlVOEglwvk/v+HFj5BvPi8/i7XviIKvVkFbkAIK/1fgFVa6984PnKuQncx0tcwIGitb9Jal2itS5G6eVNr/W3gLWRFvAEvU7hc24AtSqmx4U3HAisZxLoKsxk4VCmVEr6eplyDWl8Ouqqfl4CLwqNqDgXqHWGmfkcpNQsJUZ6qtW6JKe95SimvUqoMGA18MhBl0lov11oXaK1Lw/d/BTIQZBuDXF/Ai0hnM0qpMchAi2r6u776q9Nkb/tDRhKsRXrqbx7EchyJuPPLgCXhv5ORGP58YB0y4iBnkMo3k8joo/3CN9t64DnCoyAGuDwHAgvD9fUikL031BVwK7AaWAE8gYwEGfD6Av6G9Gu0Iwbtu13VDzJ44IHwM7AcOGiAy7UeiYWb+/5Bx/43h8u1BjhpIMsV8/kmIh3Ng11ficCT4XtsMXDMQNSXndFssVgslg72lfCRxWKxWHqBFQWLxWKxdGBFwWKxWCwdWFGwWCwWSwdWFCwWi8XSgRUFi2UAUUrNVOEstBbL3ogVBYvFYrF0YEXBYomDUuoCpdQnSqklSqm5StaaaFJK/Tac236+Uio/vO+BSqkFjnUCzPoFo5RSbyilliqlFiulRoYPn6Yia0Q8NRD5bCyW3mJFwWKJQSm1P3AucITW+kAgCHwbSXy3UGs9AXgH+GX4K48DP9FaT0JmvprtTwEPaK0nA4cjM1ZBMuP+AMmLvx+SN8li2Svw9LyLxbLPcSwwDfg03IhPRpLKhYBnwvs8CfxDKZUJZGmt3wlvfwx4TimVDhRrrf8JoLX2AYSP94nWuiL8fgmSR//9/v9ZFkvPWFGwWDqjgMe01jdFbVTq5zH77WqOmDbH/0Hsc2jZi7DhI4ulM/OBs5RSBdCx5vEI5HkxWVDnAO9rreuBWqXUjPD2C4F3tNaNQIVS6vTwMbzh3P0Wy16NbaFYLDForVcqpX4G/Ecp5UIyV16NLPJzcPizHUi/A0h66gfDRn8D8F/h7RcCc5VSt4WPcfYA/gyLZZewWVItll6ilGrSWqcNdjkslv7Eho8sFovF0oH1FCwWi8XSgfUULBaLxdKBFQWLxWKxdGBFwWKxWCwdWFGwWCwWSwdWFCwWi8XSgRUFi8VisXTw/x4rVi2gn2eyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.4646 - acc: 0.8924\n",
      "Loss: 0.4645967050009675 Accuracy: 0.8924195\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0352 - acc: 0.1896\n",
      "Epoch 00001: val_loss improved from inf to 2.07625, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/001-2.0763.hdf5\n",
      "36805/36805 [==============================] - 197s 5ms/sample - loss: 3.0353 - acc: 0.1896 - val_loss: 2.0763 - val_acc: 0.3510\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1884 - acc: 0.3474\n",
      "Epoch 00002: val_loss improved from 2.07625 to 1.75526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/002-1.7553.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 2.1885 - acc: 0.3474 - val_loss: 1.7553 - val_acc: 0.4524\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7602 - acc: 0.4569\n",
      "Epoch 00003: val_loss improved from 1.75526 to 1.27825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/003-1.2783.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.7601 - acc: 0.4569 - val_loss: 1.2783 - val_acc: 0.5928\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4651 - acc: 0.5419\n",
      "Epoch 00004: val_loss improved from 1.27825 to 1.19225, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/004-1.1922.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.4650 - acc: 0.5420 - val_loss: 1.1922 - val_acc: 0.6336\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2390 - acc: 0.6127\n",
      "Epoch 00005: val_loss improved from 1.19225 to 1.00942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/005-1.0094.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.2390 - acc: 0.6127 - val_loss: 1.0094 - val_acc: 0.6918\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0802 - acc: 0.6668\n",
      "Epoch 00006: val_loss improved from 1.00942 to 0.86276, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/006-0.8628.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.0802 - acc: 0.6668 - val_loss: 0.8628 - val_acc: 0.7431\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9656 - acc: 0.7046\n",
      "Epoch 00007: val_loss improved from 0.86276 to 0.78981, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/007-0.7898.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.9656 - acc: 0.7046 - val_loss: 0.7898 - val_acc: 0.7699\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8546 - acc: 0.7392\n",
      "Epoch 00008: val_loss improved from 0.78981 to 0.64376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/008-0.6438.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.8546 - acc: 0.7392 - val_loss: 0.6438 - val_acc: 0.8137\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7701\n",
      "Epoch 00009: val_loss improved from 0.64376 to 0.59268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/009-0.5927.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.7646 - acc: 0.7701 - val_loss: 0.5927 - val_acc: 0.8351\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6910 - acc: 0.7934\n",
      "Epoch 00010: val_loss improved from 0.59268 to 0.53874, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/010-0.5387.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.6909 - acc: 0.7935 - val_loss: 0.5387 - val_acc: 0.8460\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6277 - acc: 0.8157\n",
      "Epoch 00011: val_loss improved from 0.53874 to 0.53191, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/011-0.5319.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.6278 - acc: 0.8157 - val_loss: 0.5319 - val_acc: 0.8488\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8292\n",
      "Epoch 00012: val_loss did not improve from 0.53191\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5767 - acc: 0.8292 - val_loss: 0.7866 - val_acc: 0.7761\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.8434\n",
      "Epoch 00013: val_loss improved from 0.53191 to 0.44598, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/013-0.4460.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5395 - acc: 0.8434 - val_loss: 0.4460 - val_acc: 0.8782\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8568\n",
      "Epoch 00014: val_loss did not improve from 0.44598\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4905 - acc: 0.8568 - val_loss: 0.7849 - val_acc: 0.7713\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8662\n",
      "Epoch 00015: val_loss improved from 0.44598 to 0.42083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/015-0.4208.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4627 - acc: 0.8662 - val_loss: 0.4208 - val_acc: 0.8800\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8705\n",
      "Epoch 00016: val_loss did not improve from 0.42083\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4421 - acc: 0.8705 - val_loss: 0.4810 - val_acc: 0.8602\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8787\n",
      "Epoch 00017: val_loss did not improve from 0.42083\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4155 - acc: 0.8787 - val_loss: 0.7166 - val_acc: 0.7911\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8867\n",
      "Epoch 00018: val_loss improved from 0.42083 to 0.39432, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/018-0.3943.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3865 - acc: 0.8867 - val_loss: 0.3943 - val_acc: 0.8819\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8919\n",
      "Epoch 00019: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3737 - acc: 0.8919 - val_loss: 0.4965 - val_acc: 0.8586\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8968\n",
      "Epoch 00020: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3523 - acc: 0.8969 - val_loss: 0.4175 - val_acc: 0.8852\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.9001\n",
      "Epoch 00021: val_loss improved from 0.39432 to 0.37710, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/021-0.3771.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3454 - acc: 0.9001 - val_loss: 0.3771 - val_acc: 0.8917\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.9045\n",
      "Epoch 00022: val_loss improved from 0.37710 to 0.37707, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/022-0.3771.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3272 - acc: 0.9045 - val_loss: 0.3771 - val_acc: 0.8947\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9060\n",
      "Epoch 00023: val_loss did not improve from 0.37707\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3150 - acc: 0.9060 - val_loss: 0.7672 - val_acc: 0.7934\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9104\n",
      "Epoch 00024: val_loss improved from 0.37707 to 0.35486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/024-0.3549.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3005 - acc: 0.9104 - val_loss: 0.3549 - val_acc: 0.9003\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9147\n",
      "Epoch 00025: val_loss improved from 0.35486 to 0.34835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/025-0.3484.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2898 - acc: 0.9147 - val_loss: 0.3484 - val_acc: 0.9066\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9165\n",
      "Epoch 00026: val_loss improved from 0.34835 to 0.32993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/026-0.3299.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2818 - acc: 0.9165 - val_loss: 0.3299 - val_acc: 0.9052\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9227\n",
      "Epoch 00027: val_loss improved from 0.32993 to 0.30678, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/027-0.3068.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2674 - acc: 0.9227 - val_loss: 0.3068 - val_acc: 0.9099\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9211\n",
      "Epoch 00028: val_loss did not improve from 0.30678\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2670 - acc: 0.9211 - val_loss: 0.5323 - val_acc: 0.8432\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9269\n",
      "Epoch 00029: val_loss did not improve from 0.30678\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2463 - acc: 0.9269 - val_loss: 0.4613 - val_acc: 0.8730\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9284\n",
      "Epoch 00030: val_loss did not improve from 0.30678\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2410 - acc: 0.9284 - val_loss: 0.3209 - val_acc: 0.9096\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9280\n",
      "Epoch 00031: val_loss did not improve from 0.30678\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2444 - acc: 0.9280 - val_loss: 0.4244 - val_acc: 0.8800\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9336\n",
      "Epoch 00032: val_loss did not improve from 0.30678\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2236 - acc: 0.9335 - val_loss: 0.3933 - val_acc: 0.8919\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9357\n",
      "Epoch 00033: val_loss improved from 0.30678 to 0.26077, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/033-0.2608.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2198 - acc: 0.9357 - val_loss: 0.2608 - val_acc: 0.9192\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9350\n",
      "Epoch 00034: val_loss did not improve from 0.26077\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2162 - acc: 0.9350 - val_loss: 0.3316 - val_acc: 0.9019\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9379\n",
      "Epoch 00035: val_loss did not improve from 0.26077\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2081 - acc: 0.9379 - val_loss: 0.6493 - val_acc: 0.8295\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9407\n",
      "Epoch 00036: val_loss did not improve from 0.26077\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2002 - acc: 0.9406 - val_loss: 0.4636 - val_acc: 0.8749\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9424\n",
      "Epoch 00037: val_loss did not improve from 0.26077\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1932 - acc: 0.9424 - val_loss: 0.2767 - val_acc: 0.9199\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9440\n",
      "Epoch 00038: val_loss did not improve from 0.26077\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1862 - acc: 0.9441 - val_loss: 0.3867 - val_acc: 0.8856\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9459\n",
      "Epoch 00039: val_loss improved from 0.26077 to 0.23096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/039-0.2310.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1805 - acc: 0.9459 - val_loss: 0.2310 - val_acc: 0.9299\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9483\n",
      "Epoch 00040: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1734 - acc: 0.9483 - val_loss: 0.3853 - val_acc: 0.8877\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9469\n",
      "Epoch 00041: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1754 - acc: 0.9469 - val_loss: 0.4075 - val_acc: 0.8803\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9506\n",
      "Epoch 00042: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1662 - acc: 0.9506 - val_loss: 0.5663 - val_acc: 0.8381\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9489\n",
      "Epoch 00043: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1698 - acc: 0.9488 - val_loss: 0.3156 - val_acc: 0.9157\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9506\n",
      "Epoch 00044: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1603 - acc: 0.9506 - val_loss: 0.3553 - val_acc: 0.9001\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9563\n",
      "Epoch 00045: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1484 - acc: 0.9563 - val_loss: 0.2313 - val_acc: 0.9292\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9573\n",
      "Epoch 00046: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1409 - acc: 0.9573 - val_loss: 0.2964 - val_acc: 0.9168\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9578\n",
      "Epoch 00047: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1394 - acc: 0.9578 - val_loss: 0.2818 - val_acc: 0.9185\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9539\n",
      "Epoch 00048: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1537 - acc: 0.9539 - val_loss: 0.2430 - val_acc: 0.9350\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9592\n",
      "Epoch 00049: val_loss did not improve from 0.23096\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1294 - acc: 0.9591 - val_loss: 0.2621 - val_acc: 0.9285\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9585\n",
      "Epoch 00050: val_loss improved from 0.23096 to 0.19747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv_checkpoint/050-0.1975.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1390 - acc: 0.9584 - val_loss: 0.1975 - val_acc: 0.9427\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9598\n",
      "Epoch 00051: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1340 - acc: 0.9598 - val_loss: 0.2694 - val_acc: 0.9238\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9617\n",
      "Epoch 00052: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1250 - acc: 0.9617 - val_loss: 0.2742 - val_acc: 0.9241\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9627\n",
      "Epoch 00053: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1226 - acc: 0.9627 - val_loss: 0.2890 - val_acc: 0.9217\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9648\n",
      "Epoch 00054: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1146 - acc: 0.9648 - val_loss: 0.2953 - val_acc: 0.9185\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9631\n",
      "Epoch 00055: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1156 - acc: 0.9631 - val_loss: 0.2784 - val_acc: 0.9245\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9664\n",
      "Epoch 00056: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1118 - acc: 0.9664 - val_loss: 0.4125 - val_acc: 0.8949\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9638\n",
      "Epoch 00057: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1151 - acc: 0.9638 - val_loss: 0.2757 - val_acc: 0.9248\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9665\n",
      "Epoch 00058: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1082 - acc: 0.9664 - val_loss: 0.2810 - val_acc: 0.9175\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9634\n",
      "Epoch 00059: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1178 - acc: 0.9634 - val_loss: 0.4724 - val_acc: 0.8772\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9677\n",
      "Epoch 00060: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1038 - acc: 0.9677 - val_loss: 0.2493 - val_acc: 0.9329\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9728\n",
      "Epoch 00061: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0897 - acc: 0.9728 - val_loss: 0.4407 - val_acc: 0.8896\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9715\n",
      "Epoch 00062: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0919 - acc: 0.9715 - val_loss: 0.3361 - val_acc: 0.9129\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9689\n",
      "Epoch 00063: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1010 - acc: 0.9689 - val_loss: 0.2543 - val_acc: 0.9236\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9699\n",
      "Epoch 00064: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0967 - acc: 0.9699 - val_loss: 0.7564 - val_acc: 0.8225\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9740\n",
      "Epoch 00065: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0858 - acc: 0.9739 - val_loss: 0.2932 - val_acc: 0.9229\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9699\n",
      "Epoch 00066: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0986 - acc: 0.9699 - val_loss: 0.2206 - val_acc: 0.9406\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9770\n",
      "Epoch 00067: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0748 - acc: 0.9770 - val_loss: 0.2565 - val_acc: 0.9283\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9744\n",
      "Epoch 00068: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0809 - acc: 0.9744 - val_loss: 0.2159 - val_acc: 0.9352\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9766\n",
      "Epoch 00069: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0751 - acc: 0.9766 - val_loss: 0.2414 - val_acc: 0.9394\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9669\n",
      "Epoch 00070: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1104 - acc: 0.9669 - val_loss: 0.3188 - val_acc: 0.9164\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9796\n",
      "Epoch 00071: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0671 - acc: 0.9796 - val_loss: 0.2571 - val_acc: 0.9285\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9793\n",
      "Epoch 00072: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0681 - acc: 0.9793 - val_loss: 0.2503 - val_acc: 0.9301\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9753\n",
      "Epoch 00073: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0768 - acc: 0.9753 - val_loss: 0.2753 - val_acc: 0.9285\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9798\n",
      "Epoch 00074: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0652 - acc: 0.9797 - val_loss: 0.2762 - val_acc: 0.9306\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9763\n",
      "Epoch 00075: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0749 - acc: 0.9763 - val_loss: 0.2329 - val_acc: 0.9380\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9791\n",
      "Epoch 00076: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0682 - acc: 0.9790 - val_loss: 0.3011 - val_acc: 0.9224\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9785\n",
      "Epoch 00077: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0697 - acc: 0.9785 - val_loss: 0.3339 - val_acc: 0.9154\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9788\n",
      "Epoch 00078: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0659 - acc: 0.9788 - val_loss: 0.2889 - val_acc: 0.9255\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9778\n",
      "Epoch 00079: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0705 - acc: 0.9778 - val_loss: 0.3629 - val_acc: 0.9047\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9799\n",
      "Epoch 00080: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0659 - acc: 0.9799 - val_loss: 0.3351 - val_acc: 0.9164\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9804\n",
      "Epoch 00081: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0628 - acc: 0.9804 - val_loss: 0.3185 - val_acc: 0.9224\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9806\n",
      "Epoch 00082: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0637 - acc: 0.9806 - val_loss: 0.2782 - val_acc: 0.9317\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9794\n",
      "Epoch 00083: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0634 - acc: 0.9794 - val_loss: 0.2627 - val_acc: 0.9292\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9851\n",
      "Epoch 00084: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0500 - acc: 0.9851 - val_loss: 0.5068 - val_acc: 0.8840\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9817\n",
      "Epoch 00085: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0585 - acc: 0.9817 - val_loss: 0.2421 - val_acc: 0.9341\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9828\n",
      "Epoch 00086: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0570 - acc: 0.9828 - val_loss: 0.2821 - val_acc: 0.9257\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9833\n",
      "Epoch 00087: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0521 - acc: 0.9833 - val_loss: 0.3920 - val_acc: 0.9094\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9749\n",
      "Epoch 00088: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0796 - acc: 0.9749 - val_loss: 0.2965 - val_acc: 0.9308\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9843\n",
      "Epoch 00089: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0502 - acc: 0.9843 - val_loss: 0.2382 - val_acc: 0.9383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9821\n",
      "Epoch 00090: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0574 - acc: 0.9821 - val_loss: 0.2444 - val_acc: 0.9341\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9846\n",
      "Epoch 00091: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0502 - acc: 0.9846 - val_loss: 0.2692 - val_acc: 0.9297\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9827\n",
      "Epoch 00092: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0538 - acc: 0.9826 - val_loss: 0.4183 - val_acc: 0.9022\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9784\n",
      "Epoch 00093: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0704 - acc: 0.9784 - val_loss: 0.2561 - val_acc: 0.9385\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9833\n",
      "Epoch 00094: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0545 - acc: 0.9833 - val_loss: 0.2587 - val_acc: 0.9327\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9860\n",
      "Epoch 00095: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0468 - acc: 0.9860 - val_loss: 0.2222 - val_acc: 0.9429\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9865\n",
      "Epoch 00096: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0447 - acc: 0.9865 - val_loss: 0.2197 - val_acc: 0.9460\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9870\n",
      "Epoch 00097: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0435 - acc: 0.9870 - val_loss: 0.2641 - val_acc: 0.9359\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9809\n",
      "Epoch 00098: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0585 - acc: 0.9809 - val_loss: 0.2626 - val_acc: 0.9406\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9855\n",
      "Epoch 00099: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0461 - acc: 0.9854 - val_loss: 0.2700 - val_acc: 0.9313\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9844\n",
      "Epoch 00100: val_loss did not improve from 0.19747\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0504 - acc: 0.9844 - val_loss: 0.2261 - val_acc: 0.9418\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2594 - acc: 0.9269\n",
      "Loss: 0.25936675077905785 Accuracy: 0.92689514\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7276 - acc: 0.2619\n",
      "Epoch 00001: val_loss improved from inf to 1.90959, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/001-1.9096.hdf5\n",
      "36805/36805 [==============================] - 222s 6ms/sample - loss: 2.7275 - acc: 0.2619 - val_loss: 1.9096 - val_acc: 0.4051\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7707 - acc: 0.4736\n",
      "Epoch 00002: val_loss improved from 1.90959 to 1.78342, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/002-1.7834.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 1.7705 - acc: 0.4737 - val_loss: 1.7834 - val_acc: 0.4745\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3110 - acc: 0.5988\n",
      "Epoch 00003: val_loss improved from 1.78342 to 0.84203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/003-0.8420.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 1.3109 - acc: 0.5988 - val_loss: 0.8420 - val_acc: 0.7370\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0071 - acc: 0.6916\n",
      "Epoch 00004: val_loss improved from 0.84203 to 0.60341, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/004-0.6034.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 1.0070 - acc: 0.6917 - val_loss: 0.6034 - val_acc: 0.8211\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7607\n",
      "Epoch 00005: val_loss improved from 0.60341 to 0.56201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/005-0.5620.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.7860 - acc: 0.7607 - val_loss: 0.5620 - val_acc: 0.8307\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6357 - acc: 0.8090\n",
      "Epoch 00006: val_loss improved from 0.56201 to 0.52541, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/006-0.5254.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6361 - acc: 0.8089 - val_loss: 0.5254 - val_acc: 0.8360\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.8344\n",
      "Epoch 00007: val_loss improved from 0.52541 to 0.40445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/007-0.4045.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.5464 - acc: 0.8344 - val_loss: 0.4045 - val_acc: 0.8777\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8584\n",
      "Epoch 00008: val_loss improved from 0.40445 to 0.35874, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/008-0.3587.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.4743 - acc: 0.8584 - val_loss: 0.3587 - val_acc: 0.8910\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8768\n",
      "Epoch 00009: val_loss improved from 0.35874 to 0.33603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/009-0.3360.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.4121 - acc: 0.8768 - val_loss: 0.3360 - val_acc: 0.9003\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8876\n",
      "Epoch 00010: val_loss improved from 0.33603 to 0.33602, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/010-0.3360.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.3752 - acc: 0.8876 - val_loss: 0.3360 - val_acc: 0.8984\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8952\n",
      "Epoch 00011: val_loss did not improve from 0.33602\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.3460 - acc: 0.8952 - val_loss: 0.3541 - val_acc: 0.8912\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9065\n",
      "Epoch 00012: val_loss improved from 0.33602 to 0.26032, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/012-0.2603.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.3128 - acc: 0.9065 - val_loss: 0.2603 - val_acc: 0.9220\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9105\n",
      "Epoch 00013: val_loss did not improve from 0.26032\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2973 - acc: 0.9105 - val_loss: 0.2966 - val_acc: 0.9096\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9193\n",
      "Epoch 00014: val_loss did not improve from 0.26032\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2705 - acc: 0.9193 - val_loss: 0.3967 - val_acc: 0.8833\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9239\n",
      "Epoch 00015: val_loss improved from 0.26032 to 0.23241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/015-0.2324.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2528 - acc: 0.9239 - val_loss: 0.2324 - val_acc: 0.9308\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9267\n",
      "Epoch 00016: val_loss did not improve from 0.23241\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2412 - acc: 0.9267 - val_loss: 0.3578 - val_acc: 0.8884\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9336\n",
      "Epoch 00017: val_loss did not improve from 0.23241\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2208 - acc: 0.9336 - val_loss: 0.2508 - val_acc: 0.9269\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9391\n",
      "Epoch 00018: val_loss did not improve from 0.23241\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.2046 - acc: 0.9391 - val_loss: 0.2560 - val_acc: 0.9259\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9398\n",
      "Epoch 00019: val_loss did not improve from 0.23241\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1996 - acc: 0.9397 - val_loss: 0.3406 - val_acc: 0.8994\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9431\n",
      "Epoch 00020: val_loss did not improve from 0.23241\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1889 - acc: 0.9431 - val_loss: 0.2405 - val_acc: 0.9301\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9497\n",
      "Epoch 00021: val_loss improved from 0.23241 to 0.21231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/021-0.2123.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1737 - acc: 0.9497 - val_loss: 0.2123 - val_acc: 0.9352\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9524\n",
      "Epoch 00022: val_loss improved from 0.21231 to 0.21200, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/022-0.2120.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1593 - acc: 0.9524 - val_loss: 0.2120 - val_acc: 0.9392\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9471\n",
      "Epoch 00023: val_loss improved from 0.21200 to 0.20297, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/023-0.2030.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1707 - acc: 0.9472 - val_loss: 0.2030 - val_acc: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9552\n",
      "Epoch 00024: val_loss did not improve from 0.20297\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1489 - acc: 0.9552 - val_loss: 0.2313 - val_acc: 0.9311\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9590\n",
      "Epoch 00025: val_loss did not improve from 0.20297\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1376 - acc: 0.9590 - val_loss: 0.2245 - val_acc: 0.9350\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9615\n",
      "Epoch 00026: val_loss did not improve from 0.20297\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1297 - acc: 0.9616 - val_loss: 0.2133 - val_acc: 0.9383\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9638\n",
      "Epoch 00027: val_loss did not improve from 0.20297\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1209 - acc: 0.9638 - val_loss: 0.2365 - val_acc: 0.9338\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9644\n",
      "Epoch 00028: val_loss did not improve from 0.20297\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1170 - acc: 0.9644 - val_loss: 0.2812 - val_acc: 0.9203\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9652\n",
      "Epoch 00029: val_loss improved from 0.20297 to 0.19419, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/029-0.1942.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1160 - acc: 0.9652 - val_loss: 0.1942 - val_acc: 0.9464\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9670\n",
      "Epoch 00030: val_loss improved from 0.19419 to 0.18923, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/030-0.1892.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1090 - acc: 0.9670 - val_loss: 0.1892 - val_acc: 0.9439\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9676\n",
      "Epoch 00031: val_loss did not improve from 0.18923\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1053 - acc: 0.9675 - val_loss: 0.2774 - val_acc: 0.9248\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9702\n",
      "Epoch 00032: val_loss did not improve from 0.18923\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0988 - acc: 0.9702 - val_loss: 0.2127 - val_acc: 0.9418\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9718\n",
      "Epoch 00033: val_loss did not improve from 0.18923\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0934 - acc: 0.9718 - val_loss: 0.2188 - val_acc: 0.9357\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9736\n",
      "Epoch 00034: val_loss improved from 0.18923 to 0.17494, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/034-0.1749.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0848 - acc: 0.9736 - val_loss: 0.1749 - val_acc: 0.9541\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9737\n",
      "Epoch 00035: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0875 - acc: 0.9737 - val_loss: 0.2042 - val_acc: 0.9401\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9738\n",
      "Epoch 00036: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0854 - acc: 0.9738 - val_loss: 0.4323 - val_acc: 0.8952\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9721\n",
      "Epoch 00037: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0902 - acc: 0.9722 - val_loss: 0.1936 - val_acc: 0.9450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9788\n",
      "Epoch 00038: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0724 - acc: 0.9788 - val_loss: 0.1889 - val_acc: 0.9457\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9798\n",
      "Epoch 00039: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0678 - acc: 0.9798 - val_loss: 0.2000 - val_acc: 0.9485\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9813\n",
      "Epoch 00040: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0628 - acc: 0.9813 - val_loss: 0.1831 - val_acc: 0.9492\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9799\n",
      "Epoch 00041: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0661 - acc: 0.9799 - val_loss: 0.1944 - val_acc: 0.9499\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9804\n",
      "Epoch 00042: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0663 - acc: 0.9804 - val_loss: 0.2830 - val_acc: 0.9262\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9817\n",
      "Epoch 00043: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0603 - acc: 0.9816 - val_loss: 0.2090 - val_acc: 0.9434\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9814\n",
      "Epoch 00044: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0606 - acc: 0.9814 - val_loss: 0.2066 - val_acc: 0.9425\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9837\n",
      "Epoch 00045: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0540 - acc: 0.9837 - val_loss: 0.1820 - val_acc: 0.9506\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9811\n",
      "Epoch 00046: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0639 - acc: 0.9811 - val_loss: 0.2427 - val_acc: 0.9376\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9840\n",
      "Epoch 00047: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0528 - acc: 0.9839 - val_loss: 0.5209 - val_acc: 0.8684\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9832\n",
      "Epoch 00048: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0540 - acc: 0.9831 - val_loss: 0.3986 - val_acc: 0.8996\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9841\n",
      "Epoch 00049: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0514 - acc: 0.9841 - val_loss: 0.2941 - val_acc: 0.9259\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9860\n",
      "Epoch 00050: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0461 - acc: 0.9860 - val_loss: 0.1834 - val_acc: 0.9495\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9877\n",
      "Epoch 00051: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0415 - acc: 0.9877 - val_loss: 0.2287 - val_acc: 0.9408\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9837\n",
      "Epoch 00052: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0533 - acc: 0.9837 - val_loss: 0.2842 - val_acc: 0.9322\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9884\n",
      "Epoch 00053: val_loss improved from 0.17494 to 0.17455, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/053-0.1745.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0365 - acc: 0.9884 - val_loss: 0.1745 - val_acc: 0.9548\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9877\n",
      "Epoch 00054: val_loss improved from 0.17455 to 0.17144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/054-0.1714.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0408 - acc: 0.9877 - val_loss: 0.1714 - val_acc: 0.9532\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9868\n",
      "Epoch 00055: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0433 - acc: 0.9868 - val_loss: 0.3115 - val_acc: 0.9208\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9876\n",
      "Epoch 00056: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0418 - acc: 0.9876 - val_loss: 0.2254 - val_acc: 0.9457\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9850\n",
      "Epoch 00057: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0472 - acc: 0.9850 - val_loss: 0.1749 - val_acc: 0.9574\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9895\n",
      "Epoch 00058: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0344 - acc: 0.9895 - val_loss: 0.2010 - val_acc: 0.9539\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9902\n",
      "Epoch 00059: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0324 - acc: 0.9902 - val_loss: 0.3772 - val_acc: 0.9068\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9862\n",
      "Epoch 00060: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0454 - acc: 0.9861 - val_loss: 0.1978 - val_acc: 0.9550\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9888\n",
      "Epoch 00061: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0364 - acc: 0.9888 - val_loss: 0.1930 - val_acc: 0.9536\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9921\n",
      "Epoch 00062: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0270 - acc: 0.9921 - val_loss: 0.5061 - val_acc: 0.8877\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9895\n",
      "Epoch 00063: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0338 - acc: 0.9895 - val_loss: 0.2214 - val_acc: 0.9455\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9880\n",
      "Epoch 00064: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0378 - acc: 0.9880 - val_loss: 0.1916 - val_acc: 0.9560\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9902\n",
      "Epoch 00065: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0323 - acc: 0.9902 - val_loss: 0.2124 - val_acc: 0.9502\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9893\n",
      "Epoch 00066: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0334 - acc: 0.9893 - val_loss: 0.2039 - val_acc: 0.9536\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9928\n",
      "Epoch 00067: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0232 - acc: 0.9928 - val_loss: 0.2442 - val_acc: 0.9397\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9899\n",
      "Epoch 00068: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0342 - acc: 0.9899 - val_loss: 0.1752 - val_acc: 0.9562\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9922\n",
      "Epoch 00069: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0244 - acc: 0.9922 - val_loss: 0.1832 - val_acc: 0.9560\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9917\n",
      "Epoch 00070: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0266 - acc: 0.9917 - val_loss: 0.1942 - val_acc: 0.9527\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9892\n",
      "Epoch 00071: val_loss did not improve from 0.17144\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0347 - acc: 0.9891 - val_loss: 0.2429 - val_acc: 0.9460\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9861\n",
      "Epoch 00072: val_loss improved from 0.17144 to 0.16866, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv_checkpoint/072-0.1687.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0479 - acc: 0.9861 - val_loss: 0.1687 - val_acc: 0.9581\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9937\n",
      "Epoch 00073: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0195 - acc: 0.9938 - val_loss: 0.2192 - val_acc: 0.9467\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 00074: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0266 - acc: 0.9918 - val_loss: 0.1890 - val_acc: 0.9546\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9880\n",
      "Epoch 00075: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0381 - acc: 0.9880 - val_loss: 0.2142 - val_acc: 0.9518\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9862\n",
      "Epoch 00076: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0463 - acc: 0.9862 - val_loss: 0.1772 - val_acc: 0.9581\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9952\n",
      "Epoch 00077: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0167 - acc: 0.9952 - val_loss: 0.2088 - val_acc: 0.9534\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9943\n",
      "Epoch 00078: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0193 - acc: 0.9943 - val_loss: 0.2161 - val_acc: 0.9525\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9885\n",
      "Epoch 00079: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0360 - acc: 0.9885 - val_loss: 0.2105 - val_acc: 0.9541\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9946\n",
      "Epoch 00080: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0182 - acc: 0.9946 - val_loss: 0.1828 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9938\n",
      "Epoch 00081: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.2236 - val_acc: 0.9534\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9923\n",
      "Epoch 00082: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0250 - acc: 0.9923 - val_loss: 0.2104 - val_acc: 0.9525\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9929\n",
      "Epoch 00083: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0243 - acc: 0.9929 - val_loss: 0.1794 - val_acc: 0.9574\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9934\n",
      "Epoch 00084: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0209 - acc: 0.9934 - val_loss: 0.2030 - val_acc: 0.9564\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9915\n",
      "Epoch 00085: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0273 - acc: 0.9914 - val_loss: 0.2564 - val_acc: 0.9392\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9901\n",
      "Epoch 00086: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0331 - acc: 0.9901 - val_loss: 0.2187 - val_acc: 0.9497\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9934\n",
      "Epoch 00087: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0203 - acc: 0.9934 - val_loss: 0.1863 - val_acc: 0.9581\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9952\n",
      "Epoch 00088: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 0.2075 - val_acc: 0.9527\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9927\n",
      "Epoch 00089: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0235 - acc: 0.9927 - val_loss: 0.1878 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9915\n",
      "Epoch 00090: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0265 - acc: 0.9916 - val_loss: 0.1763 - val_acc: 0.9609\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9950\n",
      "Epoch 00091: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0164 - acc: 0.9950 - val_loss: 0.1989 - val_acc: 0.9548\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9936\n",
      "Epoch 00092: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0229 - acc: 0.9936 - val_loss: 0.2453 - val_acc: 0.9483\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9932\n",
      "Epoch 00093: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0217 - acc: 0.9931 - val_loss: 0.2094 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 00094: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0277 - acc: 0.9914 - val_loss: 0.2154 - val_acc: 0.9520\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9964\n",
      "Epoch 00095: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0121 - acc: 0.9964 - val_loss: 0.1894 - val_acc: 0.9616\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9954\n",
      "Epoch 00096: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0163 - acc: 0.9953 - val_loss: 0.2760 - val_acc: 0.9362\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9899\n",
      "Epoch 00097: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0320 - acc: 0.9899 - val_loss: 0.1984 - val_acc: 0.9548\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9959\n",
      "Epoch 00098: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0133 - acc: 0.9958 - val_loss: 0.2087 - val_acc: 0.9546\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9904\n",
      "Epoch 00099: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0293 - acc: 0.9903 - val_loss: 0.2250 - val_acc: 0.9485\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 00100: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0272 - acc: 0.9916 - val_loss: 0.1949 - val_acc: 0.9604\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 00101: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.1747 - val_acc: 0.9585\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 00102: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0171 - acc: 0.9947 - val_loss: 0.1978 - val_acc: 0.9560\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9948\n",
      "Epoch 00103: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0172 - acc: 0.9948 - val_loss: 0.2101 - val_acc: 0.9520\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 00104: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0217 - acc: 0.9936 - val_loss: 0.2150 - val_acc: 0.9506\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9918\n",
      "Epoch 00105: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0255 - acc: 0.9918 - val_loss: 0.2045 - val_acc: 0.9585\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9935\n",
      "Epoch 00106: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0204 - acc: 0.9935 - val_loss: 0.1849 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9957\n",
      "Epoch 00107: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0144 - acc: 0.9957 - val_loss: 0.1978 - val_acc: 0.9555\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9891\n",
      "Epoch 00108: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0369 - acc: 0.9891 - val_loss: 0.1843 - val_acc: 0.9602\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9965\n",
      "Epoch 00109: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0107 - acc: 0.9965 - val_loss: 0.2219 - val_acc: 0.9564\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 00110: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.2106 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9953\n",
      "Epoch 00111: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0157 - acc: 0.9953 - val_loss: 0.2195 - val_acc: 0.9543\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9937\n",
      "Epoch 00112: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.2010 - val_acc: 0.9581\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 00113: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0266 - acc: 0.9918 - val_loss: 0.2233 - val_acc: 0.9536\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9969\n",
      "Epoch 00114: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0112 - acc: 0.9968 - val_loss: 0.2327 - val_acc: 0.9495\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9902\n",
      "Epoch 00115: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0337 - acc: 0.9901 - val_loss: 0.1905 - val_acc: 0.9569\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9952\n",
      "Epoch 00116: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0169 - acc: 0.9952 - val_loss: 0.2512 - val_acc: 0.9446\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9944\n",
      "Epoch 00117: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.1957 - val_acc: 0.9604\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9938\n",
      "Epoch 00118: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0203 - acc: 0.9938 - val_loss: 0.1897 - val_acc: 0.9588\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9952\n",
      "Epoch 00119: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 0.1762 - val_acc: 0.9634\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00120: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1784 - val_acc: 0.9623\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9945\n",
      "Epoch 00121: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0167 - acc: 0.9945 - val_loss: 0.2286 - val_acc: 0.9525\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9930\n",
      "Epoch 00122: val_loss did not improve from 0.16866\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0206 - acc: 0.9930 - val_loss: 0.2142 - val_acc: 0.9520\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2445 - acc: 0.9448\n",
      "Loss: 0.244535535413033 Accuracy: 0.944756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN'\n",
    "\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 936us/sample - loss: 7.1497 - acc: 0.0860\n",
      "Loss: 7.1496533096765065 Accuracy: 0.08598131\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 3.1928 - acc: 0.2309\n",
      "Loss: 3.1927570615849636 Accuracy: 0.23094496\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 2.8369 - acc: 0.5753\n",
      "Loss: 2.836937515114203 Accuracy: 0.57528555\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.2439 - acc: 0.7279\n",
      "Loss: 1.2438688593366312 Accuracy: 0.7279335\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.0801 - acc: 0.7607\n",
      "Loss: 1.0800980104464237 Accuracy: 0.7607477\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.8138 - acc: 0.7896\n",
      "Loss: 0.8137692730251007 Accuracy: 0.7896158\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4646 - acc: 0.8924\n",
      "Loss: 0.4645967050009675 Accuracy: 0.8924195\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2594 - acc: 0.9269\n",
      "Loss: 0.25936675077905785 Accuracy: 0.92689514\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2445 - acc: 0.9448\n",
      "Loss: 0.244535535413033 Accuracy: 0.944756\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 11.3414 - acc: 0.1929\n",
      "Loss: 11.341394414337254 Accuracy: 0.19293873\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 7.9129 - acc: 0.3306\n",
      "Loss: 7.9129345960211035 Accuracy: 0.33063343\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 5.6245 - acc: 0.4378\n",
      "Loss: 5.624517787010375 Accuracy: 0.43779856\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.5850 - acc: 0.7028\n",
      "Loss: 1.5850401088332338 Accuracy: 0.70280373\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.1920 - acc: 0.7593\n",
      "Loss: 1.1919883197713121 Accuracy: 0.75929385\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 2.4760 - acc: 0.6353\n",
      "Loss: 2.4760207142042594 Accuracy: 0.63530636\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.8675 - acc: 0.8235\n",
      "Loss: 0.8674916854404834 Accuracy: 0.8234683\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2982 - acc: 0.9271\n",
      "Loss: 0.2982376911552663 Accuracy: 0.9271028\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2978 - acc: 0.9398\n",
      "Loss: 0.29782334990753007 Accuracy: 0.93977153\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
