{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4294 - acc: 0.2413\n",
      "Epoch 00001: val_loss improved from inf to 2.24727, saving model to model/checkpoint/1D_CNN_1_conv_custom_DO_checkpoint/001-2.2473.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 2.4293 - acc: 0.2414 - val_loss: 2.2473 - val_acc: 0.3189\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9645 - acc: 0.4201\n",
      "Epoch 00002: val_loss improved from 2.24727 to 2.13194, saving model to model/checkpoint/1D_CNN_1_conv_custom_DO_checkpoint/002-2.1319.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 1.9646 - acc: 0.4201 - val_loss: 2.1319 - val_acc: 0.3445\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6755 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 2.13194 to 2.11547, saving model to model/checkpoint/1D_CNN_1_conv_custom_DO_checkpoint/003-2.1155.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 1.6754 - acc: 0.5128 - val_loss: 2.1155 - val_acc: 0.3471\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4447 - acc: 0.5875\n",
      "Epoch 00004: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 1.4447 - acc: 0.5875 - val_loss: 2.1598 - val_acc: 0.3399\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2622 - acc: 0.6464\n",
      "Epoch 00005: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 1.2622 - acc: 0.6464 - val_loss: 2.1849 - val_acc: 0.3308\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1107 - acc: 0.6945\n",
      "Epoch 00006: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 1.1107 - acc: 0.6945 - val_loss: 2.2224 - val_acc: 0.3433\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9821 - acc: 0.7351\n",
      "Epoch 00007: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.9821 - acc: 0.7351 - val_loss: 2.2724 - val_acc: 0.3368\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8744 - acc: 0.7689\n",
      "Epoch 00008: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.8743 - acc: 0.7689 - val_loss: 2.3417 - val_acc: 0.3326\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.8021\n",
      "Epoch 00009: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.7734 - acc: 0.8021 - val_loss: 2.4044 - val_acc: 0.3375\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6964 - acc: 0.8262\n",
      "Epoch 00010: val_loss did not improve from 2.11547\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 0.6964 - acc: 0.8262 - val_loss: 2.4904 - val_acc: 0.3226\n",
      "Epoch 11/500\n",
      "11008/36805 [=======>......................] - ETA: 26s - loss: 0.5734 - acc: 0.8750"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_{}_conv_custom_DO'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_1_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 461us/sample - loss: 2.1419 - acc: 0.3321\n",
      "Loss: 2.141924336716641 Accuracy: 0.33208722\n",
      "\n",
      "1D_CNN_2_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 744us/sample - loss: 1.8082 - acc: 0.4434\n",
      "Loss: 1.808169612854812 Accuracy: 0.44340602\n",
      "\n",
      "1D_CNN_3_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 847us/sample - loss: 1.4653 - acc: 0.5412\n",
      "Loss: 1.4653394272022902 Accuracy: 0.5412253\n",
      "\n",
      "1D_CNN_4_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 904us/sample - loss: 1.0939 - acc: 0.6698\n",
      "Loss: 1.0938899715370107 Accuracy: 0.6697819\n",
      "\n",
      "1D_CNN_5_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 921us/sample - loss: 0.8304 - acc: 0.7537\n",
      "Loss: 0.8304384512940919 Accuracy: 0.75368637\n",
      "\n",
      "1D_CNN_6_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 942us/sample - loss: 0.4508 - acc: 0.8754\n",
      "Loss: 0.4507794922138424 Accuracy: 0.8753894\n",
      "\n",
      "1D_CNN_7_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 973us/sample - loss: 0.2110 - acc: 0.9360\n",
      "Loss: 0.21100966663932502 Accuracy: 0.93603325\n",
      "\n",
      "1D_CNN_8_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 993us/sample - loss: 0.1919 - acc: 0.9545\n",
      "Loss: 0.19189429886615908 Accuracy: 0.9545171\n",
      "\n",
      "1D_CNN_9_conv_custom_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1931 - acc: 0.9406\n",
      "Loss: 0.19314148334838518 Accuracy: 0.9406023\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_{}_conv_custom_DO'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4484 - acc: 0.2354\n",
      "Epoch 00001: val_loss improved from inf to 2.24971, saving model to model/checkpoint/1D_CNN_custom_DO_1_conv_checkpoint/001-2.2497.hdf5\n",
      "36805/36805 [==============================] - 36s 985us/sample - loss: 2.4483 - acc: 0.2355 - val_loss: 2.2497 - val_acc: 0.3336\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9852 - acc: 0.4145\n",
      "Epoch 00002: val_loss improved from 2.24971 to 2.14756, saving model to model/checkpoint/1D_CNN_custom_DO_1_conv_checkpoint/002-2.1476.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.9851 - acc: 0.4145 - val_loss: 2.1476 - val_acc: 0.3417\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6961 - acc: 0.5076\n",
      "Epoch 00003: val_loss improved from 2.14756 to 2.12015, saving model to model/checkpoint/1D_CNN_custom_DO_1_conv_checkpoint/003-2.1202.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.6960 - acc: 0.5076 - val_loss: 2.1202 - val_acc: 0.3468\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4715 - acc: 0.5777\n",
      "Epoch 00004: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.4715 - acc: 0.5777 - val_loss: 2.1574 - val_acc: 0.3447\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2885 - acc: 0.6351\n",
      "Epoch 00005: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.2885 - acc: 0.6351 - val_loss: 2.1678 - val_acc: 0.3375\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1395 - acc: 0.6833\n",
      "Epoch 00006: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.1395 - acc: 0.6833 - val_loss: 2.2218 - val_acc: 0.3394\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0105 - acc: 0.7241\n",
      "Epoch 00007: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 1.0107 - acc: 0.7241 - val_loss: 2.3193 - val_acc: 0.3296\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7538\n",
      "Epoch 00008: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.9070 - acc: 0.7538 - val_loss: 2.3592 - val_acc: 0.3303\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8070 - acc: 0.7859\n",
      "Epoch 00009: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.8072 - acc: 0.7858 - val_loss: 2.4288 - val_acc: 0.3282\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7292 - acc: 0.8101\n",
      "Epoch 00010: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.7291 - acc: 0.8101 - val_loss: 2.5043 - val_acc: 0.3319\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6544 - acc: 0.8350\n",
      "Epoch 00011: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.6543 - acc: 0.8350 - val_loss: 2.5319 - val_acc: 0.3308\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8585\n",
      "Epoch 00012: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.5865 - acc: 0.8584 - val_loss: 2.6055 - val_acc: 0.3359\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8760\n",
      "Epoch 00013: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.5280 - acc: 0.8760 - val_loss: 2.7125 - val_acc: 0.3226\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4790 - acc: 0.8900\n",
      "Epoch 00014: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.4791 - acc: 0.8900 - val_loss: 2.8089 - val_acc: 0.3322\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8981\n",
      "Epoch 00015: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.4420 - acc: 0.8981 - val_loss: 2.8770 - val_acc: 0.3205\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.9153\n",
      "Epoch 00016: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.3929 - acc: 0.9153 - val_loss: 2.9651 - val_acc: 0.3329\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.9226\n",
      "Epoch 00017: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.3605 - acc: 0.9226 - val_loss: 2.9834 - val_acc: 0.3294\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9327\n",
      "Epoch 00018: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.3268 - acc: 0.9327 - val_loss: 3.0907 - val_acc: 0.3277\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9379\n",
      "Epoch 00019: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.3070 - acc: 0.9379 - val_loss: 3.1413 - val_acc: 0.3312\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9460\n",
      "Epoch 00020: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2771 - acc: 0.9458 - val_loss: 3.2664 - val_acc: 0.3224\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9512\n",
      "Epoch 00021: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2546 - acc: 0.9512 - val_loss: 3.2730 - val_acc: 0.3319\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9583\n",
      "Epoch 00022: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2289 - acc: 0.9583 - val_loss: 3.4023 - val_acc: 0.3238\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9611\n",
      "Epoch 00023: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2144 - acc: 0.9610 - val_loss: 3.4824 - val_acc: 0.3233\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9661\n",
      "Epoch 00024: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1970 - acc: 0.9661 - val_loss: 3.5331 - val_acc: 0.3219\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9684\n",
      "Epoch 00025: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1863 - acc: 0.9684 - val_loss: 3.5572 - val_acc: 0.3226\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9722\n",
      "Epoch 00026: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.1704 - acc: 0.9722 - val_loss: 3.6619 - val_acc: 0.3168\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9753\n",
      "Epoch 00027: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1577 - acc: 0.9752 - val_loss: 3.7236 - val_acc: 0.3175\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9783\n",
      "Epoch 00028: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1415 - acc: 0.9783 - val_loss: 3.8017 - val_acc: 0.3159\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9797\n",
      "Epoch 00029: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1363 - acc: 0.9796 - val_loss: 3.8632 - val_acc: 0.3210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9804\n",
      "Epoch 00030: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1297 - acc: 0.9804 - val_loss: 3.9025 - val_acc: 0.3194\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9832\n",
      "Epoch 00031: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1192 - acc: 0.9831 - val_loss: 3.9979 - val_acc: 0.3149\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9833\n",
      "Epoch 00032: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1137 - acc: 0.9833 - val_loss: 4.0343 - val_acc: 0.3147\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9858\n",
      "Epoch 00033: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1057 - acc: 0.9858 - val_loss: 4.0351 - val_acc: 0.3175\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9861\n",
      "Epoch 00034: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1012 - acc: 0.9861 - val_loss: 4.1101 - val_acc: 0.3175\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9878\n",
      "Epoch 00035: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0941 - acc: 0.9878 - val_loss: 4.1773 - val_acc: 0.3124\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9887\n",
      "Epoch 00036: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0905 - acc: 0.9887 - val_loss: 4.1787 - val_acc: 0.3212\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9891\n",
      "Epoch 00037: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.0849 - acc: 0.9891 - val_loss: 4.3280 - val_acc: 0.3208\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9885\n",
      "Epoch 00038: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0857 - acc: 0.9885 - val_loss: 4.3705 - val_acc: 0.3152\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9903\n",
      "Epoch 00039: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0760 - acc: 0.9903 - val_loss: 4.3844 - val_acc: 0.3121\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9890\n",
      "Epoch 00040: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0816 - acc: 0.9889 - val_loss: 4.4228 - val_acc: 0.3173\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9904\n",
      "Epoch 00041: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0751 - acc: 0.9904 - val_loss: 4.4749 - val_acc: 0.3229\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9912\n",
      "Epoch 00042: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0709 - acc: 0.9912 - val_loss: 4.4936 - val_acc: 0.3140\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9932\n",
      "Epoch 00043: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0596 - acc: 0.9932 - val_loss: 4.5057 - val_acc: 0.3217\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9926\n",
      "Epoch 00044: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0627 - acc: 0.9926 - val_loss: 4.5778 - val_acc: 0.3103\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9929\n",
      "Epoch 00045: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0587 - acc: 0.9929 - val_loss: 4.6173 - val_acc: 0.3121\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9924\n",
      "Epoch 00046: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0604 - acc: 0.9924 - val_loss: 4.6828 - val_acc: 0.3147\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9940\n",
      "Epoch 00047: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0554 - acc: 0.9940 - val_loss: 4.7006 - val_acc: 0.3175\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9931\n",
      "Epoch 00048: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0570 - acc: 0.9931 - val_loss: 4.6778 - val_acc: 0.3170\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9939\n",
      "Epoch 00049: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0530 - acc: 0.9939 - val_loss: 4.7285 - val_acc: 0.3156\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9938\n",
      "Epoch 00050: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0511 - acc: 0.9938 - val_loss: 4.7806 - val_acc: 0.3152\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9938\n",
      "Epoch 00051: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0521 - acc: 0.9938 - val_loss: 4.7921 - val_acc: 0.3198\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9950\n",
      "Epoch 00052: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0459 - acc: 0.9951 - val_loss: 4.8322 - val_acc: 0.3140\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9942\n",
      "Epoch 00053: val_loss did not improve from 2.12015\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0471 - acc: 0.9942 - val_loss: 4.8755 - val_acc: 0.3159\n",
      "\n",
      "1D_CNN_custom_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 519us/sample - loss: 2.1382 - acc: 0.3319\n",
      "Loss: 2.1382002996383305 Accuracy: 0.33187956\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2575 - acc: 0.3009\n",
      "Epoch 00001: val_loss improved from inf to 1.95516, saving model to model/checkpoint/1D_CNN_custom_DO_2_conv_checkpoint/001-1.9552.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.2574 - acc: 0.3010 - val_loss: 1.9552 - val_acc: 0.4186\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7738 - acc: 0.4666\n",
      "Epoch 00002: val_loss improved from 1.95516 to 1.81058, saving model to model/checkpoint/1D_CNN_custom_DO_2_conv_checkpoint/002-1.8106.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.7737 - acc: 0.4666 - val_loss: 1.8106 - val_acc: 0.4510\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5183 - acc: 0.5481\n",
      "Epoch 00003: val_loss improved from 1.81058 to 1.78155, saving model to model/checkpoint/1D_CNN_custom_DO_2_conv_checkpoint/003-1.7816.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.5183 - acc: 0.5481 - val_loss: 1.7816 - val_acc: 0.4561\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3222 - acc: 0.6076\n",
      "Epoch 00004: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.3221 - acc: 0.6077 - val_loss: 1.7994 - val_acc: 0.4531\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1501 - acc: 0.6629\n",
      "Epoch 00005: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.1500 - acc: 0.6629 - val_loss: 1.8487 - val_acc: 0.4370\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0017 - acc: 0.7095\n",
      "Epoch 00006: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0017 - acc: 0.7095 - val_loss: 1.8894 - val_acc: 0.4393\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8746 - acc: 0.7468\n",
      "Epoch 00007: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8746 - acc: 0.7468 - val_loss: 1.9502 - val_acc: 0.4503\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7589 - acc: 0.7814\n",
      "Epoch 00008: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7589 - acc: 0.7814 - val_loss: 2.0160 - val_acc: 0.4442\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6619 - acc: 0.8126\n",
      "Epoch 00009: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6618 - acc: 0.8127 - val_loss: 2.1153 - val_acc: 0.4391\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8366\n",
      "Epoch 00010: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5830 - acc: 0.8366 - val_loss: 2.2166 - val_acc: 0.4305\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8604\n",
      "Epoch 00011: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5072 - acc: 0.8604 - val_loss: 2.2825 - val_acc: 0.4384\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.8774\n",
      "Epoch 00012: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4449 - acc: 0.8774 - val_loss: 2.3655 - val_acc: 0.4342\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8970\n",
      "Epoch 00013: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3846 - acc: 0.8971 - val_loss: 2.4609 - val_acc: 0.4258\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.9064\n",
      "Epoch 00014: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3491 - acc: 0.9064 - val_loss: 2.5710 - val_acc: 0.4237\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9194\n",
      "Epoch 00015: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3115 - acc: 0.9194 - val_loss: 2.6358 - val_acc: 0.4232\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9289\n",
      "Epoch 00016: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2736 - acc: 0.9289 - val_loss: 2.7819 - val_acc: 0.4225\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9366\n",
      "Epoch 00017: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2468 - acc: 0.9366 - val_loss: 2.8022 - val_acc: 0.4298\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9427\n",
      "Epoch 00018: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2238 - acc: 0.9427 - val_loss: 2.8309 - val_acc: 0.4314\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9514\n",
      "Epoch 00019: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1974 - acc: 0.9514 - val_loss: 2.9252 - val_acc: 0.4326\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9542\n",
      "Epoch 00020: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1866 - acc: 0.9542 - val_loss: 3.0430 - val_acc: 0.4309\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9594\n",
      "Epoch 00021: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1689 - acc: 0.9594 - val_loss: 3.1160 - val_acc: 0.4365\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9629\n",
      "Epoch 00022: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1535 - acc: 0.9629 - val_loss: 3.2007 - val_acc: 0.4384\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9657\n",
      "Epoch 00023: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1443 - acc: 0.9657 - val_loss: 3.1907 - val_acc: 0.4368\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9677\n",
      "Epoch 00024: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1337 - acc: 0.9677 - val_loss: 3.3600 - val_acc: 0.4333\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9704\n",
      "Epoch 00025: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1258 - acc: 0.9704 - val_loss: 3.3121 - val_acc: 0.4340\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9731\n",
      "Epoch 00026: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1152 - acc: 0.9731 - val_loss: 3.4039 - val_acc: 0.4333\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9762\n",
      "Epoch 00027: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1069 - acc: 0.9762 - val_loss: 3.4786 - val_acc: 0.4300\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9763\n",
      "Epoch 00028: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1040 - acc: 0.9763 - val_loss: 3.5182 - val_acc: 0.4235\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9776\n",
      "Epoch 00029: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0971 - acc: 0.9776 - val_loss: 3.6421 - val_acc: 0.4270\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9783\n",
      "Epoch 00030: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0931 - acc: 0.9783 - val_loss: 3.5722 - val_acc: 0.4274\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9783\n",
      "Epoch 00031: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0942 - acc: 0.9783 - val_loss: 3.6191 - val_acc: 0.4347\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9822\n",
      "Epoch 00032: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0821 - acc: 0.9822 - val_loss: 3.6836 - val_acc: 0.4293\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9820\n",
      "Epoch 00033: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0836 - acc: 0.9820 - val_loss: 3.7326 - val_acc: 0.4279\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9814\n",
      "Epoch 00034: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0807 - acc: 0.9814 - val_loss: 3.7515 - val_acc: 0.4312\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9830\n",
      "Epoch 00035: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0770 - acc: 0.9830 - val_loss: 3.7441 - val_acc: 0.4365\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9836\n",
      "Epoch 00036: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0756 - acc: 0.9836 - val_loss: 3.7723 - val_acc: 0.4349\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9850\n",
      "Epoch 00037: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0690 - acc: 0.9850 - val_loss: 3.8549 - val_acc: 0.4274\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9851\n",
      "Epoch 00038: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0677 - acc: 0.9851 - val_loss: 3.8214 - val_acc: 0.4384\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9860\n",
      "Epoch 00039: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0672 - acc: 0.9860 - val_loss: 3.8945 - val_acc: 0.4465\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9874\n",
      "Epoch 00040: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0621 - acc: 0.9874 - val_loss: 3.9000 - val_acc: 0.4389\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9873\n",
      "Epoch 00041: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0620 - acc: 0.9873 - val_loss: 3.9161 - val_acc: 0.4379\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9864\n",
      "Epoch 00042: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0629 - acc: 0.9864 - val_loss: 3.9609 - val_acc: 0.4393\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9875\n",
      "Epoch 00043: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0585 - acc: 0.9875 - val_loss: 3.8991 - val_acc: 0.4389\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9887\n",
      "Epoch 00044: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0547 - acc: 0.9887 - val_loss: 4.0272 - val_acc: 0.4419\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9883\n",
      "Epoch 00045: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0559 - acc: 0.9883 - val_loss: 3.9776 - val_acc: 0.4426\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9891\n",
      "Epoch 00046: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0528 - acc: 0.9891 - val_loss: 4.0076 - val_acc: 0.4361\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0529 - acc: 0.9895 - val_loss: 4.1638 - val_acc: 0.4356\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9887\n",
      "Epoch 00048: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0546 - acc: 0.9887 - val_loss: 4.1464 - val_acc: 0.4375\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9888\n",
      "Epoch 00049: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0541 - acc: 0.9888 - val_loss: 4.1743 - val_acc: 0.4365\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9886\n",
      "Epoch 00050: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0550 - acc: 0.9886 - val_loss: 4.1159 - val_acc: 0.4421\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9899\n",
      "Epoch 00051: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0486 - acc: 0.9899 - val_loss: 4.1788 - val_acc: 0.4398\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9904\n",
      "Epoch 00052: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0485 - acc: 0.9904 - val_loss: 4.1457 - val_acc: 0.4396\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9899\n",
      "Epoch 00053: val_loss did not improve from 1.78155\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0479 - acc: 0.9899 - val_loss: 4.1471 - val_acc: 0.4384\n",
      "\n",
      "1D_CNN_custom_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 659us/sample - loss: 1.8384 - acc: 0.4411\n",
      "Loss: 1.8384307471886354 Accuracy: 0.4411215\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1674 - acc: 0.3195\n",
      "Epoch 00001: val_loss improved from inf to 1.76617, saving model to model/checkpoint/1D_CNN_custom_DO_3_conv_checkpoint/001-1.7662.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 2.1673 - acc: 0.3195 - val_loss: 1.7662 - val_acc: 0.4559\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5951 - acc: 0.5156\n",
      "Epoch 00002: val_loss improved from 1.76617 to 1.52934, saving model to model/checkpoint/1D_CNN_custom_DO_3_conv_checkpoint/002-1.5293.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.5950 - acc: 0.5157 - val_loss: 1.5293 - val_acc: 0.5276\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3552 - acc: 0.5893\n",
      "Epoch 00003: val_loss improved from 1.52934 to 1.43920, saving model to model/checkpoint/1D_CNN_custom_DO_3_conv_checkpoint/003-1.4392.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.3552 - acc: 0.5893 - val_loss: 1.4392 - val_acc: 0.5516\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1937 - acc: 0.6342\n",
      "Epoch 00004: val_loss improved from 1.43920 to 1.39914, saving model to model/checkpoint/1D_CNN_custom_DO_3_conv_checkpoint/004-1.3991.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1937 - acc: 0.6342 - val_loss: 1.3991 - val_acc: 0.5709\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0698 - acc: 0.6739\n",
      "Epoch 00005: val_loss did not improve from 1.39914\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0697 - acc: 0.6739 - val_loss: 1.4216 - val_acc: 0.5635\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9593 - acc: 0.7089\n",
      "Epoch 00006: val_loss improved from 1.39914 to 1.39309, saving model to model/checkpoint/1D_CNN_custom_DO_3_conv_checkpoint/006-1.3931.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9592 - acc: 0.7089 - val_loss: 1.3931 - val_acc: 0.5756\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8627 - acc: 0.7388\n",
      "Epoch 00007: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8626 - acc: 0.7388 - val_loss: 1.4161 - val_acc: 0.5786\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7639\n",
      "Epoch 00008: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7787 - acc: 0.7639 - val_loss: 1.4219 - val_acc: 0.5802\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7880\n",
      "Epoch 00009: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6961 - acc: 0.7880 - val_loss: 1.5026 - val_acc: 0.5621\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8096\n",
      "Epoch 00010: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6336 - acc: 0.8096 - val_loss: 1.4639 - val_acc: 0.5884\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.8253\n",
      "Epoch 00011: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5669 - acc: 0.8253 - val_loss: 1.5165 - val_acc: 0.5828\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8438\n",
      "Epoch 00012: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5077 - acc: 0.8438 - val_loss: 1.5202 - val_acc: 0.5947\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8625\n",
      "Epoch 00013: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4545 - acc: 0.8625 - val_loss: 1.5874 - val_acc: 0.5865\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8724\n",
      "Epoch 00014: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4181 - acc: 0.8724 - val_loss: 1.6713 - val_acc: 0.5858\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8845\n",
      "Epoch 00015: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3798 - acc: 0.8844 - val_loss: 1.6741 - val_acc: 0.5886\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8919\n",
      "Epoch 00016: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3529 - acc: 0.8919 - val_loss: 1.7143 - val_acc: 0.5924\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9072\n",
      "Epoch 00017: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3086 - acc: 0.9072 - val_loss: 1.7563 - val_acc: 0.5900\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9109\n",
      "Epoch 00018: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2964 - acc: 0.9109 - val_loss: 1.7686 - val_acc: 0.6003\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9169\n",
      "Epoch 00019: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2720 - acc: 0.9169 - val_loss: 1.8146 - val_acc: 0.5973\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9238\n",
      "Epoch 00020: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2535 - acc: 0.9238 - val_loss: 1.8235 - val_acc: 0.6005\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9300\n",
      "Epoch 00021: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2337 - acc: 0.9300 - val_loss: 1.9154 - val_acc: 0.5968\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9347\n",
      "Epoch 00022: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2211 - acc: 0.9347 - val_loss: 1.9444 - val_acc: 0.5919\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9413\n",
      "Epoch 00023: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2000 - acc: 0.9413 - val_loss: 1.9939 - val_acc: 0.5959\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9436\n",
      "Epoch 00024: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1921 - acc: 0.9436 - val_loss: 2.0018 - val_acc: 0.5984\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9474\n",
      "Epoch 00025: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1819 - acc: 0.9474 - val_loss: 2.0271 - val_acc: 0.6087\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9493\n",
      "Epoch 00026: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1728 - acc: 0.9494 - val_loss: 2.0330 - val_acc: 0.6070\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9526\n",
      "Epoch 00027: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1664 - acc: 0.9526 - val_loss: 2.0809 - val_acc: 0.5991\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9553\n",
      "Epoch 00028: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1535 - acc: 0.9553 - val_loss: 2.1694 - val_acc: 0.5910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9574\n",
      "Epoch 00029: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1463 - acc: 0.9574 - val_loss: 2.0972 - val_acc: 0.6138\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9573\n",
      "Epoch 00030: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1464 - acc: 0.9573 - val_loss: 2.1399 - val_acc: 0.6112\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9612\n",
      "Epoch 00031: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1362 - acc: 0.9612 - val_loss: 2.1435 - val_acc: 0.6131\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9632\n",
      "Epoch 00032: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1339 - acc: 0.9632 - val_loss: 2.1817 - val_acc: 0.6038\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9629\n",
      "Epoch 00033: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1307 - acc: 0.9629 - val_loss: 2.2168 - val_acc: 0.6031\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9670\n",
      "Epoch 00034: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1185 - acc: 0.9670 - val_loss: 2.1886 - val_acc: 0.6103\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9666\n",
      "Epoch 00035: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1184 - acc: 0.9666 - val_loss: 2.2957 - val_acc: 0.6068\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9666\n",
      "Epoch 00036: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1177 - acc: 0.9666 - val_loss: 2.2255 - val_acc: 0.6136\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9684\n",
      "Epoch 00037: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1160 - acc: 0.9684 - val_loss: 2.2664 - val_acc: 0.6187\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9731\n",
      "Epoch 00038: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1018 - acc: 0.9731 - val_loss: 2.3110 - val_acc: 0.6096\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9707\n",
      "Epoch 00039: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1064 - acc: 0.9707 - val_loss: 2.3280 - val_acc: 0.6131\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9732\n",
      "Epoch 00040: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0980 - acc: 0.9732 - val_loss: 2.3330 - val_acc: 0.6101\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9725\n",
      "Epoch 00041: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0997 - acc: 0.9725 - val_loss: 2.3303 - val_acc: 0.6157\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9740\n",
      "Epoch 00042: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0936 - acc: 0.9740 - val_loss: 2.3018 - val_acc: 0.6198\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9715\n",
      "Epoch 00043: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1019 - acc: 0.9715 - val_loss: 2.3321 - val_acc: 0.6136\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9767\n",
      "Epoch 00044: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0884 - acc: 0.9767 - val_loss: 2.4011 - val_acc: 0.6087\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9763\n",
      "Epoch 00045: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0867 - acc: 0.9763 - val_loss: 2.3893 - val_acc: 0.6189\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9770\n",
      "Epoch 00046: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0869 - acc: 0.9770 - val_loss: 2.4058 - val_acc: 0.6177\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9777\n",
      "Epoch 00047: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0843 - acc: 0.9777 - val_loss: 2.3584 - val_acc: 0.6184\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9793\n",
      "Epoch 00048: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0796 - acc: 0.9793 - val_loss: 2.4531 - val_acc: 0.6082\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9794\n",
      "Epoch 00049: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0788 - acc: 0.9794 - val_loss: 2.3856 - val_acc: 0.6243\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9785\n",
      "Epoch 00050: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0789 - acc: 0.9785 - val_loss: 2.5275 - val_acc: 0.6077\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9794\n",
      "Epoch 00051: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0783 - acc: 0.9794 - val_loss: 2.4186 - val_acc: 0.6231\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9811\n",
      "Epoch 00052: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0746 - acc: 0.9811 - val_loss: 2.4672 - val_acc: 0.6129\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9804\n",
      "Epoch 00053: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0753 - acc: 0.9804 - val_loss: 2.4832 - val_acc: 0.6210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9801\n",
      "Epoch 00054: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0765 - acc: 0.9801 - val_loss: 2.4070 - val_acc: 0.6236\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9806\n",
      "Epoch 00055: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0763 - acc: 0.9806 - val_loss: 2.3999 - val_acc: 0.6292\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9810\n",
      "Epoch 00056: val_loss did not improve from 1.39309\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0744 - acc: 0.9810 - val_loss: 2.4538 - val_acc: 0.6191\n",
      "\n",
      "1D_CNN_custom_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 778us/sample - loss: 1.4752 - acc: 0.5562\n",
      "Loss: 1.4751907923758834 Accuracy: 0.5561786\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1258 - acc: 0.3139\n",
      "Epoch 00001: val_loss improved from inf to 1.61264, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/001-1.6126.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 2.1257 - acc: 0.3139 - val_loss: 1.6126 - val_acc: 0.4934\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5169 - acc: 0.5221\n",
      "Epoch 00002: val_loss improved from 1.61264 to 1.37533, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/002-1.3753.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5170 - acc: 0.5220 - val_loss: 1.3753 - val_acc: 0.5828\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3152 - acc: 0.5947\n",
      "Epoch 00003: val_loss improved from 1.37533 to 1.25545, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/003-1.2555.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3152 - acc: 0.5947 - val_loss: 1.2555 - val_acc: 0.6145\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1747 - acc: 0.6397\n",
      "Epoch 00004: val_loss improved from 1.25545 to 1.18085, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/004-1.1808.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1746 - acc: 0.6397 - val_loss: 1.1808 - val_acc: 0.6529\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.6752\n",
      "Epoch 00005: val_loss improved from 1.18085 to 1.11536, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/005-1.1154.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0677 - acc: 0.6752 - val_loss: 1.1154 - val_acc: 0.6648\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.7041\n",
      "Epoch 00006: val_loss did not improve from 1.11536\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9725 - acc: 0.7041 - val_loss: 1.1196 - val_acc: 0.6459\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.7338\n",
      "Epoch 00007: val_loss improved from 1.11536 to 1.01954, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/007-1.0195.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8830 - acc: 0.7338 - val_loss: 1.0195 - val_acc: 0.6813\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.7573\n",
      "Epoch 00008: val_loss did not improve from 1.01954\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8038 - acc: 0.7572 - val_loss: 1.0336 - val_acc: 0.6825\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7294 - acc: 0.7777\n",
      "Epoch 00009: val_loss improved from 1.01954 to 0.98069, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/009-0.9807.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7299 - acc: 0.7777 - val_loss: 0.9807 - val_acc: 0.6981\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6548 - acc: 0.8010\n",
      "Epoch 00010: val_loss did not improve from 0.98069\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6548 - acc: 0.8010 - val_loss: 1.0185 - val_acc: 0.6930\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6000 - acc: 0.8188\n",
      "Epoch 00011: val_loss did not improve from 0.98069\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6001 - acc: 0.8188 - val_loss: 0.9904 - val_acc: 0.7011\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8337\n",
      "Epoch 00012: val_loss did not improve from 0.98069\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5482 - acc: 0.8337 - val_loss: 1.0014 - val_acc: 0.6997\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8467\n",
      "Epoch 00013: val_loss improved from 0.98069 to 0.97515, saving model to model/checkpoint/1D_CNN_custom_DO_4_conv_checkpoint/013-0.9752.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4950 - acc: 0.8467 - val_loss: 0.9752 - val_acc: 0.7177\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8621\n",
      "Epoch 00014: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4503 - acc: 0.8621 - val_loss: 1.0527 - val_acc: 0.7011\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8732\n",
      "Epoch 00015: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4127 - acc: 0.8732 - val_loss: 1.0319 - val_acc: 0.7079\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8831\n",
      "Epoch 00016: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3804 - acc: 0.8831 - val_loss: 1.0650 - val_acc: 0.7065\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8916\n",
      "Epoch 00017: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3522 - acc: 0.8916 - val_loss: 1.0383 - val_acc: 0.7186\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8995\n",
      "Epoch 00018: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3231 - acc: 0.8994 - val_loss: 1.0631 - val_acc: 0.7154\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9069\n",
      "Epoch 00019: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2958 - acc: 0.9069 - val_loss: 1.0771 - val_acc: 0.7188\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9146\n",
      "Epoch 00020: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2763 - acc: 0.9145 - val_loss: 1.0759 - val_acc: 0.7172\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9173\n",
      "Epoch 00021: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2621 - acc: 0.9173 - val_loss: 1.0919 - val_acc: 0.7282\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9224\n",
      "Epoch 00022: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2481 - acc: 0.9224 - val_loss: 1.1491 - val_acc: 0.7212\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9275\n",
      "Epoch 00023: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2281 - acc: 0.9275 - val_loss: 1.1017 - val_acc: 0.7272\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9320\n",
      "Epoch 00024: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2183 - acc: 0.9320 - val_loss: 1.1448 - val_acc: 0.7303\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9350\n",
      "Epoch 00025: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2022 - acc: 0.9350 - val_loss: 1.1651 - val_acc: 0.7321\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9383\n",
      "Epoch 00026: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1940 - acc: 0.9383 - val_loss: 1.1624 - val_acc: 0.7345\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9413\n",
      "Epoch 00027: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1864 - acc: 0.9413 - val_loss: 1.2080 - val_acc: 0.7195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9433\n",
      "Epoch 00028: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1753 - acc: 0.9434 - val_loss: 1.1835 - val_acc: 0.7291\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9455\n",
      "Epoch 00029: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1663 - acc: 0.9455 - val_loss: 1.2515 - val_acc: 0.7233\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9484\n",
      "Epoch 00030: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1647 - acc: 0.9484 - val_loss: 1.2139 - val_acc: 0.7298\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9522\n",
      "Epoch 00031: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1559 - acc: 0.9522 - val_loss: 1.2086 - val_acc: 0.7386\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9502\n",
      "Epoch 00032: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1548 - acc: 0.9502 - val_loss: 1.2535 - val_acc: 0.7270\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9530\n",
      "Epoch 00033: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1475 - acc: 0.9530 - val_loss: 1.2311 - val_acc: 0.7372\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9565\n",
      "Epoch 00034: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1408 - acc: 0.9566 - val_loss: 1.2733 - val_acc: 0.7354\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9578\n",
      "Epoch 00035: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1337 - acc: 0.9578 - val_loss: 1.2289 - val_acc: 0.7331\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9582\n",
      "Epoch 00036: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1310 - acc: 0.9582 - val_loss: 1.2559 - val_acc: 0.7352\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9574\n",
      "Epoch 00037: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1341 - acc: 0.9574 - val_loss: 1.3148 - val_acc: 0.7424\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9590\n",
      "Epoch 00038: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1272 - acc: 0.9591 - val_loss: 1.3171 - val_acc: 0.7354\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9644\n",
      "Epoch 00039: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1175 - acc: 0.9644 - val_loss: 1.2610 - val_acc: 0.7414\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9649\n",
      "Epoch 00040: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1129 - acc: 0.9649 - val_loss: 1.3027 - val_acc: 0.7368\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9626\n",
      "Epoch 00041: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1171 - acc: 0.9626 - val_loss: 1.2951 - val_acc: 0.7440\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9657\n",
      "Epoch 00042: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1110 - acc: 0.9657 - val_loss: 1.3028 - val_acc: 0.7466\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9652\n",
      "Epoch 00043: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1095 - acc: 0.9652 - val_loss: 1.2872 - val_acc: 0.7508\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9666\n",
      "Epoch 00044: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1078 - acc: 0.9666 - val_loss: 1.2894 - val_acc: 0.7494\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9682\n",
      "Epoch 00045: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1064 - acc: 0.9682 - val_loss: 1.3374 - val_acc: 0.7431\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9671\n",
      "Epoch 00046: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1059 - acc: 0.9671 - val_loss: 1.3365 - val_acc: 0.7475\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9699\n",
      "Epoch 00047: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0962 - acc: 0.9699 - val_loss: 1.3063 - val_acc: 0.7496\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9698\n",
      "Epoch 00048: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0992 - acc: 0.9698 - val_loss: 1.3479 - val_acc: 0.7480\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9704\n",
      "Epoch 00049: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0964 - acc: 0.9704 - val_loss: 1.3353 - val_acc: 0.7540\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9726\n",
      "Epoch 00050: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0941 - acc: 0.9726 - val_loss: 1.2904 - val_acc: 0.7561\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9709\n",
      "Epoch 00051: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0952 - acc: 0.9709 - val_loss: 1.3360 - val_acc: 0.7501\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9722\n",
      "Epoch 00052: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0908 - acc: 0.9722 - val_loss: 1.3435 - val_acc: 0.7480\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9725\n",
      "Epoch 00053: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 1.3485 - val_acc: 0.7512\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9745\n",
      "Epoch 00054: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0855 - acc: 0.9745 - val_loss: 1.3408 - val_acc: 0.7494\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9730\n",
      "Epoch 00055: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0890 - acc: 0.9730 - val_loss: 1.3934 - val_acc: 0.7554\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9734\n",
      "Epoch 00056: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0883 - acc: 0.9734 - val_loss: 1.2901 - val_acc: 0.7575\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9735\n",
      "Epoch 00057: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0857 - acc: 0.9735 - val_loss: 1.3507 - val_acc: 0.7554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9743\n",
      "Epoch 00058: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0837 - acc: 0.9743 - val_loss: 1.3172 - val_acc: 0.7598\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9735\n",
      "Epoch 00059: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0874 - acc: 0.9735 - val_loss: 1.2900 - val_acc: 0.7612\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9765\n",
      "Epoch 00060: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0797 - acc: 0.9765 - val_loss: 1.3859 - val_acc: 0.7529\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9768\n",
      "Epoch 00061: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0779 - acc: 0.9768 - val_loss: 1.3675 - val_acc: 0.7552\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9771\n",
      "Epoch 00062: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0793 - acc: 0.9771 - val_loss: 1.3691 - val_acc: 0.7538\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9770\n",
      "Epoch 00063: val_loss did not improve from 0.97515\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0771 - acc: 0.9770 - val_loss: 1.3890 - val_acc: 0.7556\n",
      "\n",
      "1D_CNN_custom_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 785us/sample - loss: 1.0685 - acc: 0.6881\n",
      "Loss: 1.0684766499795646 Accuracy: 0.68805814\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1114 - acc: 0.3073\n",
      "Epoch 00001: val_loss improved from inf to 1.62311, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/001-1.6231.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.1113 - acc: 0.3074 - val_loss: 1.6231 - val_acc: 0.4680\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5184 - acc: 0.5172\n",
      "Epoch 00002: val_loss improved from 1.62311 to 1.37160, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/002-1.3716.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5185 - acc: 0.5172 - val_loss: 1.3716 - val_acc: 0.5653\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2959 - acc: 0.5985\n",
      "Epoch 00003: val_loss improved from 1.37160 to 1.18479, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/003-1.1848.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2959 - acc: 0.5986 - val_loss: 1.1848 - val_acc: 0.6434\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1385 - acc: 0.6508\n",
      "Epoch 00004: val_loss improved from 1.18479 to 1.08039, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/004-1.0804.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1385 - acc: 0.6508 - val_loss: 1.0804 - val_acc: 0.6753\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6854\n",
      "Epoch 00005: val_loss improved from 1.08039 to 0.97341, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/005-0.9734.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0298 - acc: 0.6854 - val_loss: 0.9734 - val_acc: 0.7021\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9386 - acc: 0.7143\n",
      "Epoch 00006: val_loss improved from 0.97341 to 0.93565, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/006-0.9357.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9385 - acc: 0.7143 - val_loss: 0.9357 - val_acc: 0.7140\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8601 - acc: 0.7386\n",
      "Epoch 00007: val_loss improved from 0.93565 to 0.87499, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/007-0.8750.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8600 - acc: 0.7386 - val_loss: 0.8750 - val_acc: 0.7368\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7898 - acc: 0.7601\n",
      "Epoch 00008: val_loss improved from 0.87499 to 0.84520, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/008-0.8452.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7897 - acc: 0.7601 - val_loss: 0.8452 - val_acc: 0.7454\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7253 - acc: 0.7815\n",
      "Epoch 00009: val_loss improved from 0.84520 to 0.79237, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/009-0.7924.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7253 - acc: 0.7815 - val_loss: 0.7924 - val_acc: 0.7631\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6739 - acc: 0.7967\n",
      "Epoch 00010: val_loss did not improve from 0.79237\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6738 - acc: 0.7967 - val_loss: 0.7987 - val_acc: 0.7673\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8126\n",
      "Epoch 00011: val_loss improved from 0.79237 to 0.76156, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/011-0.7616.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6205 - acc: 0.8126 - val_loss: 0.7616 - val_acc: 0.7759\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8245\n",
      "Epoch 00012: val_loss did not improve from 0.76156\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5752 - acc: 0.8245 - val_loss: 0.8393 - val_acc: 0.7473\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8398\n",
      "Epoch 00013: val_loss improved from 0.76156 to 0.74495, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/013-0.7449.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5235 - acc: 0.8398 - val_loss: 0.7449 - val_acc: 0.7883\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8510\n",
      "Epoch 00014: val_loss improved from 0.74495 to 0.73225, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/014-0.7322.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4875 - acc: 0.8510 - val_loss: 0.7322 - val_acc: 0.7885\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8617\n",
      "Epoch 00015: val_loss did not improve from 0.73225\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4493 - acc: 0.8617 - val_loss: 0.8383 - val_acc: 0.7643\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8725\n",
      "Epoch 00016: val_loss improved from 0.73225 to 0.72118, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/016-0.7212.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4159 - acc: 0.8725 - val_loss: 0.7212 - val_acc: 0.7976\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8803\n",
      "Epoch 00017: val_loss did not improve from 0.72118\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3871 - acc: 0.8803 - val_loss: 0.7232 - val_acc: 0.7985\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8905\n",
      "Epoch 00018: val_loss did not improve from 0.72118\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3599 - acc: 0.8905 - val_loss: 0.7427 - val_acc: 0.7962\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8964\n",
      "Epoch 00019: val_loss improved from 0.72118 to 0.72040, saving model to model/checkpoint/1D_CNN_custom_DO_5_conv_checkpoint/019-0.7204.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3323 - acc: 0.8964 - val_loss: 0.7204 - val_acc: 0.7987\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9033\n",
      "Epoch 00020: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3080 - acc: 0.9033 - val_loss: 0.7409 - val_acc: 0.8008\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9105\n",
      "Epoch 00021: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2875 - acc: 0.9105 - val_loss: 0.7621 - val_acc: 0.7976\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9154\n",
      "Epoch 00022: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2707 - acc: 0.9154 - val_loss: 0.7642 - val_acc: 0.8006\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9176\n",
      "Epoch 00023: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2555 - acc: 0.9176 - val_loss: 0.7743 - val_acc: 0.7913\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9270\n",
      "Epoch 00024: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2339 - acc: 0.9270 - val_loss: 0.8195 - val_acc: 0.7936\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9282\n",
      "Epoch 00025: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2236 - acc: 0.9281 - val_loss: 0.8196 - val_acc: 0.7911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9316\n",
      "Epoch 00026: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2161 - acc: 0.9316 - val_loss: 0.8165 - val_acc: 0.7976\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9355\n",
      "Epoch 00027: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2023 - acc: 0.9355 - val_loss: 0.8019 - val_acc: 0.8020\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9410\n",
      "Epoch 00028: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1891 - acc: 0.9410 - val_loss: 0.8016 - val_acc: 0.8069\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9438\n",
      "Epoch 00029: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1763 - acc: 0.9438 - val_loss: 0.8476 - val_acc: 0.7987\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9442\n",
      "Epoch 00030: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1738 - acc: 0.9442 - val_loss: 0.8533 - val_acc: 0.8041\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9462\n",
      "Epoch 00031: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1667 - acc: 0.9462 - val_loss: 0.8482 - val_acc: 0.8020\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9490\n",
      "Epoch 00032: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1563 - acc: 0.9491 - val_loss: 0.8223 - val_acc: 0.8132\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9515\n",
      "Epoch 00033: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1504 - acc: 0.9515 - val_loss: 0.8324 - val_acc: 0.8102\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9527\n",
      "Epoch 00034: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1470 - acc: 0.9527 - val_loss: 0.8737 - val_acc: 0.8018\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9540\n",
      "Epoch 00035: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1441 - acc: 0.9540 - val_loss: 0.8314 - val_acc: 0.8123\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9533\n",
      "Epoch 00036: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1430 - acc: 0.9533 - val_loss: 0.8528 - val_acc: 0.8053\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9593\n",
      "Epoch 00037: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1272 - acc: 0.9593 - val_loss: 0.8703 - val_acc: 0.8088\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9584\n",
      "Epoch 00038: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1303 - acc: 0.9584 - val_loss: 0.9232 - val_acc: 0.8074\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9555\n",
      "Epoch 00039: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1367 - acc: 0.9554 - val_loss: 0.9104 - val_acc: 0.7997\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9596\n",
      "Epoch 00040: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1237 - acc: 0.9596 - val_loss: 0.8750 - val_acc: 0.8104\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9630\n",
      "Epoch 00041: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1183 - acc: 0.9630 - val_loss: 0.8865 - val_acc: 0.8069\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9626\n",
      "Epoch 00042: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1178 - acc: 0.9626 - val_loss: 0.9203 - val_acc: 0.8041\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9616\n",
      "Epoch 00043: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1219 - acc: 0.9616 - val_loss: 0.8863 - val_acc: 0.8127\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9642\n",
      "Epoch 00044: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1143 - acc: 0.9642 - val_loss: 0.8654 - val_acc: 0.8190\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9686\n",
      "Epoch 00045: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1035 - acc: 0.9686 - val_loss: 0.9170 - val_acc: 0.8062\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9649\n",
      "Epoch 00046: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1093 - acc: 0.9650 - val_loss: 0.9267 - val_acc: 0.8109\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9690\n",
      "Epoch 00047: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1015 - acc: 0.9689 - val_loss: 0.9009 - val_acc: 0.8185\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9680\n",
      "Epoch 00048: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1037 - acc: 0.9680 - val_loss: 0.9141 - val_acc: 0.8116\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9692\n",
      "Epoch 00049: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0992 - acc: 0.9692 - val_loss: 0.9257 - val_acc: 0.8106\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9698\n",
      "Epoch 00050: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0974 - acc: 0.9698 - val_loss: 0.9074 - val_acc: 0.8148\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9704\n",
      "Epoch 00051: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0971 - acc: 0.9704 - val_loss: 0.9702 - val_acc: 0.8099\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9713\n",
      "Epoch 00052: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0943 - acc: 0.9713 - val_loss: 0.9266 - val_acc: 0.8104\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9710\n",
      "Epoch 00053: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0976 - acc: 0.9710 - val_loss: 0.9132 - val_acc: 0.8160\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9723\n",
      "Epoch 00054: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0880 - acc: 0.9723 - val_loss: 0.9571 - val_acc: 0.8164\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9722\n",
      "Epoch 00055: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0873 - acc: 0.9722 - val_loss: 0.9124 - val_acc: 0.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9721\n",
      "Epoch 00056: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0887 - acc: 0.9721 - val_loss: 0.9569 - val_acc: 0.8232\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9746\n",
      "Epoch 00057: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0845 - acc: 0.9746 - val_loss: 0.8882 - val_acc: 0.8234\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9723\n",
      "Epoch 00058: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0861 - acc: 0.9723 - val_loss: 0.9224 - val_acc: 0.8248\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9739\n",
      "Epoch 00059: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0847 - acc: 0.9739 - val_loss: 0.9372 - val_acc: 0.8244\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9754\n",
      "Epoch 00060: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0817 - acc: 0.9754 - val_loss: 0.9942 - val_acc: 0.8146\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9742\n",
      "Epoch 00061: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0845 - acc: 0.9742 - val_loss: 0.9410 - val_acc: 0.8157\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9752\n",
      "Epoch 00062: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0791 - acc: 0.9752 - val_loss: 0.9634 - val_acc: 0.8213\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9763\n",
      "Epoch 00063: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0778 - acc: 0.9763 - val_loss: 0.9753 - val_acc: 0.8139\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9761\n",
      "Epoch 00064: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0791 - acc: 0.9761 - val_loss: 0.9411 - val_acc: 0.8290\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9747\n",
      "Epoch 00065: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0817 - acc: 0.9747 - val_loss: 0.9636 - val_acc: 0.8176\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9770\n",
      "Epoch 00066: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0762 - acc: 0.9770 - val_loss: 0.9398 - val_acc: 0.8218\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9772\n",
      "Epoch 00067: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0765 - acc: 0.9772 - val_loss: 0.8955 - val_acc: 0.8288\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9777\n",
      "Epoch 00068: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0729 - acc: 0.9777 - val_loss: 0.9683 - val_acc: 0.8199\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9786\n",
      "Epoch 00069: val_loss did not improve from 0.72040\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0713 - acc: 0.9786 - val_loss: 1.0334 - val_acc: 0.8162\n",
      "\n",
      "1D_CNN_custom_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmWSy7yFsIRBEdgIBAqKooFZEUcQqol/3tVqXqi0VrT+lVetatViXUktFq6KiVm2puAGpAmpA9kW2AAmQfd8z8/z+OJMQIAkBMkxInvfrdV+Tueszk+Q8995z7jlGRFBKKaUOx+HrAJRSSp0YNGEopZRqEU0YSimlWkQThlJKqRbRhKGUUqpFNGEopZRqEU0YSimlWkQThlJKqRbRhKGUUqpF/H0dQGvq1KmTJCYm+joMpZQ6YaxYsSJXROJasm67ShiJiYmkpaX5OgyllDphGGN2tnRdvSWllFKqRTRhKKWUahFNGEoppVqkXdVhNKampoaMjAwqKyt9HcoJKSgoiB49euB0On0dilLKx9p9wsjIyCA8PJzExESMMb4O54QiIuTl5ZGRkUHv3r19HY5Sysfa/S2pyspKYmNjNVkcBWMMsbGxenWmlAI6QMIANFkcA/3ulFJ1OkTCaI6IUFW1h9raIl+HopRSbVqHTxjGGKqrs7yWMAoLC3n55ZePatsLLriAwsLCFq8/c+ZMnn322aM6llJKHU6HTxgAxvgjUuuVfTeXMGprmz/mggULiIqK8kZYSil1xDRh4N2EMWPGDLZt20ZycjLTp09n8eLFnHHGGUyePJlBgwYBMGXKFEaOHMngwYOZPXt2/baJiYnk5uaSnp7OwIEDueWWWxg8eDATJkygoqKi2eOuWrWKMWPGMHToUC655BIKCgoAmDVrFoMGDWLo0KFcccUVACxZsoTk5GSSk5MZPnw4JSUlXvkulFIntnbfrLahLVvuobR01SHz3e4KQHA4Qo54n2FhyfTt+0KTy5988knWrVvHqlX2uIsXL2blypWsW7euvqnqnDlziImJoaKiglGjRnHppZcSGxt7UOxbeOedd/jb3/7G5ZdfzgcffMDVV1/d5HGvvfZaXnzxRcaNG8fDDz/M73//e1544QWefPJJduzYQWBgYP3trmeffZaXXnqJsWPHUlpaSlBQ0BF/D0qp9k+vMAAwiMhxO9ro0aMPeK5h1qxZDBs2jDFjxrB79262bNlyyDa9e/cmOTkZgJEjR5Kent7k/ouKiigsLGTcuHEAXHfddaSmpgIwdOhQrrrqKv75z3/i72/PF8aOHct9993HrFmzKCwsrJ+vlFINdaiSoakrgcrK3dTU5BAePuK4xBEaGlr/8+LFi/nyyy9ZtmwZISEhjB8/vtHnHgIDA+t/9vPzO+wtqab85z//ITU1lU8//ZTHH3+ctWvXMmPGDCZNmsSCBQsYO3YsCxcuZMCAAUe1f6VU+6VXGNg6DHAj4m71fYeHhzdbJ1BUVER0dDQhISFs2rSJ5cuXH/MxIyMjiY6O5n//+x8Ab775JuPGjcPtdrN7927OOussnnrqKYqKiigtLWXbtm0kJSVx//33M2rUKDZt2nTMMSil2h+vJQxjTIIxZpExZoMxZr0x5leNrGOMMbOMMVuNMWuMMSMaLLvOGLPFM13nrTjtseyFljcqvmNjYxk7dixDhgxh+vTphyyfOHEitbW1DBw4kBkzZjBmzJhWOe7cuXOZPn06Q4cOZdWqVTz88MO4XC6uvvpqkpKSGD58OHfffTdRUVG88MILDBkyhKFDh+J0Ojn//PNbJQalVPtivHXv3hjTDegmIiuNMeHACmCKiGxosM4FwF3ABcApwJ9F5BRjTAyQBqQA4tl2pIgUNHfMlJQUOXgApY0bNzJw4MBmY62pKaCychshIYPw8zvyiu/2riXfoVLqxGSMWSEiKS1Z12tXGCKyV0RWen4uATYC8QetdjHwhljLgShPojkP+EJE8j1J4gtgordi9eYVhlJKtRfHpQ7DGJMIDAe+O2hRPLC7wfsMz7ym5nspPk0YSil1OF5PGMaYMOAD4B4RKfbC/m81xqQZY9JycnKOch+aMJRS6nC8mjCMMU5ssnhLRD5sZJVMIKHB+x6eeU3NP4SIzBaRFBFJiYuLO8o4NWEopdTheLOVlAH+DmwUkeeaWO0T4FpPa6kxQJGI7AUWAhOMMdHGmGhggmeet2IF/DRhKKVUM7z54N5Y4BpgrTGmrj+OB4GeACLyKrAA20JqK1AO3OBZlm+MeRT4wbPdH0Qk34uxevqTqvHmIZRS6oTmtYQhIt8AzY6+I7ZN7x1NLJsDzPFCaI3yZgeERyosLIzS0tIWz1dKqeNBn/T2aEsJQyml2iJNGB7GOL2SMGbMmMFLL71U/75ukKPS0lLOOeccRowYQVJSEh9//HGL9ykiTJ8+nSFDhpCUlMS7774LwN69eznzzDNJTk5myJAh/O9//8PlcnH99dfXr/v888+3+mdUSnUMHarzQe65B1Yd2r05QKC7CrdUI37hzd9HO1hyMrzQdPfm06ZN45577uGOO+ydt/fee4+FCxcSFBTERx99REREBLm5uYwZM4bJkye3aAztDz/8kFWrVrF69Wpyc3MZNWoUZ555Jm+//TbnnXcev/vd73C5XJSXl7Nq1SoyMzNZt24dwBGN4KeUUg11rITRrLqCWjhM1csRGT58ONnZ2ezZs4ecnByio6NJSEigpqaGBx98kNTUVBwOB5mZmWRlZdG1a9fD7vObb77hyiuvxM/Pjy5dujBu3Dh++OEHRo0axY033khNTQ1TpkwhOTmZk046ie3bt3PXXXcxadIkJkyY0GqfTSnVsXSshNHMlUBtdS5VVemEhiZhHIFNrnc0pk6dyvz589m3bx/Tpk0D4K233iInJ4cVK1bgdDpJTExstFvzI3HmmWeSmprKf/7zH66//nruu+8+rr32WlavXs3ChQt59dVXee+995gz57i1JVBKtSNah+HhzYf3pk2bxrx585g/fz5Tp04FbLfmnTt3xul0smjRInbu3Nni/Z1xxhm8++67uFwucnJySE1NZfTo0ezcuZMuXbpwyy23cPPNN7Ny5Upyc3Nxu91ceumlPPbYY6xcubLVP59SqmPoWFcYzfBmwhg8eDAlJSXEx8fTrVs3AK666iouuugikpKSSElJOaIBiy655BKWLVvGsGHDMMbw9NNP07VrV+bOncszzzyD0+kkLCyMN954g8zMTG644QbcbjvWxxNPPNHqn08p1TF4rXtzXzja7s0BXK5KysvXERTUG6cz9rDrdyTavblS7Veb6N78RKP9SSmlVPM0YXgY4wdowlBKqaZowvAwxujT3kop1QxNGA1owlBKqaZpwmhAE4ZSSjVNE0YDmjCUUqppmjAa8EbCKCws5OWXXz6qbS+44ALt+0kp1WZowmigLmG05rMpzSWM2trmk9OCBQuIiopqtViUUupYeHOI1jnGmGxjzLomlk83xqzyTOuMMS5jTIxnWboxZq1nWVpj23snZn9s54OuVtvnjBkz2LZtG8nJyUyfPp3FixdzxhlnMHnyZAYNGgTAlClTGDlyJIMHD2b27Nn12yYmJpKbm0t6ejoDBw7klltuYfDgwUyYMIGKiopDjvXpp59yyimnMHz4cH72s5+RlZUFQGlpKTfccANJSUkMHTqUDz74AIDPPvuMESNGMGzYMM4555xW+8xKqfbJa096G2POBEqBN0RkyGHWvQi4V0TO9rxPB1JEJPdIjnm4J72b6d0cAJEa3O5K/PxCaWkuPUzv5qSnp3PhhRfWdy++ePFiJk2axLp16+jduzcA+fn5xMTEUFFRwahRo1iyZAmxsbEkJiaSlpZGaWkpJ598MmlpaSQnJ3P55ZczefJkrr766gOOVVBQQFRUFMYYXnvtNTZu3Mif/vQn7r//fqqqqnjBE2hBQQG1tbWMGDGC1NRUevfuXR9DY/RJb6XaryN50tubQ7SmGmMSW7j6lcA73oql5Wy35iJCC4alOGqjR4+uTxYAs2bN4qOPPgJg9+7dbNmyhdjYA7sn6d27N8nJyQCMHDmS9PT0Q/abkZHBtGnT2Lt3L9XV1fXH+PLLL5k3b179etHR0Xz66aeceeaZ9es0lSyUUqqOzzsfNMaEABOBOxvMFuBzY4wAfxWR2Y1ubLe/FbgVoGfPns0eq7krAajrT2ozwcF98fePbFH8RyM0NLT+58WLF/Pll1+ybNkyQkJCGD9+fKPdnAcG7u9y3c/Pr9FbUnfddRf33XcfkydPZvHixcycOdMr8SulOqa2UOl9EfCtiOQ3mHe6iIwAzgfu8NzeapSIzBaRFBFJiYuLO6ZA9vcnVXNM+2koPDyckpKSJpcXFRURHR1NSEgImzZtYvny5Ud9rKKiIuLj4wGYO3du/fxzzz33gGFiCwoKGDNmDKmpqezYsQOwt8WUUqo5bSFhXMFBt6NEJNPzmg18BIw+HoF4owPC2NhYxo4dy5AhQ5g+ffohyydOnEhtbS0DBw5kxowZjBkz5qiPNXPmTKZOncrIkSPp1KlT/fyHHnqIgoIChgwZwrBhw1i0aBFxcXHMnj2bn//85wwbNqx+YCellGqKV7s399Rh/LupSm9jTCSwA0gQkTLPvFDAISIlnp+/AP4gIp8d7njH0r052LqL0tKVBAR0ITCwR4u26Qi00lup9qtNVHobY94BxgOdjDEZwCOAE0BEXvWsdgnweV2y8OgCfGRsrbM/8HZLkkUrxaxPeyulVBO82Urqyhas8zrw+kHztgPDvBPV4WnCUEqpxrWFOow2RROGUko1ThPGQTRhKKVU4zRhHEQThlJKNU4TxkG80QGhUkq1B5owDrL/WYzW64DwSIWFhfns2Eop1RRNGCJQUQFVVYB3Ht5TSqn2QBMGwIYNkJ0NtH7CmDFjxgHdcsycOZNnn32W0tJSzjnnHEaMGEFSUhIff/zxYffVVDfojXVT3lSX5kopdbR83vng8XTPZ/ewal8j/ZuXl4MxEByMiAu3uxyHI7g+eTQnuWsyL0xsulfDadOmcc8993DHHXcA8N5777Fw4UKCgoL46KOPiIiIIDc3lzFjxjB58mRMM93kzpkz54Bu0C+99FLcbje33HLLAd2UAzz66KNERkaydu1awPYfpZRSx6JDJYwmORzgGf1uf4HdOpXew4cPJzs7mz179pCTk0N0dDQJCQnU1NTw4IMPkpqaisPhIDMzk6ysLLp27drkvhrrBj0nJ6fRbsob69JcKaWORYdKGE1eCWRlwe7dMHQo4vSjtPRHAgJ6EBjYdOF9JKZOncr8+fPZt29ffSd/b731Fjk5OaxYsQKn00liYmKj3ZrXaWk36Eop5S1ahwEQEmJfKyqwX4lp1UrvadOmMW/ePObPn8/UqVMB2xV5586dcTqdLFq0iJ07dza7j6a6QW+qm/LGujRXSqljoQkDIDjYvpaXe6UDwsGDB1NSUkJ8fDzdunUD4KqrriItLY2kpCTeeOMNBgwY0Ow+muoGvaluyhvr0lwppY6FV7s3P96OqXvzNWsgLAxOOomysvUYE0hIyMleivTEot2bK9V+HUn35nqFUSc42LaWQrsHUUqpxmjCqBMSApWV4HZrwlBKqUZ4LWEYY+YYY7KNMeuaWD7eGFNkjFnlmR5usGyiMWazMWarMWbGscbSottudfUYFRUY49SE4dGeblkqpY6NN68wXgcmHmad/4lIsmf6A4Axxg94CTgfGARcaYwZdLRBBAUFkZeXd/iCr0FLKfvAXi0i7qM9bLsgIuTl5REUFOTrUJRSbYA3R9xL9YzpfaRGA1s9I+9hjJkHXAxsOJo4evToQUZGBjk5Oc2vKAJ5eVBVhSsyiJqaXAIC1uBwBB7NYduNoKAgevTQ8c2VUr5/cO9UY8xqYA/wGxFZD8QDuxuskwGc0tQOjDG3ArcC9OzZ85DlTqez/inow7rpJggMpPKzuSxfnkLfvi8TH397Sz+LUkq1a76s9F4J9BKRYcCLwL+OZiciMltEUkQkJS4u7tgiGjYMVq8mMCABpzOOkpIfjm1/SinVjvgsYYhIsYiUen5eADiNMZ2ATCChwao9PPO8b9gwKCzEZGQQHj5KE4ZSSjXgs4RhjOlqPD39GWNGe2LJA34A+hpjehtjAoArgE+OS1DDhtnX1asJDx9FWdkGamtLj8uhlVKqrfNaHYYx5h1gPNDJGJMBPAI4AUTkVeAy4HZjTC1QAVwhtilTrTHmTmAh4AfM8dRteF9Skn1dvZqI00YBbkpLVxIVdeZxObxSSrVl3mwldeVhlv8F+EsTyxYAC7wRV7PCw6FPH88Vxq0AlJT8oAlDKaXQJ70P5an4DgjoTGBgT4qLtR5DKaVAE8ahhg2DrVuhtFQrvpVSqgFNGAcbNsw+xLd2LRERo6is3E5NTZ6vo1JKKZ/ThHGwg1pKAZSUpDWzgVJKdQyaMA7WqxdERnoSxkgArcdQSik0YRzKGBg6FFavxt8/kpCQAZSUfO/rqJRSyuc0YTRm2DA7Ap/LVV/xrd18K6U6Ok0YjRk9GsrKYO1awsNHUV29j6qq49M7iVJKtVWaMBozbpx9XbKkQcW31mMopTo2TRiN6dkTEhNhyRLCwpIxxl8ThlKqw9OE0ZTx4yE1FT8TQGhokiYMpVSHpwmjKePG2RH41q/3VHynacW3UqpD04TRlIPqMWprC6mo2OrbmJRSyoc0YTQlMdHWZSxZQkSEVnwrpZQmjKYYY68yliwhJHgQfn7hFBYu8XVUSinlM15LGMaYOcaYbGPMuiaWX2WMWWOMWWuMWWqMGdZgWbpn/ipjjO86cho3DnJycGzeQnT0ueTnL9B6DKVUh+XNK4zXgYnNLN8BjBORJOBRYPZBy88SkWQRSfFSfIc3frx9XbKE2NhJVFVlUFa2xmfhKKWUL3ktYYhIKpDfzPKlIlLgebsc6OGtWI7aSSdBfDwsWUJMzAUA5OX9x8dBKaWUb7SVOoybgP82eC/A58aYFcaYW30U0/56jMWLCQzoQljYSPLy/u2zcJRSypd8njCMMWdhE8b9DWafLiIjgPOBO4wxTQ6qbYy51RiTZoxJy8nJaf0Ax42DrCz46SdiYydRXLyc6urc1j+OUkq1cT5NGMaYocBrwMUiUj+snYhkel6zgY+A0U3tQ0Rmi0iKiKTExcW1fpAH1GNcCAj5+Z+1/nGUUqqN81nCMMb0BD4ErhGRnxrMDzXGhNf9DEwAGm1pdVz07Qtdu3oe4BuJ09mF/Hytx1BKdTz+3tqxMeYdYDzQyRiTATwCOAFE5FXgYSAWeNkYA1DraRHVBfjIM88feFtEfHdK36Aew2CIjT2f3Nx/4XbX4nB47etTSqk2x2slnohceZjlNwM3NzJ/OzDs0C18aPx4ePdd2LaNmJhJ7Nv3OsXFy4iKOsPXkSml1HHj80rvE0Jdv1KLFhETcy7G+GvzWqVUh6MJoyUGDICTT4Y5c/D3jyQy8gxtXquU6nA0YbSEMXD33bB8OSxfTmzsJMrL11NZudPXkSml1HHTooRhjPmVMSbCWH83xqw0xkzwdnBtyg03QGQkPP88MTGTAH3qWynVsbT0CuNGESnGNnGNBq4BnvRaVG1RWBjceivMn09IThBBQX00YSilOpSWJgzjeb0AeFNE1jeY13HcdRcYg/nLX4iNvZCCgq+oqWmyuyyllGpXWpowVhhjPscmjIWeB+vc3gurjUpIgKlT4W9/o1vYFYhUsXfv330dlVJKHRctTRg3ATOAUSJSjn0A7wavRdWW3XsvFBcT9t53REaOY8+elxFx+ToqpZTyupYmjFOBzSJSaIy5GngIKPJeWG3Y6NFw2mnw5z8T3/V2KivTtYmtUqpDaGnCeAUo94yK92tgG/CG16Jq6+67D3bsoNNSPwIDe5CR8aKvI1JKKa9racKoFTs26cXAX0TkJSDce2G1cVOmQGIijhf+TPfut1NY+BVlZRt9HZVSSnlVSxNGiTHmAWxz2v8YYxx4OhLskPz87IN833xD972jMSaQzMy/+DoqpZTyqpYmjGlAFfZ5jH3Y4VSf8VpUJ4Ibb4TQUJyvvEnnzlewb99cams7ZrWOUqpjaFHC8CSJt4BIY8yFQKWIdNw6DLBPfV9/PcybRw/nlbjdZezb97qvo1JKKa9padcglwPfA1OBy4HvjDGXeTOwE8Jdd0F1NeHvfE9ExBgyM/+CSMd7PEUp5UPFxZCdfVwO1dJbUr/DPoNxnYhcix0y9f8dbiNjzBxjTLYxptER8zx9U80yxmw1xqwxxoxosOw6Y8wWz3RdC+M8vvr3h4kT4eWXiY+7nYqKrTp8q1Lq+LrrLhgxAkpLvX6oliYMh2d87Tp5Ldz2dWBiM8vPB/p6pluxzXcxxsRgR+g7BZucHjHGRLcw1uPrV7+CffuIWwKBgT3YufNxbIMypZTysnffhTfegJtvtv3deVlLE8ZnxpiFxpjrjTHXA/8BFhxuIxFJBZrrbOli4A2xlgNRxphuwHnAFyKSLyIFwBc0n3h8Z8IE6NcPx4sv0bPngxQXL6Wg4HNfR6WUau927YLbboMxY+Chh47LIVta6T0dmA0M9UyzReT+Vjh+PLC7wfsMz7ym5rc9DodtYvv993TbNYTAwJ7s2PGIXmUo1V7s2XNcbvccEZcLrr0Wamvhn/8Ef6+Ntn2AFg+gJCIfiMh9nukjbwZ1JIwxtxpj0owxaTk5Ob4J4tprISICx4uv0KvXQ5SUfEd+/n99E4tSqvVs3GhH3Dz1VChqQ83mn3kGliyBF1+EPn2O22GbTUvGmBKgsVNlA4iIRBzj8TOBhAbve3jmZQLjD5q/uLEdiMhs7NUPKSkpvjmtDw+Hm26CF1+k69NPsCsokfT0R4iJOR9jOl4v8EodDRF74lxbC263HejSGHsRD1BdDVVV+yeXp8/PuvXqprp5YNcrLt4/lZdDSIj9l62bACoq7LKKCjtVVkJlfhmVj7xNpesO2FBJ4KmvE/jrOwgM8cfphJoaG9MhU3kN1dszqS2vwvTrh3GY+s/hdEJQEAQG2ldj7MVLaSmUlNjX2lr7XdRNbrf9rPVTbgE1Xw+kpusKat8ZTs0btpX/R8fhNL7ZhCEi3u7+4xPgTmPMPGwFd5GI7DXGLAT+2KCiewLwgJdjOTZ33gmzZuG4bBq9X7mTjUW/IS/v33TqdJGvI1PtnNsNhYWQlwf5+bbAcTgOnAIC7OR02qmycn8hWlQEBQWwbx/s3Wtfs7IgOBg6dYLYWPsaGAg5ObYFZ3a2/VnE7q9u38bsL9jrXmtr9xd2dT/Xv1a7cJVXUeMIpMbl5+uv8iChwKP7324Ebm7Jdk4MPfGnFkwtboc/Igb3YVrcBwVBaOj+77FhwvTz80wON34783D698XZpR/OYoPTuT+peptXb3wZY97BXil0MsZkYFs+OQFE5FVsxfkFwFagHE+X6SKSb4x5FPjBs6s/iEjbHqnopJPg/ffhmmvofOFush/tTnrYI8TGXqhXGe2QiD0jrTtrbXjm29gkYm8z100ul62z3L4dduywr+XlBxa+gYEQFXXgVFFhC/O6KTvbFvatVWUWGQldu0Lnzna/W7ZAbq79nGALtS5d7PLOnW1BVVOz/2zb7bZxh4TY14AA+3n9/Pa/1v3sTy1+772DX1kWTlcNzonn4DxtNP7+dr8Nz7JF7L4CAz3T8iX4U4ucfc4B69T9bhBBlqQSFBVE+M9OISICIiJsEiwvt2fzdZMxdn5wsI07KAiCX3mO4Dl/IeipPxB409UAVD32DFUvvEzVPTOoufEXNgmv/oGAl1/AueQLAqkiYEh/As45A7+fnQW7d9uOSqOi4K234Oyzqa2Fyt05VL3wClWvvYmrvJLwscMIfXQGzrNOP/wv6IHfwZNPwpdfwjmDWueXfgRMe6qcTUlJkbS0NN8GsWYNXHwxsjeDTb+updM9HxEXN8W3MXVwbrct9PLz7Zl4UZF9LS/ff7lf91peDmVl+6eSErtu3VRUZAvPkhIOe8bYEhER0Lu3ncLD9xe+NTX2KqAu1sJCW4DXFdhdu+4vuGNj7RQTY6eAABtb3VRbe2ChXl1t9xMZaY8fGWnLtC5dbIHZmLrtQkP33+45Zr/8Jbzyir2X8vrr8PHH8Mc/wgPN3EwQgYcfhsces+8ffND+3DAoETv/ySftl7FiBQwZ0vK43nkH/u//bAukV145cL/XXGML/0cegcWLbT1CXBz8+tdwww32F9LQmjUwbRps3mxjqqiw+6ystPNHjoQ//cle1p13Hjz6KIwa1Xhca9bY5y2uvRbmzGn55zkMY8wKEUlp0bqaMLwgNxe57FLMklT2XN+ZbnP2YExbu9w+8VVU2NsweXkH3irJzra3VnbtstPu3fYs/0gEB9vCMTz8wDP8ukI2IsIui4jYfzbd3GTM/lsxtbX2fUICREe3YgF8IvngA7jsMvjNb2wFbk2NLXDfegvuvx+eeOLQL0bEnrG/8IJ97sDhgNmz7aBmf/qTXV/Evv/zn+G662DBAujZE5Yts5dtTRGxFdwffmiT1siR8NVXNuE0VFVlm9KnpkL37vDb38IttzSdacFWTNx5J8yday+vrrrKJo/+/e3y8nJ4+WWb4PLy7D6ffPLAz+9y2Yr39HTYtMmeGbSSI0kYiEi7mUaOHCltRnW1lF91tghI/nPX+jqaE4bbLbJrl8hHH4n87nciF1wgctppIikpIkOHigwYIJKQIBIScvANi/2Tv79IfLzIqaeKXHGFyG9/K/LiiyJvvy2yYIHIt9+KrFsnsmOHPVZGhsjevSJZWSIlJSIul6+/BREpKxP54AOR3buPfV8VFUe3XW2t/aL+8Q+R228XufVWka1bW7btrl0ib7whcvfdIvPmiVRW7l+2Y4dIZKTIqFEiVVX757tcIrfdZn+JU6aIvP++SGHh/lhuvtku+9Wv7B+K2233Dza+2lqRX/ziwHXmz7fvH3208Tg3bBCZMUOkX7/9f0Djx4vs29f0Zysqsn+gDT9TS3zxRfPfX3Gx/Y5B5Je/PPAP8c9/tvPffvvIjtkCQJq0sIz1eSHfmlObShgi4q6uluKUSKkVbij+AAAgAElEQVQNRKpX/M/X4fhUaanImjW2wJ49W+SRR+z//2WXiUycKHL66SLJySKdO+//v/XzE0lKEvnZz2zimDJF5PLLRa67TuS++0T++EeRv/7VlgmpqSKbNonk5bWRAv9Y1NSIXHjh/i9i0CCRe+8V+eyzAwvYw8nNFbn4YpHgYPtFud0t2664WOTnPxcJC9sfQ0SE3Y/Tab/8/PwDt8nLE3nnHZGbbhLp02f/dk6nfe3USeTXv7YJ6NRT7f62bTv02G63yMyZIlFR+7P/+PEi551n3z/00IGfw+0WmT7dLuvb177OmHHgOldeaeNYterA7Z57zs738xM591yRl18W2bOn5d+vNzT8PDfcYJPgzp0ioaH2H6Wlv8MjoAmjDSnd+pVUxiBVvaPsP2I75nLZv+0vvhB56SWRu+4SmTBBpGdPOeQqwBiRLl1EBg4UGT1a5OyzRSZPtv8jL74osmyZSHm5rz+RD7jd9my57qz4mWdsxgwIsPOGDBHZvPnw+1myRKRHD1sgjhplt506VaSgoPntqqps4ennZ+OYO1dk40b7y83MFLnxRvvLi44WefppG+Opp4o4HPYYUVE2ST3/vC2ga2psorv0Ulv41/0BzJvXfBw1NSL/+58t/IcOtZ/jqaea/s4eftju9/e/P7RQzc21f2zJyfbz5eaKXHSRXX/yZHtp2ZbUJU2wl8gXXGAvqXfs8MrhNGG0MbvfnCJuB1J96XleOUM4ntxuW1598om9Sr73XnvmP3SoPQFtmBTCw+2tpKuvFnnsMZF33xVZutQmlepqX3+SNurpp+2X99vfHji/rMwWsrGx9ot9773Gt6+psZdvDoc9416xwhb2Tz1lC+zERJHlyxvf1uUSueoqe/x//KPpGFevtkmlLvOPGmUL7GXL7BlxU/buFXnySZtMjlRNzeHXyc1tetm//mXjvfpqm0gDAuwfcFv+f6z7WwCRP/3Ja4fRhNHGVFfnyY5b7U1394sv+jqcI5KVJfL55yJ/+IPI+eeLxMQcmBRCQuwdk0mTbPJ49VWRr7+29QJt+X+xTZo3z36p06Y1fV9t1y6RMWOk/j59VZWteFm4UOTBB23mBpFrrz30inbZMpsw/P1F7rlHJD39wOV1t0Ief/zwsbrdNnG0tbPz5lxzjf18J58skpbm62ha5rXX7FVdSxLmUdKE0QZl7n5Fck9B3E4/W2HWxrjd9s7D3/4mcscd9rZxp077E4MxIoMH23qH116zJ6lZWZoUDrFvn70tcqQF6ddf27Pe008/fCV1VZVNFmBr9+tu9fj52WTy1ltNb1tQYO/7+fvb9a+80l6FvPCC1Fe2ttdfalGRyCuv2FdV70gShjarPU5EXKz6egT9bttI6NYauPxymDXLNn73gaoq2zz922/hm2/sa16eXRYebputDx5sX4cMgZQU26RUNWPLFjs+yvbt0LcvfP45JCY2vX5xMbz3HvzjH7B0KfTrZ19jY1t2vPnz7bbJyTBuHJx2Wsu7uN692/79/fWv9qESgJ//3Mbjp03AOxJ9DqONKipayqrvxzJkwWnEvpxmG/o//7x9EMfLjfGLivYnh2++ge+/3/9sQr9+MHYsnH66nfr27QDPBojYXkh37rRt29PT7fvoaOjRw07x8fbLaa6NfZ3vv4dJk+zPjz0GM2bYhzk+//zQh8ZWrLCF9fvv24dJBg60zyDceGPLk0VrKSqC116DbdvsswzBwcf3+Mrn9DmMNmzz5ttk0SKHFH/3jsjYsVLf5vxYa4FLSw9oAVNSYpvx3323bRxijNS3UjzlFNvC8cMPRbJ+KjzGT3QCqqg4sNlq3RQVtb+1T90UGysya1bzv58FC2xlTu/e+1swrV0r0r273ee339rbPJ9/LnLOOVLfIuAXv7D39trrLSB1QkDrMNqumpoiWbo0Qb77bqDUVpfZZpN1ba6PtuCorhZJSpLifiPl7X+65JJLRIKC7G6Dg20ZNXOmvU1eVtZgu08/tfexv/++VT7bCaG83Lb1Nca27FmwwD68VVpql9fU2Iflli+3LZHqCvh+/WxLm7rfUUGBrWieMcN+hyNG2FZADe3YYVsqBQeLDBtm99Otm239ovfROxS32y1r9q2RjKKMFm9T66qVrXlbZfW+1VJVewTP3xyhI0kYekvKB/LzF7JmzUR69nyQk056HGbOhN//3k4PP9zi/bhcsHo1LPr9Er7+pJSvOIcqgujeHS691Pa8MGbMob0b1DvzTPjf/+ztkFbsm6ZVLV9ubxvVnfOD7bmuruOk6Gh7Gye8BR0rV1TAxRfbjttee83eAjocEdu9xG9+Y7tkGDXKdvWwcaNdbgxcdJEdxKaxGLKzYfJke+vnN7+Bq6+2fYW0EcVVxWzL30ZlbSUju48kwK+pP5YDFVUWsSV/C7HBsfSM7Imfw/v1HiKCS1y4xY1b3LjcLlziwuW28yprK0kvTGdbwTa2F2xnW8E2CisLbWGHLfDc4qbKVUVlbSWVtZVU1VbhFjcO48DP4YfDOAhxhjC863BGx49mdPxoBnQagMMc2B1srbuWPSV72Fm4k51FO9ldtJvIoEgGdhrIwLiBdAntgiB8l/Ed8zfMZ/7G+ewq2gXAyTEnM67XOMYnjmdw3GByy3PJKssiqzSLvaV72Vawjc25m9lWsI1qVzUAToeTwZ0HM7zrcIZ3HU5CZAJxIXF0CulEXGgcUUFRh8TYUlqHcQLYtOlG9u17g5Ejvyc8bLgttOfOtZ2wXXddk9uVlsInn9iueBYtsh3SAQwI3cWEoFSmhi/ktC1zcfgf5o9n5UrbX05srC1I9+61HSO1JS++aEczbIkhQ+CSS2DKFBg+/IBKGJfbRX5BJjFX3oTfl1/B3/9uv+8Wqqip4Lud35L60Qus2JpKYFAY0bHxRMf3ITpxIAmd+zK0y1D6x/bH6be/v6Ks0iyW7l7Kst1LqXbX0D+2P/079adfbD/iw+OpqK0grzyPvIo8cstzcbldBPkHEeQfRKB/IOEB4fSK6oW/49BOpYurilmesZzNuZuJDIokNjiWTiGdiA2JJToomsigyAO2yy3PZdW+Vazet5rVWav5Ke8nthVsI7c8t36dsIAwzko8i/P6nMe5fc7F6XCyt3Qve0r2sKdkD+mF6azPWc/67PVklmTWbxfoF0jf2L70i+1HQkQCAX4BOB1OnH5O/B3+5Ffks690X32hWOuuJT4invhwO3UP746fw68+AbjFTUFFATuLbGG8s3AnmSWZ1LprW/w7MxgSIhOICY7BYRw4jAODwWEcBPoH2u/YL5BA/0D8jF/9cV1uF0VVRazcu5LiKttFb3hAODHBMVS7qqlyVVFVW0VFbQVuabr3yeigaAL8AsgqyyLAL4AJfSYwpf8UiqqKWLJzCak7UymsLDxku0C/QHpH97Z/K56/lxBnCKv3rWblvpX8uPdHcsoPHSguLiSO7OnZLf5+DviuNGG0fTU1Bfzww2CczjhGjvwBRy220nTxYvjvf+FnP7NdjWZmUrlxB59t68s7i7vx6ae2fI+Ptw1yztr8Kmd99yTd139hk8AVV9jK1Msuaz6A666zWeejj2xnaq+8ArfdRk5ZDlvzt1LjrqHGVUONuwa3uAl1hhIeGE5YQBjhAeG4xEVRZRHFVcX1U1lNGaXVpZRWl1JWXUagfyCxwbHEBMcQExxDsDOYosoiCisLKagsoKiyiPiIeFK6pzCg04ADC8bnnrM9gF5yyf7eSOum6mqbKQsKKMnNZNu+DWSuXMK+9PXsDRP2dQtnT0IUmc4KMv3K2edXgcsIwTUwKDSRpEHjSeqcRExwDLuLdrOraBe7inext2Qv/g5/wgLC6qc9JXv4PvN7atw1GAwDOg2wBVplAQUVBdS4a+pDDvALYFDcIBKjElmTtYbtBdvr5/s7/CmvKa9ft66QOpwAvwD6x/ZncOfBDOw0kOyybL7Z9Q1rs9c2W2ABhDpDiQyKxC1u9pXuq5/fPbw7AzsNpE90H/rE9OGk6JNwGAdfbPuChdsWsqNwR6P7C/YPZmDcQAbHDWZQ3CD6x/YnryKPzbmb2Zxnp70le+v/duo+X4gzhK5hXeka1pUuoV3wc/iRWZxJZkkme0r2NJoIHMZBfHg8vaJ60SuyFwkRCQT5B9UX/nVXA37Gr/7nAL8Aekb2pE90HxKjEgn0P/orObe4+SnvJ77L+I4f9vxASXUJgX6BBPgFEOgXSIgzhITIBHpF9qJXVC96RvakoKKADTkb2Ji7kQ05GyiuKmZS30lc2O9CIoMObGLocrtYm72WbfnbiAuNo0toF7qGdSUiMKLZ4RBEhH2l+9hbupfc8lxyynLILc/FLW7uPfXeo/qsmjBOELm5n7Bu3cUkJs4kMfERe9vijDNgxw4q4k/ms219mV97MZ9yESVEEBdewdRrgrniCtuqybHsW9us6YEHbA+bLpdtCxsYiHvlChbtXEJJdQlucddflid1TqJ/bST06gW33mpb6yQng78/b/3jPm7/z+2UVJcc82czGKTRwRobF+IMIblrMn1j+lK+diUlm9dS0j2W0t7xBDqDiAiMICIwgsjASFziYlv+NrbkbyG77NCzquhaJ91KIL7cj/gyB/GlDuKq/dl53imsjXWxNmstWWVZ9et3Du1Mz8iedA/vjsvtOiDxRQVFcUbPMziz15mMTRhLdHB0/XYiQnlNOTsKd7B632rWZK1hddZqdhTuYEjnIZzW4zROSziNEd1GEOAXQGZJJptzN/NT3k/sKtpFRGBE/VVBbHAsTj9n/W2SytpKCisL6wuf9TnrSS9MJ9QZypgeYzi95+mMTRhLUpckSqtLD7hSKawspLCysD45u3EzJG4IyV2TGdZ1GJ1COjX5exARtuZvZVH6Ivwd/nQP714/1Z2tt1TdGXvDq67G1skrz0OQAxJAsH9ws9up1tNmEoYxZiLwZ8APeE1Enjxo+fPAWZ63IUBnEYnyLHMBaz3LdonI5MMd70RLGAAbNlxNTs67DB++lIiIUSz7OJtZd2zi032jKHMFExtawSVn5HFZ1kuc8+Mz+P/yF8hzz1HqqiB87Fl2oIdNm2wTXYA336Tyxmu59tnTeL9w6SHHMxiukaHM/PNqen+3Gfr1o3jWM9y5+Le8OQzGJozlgdMfIMg/CKefE6fDicM4KKspo6SqhJLqEkqqSnD6OesL8YjACMIDwg84Mw/yD6LaVU1BZQH5FfnkV+RTXlNOZGAk0cHRRAdFEx4YTnphOml70lixZwVpe9NI37WGsJxiwsNiCB84jLDAcKpd1QdczQhCn+g+nBxzMifHnEyf6D4kRCbQLawbXcK6EOQfdNjvPbssm+KqYnpE9GjR+m1BeU15/dWKUq2lTSQMYweA+Ak4F8jAjp53pYhsaGL9u4DhInKj532piLTwKSTrREwYNTWFfP99MsuXT+Bf/3qZb77xJzrajq0y6ZJS+g7PIq8yi605m1n14Uus3r2CVQn+5DtrmbYOnr/kr3T7v1vr91dQksOU+3uS2qWSP579OBNPPt/evzUGt7h5e9WbvPjtc7gchltG386F/S7kzn//kvTCdB4uS+F3Ty87fgWS220fdvvuO1u5vWwZrFplB6n5xz/0ATKljoO2kjBOBWaKyHme9w8AiMgTTay/FHhERL7wvG/3CaOmxg7u9cQTZWzaFEpc35WMuPET0kPfI6NkF2U1ZQesH+QfxFD/eIYtTye0wsUrpzgIDA7jj2f/kdtSbiOzJJPz3zqfLTmbmTvfxZVP/QcuuODAg86dy567rufRZy7ktezPqHXX0jOyJ29tGMjp85bayu+6q5XWUlAAr74K//63fbq54aj3dU8PhofD6NF21LH77tNkodRx0lYSxmXARBG52fP+GuAUEbmzkXV7AcuBHiK2pswYUwusAmqBJ0XkX4c75omSMEpLbavO556D3dnFJJz9H4JOe4ItrrUYDGf3PpuhXYbWV4R1CetCr8he9I3ta8/+f/wRHnuMLQ/+gtvXP81XO75iVPdRZJZkUlpdyr8unc9Z595ix/Fctmx/iyERO8RjTQ2sXcu2gu18vPljbki+gei0dbaZ7Zw5R9SCqFm7dtnR0f72N/uhx4yxo5SFhdkpNNSOOjZmDAwYoElCKR84ERPG/dhkcVeDefEikmmMOQn4GjhHRLY1su2twK0APXv2HLlz506vfJ7WkF/g5q4XvuDD776jMnI1Qb1XURlsW9IkRiVyXpdazokp4qIz1xIU1KtF+xQR3ln3DvcuvBenw8l/r/ovSV2SbB9Bt90GJ51kW0FNmGCHqLzoIjus5S23HLwjGDTIPtew9NC6D8COQ/zuu7ZZapcutvuLkSMPXW/lStvlybx5dr9XXAHTp8OwYUfydSmljoM20TUIcCqwsMH7B4AHmlj3R+C0Zvb1OnDZ4Y7ZVp/0Li6plSsee0ccdw0SZiI8YqTn0/1k6ntT5fHUx2XRjkXicrukvHybpKaGy8qVp4vLdWTdGZdVl0lJVcn+GS6XHdruoosOHDktJuagx70bePZZu866dQfOT0+3TzTHxtrl/fvbITbB9nn+zTd2HIR//Utk3Dg7Pyys8S60lVJtCm2haxDAH9gO9AYCgNXA4EbWGwCk47na8cyLBgI9P3cCtgCDDnfMtpYwyipq5Lo//VP8fjVAmImE/XawPPnvdw4s2A+yd++bsmgR8tNPd4u7tfoYqqoSWbzYDm/ZXNfq2dl2ZLMuXewwebGx+/sYcThELrlE5KuvbPcYhYV2jNS6PtDrkknPnjbxFHbAPqqUOgEdScLwWnMYEak1xtwJLMQ2q50jIuuNMX/wBPiJZ9UrgHmewOsMBP5qjHEDDmwdRqOtq9oSt7hZk7WGr3d8zfwVX/PdvlTczhJC/JOYMeR9fvfznx+2HXvXrldTWrqSjIznCQyMp2fP3x57YAEBtvvrceOaXy8uDp55xnYXEhpqp7Aw6NTJ3lbq2XP/upGR9vmPu++2dRSpqXadn//cdt2hlGp39MG9VuAWN3NXzeWhRQ+xp2SPnZnbj7Dcs7l30iQe+b8L8HO0/IEnETcbN15FdvY8Bgx4g65dr/FS5Eqpju5I6jD0VPAYLdu9jLs/u5u0PWn0DTqV8C+fpGzdWfz6lh488trRtVA1xsGAAa9TXZ3N5s03EhDQmZiY81o/eKWUOgJH172hYm/JXq7+8GpOm3MamcV7GJv1T7bM+JaB1dewcnEPnn762B5ncDgCGTLkI0JCBrNu3aUUF7f95sJKqfZNE8ZReG/9ewx5ZQjzN8znlgG/I/z1zSx99SpmzjQsXdp6rUf9/SMYOvS/BATEsXbtJCoqGu8UTimljgdNGEegoKKAqz68imnzp3Fy9Mn8v06r+ecNj1GUE8aXX8Ijj7T+s2eBgd1ISvovIjWsXXsBNTUFrXsApZRqIU0YLfTV9q9IeiWJd9e9y+/H/54zt37LQ7f357TTbPdHZ5/tvWOHhg5gyJCPqKjYxvr1P8ftrvbewZRSqgmaMFpgY85Gzn/rfMIDw1l+83IiVz3Ms0/7c/vtsHCh7YHD26KixjFgwD8oLFzM5s03055atymlTgzaSuowRIS7P7ub0IBQUq9PJfWzOO69147r8+KLx7f7oy5drqKiYgfp6f+PoKCT6N175vE7uFKqw9OEcRgfbfqIL7d/yayJs9j8YxxXXQWnngpvveWbvvJ69fodlZXb2bnz9zidnejR45CuuZRSyis0YTSjvKacexfeS1LnJM4Kv50zT7cD1X3yCQQH+yYmYwz9+v2Vmpp8tm69C5EaEhKObmhGpZQ6EpowmvHUN0+xq2gX/522mMkX+ON0wmefQWysb+NyOJwMHvw+GzZcybZt9yFS0zpdiCilVDO00rsJ2wu289S3T3HlkCtZPm8cO3bA++9D796+jsxyOJwMGvQOcXHT2L79fnbufNzXISml2jm9wmjCfQvvw9/hz71DnmHcNXD55XZ8obbE4XAycOA/McafHTseQsRNYuL/83VYSql2ShNGIxZuXcjHmz/myXOeZNZj8bjd8NRTvo6qcQ6HPwMHzsUYP9LTHyYgoDPdu//C12EppdohTRgHEREeWfwIiVGJjPW7hxn/tL14Jyb6OrKmGeNH//6vUVOTy08//RKnswtxcVN8HZZSqp3ROoyDLE5fzHeZ3zH9tN9y/28C6dLFJoy2zlaEv0d4eAobN15JUVETw6wqpdRR0oRxkD9+80e6hHYhfOsNLF0Kjz8O4eG+jqpl/PxCSUr6N4GBCaxdexFlZZt8HZJSqh3xasIwxkw0xmw2xmw1xsxoZPn1xpgcY8wqz3Rzg2XXGWO2eKbrvBlnnbQ9aXy5/UvuSrmP//dAEMnJcP31x+PIrScgII6hQz/DGCdr1pxHeflWX4eklGonvJYwjDF+wEvA+cAg4EpjzKBGVn1XRJI902uebWOAR4BTgNHAI8aYaG/FWueJb54gKiiKwLW3sXMnPPecb57mPlbBwScxdOgCXK5SVqxIITf3k8NvpJRSh+HNK4zRwFYR2S4i1cA84OIWbnse8IWI5ItIAfAFMNFLcQK2g8EPN37InaPuZNniCE46Cc46y5tH9K7w8BGMHLmC4OCTWbfuYrZvfwgRl6/DUkqdwLyZMOKB3Q3eZ3jmHexSY8waY8x8Y0zCEW6LMeZWY0yaMSYtJyfnqIN96tunCHGGcPcpv2LZMttf1IkuODiR4cO/oVu3m9m163HWrDmf6upcX4ellDpB+brS+1MgUUSGYq8i5h7pDkRktoikiEhKXFzcUQWxs3Anb619i1tG3EJFXif27oUxY45qV22On18Q/fv/jf79X6OwMJUVK1IoLV3r67CUUicgbyaMTCChwfsennn1RCRPRKo8b18DRrZ029b07NJnMRh+feqvWb7czmsvCaNOt243MXz4N4jU8OOPp2m9hlLqiHkzYfwA9DXG9DbGBABXAAeUUsaYbg3eTgY2en5eCEwwxkR7KrsneOa1uqLKIv7+49+5Zug1JEQmsHw5BAW13rjcbUlERAojR/5ASMhA1q2bws6dT+pATEqpFvPak94iUmuMuRNb0PsBc0RkvTHmD0CaiHwC3G2MmQzUAvnA9Z5t840xj2KTDsAfRCTfG3FGBkXaUfQCIwFYtgxSUsDp9MbRfC8wsDvJyUvYvPkmdux4gLKydfTvPxs/vxBfh6aUauNMezrDTElJkbS0tKPevqoKIiLg7rvhmWdaMbA2SETYtesJdux4iJCQAQwa9A5hYe3wskop1SxjzAoRSWnJur6u9G5TfvwRqqvbRwupwzHG0KvXgwwb9gW1tYWsWDGajIxZeotKKdUkTRgNtNcK7+ZER59DSsoaYmLOY+vWX7F27YVUV2f7OiylVBukCaOB5cuhZ0/o3t3XkRxfAQGdGDLkY/r2/QsFBV/x/fcD2bt3jl5tKKUOoAmjgWXLOtbVRUPGGOLj7yAlZSWhoYPYvPkmVq0aT1nZxsNvrJTqEDRheOzZA7t2ddyEUSc0dBDJyUvo3/81ysrWkpY2jO3bH8LlKvd1aEopH9OE4fHdd/a1I1R4H44xDrp1u4nRozfRufMV7Nr1ON9/P5CcnA/0NpVSHZgmDI9lyyAgAIYP93UkbUdAQGcGDnyD5OTF+PtHsn79ZaxefS5lZRt8HZpSygc0YXgsXw4jRkBgoK8jaXuiosYxcuRKTj75RUpLV5CWNoytW39NbW2xr0NTSh1HmjCAmhpIS9P6i+Y4HP706HEno0f/RNeuN5CR8Tzff9+fffv+qbeplOogNGEAa9ZARYUmjJYICIijf//ZjBixnMDABDZtuoZVq8ZRWrrG16EppbxMEwb7H9jTCu+Wi4gYzYgRy+nX72+UlW0gLW04GzdeT0VFuq9DU0p5iSYMbIV3t26QkHD4ddV+xjjo3v1mTjnlJ3r0uJfs7Hl8/30/tmy5m+rqLF+Hp5RqZZowsFcYY8aAMb6O5MTkdMZw8snPcsopW+na9QYyM19m+fKT2Lr111RW7vR1eEqpVtLhE0ZVle0O5OyzfR3JiS8oqAf9+/+V0aM30qnTJWRk/Jnly/uwfv00iou/83V4SqljpN2bK6+prNxNZuaL7NkzG5eriIiIMXTtegNxcVNxOqN9HZ5SijbUvbkxZqIxZrMxZqsxZkYjy+8zxmwwxqwxxnxljOnVYJnLGLPKM+l4oiegoKAE+vR5mlNP3c3JJ/+Z2tpCfvrpFyxd2o3166eSm/spbne1r8NUSrWQ164wjDF+wE/AuUAGdvS8K0VkQ4N1zgK+E5FyY8ztwHgRmeZZVioiYUdyTL3CaNtEhJKSFWRlvUl29tvU1OTi7x9FbOzFxMVdRkzMuTgc+uSkUsfTkVxheG2IVmA0sFVEtnuCmgdcDNQnDBFZ1GD95cDVXoxH+ZgxhoiIFCIiUujT51kKCj4nO/t98vI+JitrLn5+EXTqNIWEhPt09D+l2iBvJox4YHeD9xnAKc2sfxPw3wbvg4wxadjxvp8UkX+1fojKVxwOJ7Gxk4iNnYTbXU1Bwdfk5MwnJ+ddsrLeICbmfHr2nEFk5BkYbb6mVJvgzYTRYsaYq4EUYFyD2b1EJNMYcxLwtTFmrYhsa2TbW4FbAXr27Hlc4lWty+EIIDZ2IrGxE+nT5xn27HmFjIwXWLVqHBERp9K58zQiIk4lLCwZhyPA1+Eq1WF5M2FkAg0fhevhmXcAY8zPgN8B40Skqm6+iGR6XrcbYxYDw4FDEoaIzAZmg63DaMX4lQ84ndH06vUgPXrcy759/yAj4wW2br0HAIcjiLCwkUREjCY0dDAhIQMJCRmoLa6UOk68mTB+APoaY3pjE8UVwP81XMEYMxz4KzBRRLIbzI8GykWkyhjTCRgLPO3FWFUb4+cXTHz8L4mP/yVVVZkUFS2juNhOe/a8gttdWb9uQEA3oqLGERt7ITExE3E6Y/Rz/5wAAA05SURBVH0YuVLtl9cShojUGmPuBBYCfsAcEVlvjPkDkCYinwDPAGHA+5771LtEZDIwEPirMcaNbfr7ZMPWVapjCQyMp3Pny+jc+TIARFxUVu6krGwD5eUbKStbQ37+52RnzwMcREaeRkzMJGJjLyA0NEnrQJRqJfrgnmoXRNyUlKSRl/dv8vI+pbR0FQABAfHExEwkNvYCoqPPwd8/0seRKtW2HEmzWk0Yql2qqtpDfv5n5OUtoKDgC1yuYsCPiIgxxMScR0zMeYSHj8Q+LqRUx6UJQ6kG3O4aiouXkZ+/kPz8hZSWrvAsceDnF46/fwR+fhH4+0cRHNybkJABBAf3JySkPyEh/fRhQtWuacJQqhnV1TkUFHxBWdkGXK4SXK5iamuLqa3Np6JiG1VV+x8fMiaA8PCRREScSmTkaUREjCEgoLvWi6h2o6086a3U/2/vXmPkKu87jn9/Z87cdr3e9a7XxuG2C6EQowY7TYE0pEpBTR1Uhb6gDUkaRVWkvKFSkCq1sXpT865vQvuCtonatLRFIQkNDeJFaOIgKmiLsY0hBsfBwS7Ywd71Za+zc//3xXl2GDZ2PXh3PWe8/490NHMuc/zb0Vn/9zznnOdJpVxulM2bP33e9Y3GPKXSTyiVDjE39yIzM//F8eMPcezYVwCIon4KhTGKxXEKhXHy+SuJ4xGy2WGy2ZHwfgNxPEQU9XlxcZcNLxjOLZHJ9DMwsJ2Bge1s3nwfAM1mlbm5/czM7KZc/ikLC0col48wNfUMjcbsefclxaFw9BNFOaQcUZQjkxlgZORuRkc/SbE4dsFMZg0qlWPE8Qhx/K66WHNuxXiTlHPLYGY0myVqtdPUameo109Tq52mXp9qm87SaJQwq9JsVjGrUqn8jLm5fQAMDNzGpk330df3C9TrZ8N+zlKtTlAuv87CwmHK5aOY1ZCyDA7ewfDwDoaHd/htw27Z/BqGcz1gYeEIk5PfYmLi0dZtwO3ieIhCYZxi8XoKhespFsdZWHidM2e+x/z8ywBks6MUCtdRKFxDPn8NhcI1ZDIDSBGQQYowq1OrTVKtTlCrTVCtTpLNbqS/fyt9fe+jv38rhcKY3zG2RnnBcK7HlEqvUa+fIY6HiePF6x/nbzGuVI5z5sxTTE8/R6XyBuXyG1Qqb7zjCfilpBy53Gay2RGq1ZNUq2+11kVRP+vX387Q0EcYHLyDgYHbgCbl8tHW1GwuhAJzM4XCeChKrtd5wXBuDTIzarVTNJslzBqYNYEGkCGXGyWTWf+O5qtabYpS6SCl0kFmZ/cxM/Mcc3MvAQYovJ5bFPXR13cjENFozIW7zWYBkctdQT7/HnK5LeRyW8hkBshkikRRH1FUJIryodhErTOhOB4gkxkkjoeI4yEA6vUzrWa+RmOevr6b6O//RaIou1pf4ZrkBcM5d1Hq9Wmmp/+b2dnniaI+CoUxCoVrKRTGiKI88/MHmZ8/QKn0CqXSj0meZVkXisI6oEm1eoJq9S0qlZ9RrZ6g2SytWD4pz7p1tzAw8MsUi9chxa0JonCdqNyaoqhALncFudxmcrkriOMRzKo0GiWazQWazQVARFGhbSqGwrX+586izJrU6zOYVclmN14WZ1leMJxzqWHWpNkst/0nXSE5e2li1sSsTqMxG24QmKZenwIgmx0mjpNblaOowPz8AWZnX2BmZjezs3tpNudXObmI40HieANmDer1qXAWlfyfKWXJ569uXT+K48Fw9pQPr3EoYJXW9PZzP9PU6zNAg3z+KvL5a8N+riaKCuHfMMyaSFmy2Y1t04YVvd7kz2E451JDishk+shk+pa1n/7+rWza9DtAcptxozEXmt7qYWoQRfnWmYKUpdksU62epFY7SbV6glrtdNim2JoAms0yZpVWYVu8uy25a+1s6/bopIAMImWpVI5TqbxBpfImU1NPhzOPSltBXPz5s61CksmsC/tYTy53BQDl8ptMTz/bKpQdfKMhe6H1ms+/h+3b/3NZ328nvGA453qOlOmoI8lMpkixONbRsy4rxcxahSx59qazZqt6fYZK5RjNZjV8RoAwq4bbtk9Rq01Sq50KZ2vlcMZWXnYx7pQXDOecW0GSwjWVd/ffaxyvJ463rk6oFdL7V2ycc85dEqtaMCTtkHRI0mFJXzrH+rykb4b1z0saa1u3Myw/JOk3VjOnc865C1u1gqHkMv5DwMeBrcCnJC093/o8cNbM3gs8CPxl+OxWkiFdbwZ2AH8jfwzVOee6ajXPMG4FDpvZ62ZWBR4F7lmyzT3Aw+H9Y8BdSp4sugd41MwqZnYEOBz255xzrktWs2BcCbzZNn8sLDvnNmZWB6aBkQ4/65xz7hLq+Yvekr4gaY+kPZOTk92O45xzl63VLBjHgavb5q8Ky865jZL70AaB0x1+FgAz+5qZfdDMPjg6OrpC0Z1zzi21mgXjBeAGSeOSciQXsZ9Yss0TwOfC+3uBH1rSV8kTwH3hLqpx4AZg9ypmdc45dwGr9uCemdUl/T7wFJABvm5mr0j6MrDHzJ4A/gH4F0mHgTMkRYWw3beAV4E6cL+ZNS70b+7du/eUpP+9yMgbgVMX+dlu6bXMvZYXPPOl0muZey0vnD/ztZ3u4LLqfHA5JO3ptAOutOi1zL2WFzzzpdJrmXstL6xM5p6/6O2cc+7S8ILhnHOuI14w3va1bge4CL2Wudfygme+VHotc6/lhRXI7NcwnHPOdcTPMJxzznVkzReMC/WomwaSvi5pQtKBtmXDkr4v6bXwuqGbGZeSdLWkpyW9KukVSV8My1ObW1JB0m5JL4XMfxGWj4felA+H3pVz3c7aTlJG0ouSngzzac97VNKPJO2XtCcsS+1xASBpSNJjkn4s6aCkD6U5s6Qbw/e7OM1IemC5mdd0weiwR900+CeSXnvbfQnYZWY3ALvCfJrUgT8ws63A7cD94btNc+4KcKeZ3QJsA3ZIup2kF+UHQ6/KZ0l6WU6TLwIH2+bTnhfg18xsW9ttnmk+LgD+Gviemd0E3ELyfac2s5kdCt/vNuCXgBLwOMvNnAwnuDYn4EPAU23zO4Gd3c51nqxjwIG2+UPAlvB+C3Co2xkvkP+7wK/3Sm6gD9gH3EbysFN8rmOm2xNJtzm7gDuBJ0nG9Uxt3pDpKLBxybLUHhckXRYdIVzz7YXMS3J+DHhuJTKv6TMMertX3M1m9lZ4fwLY3M0w/58wMNZ24HlSnjs07+wHJoDvAz8FpizpTRnSd4z8FfCHQDPMj5DuvAAG/IekvZK+EJal+bgYByaBfwxNf38vqZ90Z253H/CN8H5Zmdd6wbgsWPLnQipvd5O0Dvg34AEzm2lfl8bcZtaw5DT+KpIxWG7qcqTzkvSbwISZ7e12lnfpDjP7AElT8P2SfrV9ZQqPixj4APC3ZrYdmGdJU04KMwMQrl99Avj20nUXk3mtF4yOe8VNoZOStgCE14ku5/k5krIkxeIRM/tOWJz63ABmNgU8TdKkMxR6U4Z0HSMfBj4h6SjJAGV3krS1pzUvAGZ2PLxOkLSr30q6j4tjwDEzez7MP0ZSQNKcedHHgX1mdjLMLyvzWi8YnfSom1btPf1+juQaQWpIEknnkgfN7Cttq1KbW9KopKHwvkhyzeUgSeG4N2yWmsxmttPMrjKzMZJj94dm9hlSmhdAUr+kgcX3JO3rB0jxcWFmJ4A3Jd0YFt1F0jFqajO3+RRvN0fBcjN3+4JMtyfgbuAnJG3Vf9ztPOfJ+A3gLaBG8tfO50naqncBrwE/AIa7nXNJ5jtITndfBvaH6e405wbeD7wYMh8A/iwsv46ke/3DJKf2+W5nPUf2jwJPpj1vyPZSmF5Z/J1L83ER8m0D9oRj49+BDT2QuZ9kfKHBtmXLyuxPejvnnOvIWm+Scs451yEvGM455zriBcM551xHvGA455zriBcM55xzHfGC4VwKSProYm+zzqWVFwznnHMd8YLh3Lsg6XfDmBn7JX01dFY4J+nBMIbGLkmjYdttkv5H0suSHl8ce0DSeyX9IIy7sU/S9WH369rGXHgkPC3vXGp4wXCuQ5LeB3wS+LAlHRQ2gM+QPFG7x8xuBp4B/jx85J+BPzKz9wM/alv+CPCQJeNu/ArJU/yQ9Oj7AMnYLNeR9BXlXGrEF97EORfcRTIYzQvhj/8iSedtTeCbYZt/Bb4jaRAYMrNnwvKHgW+HfpSuNLPHAcysDBD2t9vMjoX5/SRjoDy7+j+Wc53xguFc5wQ8bGY737FQ+tMl211sfzuVtvcN/PfTpYw3STnXuV3AvZI2QWsc6mtJfo8We4f9NPCsmU0DZyV9JCz/LPCMmc0CxyT9VthHXlLfJf0pnLtI/heMcx0ys1cl/QnJaHERSe/B95MMqHNrWDdBcp0Dku6j/y4UhNeB3wvLPwt8VdKXwz5++xL+GM5dNO+t1rllkjRnZuu6ncO51eZNUs455zriZxjOOec64mcYzjnnOuIFwznnXEe8YDjnnOuIFwznnHMd8YLhnHOuI14wnHPOdeT/AMbDqQVw8Z5GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 788us/sample - loss: 0.8442 - acc: 0.7595\n",
      "Loss: 0.844184148311615 Accuracy: 0.7595016\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2178 - acc: 0.2724\n",
      "Epoch 00001: val_loss improved from inf to 1.57623, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/001-1.5762.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 2.2178 - acc: 0.2724 - val_loss: 1.5762 - val_acc: 0.5108\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5468 - acc: 0.5035\n",
      "Epoch 00002: val_loss improved from 1.57623 to 1.32407, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/002-1.3241.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.5467 - acc: 0.5035 - val_loss: 1.3241 - val_acc: 0.6019\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3547 - acc: 0.5689\n",
      "Epoch 00003: val_loss improved from 1.32407 to 1.19806, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/003-1.1981.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.3547 - acc: 0.5689 - val_loss: 1.1981 - val_acc: 0.6457\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2104 - acc: 0.6205\n",
      "Epoch 00004: val_loss improved from 1.19806 to 1.10597, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/004-1.1060.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2106 - acc: 0.6204 - val_loss: 1.1060 - val_acc: 0.6548\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0817 - acc: 0.6617\n",
      "Epoch 00005: val_loss improved from 1.10597 to 0.95516, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/005-0.9552.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0817 - acc: 0.6617 - val_loss: 0.9552 - val_acc: 0.7074\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9799 - acc: 0.6986\n",
      "Epoch 00006: val_loss improved from 0.95516 to 0.91225, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/006-0.9123.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9799 - acc: 0.6986 - val_loss: 0.9123 - val_acc: 0.7186\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8820 - acc: 0.7308\n",
      "Epoch 00007: val_loss improved from 0.91225 to 0.87282, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/007-0.8728.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8821 - acc: 0.7307 - val_loss: 0.8728 - val_acc: 0.7326\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7988 - acc: 0.7586\n",
      "Epoch 00008: val_loss improved from 0.87282 to 0.77063, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/008-0.7706.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7990 - acc: 0.7586 - val_loss: 0.7706 - val_acc: 0.7771\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.7766\n",
      "Epoch 00009: val_loss improved from 0.77063 to 0.67623, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/009-0.6762.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7361 - acc: 0.7766 - val_loss: 0.6762 - val_acc: 0.7999\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6826 - acc: 0.7945\n",
      "Epoch 00010: val_loss improved from 0.67623 to 0.61364, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/010-0.6136.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6826 - acc: 0.7946 - val_loss: 0.6136 - val_acc: 0.8274\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.8114\n",
      "Epoch 00011: val_loss improved from 0.61364 to 0.57857, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/011-0.5786.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6284 - acc: 0.8115 - val_loss: 0.5786 - val_acc: 0.8374\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.8209\n",
      "Epoch 00012: val_loss improved from 0.57857 to 0.56015, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/012-0.5602.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5947 - acc: 0.8209 - val_loss: 0.5602 - val_acc: 0.8437\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.8347\n",
      "Epoch 00013: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5538 - acc: 0.8347 - val_loss: 0.5862 - val_acc: 0.8283\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.8421\n",
      "Epoch 00014: val_loss improved from 0.56015 to 0.50500, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/014-0.5050.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5265 - acc: 0.8421 - val_loss: 0.5050 - val_acc: 0.8595\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8496\n",
      "Epoch 00015: val_loss improved from 0.50500 to 0.46826, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/015-0.4683.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4965 - acc: 0.8496 - val_loss: 0.4683 - val_acc: 0.8700\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.8590\n",
      "Epoch 00016: val_loss improved from 0.46826 to 0.46056, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/016-0.4606.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4696 - acc: 0.8590 - val_loss: 0.4606 - val_acc: 0.8710\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8674\n",
      "Epoch 00017: val_loss improved from 0.46056 to 0.45049, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/017-0.4505.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4437 - acc: 0.8674 - val_loss: 0.4505 - val_acc: 0.8689\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8713\n",
      "Epoch 00018: val_loss improved from 0.45049 to 0.43379, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/018-0.4338.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4279 - acc: 0.8713 - val_loss: 0.4338 - val_acc: 0.8803\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8814\n",
      "Epoch 00019: val_loss did not improve from 0.43379\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4003 - acc: 0.8814 - val_loss: 0.5185 - val_acc: 0.8558\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8841\n",
      "Epoch 00020: val_loss improved from 0.43379 to 0.42089, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/020-0.4209.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3845 - acc: 0.8841 - val_loss: 0.4209 - val_acc: 0.8905\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8885\n",
      "Epoch 00021: val_loss did not improve from 0.42089\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3681 - acc: 0.8885 - val_loss: 0.4225 - val_acc: 0.8824\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8922\n",
      "Epoch 00022: val_loss improved from 0.42089 to 0.40386, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/022-0.4039.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3535 - acc: 0.8922 - val_loss: 0.4039 - val_acc: 0.8908\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8958\n",
      "Epoch 00023: val_loss improved from 0.40386 to 0.39944, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/023-0.3994.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3367 - acc: 0.8958 - val_loss: 0.3994 - val_acc: 0.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8985\n",
      "Epoch 00024: val_loss improved from 0.39944 to 0.38576, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/024-0.3858.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3293 - acc: 0.8985 - val_loss: 0.3858 - val_acc: 0.8996\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9025\n",
      "Epoch 00025: val_loss did not improve from 0.38576\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3153 - acc: 0.9025 - val_loss: 0.3875 - val_acc: 0.8991\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9069\n",
      "Epoch 00026: val_loss did not improve from 0.38576\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3029 - acc: 0.9069 - val_loss: 0.4062 - val_acc: 0.8947\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9102\n",
      "Epoch 00027: val_loss did not improve from 0.38576\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2868 - acc: 0.9102 - val_loss: 0.3980 - val_acc: 0.9003\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9135\n",
      "Epoch 00028: val_loss did not improve from 0.38576\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2824 - acc: 0.9135 - val_loss: 0.3993 - val_acc: 0.8991\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9170\n",
      "Epoch 00029: val_loss improved from 0.38576 to 0.38562, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/029-0.3856.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2730 - acc: 0.9170 - val_loss: 0.3856 - val_acc: 0.9017\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9195\n",
      "Epoch 00030: val_loss improved from 0.38562 to 0.37702, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/030-0.3770.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2604 - acc: 0.9194 - val_loss: 0.3770 - val_acc: 0.9087\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9212\n",
      "Epoch 00031: val_loss improved from 0.37702 to 0.37438, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/031-0.3744.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2535 - acc: 0.9212 - val_loss: 0.3744 - val_acc: 0.9057\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9252\n",
      "Epoch 00032: val_loss did not improve from 0.37438\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2385 - acc: 0.9252 - val_loss: 0.3864 - val_acc: 0.9043\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9254\n",
      "Epoch 00033: val_loss did not improve from 0.37438\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2358 - acc: 0.9254 - val_loss: 0.3773 - val_acc: 0.9064\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9294\n",
      "Epoch 00034: val_loss improved from 0.37438 to 0.37116, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/034-0.3712.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2249 - acc: 0.9294 - val_loss: 0.3712 - val_acc: 0.9047\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9331\n",
      "Epoch 00035: val_loss did not improve from 0.37116\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2142 - acc: 0.9331 - val_loss: 0.3761 - val_acc: 0.9119\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9339\n",
      "Epoch 00036: val_loss did not improve from 0.37116\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2088 - acc: 0.9339 - val_loss: 0.3726 - val_acc: 0.9057\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9370\n",
      "Epoch 00037: val_loss did not improve from 0.37116\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1962 - acc: 0.9370 - val_loss: 0.4115 - val_acc: 0.9078\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9342\n",
      "Epoch 00038: val_loss did not improve from 0.37116\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2020 - acc: 0.9342 - val_loss: 0.3718 - val_acc: 0.9173\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9398\n",
      "Epoch 00039: val_loss did not improve from 0.37116\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1878 - acc: 0.9398 - val_loss: 0.4063 - val_acc: 0.8949\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9419\n",
      "Epoch 00040: val_loss improved from 0.37116 to 0.35773, saving model to model/checkpoint/1D_CNN_custom_DO_6_conv_checkpoint/040-0.3577.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1827 - acc: 0.9419 - val_loss: 0.3577 - val_acc: 0.9178\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9438\n",
      "Epoch 00041: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1742 - acc: 0.9438 - val_loss: 0.3831 - val_acc: 0.9129\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9434\n",
      "Epoch 00042: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1746 - acc: 0.9434 - val_loss: 0.3789 - val_acc: 0.9166\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9473\n",
      "Epoch 00043: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1645 - acc: 0.9473 - val_loss: 0.3668 - val_acc: 0.9140\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9469\n",
      "Epoch 00044: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1640 - acc: 0.9469 - val_loss: 0.3908 - val_acc: 0.9143\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9482\n",
      "Epoch 00045: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1571 - acc: 0.9482 - val_loss: 0.3949 - val_acc: 0.9068\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9491\n",
      "Epoch 00046: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1552 - acc: 0.9491 - val_loss: 0.3928 - val_acc: 0.9117\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9503\n",
      "Epoch 00047: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1516 - acc: 0.9502 - val_loss: 0.4010 - val_acc: 0.9101\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9508\n",
      "Epoch 00048: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1508 - acc: 0.9508 - val_loss: 0.4167 - val_acc: 0.9036\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9530\n",
      "Epoch 00049: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1419 - acc: 0.9530 - val_loss: 0.3752 - val_acc: 0.9175\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9538\n",
      "Epoch 00050: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1409 - acc: 0.9538 - val_loss: 0.4002 - val_acc: 0.9115\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9561\n",
      "Epoch 00051: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1336 - acc: 0.9561 - val_loss: 0.4294 - val_acc: 0.9075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9557\n",
      "Epoch 00052: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1359 - acc: 0.9557 - val_loss: 0.4020 - val_acc: 0.9119\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9572\n",
      "Epoch 00053: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1309 - acc: 0.9572 - val_loss: 0.3978 - val_acc: 0.9133\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9593\n",
      "Epoch 00054: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1244 - acc: 0.9593 - val_loss: 0.3820 - val_acc: 0.9185\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9600\n",
      "Epoch 00055: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1228 - acc: 0.9600 - val_loss: 0.3848 - val_acc: 0.9145\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9602\n",
      "Epoch 00056: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1193 - acc: 0.9602 - val_loss: 0.4027 - val_acc: 0.9180\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9615\n",
      "Epoch 00057: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1148 - acc: 0.9616 - val_loss: 0.4147 - val_acc: 0.9150\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9627\n",
      "Epoch 00058: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1130 - acc: 0.9627 - val_loss: 0.4200 - val_acc: 0.9119\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9629\n",
      "Epoch 00059: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1111 - acc: 0.9629 - val_loss: 0.4154 - val_acc: 0.9140\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9627\n",
      "Epoch 00060: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1135 - acc: 0.9626 - val_loss: 0.4024 - val_acc: 0.9159\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9627\n",
      "Epoch 00061: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1092 - acc: 0.9627 - val_loss: 0.4160 - val_acc: 0.9168\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9657\n",
      "Epoch 00062: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1031 - acc: 0.9657 - val_loss: 0.4077 - val_acc: 0.9159\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9678\n",
      "Epoch 00063: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0978 - acc: 0.9678 - val_loss: 0.4225 - val_acc: 0.9147\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9664\n",
      "Epoch 00064: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1035 - acc: 0.9664 - val_loss: 0.4046 - val_acc: 0.9196\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9670\n",
      "Epoch 00065: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0989 - acc: 0.9670 - val_loss: 0.4336 - val_acc: 0.9171\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9692\n",
      "Epoch 00066: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0948 - acc: 0.9692 - val_loss: 0.4385 - val_acc: 0.9099\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9674\n",
      "Epoch 00067: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0981 - acc: 0.9674 - val_loss: 0.4099 - val_acc: 0.9178\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9671\n",
      "Epoch 00068: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1009 - acc: 0.9671 - val_loss: 0.4193 - val_acc: 0.9196\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9694\n",
      "Epoch 00069: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0921 - acc: 0.9694 - val_loss: 0.4035 - val_acc: 0.9194\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9696\n",
      "Epoch 00070: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0922 - acc: 0.9696 - val_loss: 0.4070 - val_acc: 0.9171\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9703\n",
      "Epoch 00071: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0908 - acc: 0.9703 - val_loss: 0.4286 - val_acc: 0.9199\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9717\n",
      "Epoch 00072: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0874 - acc: 0.9717 - val_loss: 0.4034 - val_acc: 0.9208\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9727\n",
      "Epoch 00073: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0839 - acc: 0.9727 - val_loss: 0.4167 - val_acc: 0.9168\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9737\n",
      "Epoch 00074: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0819 - acc: 0.9737 - val_loss: 0.4342 - val_acc: 0.9138\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9702\n",
      "Epoch 00075: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0919 - acc: 0.9702 - val_loss: 0.4054 - val_acc: 0.9201\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9731\n",
      "Epoch 00076: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0824 - acc: 0.9731 - val_loss: 0.3997 - val_acc: 0.9196\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9743\n",
      "Epoch 00077: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0766 - acc: 0.9744 - val_loss: 0.4298 - val_acc: 0.9154\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9735\n",
      "Epoch 00078: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0806 - acc: 0.9735 - val_loss: 0.4353 - val_acc: 0.9152\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9760\n",
      "Epoch 00079: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0754 - acc: 0.9760 - val_loss: 0.4192 - val_acc: 0.9159\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9728\n",
      "Epoch 00080: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0810 - acc: 0.9728 - val_loss: 0.4266 - val_acc: 0.9164\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9753\n",
      "Epoch 00081: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0752 - acc: 0.9753 - val_loss: 0.4317 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9748\n",
      "Epoch 00082: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0793 - acc: 0.9748 - val_loss: 0.4206 - val_acc: 0.9203\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9735\n",
      "Epoch 00083: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0803 - acc: 0.9735 - val_loss: 0.4570 - val_acc: 0.9119\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9768\n",
      "Epoch 00084: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0723 - acc: 0.9769 - val_loss: 0.4518 - val_acc: 0.9154\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9779\n",
      "Epoch 00085: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0690 - acc: 0.9779 - val_loss: 0.4462 - val_acc: 0.9206\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9746\n",
      "Epoch 00086: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0753 - acc: 0.9747 - val_loss: 0.4548 - val_acc: 0.9101\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9763\n",
      "Epoch 00087: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0707 - acc: 0.9763 - val_loss: 0.4626 - val_acc: 0.9113\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9764\n",
      "Epoch 00088: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0724 - acc: 0.9764 - val_loss: 0.4582 - val_acc: 0.9140\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9777\n",
      "Epoch 00089: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0665 - acc: 0.9777 - val_loss: 0.4531 - val_acc: 0.9138\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9794\n",
      "Epoch 00090: val_loss did not improve from 0.35773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0631 - acc: 0.9794 - val_loss: 0.4390 - val_acc: 0.9199\n",
      "\n",
      "1D_CNN_custom_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 802us/sample - loss: 0.4309 - acc: 0.8808\n",
      "Loss: 0.43092903614291767 Accuracy: 0.8807892\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2725 - acc: 0.2494\n",
      "Epoch 00001: val_loss improved from inf to 1.56733, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/001-1.5673.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 2.2724 - acc: 0.2494 - val_loss: 1.5673 - val_acc: 0.4852\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5238 - acc: 0.4950\n",
      "Epoch 00002: val_loss improved from 1.56733 to 1.22385, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/002-1.2239.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5240 - acc: 0.4950 - val_loss: 1.2239 - val_acc: 0.6331\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2696 - acc: 0.5850\n",
      "Epoch 00003: val_loss improved from 1.22385 to 1.00730, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/003-1.0073.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2695 - acc: 0.5850 - val_loss: 1.0073 - val_acc: 0.6883\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0995 - acc: 0.6440\n",
      "Epoch 00004: val_loss improved from 1.00730 to 0.84880, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/004-0.8488.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0995 - acc: 0.6440 - val_loss: 0.8488 - val_acc: 0.7433\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9531 - acc: 0.6977\n",
      "Epoch 00005: val_loss improved from 0.84880 to 0.75706, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/005-0.7571.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9532 - acc: 0.6977 - val_loss: 0.7571 - val_acc: 0.7801\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7368\n",
      "Epoch 00006: val_loss improved from 0.75706 to 0.63829, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/006-0.6383.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8389 - acc: 0.7369 - val_loss: 0.6383 - val_acc: 0.8143\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7650\n",
      "Epoch 00007: val_loss did not improve from 0.63829\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7437 - acc: 0.7650 - val_loss: 0.6906 - val_acc: 0.7936\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6694 - acc: 0.7887\n",
      "Epoch 00008: val_loss improved from 0.63829 to 0.62307, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/008-0.6231.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6695 - acc: 0.7887 - val_loss: 0.6231 - val_acc: 0.8127\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6161 - acc: 0.8076\n",
      "Epoch 00009: val_loss improved from 0.62307 to 0.46446, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/009-0.4645.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6160 - acc: 0.8077 - val_loss: 0.4645 - val_acc: 0.8689\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.8297\n",
      "Epoch 00010: val_loss improved from 0.46446 to 0.44706, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/010-0.4471.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5431 - acc: 0.8297 - val_loss: 0.4471 - val_acc: 0.8686\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8404\n",
      "Epoch 00011: val_loss improved from 0.44706 to 0.37051, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/011-0.3705.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5070 - acc: 0.8404 - val_loss: 0.3705 - val_acc: 0.8940\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8575\n",
      "Epoch 00012: val_loss did not improve from 0.37051\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4598 - acc: 0.8574 - val_loss: 0.3771 - val_acc: 0.8896\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8655\n",
      "Epoch 00013: val_loss did not improve from 0.37051\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4243 - acc: 0.8655 - val_loss: 0.3724 - val_acc: 0.8833\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8748\n",
      "Epoch 00014: val_loss improved from 0.37051 to 0.31158, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/014-0.3116.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3990 - acc: 0.8748 - val_loss: 0.3116 - val_acc: 0.9126\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8828\n",
      "Epoch 00015: val_loss did not improve from 0.31158\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3753 - acc: 0.8828 - val_loss: 0.3178 - val_acc: 0.9152\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8875\n",
      "Epoch 00016: val_loss improved from 0.31158 to 0.27868, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/016-0.2787.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3562 - acc: 0.8875 - val_loss: 0.2787 - val_acc: 0.9208\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8956\n",
      "Epoch 00017: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3382 - acc: 0.8956 - val_loss: 0.2794 - val_acc: 0.9194\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8989\n",
      "Epoch 00018: val_loss improved from 0.27868 to 0.26297, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/018-0.2630.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3211 - acc: 0.8989 - val_loss: 0.2630 - val_acc: 0.9245\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9035\n",
      "Epoch 00019: val_loss did not improve from 0.26297\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3087 - acc: 0.9035 - val_loss: 0.2635 - val_acc: 0.9283\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9070\n",
      "Epoch 00020: val_loss improved from 0.26297 to 0.25360, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/020-0.2536.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2934 - acc: 0.9069 - val_loss: 0.2536 - val_acc: 0.9280\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9083\n",
      "Epoch 00021: val_loss improved from 0.25360 to 0.24666, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/021-0.2467.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2910 - acc: 0.9083 - val_loss: 0.2467 - val_acc: 0.9299\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9121\n",
      "Epoch 00022: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2730 - acc: 0.9121 - val_loss: 0.2810 - val_acc: 0.9194\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9159\n",
      "Epoch 00023: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2663 - acc: 0.9159 - val_loss: 0.2544 - val_acc: 0.9243\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9171\n",
      "Epoch 00024: val_loss improved from 0.24666 to 0.23241, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/024-0.2324.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2603 - acc: 0.9171 - val_loss: 0.2324 - val_acc: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9220\n",
      "Epoch 00025: val_loss improved from 0.23241 to 0.22456, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/025-0.2246.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2473 - acc: 0.9220 - val_loss: 0.2246 - val_acc: 0.9350\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9232\n",
      "Epoch 00026: val_loss improved from 0.22456 to 0.21610, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/026-0.2161.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2397 - acc: 0.9232 - val_loss: 0.2161 - val_acc: 0.9359\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9260\n",
      "Epoch 00027: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2305 - acc: 0.9259 - val_loss: 0.2638 - val_acc: 0.9259\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9273\n",
      "Epoch 00028: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2295 - acc: 0.9273 - val_loss: 0.2298 - val_acc: 0.9355\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9311\n",
      "Epoch 00029: val_loss did not improve from 0.21610\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2153 - acc: 0.9311 - val_loss: 0.2235 - val_acc: 0.9352\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9318\n",
      "Epoch 00030: val_loss improved from 0.21610 to 0.21589, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/030-0.2159.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2106 - acc: 0.9318 - val_loss: 0.2159 - val_acc: 0.9390\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9347\n",
      "Epoch 00031: val_loss did not improve from 0.21589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2043 - acc: 0.9347 - val_loss: 0.2255 - val_acc: 0.9334\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9364\n",
      "Epoch 00032: val_loss improved from 0.21589 to 0.20350, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/032-0.2035.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1985 - acc: 0.9364 - val_loss: 0.2035 - val_acc: 0.9411\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9379\n",
      "Epoch 00033: val_loss did not improve from 0.20350\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1914 - acc: 0.9379 - val_loss: 0.2433 - val_acc: 0.9385\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9364\n",
      "Epoch 00034: val_loss did not improve from 0.20350\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1958 - acc: 0.9364 - val_loss: 0.2389 - val_acc: 0.9336\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9414\n",
      "Epoch 00035: val_loss did not improve from 0.20350\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1823 - acc: 0.9413 - val_loss: 0.2261 - val_acc: 0.9399\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9435\n",
      "Epoch 00036: val_loss improved from 0.20350 to 0.20264, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/036-0.2026.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1737 - acc: 0.9435 - val_loss: 0.2026 - val_acc: 0.9420\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9423\n",
      "Epoch 00037: val_loss improved from 0.20264 to 0.18452, saving model to model/checkpoint/1D_CNN_custom_DO_7_conv_checkpoint/037-0.1845.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1744 - acc: 0.9423 - val_loss: 0.1845 - val_acc: 0.9492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9468\n",
      "Epoch 00038: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1620 - acc: 0.9468 - val_loss: 0.1959 - val_acc: 0.9462\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9448\n",
      "Epoch 00039: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1619 - acc: 0.9448 - val_loss: 0.1960 - val_acc: 0.9462\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9476\n",
      "Epoch 00040: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1592 - acc: 0.9475 - val_loss: 0.2117 - val_acc: 0.9420\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9479\n",
      "Epoch 00041: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1558 - acc: 0.9479 - val_loss: 0.1998 - val_acc: 0.9457\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9493\n",
      "Epoch 00042: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1521 - acc: 0.9493 - val_loss: 0.2169 - val_acc: 0.9425\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9515\n",
      "Epoch 00043: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1481 - acc: 0.9515 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9512\n",
      "Epoch 00044: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1449 - acc: 0.9512 - val_loss: 0.1920 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9531\n",
      "Epoch 00045: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1394 - acc: 0.9531 - val_loss: 0.2025 - val_acc: 0.9464\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9545\n",
      "Epoch 00046: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1353 - acc: 0.9545 - val_loss: 0.1910 - val_acc: 0.9478\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9547\n",
      "Epoch 00047: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1344 - acc: 0.9547 - val_loss: 0.1931 - val_acc: 0.9469\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9530\n",
      "Epoch 00048: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1377 - acc: 0.9530 - val_loss: 0.1989 - val_acc: 0.9453\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9566\n",
      "Epoch 00049: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1305 - acc: 0.9566 - val_loss: 0.2167 - val_acc: 0.9413\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9580\n",
      "Epoch 00050: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1264 - acc: 0.9580 - val_loss: 0.1873 - val_acc: 0.9522\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9600\n",
      "Epoch 00051: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1207 - acc: 0.9600 - val_loss: 0.1990 - val_acc: 0.9522\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9591\n",
      "Epoch 00052: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1197 - acc: 0.9591 - val_loss: 0.1863 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9613\n",
      "Epoch 00053: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1165 - acc: 0.9613 - val_loss: 0.2009 - val_acc: 0.9504\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9609\n",
      "Epoch 00054: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1160 - acc: 0.9608 - val_loss: 0.1994 - val_acc: 0.9504\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9600\n",
      "Epoch 00055: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1184 - acc: 0.9600 - val_loss: 0.1892 - val_acc: 0.9490\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9643\n",
      "Epoch 00056: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1056 - acc: 0.9643 - val_loss: 0.2088 - val_acc: 0.9469\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9624\n",
      "Epoch 00057: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1097 - acc: 0.9624 - val_loss: 0.1845 - val_acc: 0.9481\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9646\n",
      "Epoch 00058: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1040 - acc: 0.9645 - val_loss: 0.1992 - val_acc: 0.9506\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9633\n",
      "Epoch 00059: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1086 - acc: 0.9633 - val_loss: 0.1890 - val_acc: 0.9488\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9645\n",
      "Epoch 00060: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1024 - acc: 0.9645 - val_loss: 0.1954 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9676\n",
      "Epoch 00061: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0973 - acc: 0.9676 - val_loss: 0.2034 - val_acc: 0.9515\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9658\n",
      "Epoch 00062: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1002 - acc: 0.9658 - val_loss: 0.2054 - val_acc: 0.9532\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9684\n",
      "Epoch 00063: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0903 - acc: 0.9684 - val_loss: 0.2021 - val_acc: 0.9527\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9670\n",
      "Epoch 00064: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0963 - acc: 0.9670 - val_loss: 0.1900 - val_acc: 0.9529\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9690\n",
      "Epoch 00065: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0929 - acc: 0.9690 - val_loss: 0.1861 - val_acc: 0.9534\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9702\n",
      "Epoch 00066: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0868 - acc: 0.9702 - val_loss: 0.1985 - val_acc: 0.9518\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9689\n",
      "Epoch 00067: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0903 - acc: 0.9689 - val_loss: 0.2071 - val_acc: 0.9534\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9705\n",
      "Epoch 00068: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0847 - acc: 0.9705 - val_loss: 0.2035 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9715\n",
      "Epoch 00069: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0844 - acc: 0.9715 - val_loss: 0.2028 - val_acc: 0.9520\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9700\n",
      "Epoch 00070: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0868 - acc: 0.9700 - val_loss: 0.2058 - val_acc: 0.9504\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9701\n",
      "Epoch 00071: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0853 - acc: 0.9700 - val_loss: 0.2185 - val_acc: 0.9513\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9708\n",
      "Epoch 00072: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0844 - acc: 0.9708 - val_loss: 0.2039 - val_acc: 0.9520\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9730\n",
      "Epoch 00073: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0774 - acc: 0.9730 - val_loss: 0.1979 - val_acc: 0.9522\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9722\n",
      "Epoch 00074: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0790 - acc: 0.9722 - val_loss: 0.2124 - val_acc: 0.9532\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9735\n",
      "Epoch 00075: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0764 - acc: 0.9735 - val_loss: 0.1968 - val_acc: 0.9527\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9729\n",
      "Epoch 00076: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0795 - acc: 0.9729 - val_loss: 0.2067 - val_acc: 0.9534\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9749\n",
      "Epoch 00077: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0717 - acc: 0.9749 - val_loss: 0.2096 - val_acc: 0.9560\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9744\n",
      "Epoch 00078: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0740 - acc: 0.9744 - val_loss: 0.2247 - val_acc: 0.9462\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9749\n",
      "Epoch 00079: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0713 - acc: 0.9749 - val_loss: 0.1997 - val_acc: 0.9509\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9754\n",
      "Epoch 00080: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0721 - acc: 0.9754 - val_loss: 0.2051 - val_acc: 0.9564\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9771\n",
      "Epoch 00081: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0686 - acc: 0.9771 - val_loss: 0.2338 - val_acc: 0.9515\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9777\n",
      "Epoch 00082: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0666 - acc: 0.9777 - val_loss: 0.2040 - val_acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9757\n",
      "Epoch 00083: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0696 - acc: 0.9757 - val_loss: 0.2135 - val_acc: 0.9548\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9769\n",
      "Epoch 00084: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0692 - acc: 0.9769 - val_loss: 0.2328 - val_acc: 0.9518\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9779\n",
      "Epoch 00085: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0635 - acc: 0.9779 - val_loss: 0.2075 - val_acc: 0.9543\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9783\n",
      "Epoch 00086: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0640 - acc: 0.9783 - val_loss: 0.2063 - val_acc: 0.9541\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9777\n",
      "Epoch 00087: val_loss did not improve from 0.18452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0646 - acc: 0.9777 - val_loss: 0.2187 - val_acc: 0.9511\n",
      "\n",
      "1D_CNN_custom_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmX0m+0JYQiABkZ2ALKIo4oYLFbciWpdqqz5trdZafaTWtrZVa60++mDdt6qlqD/Qio8LlRZErYiAICgiW4CwZSF7ZjLb+f1xJpMEkhAgkyHM9/163Vcy996599ybyfnOWa/SWiOEEEIAWOKdACGEEEcPCQpCCCGiJCgIIYSIkqAghBAiSoKCEEKIKAkKQgghoiQoCCGEiJKgIIQQIkqCghBCiChbvBNwqLKzs3V+fn68kyGEEN3KypUry7TWPQ62X7cLCvn5+axYsSLeyRBCiG5FKbWtI/tJ9ZEQQogoCQpCCCGiJCgIIYSI6nZtCq0JBAIUFxfj8/ninZRuy+Vy0bdvX+x2e7yTIoSIo2MiKBQXF5OSkkJ+fj5KqXgnp9vRWlNeXk5xcTEFBQXxTo4QIo6Oieojn89HVlaWBITDpJQiKytLSlpCiGMjKAASEI6Q3D8hBBxDQeFgQiEvDQ07CYcD8U6KEEIctRImKITDPvz+3Wjd+UGhsrKSJ5544rDee/7551NZWdnh/e+55x4eeuihwzqXEEIcTMIEBaWsAGgd6vRjtxcUgsFgu+999913SU9P7/Q0CSHE4UiYoNB0qeFOP/KsWbPYvHkzo0eP5o477mDJkiWceuqpTJ8+nWHDhgFw0UUXMXbsWIYPH84zzzwTfW9+fj5lZWUUFRUxdOhQbrjhBoYPH87UqVPxer3tnnf16tVMnDiRUaNGcfHFF1NRUQHA7NmzGTZsGKNGjeLyyy8H4MMPP2T06NGMHj2aMWPGUFNT0+n3QQjR/R0TXVKb27jxVmprV7eyJUwoVIfF4kapQ7vs5OTRDBr0aJvbH3jgAdatW8fq1ea8S5YsYdWqVaxbty7axfOFF14gMzMTr9fL+PHjufTSS8nKytov7RuZO3cuzz77LJdddhnz58/nqquuavO811xzDY899hinnXYav/nNb/jd737Ho48+ygMPPMDWrVtxOp3RqqmHHnqIxx9/nEmTJlFbW4vL5TqkeyCESAwJVFJo7F2ju+RsEyZMaNHnf/bs2RQWFjJx4kR27NjBxo0bD3hPQUEBo0ePBmDs2LEUFRW1efyqqioqKys57bTTAPj+97/P0qVLARg1ahRXXnklf/vb37DZTACcNGkSt912G7Nnz6aysjK6Xgghmjvmcoa2vtFrHaS2djVOZ18cjl4xT0dSUlL09yVLlrBo0SI+/fRTPB4PU6ZMaXVMgNPpjP5utVoPWn3UlnfeeYelS5fy9ttvc99997F27VpmzZrFtGnTePfdd5k0aRILFy5kyJAhh3V8IcSxK4FKCo0NzZ3fppCSktJuHX1VVRUZGRl4PB6++eYbli1bdsTnTEtLIyMjg48++giAV155hdNOO41wOMyOHTs4/fTT+dOf/kRVVRW1tbVs3ryZkSNHcueddzJ+/Hi++eabI06DEOLYc8yVFNpiBmdZYtL7KCsri0mTJjFixAjOO+88pk2b1mL7ueeey1NPPcXQoUMZPHgwEydO7JTzvvTSS/zoRz+ivr6eAQMG8OKLLxIKhbjqqquoqqpCa80tt9xCeno6v/71r1m8eDEWi4Xhw4dz3nnndUoahBDHFqV119Sxd5Zx48bp/R+ys379eoYOHXrQ99bWrsFmS8Plyo9R6rq3jt5HIUT3o5RaqbUed7D9Eqj6CMAak5KCEEIcKxIqKChliUmbghBCHCsSLChISUEIIdqTUEHB9ECSoCCEEG1JqKBgqo8kKAghRFsSLChYpU1BCCHakVBB4WiqPkpOTj6k9UII0RUSKiiY6bO1lBaEEKINCRYUzOV2drvCrFmzePzxx6OvGx+EU1tby5lnnskJJ5zAyJEjeeuttzp8TK01d9xxByNGjGDkyJG89tprAOzevZvJkyczevRoRowYwUcffUQoFOLaa6+N7vvII4906vUJIRLHsTfNxa23wurWps4Gmw5gCftQ1iQOKR6OHg2Ptj119syZM7n11lu56aabAHj99ddZuHAhLpeLN998k9TUVMrKypg4cSLTp0/v0POQ33jjDVavXs2aNWsoKytj/PjxTJ48mb///e+cc845/OpXvyIUClFfX8/q1avZuXMn69atAzikJ7kJIURzx15QaFckM9ZNv3aGMWPGUFJSwq5duygtLSUjI4O8vDwCgQB33XUXS5cuxWKxsHPnTvbu3UuvXgefpfXjjz/miiuuwGq10rNnT0477TQ+//xzxo8fzw9+8AMCgQAXXXQRo0ePZsCAAWzZsoWbb76ZadOmMXXq1M67OCFEQjn2gkI73+jDwWq83m9xuwdjs6V06mlnzJjBvHnz2LNnDzNnzgRgzpw5lJaWsnLlSux2O/n5+a1OmX0oJk+ezNKlS3nnnXe49tprue2227jmmmtYs2YNCxcu5KmnnuL111/nhRde6IzLEkIkmJi1KSil8pRSi5VSXyulvlJK/ayVfZRSarZSapNS6kul1AmxSo85X+Pldn4PpJkzZ/Lqq68yb948ZsyYAZgps3NycrDb7SxevJht27Z1+Hinnnoqr732GqFQiNLSUpYuXcqECRPYtm0bPXv25IYbbuD6669n1apVlJWVEQ6HufTSS7n33ntZtWpVp1+fECIxxLKkEAR+obVepZRKAVYqpT7QWn/dbJ/zgEGR5UTgycjPGIndMxWGDx9OTU0Nubm59O7dG4Arr7ySCy64gJEjRzJu3LhDeqjNxRdfzKeffkphYSFKKR588EF69erFSy+9xJ///GfsdjvJycm8/PLL7Ny5k+uuu45w2FzXH//4x06/PiFEYuiyqbOVUm8Bf9Faf9Bs3dPAEq313MjrDcAUrfXuto5zJFNnh8N+6uq+xOnsj8PR4zCv5NglU2cLcew6qqbOVkrlA2OAz/bblAvsaPa6OLIuRumwRn47OgawCSHE0SbmQUEplQzMB27VWlcf5jFuVEqtUEqtKC0tPYLUxGacghBCHCtiGhSUUnZMQJijtX6jlV12AnnNXveNrGtBa/2M1nqc1npcjx6HX+3T9EhOGdEshBCtiWXvIwU8D6zXWv9PG7stAK6J9EKaCFS1157QOemSZyoIIURbYtn7aBJwNbBWKdU4xPguoB+A1vop4F3gfGATUA9cF8P0RBw9k+IJIcTRJmZBQWv9MQcZN6xN16ebYpWG1khJQQgh2pZQE+JBbJ7TXFlZyRNPPHFY7z3//PNlriIhxFEjAYNC51cftRcUgsFgu+999913SU9P79T0CCHE4Uq4oACdX300a9YsNm/ezOjRo7njjjtYsmQJp556KtOnT2fYsGEAXHTRRYwdO5bhw4fzzDPPRN+bn59PWVkZRUVFDB06lBtuuIHhw4czdepUvF7vAed6++23OfHEExkzZgxnnXUWe/fuBaC2tpbrrruOkSNHMmrUKObPnw/A+++/zwknnEBhYSFnnnlmp163EOLYc8xNiNfOzNkAhMO90boHVmvb++zvIDNn88ADD7Bu3TpWR068ZMkSVq1axbp16ygoKADghRdeIDMzE6/Xy/jx47n00kvJyspqcZyNGzcyd+5cnn32WS677DLmz5/PVVdd1WKfU045hWXLlqGU4rnnnuPBBx/k4Ycf5g9/+ANpaWmsXbsWgIqKCkpLS7nhhhtYunQpBQUF7Nu3r+MXLYRISMdcUDg4hZk7u5Pnz97PhAkTogEBYPbs2bz55psA7Nixg40bNx4QFAoKChg9ejQAY8eOpaio6IDjFhcXM3PmTHbv3o3f74+eY9GiRbz66qvR/TIyMnj77beZPHlydJ/MzMxOvUYhxLHnmAsK7X2jB2ho2Iffv5Pk5BM69LCbw5WUlBT9fcmSJSxatIhPP/0Uj8fDlClTWp1C2+l0Rn+3Wq2tVh/dfPPN3HbbbUyfPp0lS5Zwzz33xCT9QojElHBtCo3zH3Vmu0JKSgo1NTVtbq+qqiIjIwOPx8M333zDsmXLDvtcVVVV5Oaa6aFeeuml6Pqzzz67xSNBKyoqmDhxIkuXLmXr1q0AUn0khDioBAwKnf9MhaysLCZNmsSIESO44447Dth+7rnnEgwGGTp0KLNmzWLixImHfa577rmHGTNmMHbsWLKzs6Pr7777bioqKhgxYgSFhYUsXryYHj168Mwzz3DJJZdQWFgYffiPEEK0pcumzu4sRzJ1NkAgUIHPtxmPZxhWqycWSey2ZOpsIY5dR9XU2UeTWFQfCSHEsUKCghBCiKiECwpNlyxBQQgh9pdwQaGppCDPVBBCiP0lcFCQkoIQQuwv4YKCVB8JIUTbEi4oHC2P5ExOTo7r+YUQojUJFxRAHrQjhBBtSdig0JnVR7NmzWoxxcQ999zDQw89RG1tLWeeeSYnnHACI0eO5K233jrosdqaYru1KbDbmi5bCCEO1zE3Id6t79/K6j3tzJ0NhEL1KKWwWNwdOuboXqN59Ny2Z9qbOXMmt956KzfdZJ4s+vrrr7Nw4UJcLhdvvvkmqamplJWVMXHiRKZPn97uRHytTbEdDodbnQK7temyhRDiSBxzQaEjlILOnN5jzJgxlJSUsGvXLkpLS8nIyCAvL49AIMBdd93F0qVLsVgs7Ny5k71799KrV682j9XaFNulpaWtToHd2nTZQghxJI65oNDeN/pGXu8mwuEGkpKGd9p5Z8yYwbx589izZ0904rk5c+ZQWlrKypUrsdvt5OfntzpldqOOTrEthBCxkpBtCrF4JOfMmTN59dVXmTdvHjNmzADMNNc5OTnY7XYWL17Mtm3b2j1GW1NstzUFdmvTZQshxJFIyKAQi95Hw4cPp6amhtzcXHr37g3AlVdeyYoVKxg5ciQvv/wyQ4YMafcYbU2x3dYU2K1Nly2EEEci4abOBmhoKMbv3xvzp691NzJ1thDHLpk6u11Wmp7TLIQQolFCBgWZ/0gIIVp3zASFQ6kGk6BwoO5WjSiEiI1jIii4XC7Ky8sPIWNrvGyZPhtMQCgvL8flcsU7KUKIODsmxin07duX4uJiSktLO7R/OOzD7y/D4diAxSIZIZjA2rdv33gnQwgRZ8dEULDb7dHRvh1RXf05q1adx4gRb5Od/Z0YpkwIIbqXY6L66FDZbKkAhELVcU6JEEIcXRIyKFitKQAEgxIUhBCiucQJCps2wezZUF2N1dpYUqiJc6KEEOLokjhBYc0a+NnPYPNmrNYkQEn1kRBC7CdxgkK/fubnjh0opbBaUwgGpaQghBDNJU5QyMszP7dvB0xjs5QUhBCipZgFBaXUC0qpEqXUuja2T1FKVSmlVkeW38QqLQDk5IDDATt2AKaxWdoUhBCipViOU/gr8Bfg5Xb2+Uhr3TUDBSwW6Ns3WlKw27Px+/d0yamFEKK7iFlJQWu9FNgXq+Mfln79oiUFj2cw9fUb4pwgIYQ4usS7TeEkpdQapdR7Sqk2n42plLpRKbVCKbWio1NZtCovL1pS8HiGEAiUEggcXXFLCCHiKZ5BYRXQX2tdCDwG/KOtHbXWz2itx2mtx/Xo0ePwz9ivH+zaBcEgbvdgACktCCFEM3ELClrraq11beT3dwG7Uio7pifNy4NQCHbvxuMxj8asr/8mpqcUQojuJG5BQSnVS0WehamUmhBJS3lMT9psrILLlY9SDgkKQgjRTMx6Hyml5gJTgGylVDHwW8AOoLV+Cvgu8GOlVBDwApfrWD/ppdlYBcvJJ+N2D5KgIIQQzcQsKGitrzjI9r9guqx2nWYlBTCNzXV1a7s0CUIIcTSLd++jrpWaapZoD6TB+HxbCIcDcU6YEEIcHRIrKMB+YxWGoHUQr3dznBMlhBBHh8QLCvuNVQDpgSSEEI0SLyjsN6oZJCgIIUSjxAsKeXlQVgb19dhsqTgcvSUoCCFEROIFhcYeSMXFgKlCkqAghBBG4gWFxrEKzRqbvd4NxHqIhBBCdAeJFxQaSwrNGpuDwUoCgZI4JkoIIY4OiRcUcnPNz2YlBZDGZiGEgEQMCk4n9OrVYgAbSFAQQghIxKAApl0hUlJwOvOwWNwSFIQQgkQNCv36RUsKSlnkKWxCCBGRmEGhsaQQ6XEk3VKFEMJIzKDQrx/U1UFFBQBu92B8viJCIW+cEyaEEPGVmEGhlbEKoPF6v41fmoQQ4iiQmEFhv7EKycmFANTUrIhXioQQ4qiQmEGhlZKCzZZFVdUncUyUEELEX4eCglLqZ0qpVGU8r5RapZSaGuvExUzPnmC3N+uBpEhLO5mqqo/jnDAhhIivjpYUfqC1rgamAhnA1cADMUtVrFks0LdvNCgApKWdgte7Eb9fprsQQiSujgYFFfl5PvCK1vqrZuu6p4ED4dumhuW0tFMApApJCJHQOhoUViql/okJCguVUilAOHbJ6gKFhbBuHQSDAKSkjEUppwQFIURC62hQ+CEwCxivta4H7MB1MUtVVxg1ChoaYONGACwWJ6mp46VdQQiR0DoaFE4CNmitK5VSVwF3A1WxS1YXKDTdUFmzJroqNXUStbWrCIXq45QoIYSIr44GhSeBeqVUIfALYDPwcsxS1RWGDjU9kJoFhbS0U9A6QE3N53FMmBBCxE9Hg0JQm0eTXQj8RWv9OJASu2R1AYfDBIYWQeFkQBqbhRCJq6NBoUYp9UtMV9R3lFIWTLtC9zZqVIugYLdn4vEMk3YFIUTC6mhQmAk0YMYr7AH6An+OWaq6SmEh7NoFZWXRVWlpp1BV9R+07t6dq4QQ4nB0KChEAsEcIE0p9R3Ap7Xu3m0K0NTY/OWX0VVpaZMIhaqoq/sqTokSQoj46eg0F5cBy4EZwGXAZ0qp78YyYV2ilR5IMohNCJHIbB3c71eYMQolAEqpHsAiYF6sEtYlcnLMPEjNgoLLVYDD0Yuqqo/Jzf1RHBMnhBBdr6NtCpbGgBBRfgjvPboVFraoPjKT402msnIJOvJkNiGESBQdzdjfV0otVEpdq5S6FngHeDd2yepChYXw1VcQCERXZWSchd+/Ux7RKYRIOB1taL4DeAYYFVme0VrfGcuEdZnCQvD7YcOG6KqMjLMBqKj4IF6pEkKIuOhwFZDWer7W+rbI8mYsE9WlRo0yP5u1K7jd+bjdx7Fv3z/jlCghhIiPdoOCUqpGKVXdylKjlKruqkTG1JAhZnRzs3YFgIyMqVRWLiEc9scpYUII0fXaDQpa6xStdWorS4rWOrW99yqlXlBKlSil1rWxXSmlZiulNimlvlRKnXAkF3LY7HYYNqxFSQFMFVI4XEd19bK4JEsIIeIhlj2I/gqc287284BBkeVGzKR78VFY2EpQOB2wSruCECKhxCwoaK2XAvva2eVC4GVtLAPSlVK9Y5Wedo0aBXv2QElTr1ubLY3U1AnSriCESCjxHGuQC+xo9ro4sq7rjR5tfq5c2WJ1RsbZ1NSsIBCoiEOihBCi63WLAWhKqRuVUiuUUitKS0s7/wQnngg2G3z4YYvVpmtqmMrKf3f+OYUQ4ijU0WkuYmEnkNfsdd/IugNorZ/BjJNg3LhxnT/MOCkJJkyAJUtarE5NPRGrNYV9+/5Jjx6XdvpphRDt09oMI2poMD8B3G6zWNr4ShsKmbGoNTVQXW0Wr9f0KWlcgkHw+cz6QACsVrPYbGZ9TY1ZvF5zHpvNLFqb4weD5mfzpaEB6uqgthbq681xG/ezWCAlxSzJyWadz9e0NF6f3990vHDYnK/x3FYrXHghXHFFbO95PIPCAuCnSqlXgROBKq317rilZsoU+NOfzF80ORkAi8VOevrp0tgsjgoNDU2Zm9NpMolg0GR6VVXmo6tUUwbXmAE6HGb93r1QXAw7d5pMKznZZFIejzlu43H8fnC5zOJ0NmWgDQ3mHGVlZtm3z2R2ycnme5XdDpWVTQuYY7jdJlNrzDBrasz5GzNEv9+cpzGz19psr6836WqL09kyMDQGg3jOTqOUuRcej7nvjZl5MNgUaBonT3A4mu6x02leN77HYjHva7yuYNAs48bF/hpiFhSUUnOBKUC2UqoY+C2RB/NorZ/CTJNxPrAJqAeui1VaOmTKFLj/fvjkEzjnnOjqjIyzKS9fgNe7Gbd7YPzSJ45ITUMNAMmOZJRSh/TecLjpW2V9fctvoD6fJkA9Xl2FL1yLxZ+G8mZTX2elrq4pY/P5zD+31uZ4VqvJOBozkKoqk2nv3Qvl5eZ9tYFqKtOWUk85DZH3g4KgCwJuCLoh5ICwFbTV/KTZtYWtZp+AGwIe8KdA0NlyHwAVBkcNuCrBXQFWP/iTI0uSee2oNYu9nuT0BlLS/SSn+QlZ6vHW1+OrqSeID7fLgifLijvXilJQFdAEgmFCYY09K4TDGcTlDJFut+O2JeG2JeGyuvAGvdQH6vGG6rBoO+m2nqQ7cshwZqHsPrS9hpCthjAhtN9DuCGy6BAh/ISUH638WGxBlDUA1gAudxiXS+N0h7Hbw/iCfhoii9UCTocVh92K3aYIhIIEQkH8oQABavGpCrxUgCVIYfYEJuRM4fiUMditNkKqgRJfMbvqi9heu4lt1Zsoqt4EKkxBVn8K0vPpk9KHmoYaSutLKa0rpS5QF73dOqxId6fRIymbLHcWTpuTSl9ldAmFQ1iUBaUUDquDbI/ZL9uTzZDsIUBs86GYBQWtdbuFnMjjPW+K1fkP2cknmxC9ZEmLoJCZaX4vK1tAXt7P45S4Y1edv449tXuwKAv90vphtVij2/Z59/HF7i8oqSshxZGOR2XgURnkpeeSk54crQbYuRNWf1vK51s3sLuqnMr6aiq9NVSFd1Ph+JJy65dUWYoAUGE7Nn82+NKxKAsWS+RbWTAVVZNLqDIXf2U2IWc52rOHcNIeQENdD6jvAb40SN0J6VshYysk7QVrsOVFaQX12SZTtXnB7oVUH2grKuSAsAMVcqK9SegqDwSSoL4H7mBv0lx9cB8foirrAyqSP0Wr/Y59hGzKTrIjBbvFji/owx9qoCHsO6Rj1EaW1tQcbsIc7WwLRZbmFOA6yHvaS2g7rMpKhjuDDFcGIR3ina1mMugURwpJjiT21O5psb/T6mRg5kAsysLSHYup8be8C8mOZPNlJBKMwzpMpa+ShlDDAef22D1YlRWNJqzDNAQbCOmmi79z0p08cNYDh35RhyCe1UdHlzbaFTyeQSQnj6ak5NVuGRTCOkyFt4I9tXuobqhGKYVFmTK3N+Cl1l9Lrb8Wt93Nqf1OJcOdccAxGoINLC5azFvfvMW7m96luqEat82N2+7GaXUSCAfwh/z4Q376pfXj9PzTOT3/dIZkjuCz7V/wYdFSPt31EXtr9xAOWwiHLASDYWp1KX7V9A9k0XZcvgKstXl43ZsIJm9r+8Lqs6Cqv/kWnP0NeMpbbncDYQuWisHoPSfCnhtw2hwk55ThyizHmlRJMKgJNkAwqAk5Kgllr8XX5z2CljrsOplk3YskemFRCq9lHfWU4tWVZNj60NtdQF7yWfT09CHZlk6SNQ2nJQm/pYpavZeq0F4C1JPsdOOxu3HanGito/ep8Ztxnb+eal8t+3zfsKv23+zxmXqXMb3GcOPA2znnuHPon9a/xd/TF/ThDXrxBrz4Q35COkQoHGqReQAEQoHofvWBemr9tVQ3VFPjryEQCuCyuXDZXDhtTtKcaWS4M0h3peOwOqjz11Hrr6UuUIfT6oxmbB67B6fNicPqwG6x47F78Ng9JDmScFqdhHU4mh4g+o1XobBarFiVFavFSiAUMNcfqMMX9OG2uUlyJOGxe2gINlBSV8Leur2U15fjsXtIcaaQ4kjBarFSH6iPLjaLDYfVEU2P3WrHbrFjs9hM0I+c36IsOK2RdFvNk4Qb75nWGrvVvMdmseG0OluUJvfU7uHDog9Zum0pvqCP/un96ZfWj35p/Tgu8zj6pvaN/k9pran0VbKzZiepzlR6eHrgtrsP+PhqrakP1FNWX0ZDqIF0V3r03u+/X3VDNWX1ZZTVl9EjqUfb/xOdRHW36aHHjRunV6xYEZuD/+pXpl2hsjLargCwffuDbNlyJyeeuCnuVUi+oI8dVTvYtG8Tm/ZtYkf1DmwWWzSTrvXXsrVyK1sqtrCtcht7avcQCAcOfmDMP/DY3mM5Pf90AIqqithSvo31ZV9RF6zFbU1iQtZUkkJ9Ka/2UlnrpdbrR4fs6ICTcNBGjetr6jOWg7XZOYMO2DkBKgsAbaorUFDXA1XXC6uvFw53AGevzajMTYRStpMaGkDP8AnkWk4g29EH3FVoZwUB2z72BYopCRRRHioiSD39kwczPGcoY/OHMKh3T3plpJLuTiHdlY7T5gRMlU1bDZPNNWbcje9rbfuhVj8disaMPs2VFrNziMSklFqptT5oq4QEheY++ACmToX3329RheTzbWPZsnwKCu6jf/+7YnPuNsz7eh73Lr2XvXV7qfBWHFDkdFqdhHSIYNhUMygUuam5FKQXkJ+eT25KLr1TetMruRcOncq+Ck1lpaaiMky4wYMKmLrjktpy1lT/i816EeXuZaaqo7ofel8+lB8PG6fB1jNMfXZESgrk5po68cYGs5QUSMmqw5f9H+qSv6LANYbjkyaQ5nGTng69e5slJ+fAhkIhROxIUDgcdXWQng633w5//GOLTatWTSIUqmb8+LWdcqoKbwX+kJ8eST2iRc/mfEEfty28jSdXPElhz0Im5E6IFjF7J/dmUNYgBmUOIicpB6UUdd4gK1b72LbFTmW5k9LSpt4mO3bA9u2mYbQ9qamRDLt3A3162enT20Lv3pCdbXqFNGb8vXpBfr65VTH80iyE6EQdDQrSptBcG+0KADk5V7Bp083U1q4jOXnEYZ8iGA7y8H8e5p4P78EX9OGwOshNyaV/en8GZw1maPZQ+qX143cf/o41e9dw+0m3c/+Z90frQcvK4NtvYdsn8PE22LwZVq2CtWttBAJNVV4Wi8nM+/aFgQPh9NPNt/qePc239JwcU0PW2A0wOdl84zdarzoRQhxL0yHfAAAgAElEQVT7JCjs77TT4MEHW4xXAMjJmcGmTT+jpORVkpPvPaxDr9q9iusXXM8Xe77goiEXcWbBmeyo2kFxTTFbK7by+levU+EzU2pkubOYf8n/kVk+jYcehBUrzLJ9e8tjZmbCmDHwi1+YPsxDh5oMPzNTqmaEEIdOgsL+pkwxVUf7jVdwOHqSkXEGJSVzKSj4w0EbGyu8FVww9wI27dsU7QGxp3YPOUk5zL9sPpcMvaTF/lrDpk2af39WyuIvv2XTvwZz+d09ogNdjjsOTjoJfvpTGD4c+veHfv1MHb4QQnQWCQr7axyv8OGHLYICmCqkDRt+SE3NClJTx7d5CK01P37nx3y28zOuLbwWpRRaa3om9+T2k28n3ZUe3ffrr+Gxx+C116CiQgE5uFw5jB1rvv2fcooJBpmZsbpgIYRoIkFhf8nJMH48vPce3Hdfi5bU7OxL+PbbH1FSMrfdoDBn7Rxe++o17jvjPu469cDeStXVpqPT00+bn04nzJhhaq7GjTMlAbs9JlcnhBDtkqDQmquvhp/8BD791JQcIuz2dDIzz6ekZC4DBvwJi+XAnLuosoib3r2JSXmTuHPSndH127fDiy+aILBsmZmyIDfXzKxxww2mUVgIIeJNmiJbc/XVkJYGs2cfsKl37+tpaNhDWdk/DtgWCoe45s1r0FrzysWvYLVYqaqCX/4Sjj8efvc7M/nXnXfC4sWwdavZJgFBCHG0kKDQmuRkuP56mDfPdPSPqPJVcfX7f+Hy5VZm/+duAqGmUbtfl37NzHkz+Wj7Rzx+/uP0dhfw+OOmgfiBB+Cyy6CoCJYvN7VSU6ZIFZEQ4ugjQaEtN91k5kZ40jw6envVdk558RQWbf0XOUm9uG/Ntwx+bABPfP4E3339u4x4YgTvbXqPX570W0oWXcWAAU09hVasgJdfNr2FhBDiaCZtCm0pKIDp0+Hpp1l1/TS+88Z3qQvU8f6V73NK7igeeacPL+3wctO7N5HqTOXOk+/CveZWZn8vm/JyM1jslVfgjDNk1K8QovuQoNCen/2MD9a+xcWvnEFWak8+ufoTRuSY0cwXDr2Ck7LfxNp/EZUbxzLr1nS++spMnXTPPaYbqRBCdDdSfdSOeTllTLtKMbBSsewHn0YDAkCfPj+hssLBw7fmccHZ6dTVwVtvmbn0JCAIIborCQpteH7V88ycfznjXQP48Ekfvb/c0mL78uUncsMN6/m//yvg7rs1X39tapukqkgI0Z1JUGjFX5b/hevfvp6pA6fyzx/9h3SLB+bOBcxzam+/HaZOVaSlOXjiiQn84hdLcR/4HA0hhOh2JCjs57Piz7j1/VuZPng6b13+FkkZOXD++TB/PjWVIaZOhYcfNmPbVq1yMGzYLrZu/S3dbQpyIYRojQSFZqobqvneG9+jb2pfXrropaZH482YQc3eOs47tZZPPoG//x0efxxSUjz07/9rqqo+ZN++hfFNvBBCdAIJCs389N2fUlRZxJxL5rSYtK761GmcY/mAZV8lM3cuXHFF03v69LkRl6uArVt/idbhOKRaCCE6jwSFiDlfzuGVL1/hN5N/w6R+k6Lr6+rgnEuS+FyP47XUG5lxScuHo1ssDgoK/kBt7WpKSl7v6mQLIUSnkqAAbK3Yyo/f+TGT8ibxq8m/arHtllvgs8/gtZ9/xqVVL8DHHzdt3LIFCgvJWZZEUtIotm69m3DY38WpF0KIzpPwQSEYDvK9N76HRVmYc8kcbJam8Xx//zu88ALcdRdc8vvR5rmVr0dKAz6fme/6yy9RN/2UATm/wefbzO7dz8fpSoQQ4sglfFD4/Ye/Z1nxMp76zlP0T+8fXb9pE/zXf8GkSWaEMklJMG0azJ9v5r3+xS/Mw5F/9SvYuZPMp1eSlnYqRUW/xe/fG7frEUKII5HQQeGjbR9x30f38f3C73P5iMuj6xsaYOZMM4vp3/9uHsQGmJLB3r1w663wxBNmwMK998LVV6Mefpjjrb8kFKph/fprpNFZCNEtJWxQqPRVctWbV1GQXsBj5z3WYttdd5lCwIsv7jez6bRppgrpL38xD9+5/36z/oEHwOEg6ddPcdxx/0tFxT/Zvv3BrrsYIYToJAkbFG5beBu7anbx90v/ToozJbr+o4/gkUfgxz+GCy/c701JSWZlVha8+mrTAxH69IFf/xoWLKD3mjx69LiMrVvvpqrqP113QUII0QlUdxuJO27cOL1ixYojOoY/5CfzT5l8b+T3eOaCZ6Lr6+qgsBC0hjVrzLN2DlBbC/X1kJPTcn1DA4wcCQ4HwdWfsGLFCWgdYNy41djtmUeUXiGEOFJKqZVa63EH2y8hSwqfFX9GXaCO8447r8X6X/4SNm82PY5aDQhgNuwfEACcTtN/9auvsBVXMGzYq/j9u9myZVbnX4AQQsRIQgaFD7Z8gEVZOL3g9Oi6Dz+Exx4z+fpppx3mgU+PHG/JElJTx9Onz0/Yvft56uq+OfJECyFEF0jIoLBoyyLG9xkfncqirg6uuw4GDmxqOz4sw4ZBdjYsWQJA//53Y7UmsXXrL4880UII0QUSLihU+apYvnM5Zw04K7ru2Wdh61Z47jnTlnzYlIIpU6JBweHoQV7ef1NW9g+qqj45onQLIURXSLigsKRoCSEd4uwBZwNmHNrs2WaQ2pQpnXCCKVNg2zYoKgIgL+/nOBy92bz5v2V6bSHEUS/hgsKiLYvw2D1M7DsRgAULTCnh5z/vpBM0NkhESgtWaxL5+fdQXf0fyssXdNJJhBAiNhIuKHyw5QNO638aTpsTMGMS+vdvZUzC4dqvXQGgV68f4PEMYfPm/yYQ2NdJJxJCiM4X06CglDpXKbVBKbVJKXVA30yl1LVKqVKl1OrIcn0s07OjagcbyjdE2xNWrjSD1W65pdlUFkfKYjGlhWZBwWKxMWjQ4/h82/jii8n4fMWddDIhhOhcMQsKSikr8DhwHjAMuEIpNayVXV/TWo+OLM/FKj1gqo6AaHvCo4+aYQc//GEnn2i/dgWAjIwzGDXqfRoatvPFFydLN1UhxFEpliWFCcAmrfUWrbUfeBXorEqaw7Jo6yJ6JvVkRM4Idu2C116DH/wA0tI6+USNLdYffthidUbGFEaP/pBwuIEvvjiFmpqVnXxiIYQ4MrEMCrnAjmaviyPr9nepUupLpdQ8pVRerBIT1mEWbVnEWQPOQinFk09CMGiqjjpdK+0KjVJSxjBmzCdYrcl8+eU0fL7tMUiAEEIcnng3NL8N5GutRwEfAC+1tpNS6kal1Aql1IrS0tLDOtG6knWU1JVE2xPeeAPOPtsMWOt0FgtMntxqUADweI5j1Kh3CYe9rF07nWCwNgaJEEKIQxfLoLATaP7Nv29kXZTWulxr3RB5+RwwtrUDaa2f0VqP01qP69Gjx2ElZtO+TSTZkzhrwFkEAvDttzC21bN1kilTTJtCs3aF5pKShjFs2GvU1a1l/for5fkLQoijQiyDwufAIKVUgVLKAVwOtOior5Tq3ezldGB9rBJzydBLqLizgr6pfdm82VQdDR0aq7PR1K7w8MNQWdnqLllZ53LccY9SXr6ALVtkKgwhRPzFLChorYPAT4GFmMz+da31V0qp3yulpkd2u0Up9ZVSag1wC3BtrNIDYLea5x98/bV5HdOgMHw4XHSReSBP376m8WLz5gN2y839KX36/IgdOx5ky5ZfyqhnIURcJeTzFO67D+6+G2pq2pkiu7N88YXp+zp3LjgcsGED5LZsb9c6xLff3sTu3U/Ts+c1DB78HBaLPcYJE0IkEnmeQjvWrzeP2Yx5QAAYMwZeegnWrgW/30Sk/Shl5fjjnyQ///fs3fsy69ZJ47MQIj4SNijEtOqoNYMHw/XXm6lYt249YLNSivz8X3P88c+yb98/WblyLJWVS7s4kUKIRJdwQSEcjlNQAPjVr0x31T/8oc1d+vS5nsLCRWgdZPXq09iw4UcEg1VdmEghRCJLuKCwfTt4vXEKCrm58JOfmOqkDRva3C0j43TGj19LXt7t7N79LMuXD2Pv3lelEVoIEXMJFxTWRzq9DmttFqauMGsWuN1wzz3t7ma1ehg48M+ccMJnOBy9WL/+CtasOZO6uq+6Jp1CiITUWXODdhuNQSEuJQWAnBz42c/Mcz+HDoUdO0wf2d27oU8f0wLerx98//swdCipqeMYO3Y5u3Y9y9atd7FixWh69/4v+vW7E5crZrOCCCESVMJ1Sb3+evNgnZKSTkzUoaqogOOOg337ICvLjGno29cEhm3bTB1Xnz6wbh2kpETf5veXsXXr3ezZ8zyg6NXrOvr1uxO3e0D8rkUI0S10tEtqwgWFSZPMsxP2m8C06zVGpZycA7d9+qlJ6E9+Yga/7cfn28b27Q+ye/dzaB0iJ2cGeXm3k5ISy3k7hBDdmYxTaIXWpvoobu0JzeXktB4QAE46yYyAfvxx8xSg/bhc/Tn++MeZOHEreXk/p7z8HVauHMfq1WdQXv6OzKMkhDhsCRUUSkpMzU3c2hMOxX33QX6+eQKQ19vqLk5nHwYO/DMnnbSDAQP+TH39t6xd+x2WLx9CcfFjBIM1XZtmIUS3l1BBoUvmPOosSUnw7LOwcSPcdZepUnrxRdN7aUGLeQWx2dLo1+92Jk7cytChc7Hbs9i06RY+/TSXr7++ipKSeTJCWgjRIQnV+yjuPY8O1VlnmZLCo4+aBUApUw/2zDNwww0tdrdY7PTseTk9e15OdfVydu16mrKytygpmYNSTjIzz6V37x+QmXk+FktC/emFEB2UUDnD+vWmM09ua89/O1o98giccALk5Zlo1rs3zJgBN95ohmf/13+1+rbU1Amkpk4gHH6a6ur/UFb2JiUlr1Je/hYORy969ryapKQR2Gxp2GzpuFwFuFz9uvjihBBHm4TqfXTWWWZm1M8+6+REdbWGBrj0UnjnHZg9Gy68EAIB85CIXr3afOh0OBxg37732L37ecrL3wFCzbYqsrMvJC/vdtKKkqGgAFJTu+RyhBCxJ11SW9GnD0ydCn/9a+emKS4aGkyJ4e23W653ueCKK0x31nFt//2DwWoCgVKCwUqCwSoqKxezs/hx+rxcwYDnINg7jbpHb8VxwdW4XAUolVDNT0IcczoaFBKm+qiqyowN6zbtCQfjdMK8eWbxesFuB6sVPv4YXnnFNEqPGWMiYTBolsxM8+zo00/HNmwYNltTSSAj+VT631eM5fm/su+0ZJxFVaTN/B27pv2OrTe5cGQfj8czGI9nMGlpk0lPP739dom1a+Hee810HsfMTRfi2JcwJYVly0z3/wUL4IILYpCwo0lVlQkMc+eaEoXNZgLGjh1mAejRwzykeuRIs8yZAwsXmqcP/f73BOtKCd39CxyPzSHYM5ndNw9k95QavA1bgTA2WxbZ2RfRo8fFpKRMwOFo9uzsvXth/HhzrvR0eOMNOP30uNwKIYQh1Uf7mTsXvvc908PzuONikLDuQGsoKoIlS8yQ7jVrTD9dv98EjaefNr2dmvv0U7jpJvMEufHjCT14P/uG11JaOo/y8gWEQmYshMORS0rKGFIcJ9D3mvlY125BzZljgszGjaZ77fe/f2Tp//pruPxyk96+fU3j+5Ahpk1lyJAjO7YQnSkQMKX3o4gEhVZUV5unrVmkerxJMGgybacTBrQxh1I4DH/7mxkvsXOnqQ4aO5bwmFHUDnVQPbCe6uBX1NasJO+eb+j9Pnx1j4X680fi9Hoo+O8NpHy2j9qzj8M+4Wwchaejhg0zDx6y7VcFtXu3qXqaNMmM1Wj09demtGGxwCmnQHGxKYns3Gm2Dx1qGt9vvNEECyE6W1GR6XyRmdn2PsGgqTa9/364+WZ44IHWg4PW8K9/mUGqZWXw0ENwzjkxSzp0PCigte5Wy9ixY7WIk7o6rf/nf7S+4AKt+/TR2ny0tbZatS4s1HraNK1B1915pd68+S69Zs15+osvpuiVyybqPVfkaG9Pmt4DOuy06+AJw3Xw+mt0+Ac/0HrQoKbtvXtr/fzzWgeDWq9fr3XPnlr36mV+b664WOvHHtN6yhStLRatPR6t779fa5/v8K+ztrb19StXmmNv3nz4x25POKz1Bx9o/cQTWnu9sTnHsSYU0vrTT7W+4w6tb7hB60WLzLr2lJZq3dDQseNXVWn97LNan3yy+Vympmr95JOtn2PbNq1POcXsN368+TlxolnfqLZW63/8Q+sTTzTb+/Rp+txffbVJ2/727dP6lVe0vuQSrf/6146luxXACt2BPDahSgqik+3ZAytWwPLlpp/v55/DtGnw8stmkN1+AoF9lG37G1Wfv4het5rkTZC80SwoqBuTTsOJA1EDh5Lx7HLsK79FjxqJKik1oWLx4vYbrYuK4Lbb4M03TR3hI4+Y9LSSlgPU1sKrr5pqruXLTTvLBReYpbgY/vd/TSM+mB5ev/413H47OBwHHktr2LLF9H8eNergRVO/35z74Yfhyy/NusJCeP11OP74g6e98Zw7d5qSl9sNHk/r31C//hr+9CdTvXHvvW2XDg92rh07YNUqM3dMba251kAAsrNNe1VODowebX4/mHAYvvnGNPx99pk5Vs+eZsnJMaVYu90sDQ1QXm6+XRcXw7vvwq5dTdddU2NKilddZUqUgweb6WLq6mD+fPPZXLLEpOvaa820yccfb0qoH34IS5eamYrLy80sxjt2gM9nPnfXXAOLFplv+CefDI89ZtK0ebMZBPXgg6ak8OST5vz/7/+Z6li73bxevtz8vwSD0L+/mZ3guuvM/bz/fvjjH00b3MSJ5jPmdJp0LV1q3tOnj3l6409+cuh/M6T6SBzlAoFyfL4ifL5t+LxFeL2bqfd+Q339N/j9u0BDjw9hwDNg9VkpemEy9sJT8HiGEQ7X09BQTENDMWChR4+LSU8/A4slkgkuXGgmFPz2W5Mx3XGH6b5rs5lM9733zD9oKGT+IUMh849XW2umMb/gAtOW8vHHZhuYcRs332yK+L/9ren1NXSoeTaG223aZAIB+OQTk3EUFZn39ewJ550H555r6i5rakw95t69ptpu40aToVRVmZkab7/dVE/88IcmA3z6adO1+P/+zyxffw1nnmmqys45x0zmNWeOyewa53FplJtrqtzOOMMEyb/8xWRUHo8JlMGgyWTuuMOka+5cc5ziYtNL7Ywz4NRTzbb1683y5ZewciWUlh74R7VYTAbfSCmTeU6fbq4/P9+MHlXKXO9778E//gHvv29eg8kUMzObgk170tPN9V1yCXznOyYTXbDAPNlw4cKmtDgc5pwNDTBokGmXWrfOdOcOBk0QaeyAkZJi9snKMunIzYXLLoMJE5pmE3jlFfPlo7y8ZXomTjTbmjdabtoEM2eaKtEJE8z9nDzZDJraP2ivW9dURevzmSUpyXyxufhi8zk4grpvCQqi2woGq/F6N1Jf/y3emvXUV6yjVq+nvv5boCnTsdt7Eg7XEwrVYLNl0aPHpWRmTiUl5URclhyTWf75zyYzy8szmURjG8TgwSYzB/PPPmqUaY846aSmksW+ffDPf5rM/LzzTMbf6L33TAP81q0tE5+WZjLTs84yGcy775pMr7LywAvt29d8Sx00yDSWn3tu07mLi814k8bSCcCIESZoLVpkMiSXy2R0Wps2mO9+12SMXq/5ZvzVV/Dvfzdl4MnJJrDddpvJcH7+cxPccnNNJhwImG7MQ4eab82N96qRw2Ea9MeONRnU2LHmvSkp5thKmessLW365r1ggSlRNPJ4zADLHTvM+XJyTIY+eTKceKK5H40ZX329OVZDg9m3sfE2O9tk2O015FZUmCC5YYNZAgGTuZ94YtM93rPHDFpaudKsnzLFfInYv52rNWVlpnTXowcMHGiWjIzW99XaBJ84NzxLUBDHnFDIi9e7Cas1BaezNxaLk1DIR0XFQkpKXqOsbAHhcB1gekMlJQ2HcJiUj/aQ9fpOLMkZqGkX4bnkFiy5/Y88QYGAyThDoaYSxYABB2YqwaDpvaW1yUAbGysbg1JbgkFTnRUOm4yzf/+m9R99BG+9Zb4tX3VV213qtDbB4csvTckiK6vl9vffN42chYWmd9ioUU3v27TJBKXMTBMoWru2jtixw5TEdu0yGfHu3SYgXnSRyYybB1sRMxIURMIJhxuorV1DdfUyqqs/w+vdiFI2lLIBipqalYTDdVitaaSnT8Fq9UTfa7F4sNszsNkysdszsdt7YLfn4HDk4HD0xGpNRXWkbUKIo5SMaBYJx2JxRicCbI0pVSyirOxNqqv/g9aNcz9pQqE6gsEKwmFfG8d24XD0iix9cDr74HDk4nD0wm7Pxm7PxuHogdOZh8XSSuOzEN2EBAWRMKxWF9nZ3yE7+ztt7hMKeQkG9+H3lxIIlOD3lxAI7MXv34Pfv4eGht3U16+nouJfhEJVrRzBgsvVD7f7OFyuApzOXByOXJzOPlitySjlwGKxo3WIQKA0cvxynM7eJCWNwuMZ0tRgLkQcSFAQohmr1Y3VmovTefD51YPBWgKBEgKBMgKBMvz+vfh8W/F6N+H1bqKs7B8EAq300mmHUg48nuNxOvtGg4nLlY/bPQi3exAOR0+pxhIxJUFBiMNksyVjsyXjdrfd1z8c9uP376ahYRfhsJdw2I/WAUDhcJh2C7s9k4aGYmpr11BbuybaLbe2dg1+/16a97iyWFxYLG4sFidKObDZUiPtH9nYbJlYLA6UsqKUjXC4ITILbiXhsI/09NPIzr6UpCSZEkS0TRqahTiKhcNBGhq2UV+/Ea93Iz7fNrRuIBz2Ew43EApVRUoppQSD+9A6GFlCKGWPNJ6no7Wmrm4NAB7PMJKSRkSC1U4CgVIcjl643cfhdg/Cak2OjAPZgd+/G7s9JzpDrts9GLd7AC5XfrShXuswwWAlWodxOLLjebtEO6ShWYhjgMViw+0eiNs9EDj3iI7l8xVTVvYmpaXzqa39AoejD6mpJ2K398Dv343Xu5Gqqo8Ihbw4nX1wOvPweIbj9++hrOxNAoGyFsez23PQOkQwWEFjacbtHkR6+hTS008DLNTXb8Dr/ZaGhl2RXl2mR5fL1R+3ezAezxAcjuxIicq03yjlwO0e2KJ3mOg6UlIQQkSZ/CCMUgeOHfD7yyKlla2RpShSGsnCZstC6wBVVR9RWbm0WSO8wuXKx+nMJRCoIBAojQSX5lViHsLh+gPO53D0weUqwGJxRarELITDfoLBKoLBSkKhapRyYLW6sVg8kao1Z6Qx34nLlU9y8iiSkkbicvUnHPYRCtURCtVjtbqx2TKw2TKwWFwEg/sibUPlkZLRoFbvQXcmJQUhxCEzjditZ4YORzYORzZpaSe1c4Q70DpEbe1aLBY7LtdArFZXiz20DuHzbae+/hvq6zfQ0LANmy0Dh6MndntPtG6INtb7fEWEw14gjNZhlLLhcPTE4xmM1ZqC1sHIqPZ6wmEvWgcIh30EgxVUVX3Erl01h3UfLJYkUlLG4PEMIRTyEgpVEwxWoZQ1GkxstjS0DqF1AK0DkW7LPaOlIas1DZstBas1BdAEAuUEAuUEgxWRazGBzmpNxu0ehMs14IB7FQ8SFIQQnUopKykpo9vd7nYX4HYXkJV1XszSobXG59tGXd1a/P5dWCwerFYPFoubcNhLIFARHZtit2dFG+tNo/8qampWUla2AKs1CZstDas1lXDYT339NwSDFQSD1ZGM3Y7FYo8Ej9a6KXeUioxzcUXbhbQOAqFoW1HfvreSn//bzrpFrZKgIIQ4JimlcLvzcbvzD+Pd1x7WOcPhhkjbSAnBYDWhUE0keChstqxI8MnElMbCkUb6iuhcXz7fZsLhQHQkfmNPssbfk5NPOKx0HQoJCkII0UlMW0YeLtehPegpNXV8jFJ06GL6DDKl1LlKqQ1KqU1KqVmtbHcqpV6LbP9MKZUfy/QIIYRoX8yCgjJN948D5wHDgCuUUsP22+2HQIXW+jjgEeBPsUqPEEKIg4tlSWECsElrvUVr7QdeBS7cb58LgZciv88DzlQyhl8IIeImlkEhF9jR7HVxZF2r+2jTzF4F7DfhuxBCiK4S0zaFzqKUulEptUIptaK0tccACiGE6BSxDAo7geZN8H0j61rdR5knoaQB+z34FLTWz2itx2mtx/XoyIPAhRBCHJZYBoXPgUFKqQKllAO4HFiw3z4LgO9Hfv8u8G/d3ebdEEKIY0jMxilorYNKqZ8CCzEjNV7QWn+llPo9sEJrvQB4HnhFKbUJ2IcJHEIIIeKk202Ip5QqBbYd5tuzgbKD7pWY5N60Te5N2+TetO1ouzf9tdYHrX/vdkHhSCilVnRklsBEJPembXJv2ib3pm3d9d50i95HQgghuoYEBSGEEFGJFhSeiXcCjmJyb9om96Ztcm/a1i3vTUK1KQghhGhfopUUhBBCtCNhgsLBpvFOJEqpPKXUYqXU10qpr5RSP4usz1RKfaCU2hj5mRHvtMaDUsqqlPpCKfV/kdcFkandN0WmenfEO43xoJRKV0rNU0p9o5Rar5Q6ST4zhlLq55H/pXVKqblKKVd3/dwkRFDo4DTeiSQI/EJrPQyYCNwUuR+zgH9prQcB/4q8TkQ/A9Y3e/0n4JHIFO8VmCnfE9H/Au9rrYcAhZh7lPCfGaVULnALME5rPQIzWPdyuunnJiGCAh2bxjthaK13a61XRX6vwfxz59JyKvOXgIvik8L4UUr1BaYBz0VeK+AMzNTukLj3JQ2YjJmFAK21X2tdiXxmGtkAd2QONw+wm276uUmUoNCRabwTUuRpd2OAz4CeWuvdkU17gJ5xSlY8PQr8NxCOvM4CKiNTu0PifnYKgFLgxUjV2nNKqSTkM4PWeifwELAdEwyqgJV0089NogQF0QqlVDIwH7hVa13dfFtkYsKE6pqmlPoOUKK1XhnvtByFbMAJwJNa6zFAHftVFSXiZwYg0o5yISHgEQwAAANBSURBVCZw9gGSgHPjmqgjkChBoSPTeCcUpZQdExDmaK3fiKzeq5TqHdneGyiJV/riZBIwXSlVhKliPANTj54eqRaAxP3sFAPFWuvPIq/nYYJEon9mAM4CtmqtS7XWAeANzGepW35uEiUodGQa74QRqSd/Hlivtf6fZpuaT2X+feCtrk5bPGmtf6m17qu1zsd8Rv6ttb4SWIyZ2h0S8L4AaK33ADuUUoMjq84EvibBPzMR24GJSilP5H+r8d50y89NwgxeU0qdj6kvbpzG+744JylulFKnAB8Ba2mqO78L067wOtAPMxPtZVrrfXFJZJwppaYAt2utv6OUGoApOWQCXwBXaa0b4pm+eFBKjcY0wDuALcB1mC+WCf+ZUUr9DpiJ6dn3BXA9pg2h231uEiYoCCGEOLhEqT4SQgjRARIUhBBCRElQEEIIESVBQQghRJQEBSGEEFESFIToQkqpKY2zrwpxNJKgIIQQIkqCghCtUEpdpZRarpRarZR6OvKMhVql1CORefP/pZTqEdl3tFJqmVLqS6XUm43PFFBKHaeUWqSUWqOUWqWUGhg5fHKz5xLMiYyCFeKoIEFBiP0opYZiRqdO0lqPBkLAlf+/vftXjTKIwjD+nCCIQcHKxsJgGzCFYCFY5QYsTCN4BWnsJJAQ8B6EWEa0EEF7wWIhlVpYeQWp0gTBQhB9U8zsYJIispAY8Pl1Ozs77BTfnu8P+x5a0NnnJIvABNjsH3kBPElyi/Yv8en4K+BZkiXgLi1BE1oq7WNab4+btJwc6Vy4cPIU6b+zDNwGPvWT+Eu0oLffwOs+5yXwtvcZuJpk0se3gTdVdQW4nuQdQJIfAH29j0l2++svwAKwc/rbkk5mUZCOK2A7ydqhwaqNI/NmzYj5M//mFx6HOke8fSQd9wF4UFXXYPSuvkE7Xqaplw+BnSTfgP2qutfHHwGT3tFut6ru9zUuVtX8me5CmoFnKNIRSb5W1TrwvqrmgJ/AKq2xzJ3+3h7tuQO0WOSt/qM/TQ+FViCeV9XTvsbKGW5DmokpqdJfqqrvSS7/6+8hnSZvH0mSBq8UJEmDVwqSpMGiIEkaLAqSpMGiIEkaLAqSpMGiIEkaDgCzvdXLaGkdoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 844us/sample - loss: 0.2148 - acc: 0.9394\n",
      "Loss: 0.21475390930846722 Accuracy: 0.9393562\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2630 - acc: 0.2534\n",
      "Epoch 00001: val_loss improved from inf to 1.43848, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/001-1.4385.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.2628 - acc: 0.2534 - val_loss: 1.4385 - val_acc: 0.5565\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3804 - acc: 0.5557\n",
      "Epoch 00002: val_loss improved from 1.43848 to 0.95788, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/002-0.9579.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3803 - acc: 0.5558 - val_loss: 0.9579 - val_acc: 0.7032\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0775 - acc: 0.6518\n",
      "Epoch 00003: val_loss improved from 0.95788 to 0.75843, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/003-0.7584.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0776 - acc: 0.6518 - val_loss: 0.7584 - val_acc: 0.7589\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8953 - acc: 0.7157\n",
      "Epoch 00004: val_loss improved from 0.75843 to 0.61868, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/004-0.6187.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8955 - acc: 0.7157 - val_loss: 0.6187 - val_acc: 0.8092\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7590\n",
      "Epoch 00005: val_loss improved from 0.61868 to 0.52075, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/005-0.5208.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7537 - acc: 0.7589 - val_loss: 0.5208 - val_acc: 0.8388\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.7880\n",
      "Epoch 00006: val_loss improved from 0.52075 to 0.46855, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/006-0.4685.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6658 - acc: 0.7880 - val_loss: 0.4685 - val_acc: 0.8579\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5870 - acc: 0.8126\n",
      "Epoch 00007: val_loss improved from 0.46855 to 0.38018, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/007-0.3802.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5869 - acc: 0.8126 - val_loss: 0.3802 - val_acc: 0.8889\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8321\n",
      "Epoch 00008: val_loss improved from 0.38018 to 0.33257, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/008-0.3326.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5294 - acc: 0.8321 - val_loss: 0.3326 - val_acc: 0.9017\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.8486\n",
      "Epoch 00009: val_loss improved from 0.33257 to 0.30066, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/009-0.3007.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4742 - acc: 0.8486 - val_loss: 0.3007 - val_acc: 0.9161\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8635\n",
      "Epoch 00010: val_loss improved from 0.30066 to 0.29051, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/010-0.2905.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4306 - acc: 0.8636 - val_loss: 0.2905 - val_acc: 0.9173\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8737\n",
      "Epoch 00011: val_loss improved from 0.29051 to 0.24020, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/011-0.2402.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3995 - acc: 0.8737 - val_loss: 0.2402 - val_acc: 0.9336\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8857\n",
      "Epoch 00012: val_loss did not improve from 0.24020\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3622 - acc: 0.8857 - val_loss: 0.2544 - val_acc: 0.9243\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8920\n",
      "Epoch 00013: val_loss improved from 0.24020 to 0.23084, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/013-0.2308.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3428 - acc: 0.8920 - val_loss: 0.2308 - val_acc: 0.9357\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8988\n",
      "Epoch 00014: val_loss improved from 0.23084 to 0.20664, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/014-0.2066.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3182 - acc: 0.8988 - val_loss: 0.2066 - val_acc: 0.9406\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9051\n",
      "Epoch 00015: val_loss improved from 0.20664 to 0.20077, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/015-0.2008.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2994 - acc: 0.9051 - val_loss: 0.2008 - val_acc: 0.9446\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9088\n",
      "Epoch 00016: val_loss improved from 0.20077 to 0.19262, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/016-0.1926.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2847 - acc: 0.9088 - val_loss: 0.1926 - val_acc: 0.9441\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9133\n",
      "Epoch 00017: val_loss improved from 0.19262 to 0.18700, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/017-0.1870.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2692 - acc: 0.9133 - val_loss: 0.1870 - val_acc: 0.9434\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9164\n",
      "Epoch 00018: val_loss improved from 0.18700 to 0.17450, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/018-0.1745.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2622 - acc: 0.9164 - val_loss: 0.1745 - val_acc: 0.9497\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9204\n",
      "Epoch 00019: val_loss improved from 0.17450 to 0.16719, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/019-0.1672.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2501 - acc: 0.9204 - val_loss: 0.1672 - val_acc: 0.9536\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9259\n",
      "Epoch 00020: val_loss improved from 0.16719 to 0.16713, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/020-0.1671.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2328 - acc: 0.9259 - val_loss: 0.1671 - val_acc: 0.9527\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9300\n",
      "Epoch 00021: val_loss improved from 0.16713 to 0.15937, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/021-0.1594.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2198 - acc: 0.9300 - val_loss: 0.1594 - val_acc: 0.9532\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9305\n",
      "Epoch 00022: val_loss improved from 0.15937 to 0.15334, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/022-0.1533.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2189 - acc: 0.9305 - val_loss: 0.1533 - val_acc: 0.9539\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9321\n",
      "Epoch 00023: val_loss did not improve from 0.15334\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2089 - acc: 0.9321 - val_loss: 0.1609 - val_acc: 0.9548\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9351\n",
      "Epoch 00024: val_loss improved from 0.15334 to 0.14559, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/024-0.1456.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2023 - acc: 0.9351 - val_loss: 0.1456 - val_acc: 0.9595\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9375\n",
      "Epoch 00025: val_loss did not improve from 0.14559\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1945 - acc: 0.9375 - val_loss: 0.1704 - val_acc: 0.9467\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9379\n",
      "Epoch 00026: val_loss did not improve from 0.14559\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1936 - acc: 0.9379 - val_loss: 0.1510 - val_acc: 0.9536\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9411\n",
      "Epoch 00027: val_loss improved from 0.14559 to 0.13407, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/027-0.1341.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1845 - acc: 0.9411 - val_loss: 0.1341 - val_acc: 0.9604\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9445\n",
      "Epoch 00028: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1712 - acc: 0.9445 - val_loss: 0.1479 - val_acc: 0.9592\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9446\n",
      "Epoch 00029: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1702 - acc: 0.9446 - val_loss: 0.1361 - val_acc: 0.9588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9461\n",
      "Epoch 00030: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1628 - acc: 0.9461 - val_loss: 0.1498 - val_acc: 0.9543\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9484\n",
      "Epoch 00031: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1590 - acc: 0.9484 - val_loss: 0.1382 - val_acc: 0.9625\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9504\n",
      "Epoch 00032: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1535 - acc: 0.9504 - val_loss: 0.1413 - val_acc: 0.9581\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9499\n",
      "Epoch 00033: val_loss improved from 0.13407 to 0.13171, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/033-0.1317.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1541 - acc: 0.9500 - val_loss: 0.1317 - val_acc: 0.9592\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9528\n",
      "Epoch 00034: val_loss did not improve from 0.13171\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1425 - acc: 0.9528 - val_loss: 0.1460 - val_acc: 0.9613\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9531\n",
      "Epoch 00035: val_loss did not improve from 0.13171\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1408 - acc: 0.9531 - val_loss: 0.1416 - val_acc: 0.9634\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9536\n",
      "Epoch 00036: val_loss did not improve from 0.13171\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1421 - acc: 0.9536 - val_loss: 0.1481 - val_acc: 0.9581\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9555\n",
      "Epoch 00037: val_loss improved from 0.13171 to 0.12353, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/037-0.1235.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1349 - acc: 0.9555 - val_loss: 0.1235 - val_acc: 0.9609\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9562\n",
      "Epoch 00038: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1311 - acc: 0.9562 - val_loss: 0.1338 - val_acc: 0.9613\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9586\n",
      "Epoch 00039: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1260 - acc: 0.9586 - val_loss: 0.1381 - val_acc: 0.9627\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9582\n",
      "Epoch 00040: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1258 - acc: 0.9582 - val_loss: 0.1273 - val_acc: 0.9606\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9614\n",
      "Epoch 00041: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1187 - acc: 0.9614 - val_loss: 0.1422 - val_acc: 0.9564\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9607\n",
      "Epoch 00042: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1167 - acc: 0.9607 - val_loss: 0.1341 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9617\n",
      "Epoch 00043: val_loss did not improve from 0.12353\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1142 - acc: 0.9617 - val_loss: 0.1315 - val_acc: 0.9616\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9630\n",
      "Epoch 00044: val_loss improved from 0.12353 to 0.12322, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/044-0.1232.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1102 - acc: 0.9630 - val_loss: 0.1232 - val_acc: 0.9641\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9637\n",
      "Epoch 00045: val_loss improved from 0.12322 to 0.12089, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/045-0.1209.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1099 - acc: 0.9637 - val_loss: 0.1209 - val_acc: 0.9641\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9648\n",
      "Epoch 00046: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1075 - acc: 0.9648 - val_loss: 0.1422 - val_acc: 0.9613\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9643\n",
      "Epoch 00047: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1035 - acc: 0.9644 - val_loss: 0.1372 - val_acc: 0.9639\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9653\n",
      "Epoch 00048: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1020 - acc: 0.9653 - val_loss: 0.1271 - val_acc: 0.9660\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9658\n",
      "Epoch 00049: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1006 - acc: 0.9658 - val_loss: 0.1326 - val_acc: 0.9644\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9668\n",
      "Epoch 00050: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0981 - acc: 0.9669 - val_loss: 0.1380 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9676\n",
      "Epoch 00051: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0946 - acc: 0.9676 - val_loss: 0.1357 - val_acc: 0.9602\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9688\n",
      "Epoch 00052: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0931 - acc: 0.9688 - val_loss: 0.1282 - val_acc: 0.9637\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9703\n",
      "Epoch 00053: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0885 - acc: 0.9703 - val_loss: 0.1358 - val_acc: 0.9630\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9691\n",
      "Epoch 00054: val_loss did not improve from 0.12089\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0894 - acc: 0.9691 - val_loss: 0.1361 - val_acc: 0.9639\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9701\n",
      "Epoch 00055: val_loss improved from 0.12089 to 0.11875, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/055-0.1188.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0895 - acc: 0.9700 - val_loss: 0.1188 - val_acc: 0.9660\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9726\n",
      "Epoch 00056: val_loss improved from 0.11875 to 0.11300, saving model to model/checkpoint/1D_CNN_custom_DO_8_conv_checkpoint/056-0.1130.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0820 - acc: 0.9726 - val_loss: 0.1130 - val_acc: 0.9683\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9717\n",
      "Epoch 00057: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0832 - acc: 0.9717 - val_loss: 0.1405 - val_acc: 0.9623\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9739\n",
      "Epoch 00058: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0789 - acc: 0.9739 - val_loss: 0.1168 - val_acc: 0.9672\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9745\n",
      "Epoch 00059: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0763 - acc: 0.9745 - val_loss: 0.1248 - val_acc: 0.9669\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9728\n",
      "Epoch 00060: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0785 - acc: 0.9728 - val_loss: 0.1338 - val_acc: 0.9648\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9740\n",
      "Epoch 00061: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0769 - acc: 0.9740 - val_loss: 0.1254 - val_acc: 0.9630\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9751\n",
      "Epoch 00062: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0732 - acc: 0.9751 - val_loss: 0.1280 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9752\n",
      "Epoch 00063: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0727 - acc: 0.9751 - val_loss: 0.1314 - val_acc: 0.9641\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9742\n",
      "Epoch 00064: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0747 - acc: 0.9742 - val_loss: 0.1281 - val_acc: 0.9674\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9768\n",
      "Epoch 00065: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0685 - acc: 0.9768 - val_loss: 0.1224 - val_acc: 0.9665\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9778\n",
      "Epoch 00066: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0674 - acc: 0.9778 - val_loss: 0.1199 - val_acc: 0.9674\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9771\n",
      "Epoch 00067: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0653 - acc: 0.9771 - val_loss: 0.1349 - val_acc: 0.9641\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9779\n",
      "Epoch 00068: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0662 - acc: 0.9779 - val_loss: 0.1353 - val_acc: 0.9686\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9784\n",
      "Epoch 00069: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0625 - acc: 0.9784 - val_loss: 0.1328 - val_acc: 0.9662\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9785\n",
      "Epoch 00070: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0622 - acc: 0.9785 - val_loss: 0.1223 - val_acc: 0.9660\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9785\n",
      "Epoch 00071: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0625 - acc: 0.9785 - val_loss: 0.1175 - val_acc: 0.9672\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9792\n",
      "Epoch 00072: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0610 - acc: 0.9792 - val_loss: 0.1227 - val_acc: 0.9674\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9799\n",
      "Epoch 00073: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0579 - acc: 0.9799 - val_loss: 0.1299 - val_acc: 0.9683\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9803\n",
      "Epoch 00074: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0588 - acc: 0.9803 - val_loss: 0.1477 - val_acc: 0.9604\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9808\n",
      "Epoch 00075: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0572 - acc: 0.9808 - val_loss: 0.1269 - val_acc: 0.9667\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9808\n",
      "Epoch 00076: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0559 - acc: 0.9808 - val_loss: 0.1389 - val_acc: 0.9667\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9802\n",
      "Epoch 00077: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0569 - acc: 0.9802 - val_loss: 0.1236 - val_acc: 0.9681\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9811\n",
      "Epoch 00078: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0532 - acc: 0.9810 - val_loss: 0.1330 - val_acc: 0.9669\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9811\n",
      "Epoch 00079: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0549 - acc: 0.9811 - val_loss: 0.1276 - val_acc: 0.9693\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9825\n",
      "Epoch 00080: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0525 - acc: 0.9825 - val_loss: 0.1323 - val_acc: 0.9672\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9829\n",
      "Epoch 00081: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0495 - acc: 0.9829 - val_loss: 0.1221 - val_acc: 0.9697\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9832\n",
      "Epoch 00082: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0500 - acc: 0.9832 - val_loss: 0.1267 - val_acc: 0.9697\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 00083: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.1201 - val_acc: 0.9681\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9828\n",
      "Epoch 00084: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0499 - acc: 0.9828 - val_loss: 0.1401 - val_acc: 0.9651\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9838\n",
      "Epoch 00085: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0474 - acc: 0.9838 - val_loss: 0.1456 - val_acc: 0.9651\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9830\n",
      "Epoch 00086: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0486 - acc: 0.9830 - val_loss: 0.1229 - val_acc: 0.9704\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9837\n",
      "Epoch 00087: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0482 - acc: 0.9837 - val_loss: 0.1329 - val_acc: 0.9665\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9850\n",
      "Epoch 00088: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0434 - acc: 0.9850 - val_loss: 0.1261 - val_acc: 0.9690\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9842\n",
      "Epoch 00089: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0454 - acc: 0.9842 - val_loss: 0.1548 - val_acc: 0.9669\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9844\n",
      "Epoch 00090: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0454 - acc: 0.9844 - val_loss: 0.1371 - val_acc: 0.9681\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9845\n",
      "Epoch 00091: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0455 - acc: 0.9845 - val_loss: 0.1300 - val_acc: 0.9688\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9846\n",
      "Epoch 00092: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0453 - acc: 0.9846 - val_loss: 0.1213 - val_acc: 0.9709\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9846\n",
      "Epoch 00093: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0428 - acc: 0.9846 - val_loss: 0.1314 - val_acc: 0.9697\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9858\n",
      "Epoch 00094: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.1215 - val_acc: 0.9720\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9849\n",
      "Epoch 00095: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0418 - acc: 0.9849 - val_loss: 0.1454 - val_acc: 0.9674\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9856\n",
      "Epoch 00096: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0413 - acc: 0.9856 - val_loss: 0.1504 - val_acc: 0.9658\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9853\n",
      "Epoch 00097: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0415 - acc: 0.9853 - val_loss: 0.1443 - val_acc: 0.9688\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9873\n",
      "Epoch 00098: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0379 - acc: 0.9873 - val_loss: 0.1498 - val_acc: 0.9683\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9871\n",
      "Epoch 00099: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0374 - acc: 0.9871 - val_loss: 0.1334 - val_acc: 0.9665\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9875\n",
      "Epoch 00100: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0366 - acc: 0.9875 - val_loss: 0.1316 - val_acc: 0.9702\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9874\n",
      "Epoch 00101: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0371 - acc: 0.9874 - val_loss: 0.1362 - val_acc: 0.9697\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9877\n",
      "Epoch 00102: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0355 - acc: 0.9877 - val_loss: 0.1419 - val_acc: 0.9702\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9870\n",
      "Epoch 00103: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0384 - acc: 0.9870 - val_loss: 0.1371 - val_acc: 0.9700\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9880\n",
      "Epoch 00104: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0347 - acc: 0.9880 - val_loss: 0.1302 - val_acc: 0.9713\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9873\n",
      "Epoch 00105: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0358 - acc: 0.9873 - val_loss: 0.1331 - val_acc: 0.9709\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9889\n",
      "Epoch 00106: val_loss did not improve from 0.11300\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0338 - acc: 0.9889 - val_loss: 0.1306 - val_acc: 0.9711\n",
      "\n",
      "1D_CNN_custom_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 942us/sample - loss: 0.1764 - acc: 0.9479\n",
      "Loss: 0.1763709456370689 Accuracy: 0.9478712\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0728 - acc: 0.3284\n",
      "Epoch 00001: val_loss improved from inf to 1.22667, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/001-1.2267.hdf5\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 2.0727 - acc: 0.3284 - val_loss: 1.2267 - val_acc: 0.6301\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0761 - acc: 0.6482\n",
      "Epoch 00002: val_loss improved from 1.22667 to 0.71116, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/002-0.7112.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0761 - acc: 0.6482 - val_loss: 0.7112 - val_acc: 0.7831\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7483\n",
      "Epoch 00003: val_loss improved from 0.71116 to 0.52655, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/003-0.5265.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7822 - acc: 0.7483 - val_loss: 0.5265 - val_acc: 0.8325\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.8047\n",
      "Epoch 00004: val_loss improved from 0.52655 to 0.40200, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/004-0.4020.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6121 - acc: 0.8047 - val_loss: 0.4020 - val_acc: 0.8742\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8344\n",
      "Epoch 00005: val_loss improved from 0.40200 to 0.35026, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/005-0.3503.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5087 - acc: 0.8344 - val_loss: 0.3503 - val_acc: 0.8877\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8557\n",
      "Epoch 00006: val_loss improved from 0.35026 to 0.29286, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/006-0.2929.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4463 - acc: 0.8557 - val_loss: 0.2929 - val_acc: 0.9057\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8742\n",
      "Epoch 00007: val_loss improved from 0.29286 to 0.25462, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/007-0.2546.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3909 - acc: 0.8741 - val_loss: 0.2546 - val_acc: 0.9185\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8846\n",
      "Epoch 00008: val_loss improved from 0.25462 to 0.22311, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/008-0.2231.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3583 - acc: 0.8846 - val_loss: 0.2231 - val_acc: 0.9292\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8981\n",
      "Epoch 00009: val_loss did not improve from 0.22311\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3169 - acc: 0.8981 - val_loss: 0.2282 - val_acc: 0.9269\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9071\n",
      "Epoch 00010: val_loss improved from 0.22311 to 0.19740, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/010-0.1974.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2952 - acc: 0.9071 - val_loss: 0.1974 - val_acc: 0.9366\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9161\n",
      "Epoch 00011: val_loss did not improve from 0.19740\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2638 - acc: 0.9161 - val_loss: 0.2198 - val_acc: 0.9278\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9204\n",
      "Epoch 00012: val_loss improved from 0.19740 to 0.18048, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/012-0.1805.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2443 - acc: 0.9204 - val_loss: 0.1805 - val_acc: 0.9404\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9257\n",
      "Epoch 00013: val_loss improved from 0.18048 to 0.16072, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/013-0.1607.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2279 - acc: 0.9257 - val_loss: 0.1607 - val_acc: 0.9469\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9335\n",
      "Epoch 00014: val_loss did not improve from 0.16072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2095 - acc: 0.9335 - val_loss: 0.1673 - val_acc: 0.9455\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9339\n",
      "Epoch 00015: val_loss did not improve from 0.16072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2038 - acc: 0.9339 - val_loss: 0.1751 - val_acc: 0.9429\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9381\n",
      "Epoch 00016: val_loss did not improve from 0.16072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1920 - acc: 0.9381 - val_loss: 0.1791 - val_acc: 0.9429\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9452\n",
      "Epoch 00017: val_loss improved from 0.16072 to 0.15150, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/017-0.1515.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1696 - acc: 0.9452 - val_loss: 0.1515 - val_acc: 0.9513\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9440\n",
      "Epoch 00018: val_loss improved from 0.15150 to 0.13710, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/018-0.1371.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1713 - acc: 0.9440 - val_loss: 0.1371 - val_acc: 0.9567\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9502\n",
      "Epoch 00019: val_loss did not improve from 0.13710\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1535 - acc: 0.9502 - val_loss: 0.1380 - val_acc: 0.9557\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9510\n",
      "Epoch 00020: val_loss improved from 0.13710 to 0.12109, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/020-0.1211.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1466 - acc: 0.9510 - val_loss: 0.1211 - val_acc: 0.9616\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9544\n",
      "Epoch 00021: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1377 - acc: 0.9544 - val_loss: 0.1458 - val_acc: 0.9560\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9562\n",
      "Epoch 00022: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1316 - acc: 0.9562 - val_loss: 0.1327 - val_acc: 0.9560\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9569\n",
      "Epoch 00023: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1288 - acc: 0.9569 - val_loss: 0.1289 - val_acc: 0.9590\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9604\n",
      "Epoch 00024: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1220 - acc: 0.9604 - val_loss: 0.1392 - val_acc: 0.9560\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9629\n",
      "Epoch 00025: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1120 - acc: 0.9629 - val_loss: 0.1416 - val_acc: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9633\n",
      "Epoch 00026: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1100 - acc: 0.9633 - val_loss: 0.1398 - val_acc: 0.9574\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9663\n",
      "Epoch 00027: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1015 - acc: 0.9663 - val_loss: 0.1377 - val_acc: 0.9569\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9660\n",
      "Epoch 00028: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0999 - acc: 0.9660 - val_loss: 0.1257 - val_acc: 0.9620\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9683\n",
      "Epoch 00029: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0947 - acc: 0.9683 - val_loss: 0.1386 - val_acc: 0.9569\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9692\n",
      "Epoch 00030: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0906 - acc: 0.9692 - val_loss: 0.1471 - val_acc: 0.9574\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9705\n",
      "Epoch 00031: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0893 - acc: 0.9705 - val_loss: 0.1327 - val_acc: 0.9641\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9718\n",
      "Epoch 00032: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0846 - acc: 0.9718 - val_loss: 0.1265 - val_acc: 0.9616\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9733\n",
      "Epoch 00033: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0773 - acc: 0.9733 - val_loss: 0.1792 - val_acc: 0.9506\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9749\n",
      "Epoch 00034: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0761 - acc: 0.9749 - val_loss: 0.1325 - val_acc: 0.9618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9751\n",
      "Epoch 00035: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0734 - acc: 0.9751 - val_loss: 0.1367 - val_acc: 0.9618\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9756\n",
      "Epoch 00036: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0726 - acc: 0.9756 - val_loss: 0.1266 - val_acc: 0.9644\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9771\n",
      "Epoch 00037: val_loss did not improve from 0.12109\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0657 - acc: 0.9771 - val_loss: 0.1476 - val_acc: 0.9578\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9776\n",
      "Epoch 00038: val_loss improved from 0.12109 to 0.12063, saving model to model/checkpoint/1D_CNN_custom_DO_9_conv_checkpoint/038-0.1206.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0667 - acc: 0.9776 - val_loss: 0.1206 - val_acc: 0.9646\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9796\n",
      "Epoch 00039: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0594 - acc: 0.9796 - val_loss: 0.1400 - val_acc: 0.9639\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9791\n",
      "Epoch 00040: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0622 - acc: 0.9791 - val_loss: 0.1420 - val_acc: 0.9618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9804\n",
      "Epoch 00041: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0578 - acc: 0.9804 - val_loss: 0.1511 - val_acc: 0.9616\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9798\n",
      "Epoch 00042: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0583 - acc: 0.9798 - val_loss: 0.1291 - val_acc: 0.9653\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9820\n",
      "Epoch 00043: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0541 - acc: 0.9820 - val_loss: 0.1296 - val_acc: 0.9665\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9814\n",
      "Epoch 00044: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0559 - acc: 0.9814 - val_loss: 0.1309 - val_acc: 0.9651\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9809\n",
      "Epoch 00045: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0569 - acc: 0.9809 - val_loss: 0.1530 - val_acc: 0.9618\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9826\n",
      "Epoch 00046: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0502 - acc: 0.9826 - val_loss: 0.1641 - val_acc: 0.9620\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9828\n",
      "Epoch 00047: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0497 - acc: 0.9828 - val_loss: 0.1511 - val_acc: 0.9651\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9837\n",
      "Epoch 00048: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0495 - acc: 0.9837 - val_loss: 0.1507 - val_acc: 0.9609\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9852\n",
      "Epoch 00049: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0449 - acc: 0.9852 - val_loss: 0.1483 - val_acc: 0.9634\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9842\n",
      "Epoch 00050: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0444 - acc: 0.9842 - val_loss: 0.1488 - val_acc: 0.9665\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9853\n",
      "Epoch 00051: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0447 - acc: 0.9853 - val_loss: 0.1400 - val_acc: 0.9646\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9873\n",
      "Epoch 00052: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0385 - acc: 0.9873 - val_loss: 0.1563 - val_acc: 0.9632\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9845\n",
      "Epoch 00053: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0457 - acc: 0.9845 - val_loss: 0.1567 - val_acc: 0.9627\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9863\n",
      "Epoch 00054: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0389 - acc: 0.9863 - val_loss: 0.1529 - val_acc: 0.9620\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9838\n",
      "Epoch 00055: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0476 - acc: 0.9838 - val_loss: 0.1598 - val_acc: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9877\n",
      "Epoch 00056: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0350 - acc: 0.9877 - val_loss: 0.1389 - val_acc: 0.9690\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9875\n",
      "Epoch 00057: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0360 - acc: 0.9875 - val_loss: 0.1505 - val_acc: 0.9653\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9880\n",
      "Epoch 00058: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0367 - acc: 0.9880 - val_loss: 0.1501 - val_acc: 0.9672\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9885\n",
      "Epoch 00059: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0355 - acc: 0.9885 - val_loss: 0.1555 - val_acc: 0.9616\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00060: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.1577 - val_acc: 0.9660\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9879\n",
      "Epoch 00061: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0378 - acc: 0.9879 - val_loss: 0.1264 - val_acc: 0.9695\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9889\n",
      "Epoch 00062: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0326 - acc: 0.9889 - val_loss: 0.1429 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9894\n",
      "Epoch 00063: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0302 - acc: 0.9894 - val_loss: 0.1601 - val_acc: 0.9627\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9898\n",
      "Epoch 00064: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0314 - acc: 0.9898 - val_loss: 0.1519 - val_acc: 0.9641\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9891\n",
      "Epoch 00065: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0333 - acc: 0.9891 - val_loss: 0.1567 - val_acc: 0.9613\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9903\n",
      "Epoch 00066: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0296 - acc: 0.9903 - val_loss: 0.1588 - val_acc: 0.9646\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9900\n",
      "Epoch 00067: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0327 - acc: 0.9900 - val_loss: 0.1422 - val_acc: 0.9660\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9907\n",
      "Epoch 00068: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0275 - acc: 0.9907 - val_loss: 0.1521 - val_acc: 0.9669\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9901\n",
      "Epoch 00069: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0291 - acc: 0.9901 - val_loss: 0.1674 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9904\n",
      "Epoch 00070: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0296 - acc: 0.9904 - val_loss: 0.1584 - val_acc: 0.9641\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9907\n",
      "Epoch 00071: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0277 - acc: 0.9907 - val_loss: 0.1470 - val_acc: 0.9681\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9911\n",
      "Epoch 00072: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0279 - acc: 0.9911 - val_loss: 0.1639 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9922\n",
      "Epoch 00073: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0242 - acc: 0.9922 - val_loss: 0.1602 - val_acc: 0.9644\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9912\n",
      "Epoch 00074: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0266 - acc: 0.9913 - val_loss: 0.1760 - val_acc: 0.9662\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9908\n",
      "Epoch 00075: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0277 - acc: 0.9908 - val_loss: 0.1763 - val_acc: 0.9662\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9905\n",
      "Epoch 00076: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0274 - acc: 0.9905 - val_loss: 0.1814 - val_acc: 0.9646\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9919\n",
      "Epoch 00077: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0247 - acc: 0.9919 - val_loss: 0.1463 - val_acc: 0.9683\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9925\n",
      "Epoch 00078: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0245 - acc: 0.9925 - val_loss: 0.1856 - val_acc: 0.9623\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 00079: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0218 - acc: 0.9928 - val_loss: 0.1767 - val_acc: 0.9644\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9921\n",
      "Epoch 00080: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0236 - acc: 0.9921 - val_loss: 0.1604 - val_acc: 0.9653\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9923\n",
      "Epoch 00081: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0235 - acc: 0.9923 - val_loss: 0.1763 - val_acc: 0.9665\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9923\n",
      "Epoch 00082: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0239 - acc: 0.9923 - val_loss: 0.1665 - val_acc: 0.9667\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9928\n",
      "Epoch 00083: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0228 - acc: 0.9928 - val_loss: 0.1811 - val_acc: 0.9627\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9914\n",
      "Epoch 00084: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0248 - acc: 0.9914 - val_loss: 0.1616 - val_acc: 0.9665\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9945\n",
      "Epoch 00085: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0169 - acc: 0.9945 - val_loss: 0.1670 - val_acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9930\n",
      "Epoch 00086: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0211 - acc: 0.9930 - val_loss: 0.1937 - val_acc: 0.9641\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9929\n",
      "Epoch 00087: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0212 - acc: 0.9929 - val_loss: 0.1773 - val_acc: 0.9623\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00088: val_loss did not improve from 0.12063\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.1782 - val_acc: 0.9676\n",
      "\n",
      "1D_CNN_custom_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 910us/sample - loss: 0.1941 - acc: 0.9464\n",
      "Loss: 0.19408866517624876 Accuracy: 0.94641745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 596us/sample - loss: 2.1382 - acc: 0.3319\n",
      "Loss: 2.1382002996383305 Accuracy: 0.33187956\n",
      "\n",
      "1D_CNN_custom_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 813us/sample - loss: 1.8384 - acc: 0.4411\n",
      "Loss: 1.8384307471886354 Accuracy: 0.4411215\n",
      "\n",
      "1D_CNN_custom_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 860us/sample - loss: 1.4752 - acc: 0.5562\n",
      "Loss: 1.4751907923758834 Accuracy: 0.5561786\n",
      "\n",
      "1D_CNN_custom_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 934us/sample - loss: 1.0685 - acc: 0.6881\n",
      "Loss: 1.0684766499795646 Accuracy: 0.68805814\n",
      "\n",
      "1D_CNN_custom_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 900us/sample - loss: 0.8442 - acc: 0.7595\n",
      "Loss: 0.844184148311615 Accuracy: 0.7595016\n",
      "\n",
      "1D_CNN_custom_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4309 - acc: 0.8808\n",
      "Loss: 0.43092903614291767 Accuracy: 0.8807892\n",
      "\n",
      "1D_CNN_custom_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 975us/sample - loss: 0.2148 - acc: 0.9394\n",
      "Loss: 0.21475390930846722 Accuracy: 0.9393562\n",
      "\n",
      "1D_CNN_custom_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 965us/sample - loss: 0.1764 - acc: 0.9479\n",
      "Loss: 0.1763709456370689 Accuracy: 0.9478712\n",
      "\n",
      "1D_CNN_custom_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1941 - acc: 0.9464\n",
      "Loss: 0.19408866517624876 Accuracy: 0.94641745\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 696us/sample - loss: 4.9242 - acc: 0.3040\n",
      "Loss: 4.924214981154366 Accuracy: 0.30404985\n",
      "\n",
      "1D_CNN_custom_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 885us/sample - loss: 4.4308 - acc: 0.4087\n",
      "Loss: 4.430827431366822 Accuracy: 0.40872273\n",
      "\n",
      "1D_CNN_custom_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 954us/sample - loss: 2.6307 - acc: 0.5844\n",
      "Loss: 2.630683792021168 Accuracy: 0.58442366\n",
      "\n",
      "1D_CNN_custom_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 964us/sample - loss: 1.6462 - acc: 0.7128\n",
      "Loss: 1.6462175498498934 Accuracy: 0.7127726\n",
      "\n",
      "1D_CNN_custom_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 967us/sample - loss: 1.2817 - acc: 0.7732\n",
      "Loss: 1.281714776817512 Accuracy: 0.77320874\n",
      "\n",
      "1D_CNN_custom_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5348 - acc: 0.8901\n",
      "Loss: 0.5347867643636645 Accuracy: 0.890135\n",
      "\n",
      "1D_CNN_custom_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 989us/sample - loss: 0.2259 - acc: 0.9400\n",
      "Loss: 0.22590514733876765 Accuracy: 0.93997926\n",
      "\n",
      "1D_CNN_custom_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 978us/sample - loss: 0.1984 - acc: 0.9560\n",
      "Loss: 0.1984219136498946 Accuracy: 0.95597094\n",
      "\n",
      "1D_CNN_custom_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2644 - acc: 0.9537\n",
      "Loss: 0.26437774926195035 Accuracy: 0.9536864\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
