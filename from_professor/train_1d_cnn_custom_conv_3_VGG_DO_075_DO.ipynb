{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5160 - acc: 0.1722\n",
      "Epoch 00001: val_loss improved from inf to 1.75446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/001-1.7545.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.5159 - acc: 0.1723 - val_loss: 1.7545 - val_acc: 0.4570\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6667 - acc: 0.4482\n",
      "Epoch 00002: val_loss improved from 1.75446 to 1.39848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/002-1.3985.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.6667 - acc: 0.4482 - val_loss: 1.3985 - val_acc: 0.5709\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4421 - acc: 0.5248\n",
      "Epoch 00003: val_loss improved from 1.39848 to 1.20559, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/003-1.2056.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4420 - acc: 0.5248 - val_loss: 1.2056 - val_acc: 0.6278\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3135 - acc: 0.5679\n",
      "Epoch 00004: val_loss improved from 1.20559 to 1.16683, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/004-1.1668.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3135 - acc: 0.5680 - val_loss: 1.1668 - val_acc: 0.6469\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2122 - acc: 0.6106\n",
      "Epoch 00005: val_loss improved from 1.16683 to 1.04138, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/005-1.0414.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2122 - acc: 0.6106 - val_loss: 1.0414 - val_acc: 0.6799\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.6499\n",
      "Epoch 00006: val_loss improved from 1.04138 to 0.89115, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/006-0.8912.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0992 - acc: 0.6499 - val_loss: 0.8912 - val_acc: 0.7435\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9970 - acc: 0.6880\n",
      "Epoch 00007: val_loss improved from 0.89115 to 0.80802, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/007-0.8080.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9969 - acc: 0.6880 - val_loss: 0.8080 - val_acc: 0.7673\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7217\n",
      "Epoch 00008: val_loss improved from 0.80802 to 0.71729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/008-0.7173.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9034 - acc: 0.7217 - val_loss: 0.7173 - val_acc: 0.7941\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8328 - acc: 0.7450\n",
      "Epoch 00009: val_loss improved from 0.71729 to 0.65574, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/009-0.6557.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8327 - acc: 0.7451 - val_loss: 0.6557 - val_acc: 0.8081\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7652 - acc: 0.7696\n",
      "Epoch 00010: val_loss improved from 0.65574 to 0.62170, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/010-0.6217.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7652 - acc: 0.7696 - val_loss: 0.6217 - val_acc: 0.8239\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7175 - acc: 0.7840\n",
      "Epoch 00011: val_loss improved from 0.62170 to 0.54866, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/011-0.5487.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7175 - acc: 0.7840 - val_loss: 0.5487 - val_acc: 0.8484\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6631 - acc: 0.8007\n",
      "Epoch 00012: val_loss did not improve from 0.54866\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6630 - acc: 0.8007 - val_loss: 0.5576 - val_acc: 0.8519\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.8133\n",
      "Epoch 00013: val_loss did not improve from 0.54866\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6253 - acc: 0.8133 - val_loss: 0.5558 - val_acc: 0.8456\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8218\n",
      "Epoch 00014: val_loss improved from 0.54866 to 0.47181, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/014-0.4718.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5940 - acc: 0.8218 - val_loss: 0.4718 - val_acc: 0.8719\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.8326\n",
      "Epoch 00015: val_loss did not improve from 0.47181\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5583 - acc: 0.8326 - val_loss: 0.4721 - val_acc: 0.8733\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8423\n",
      "Epoch 00016: val_loss improved from 0.47181 to 0.43467, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/016-0.4347.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5319 - acc: 0.8423 - val_loss: 0.4347 - val_acc: 0.8772\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.8462\n",
      "Epoch 00017: val_loss did not improve from 0.43467\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5094 - acc: 0.8462 - val_loss: 0.4450 - val_acc: 0.8782\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4918 - acc: 0.8524\n",
      "Epoch 00018: val_loss improved from 0.43467 to 0.41849, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/018-0.4185.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4917 - acc: 0.8524 - val_loss: 0.4185 - val_acc: 0.8863\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8597\n",
      "Epoch 00019: val_loss improved from 0.41849 to 0.38736, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/019-0.3874.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4620 - acc: 0.8597 - val_loss: 0.3874 - val_acc: 0.8947\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8641\n",
      "Epoch 00020: val_loss improved from 0.38736 to 0.36754, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/020-0.3675.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4505 - acc: 0.8641 - val_loss: 0.3675 - val_acc: 0.9036\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8711\n",
      "Epoch 00021: val_loss did not improve from 0.36754\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4282 - acc: 0.8711 - val_loss: 0.3775 - val_acc: 0.8998\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8754\n",
      "Epoch 00022: val_loss did not improve from 0.36754\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4138 - acc: 0.8753 - val_loss: 0.3978 - val_acc: 0.8880\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8801\n",
      "Epoch 00023: val_loss did not improve from 0.36754\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4038 - acc: 0.8802 - val_loss: 0.3901 - val_acc: 0.8887\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8843\n",
      "Epoch 00024: val_loss did not improve from 0.36754\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3851 - acc: 0.8843 - val_loss: 0.3702 - val_acc: 0.9017\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8888\n",
      "Epoch 00025: val_loss improved from 0.36754 to 0.34803, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/025-0.3480.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3694 - acc: 0.8888 - val_loss: 0.3480 - val_acc: 0.9080\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8895\n",
      "Epoch 00026: val_loss improved from 0.34803 to 0.32914, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/026-0.3291.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3658 - acc: 0.8895 - val_loss: 0.3291 - val_acc: 0.9089\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8915\n",
      "Epoch 00027: val_loss did not improve from 0.32914\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3534 - acc: 0.8916 - val_loss: 0.3306 - val_acc: 0.9133\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8966\n",
      "Epoch 00028: val_loss did not improve from 0.32914\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3372 - acc: 0.8966 - val_loss: 0.3334 - val_acc: 0.9108\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8993\n",
      "Epoch 00029: val_loss improved from 0.32914 to 0.32902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/029-0.3290.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3314 - acc: 0.8994 - val_loss: 0.3290 - val_acc: 0.9115\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9023\n",
      "Epoch 00030: val_loss improved from 0.32902 to 0.31153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/030-0.3115.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3172 - acc: 0.9024 - val_loss: 0.3115 - val_acc: 0.9194\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9058\n",
      "Epoch 00031: val_loss did not improve from 0.31153\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3064 - acc: 0.9058 - val_loss: 0.3146 - val_acc: 0.9194\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9049\n",
      "Epoch 00032: val_loss did not improve from 0.31153\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3112 - acc: 0.9049 - val_loss: 0.3121 - val_acc: 0.9157\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9084\n",
      "Epoch 00033: val_loss did not improve from 0.31153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2993 - acc: 0.9084 - val_loss: 0.3325 - val_acc: 0.9057\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9111\n",
      "Epoch 00034: val_loss improved from 0.31153 to 0.30885, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/034-0.3089.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2880 - acc: 0.9111 - val_loss: 0.3089 - val_acc: 0.9201\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9143\n",
      "Epoch 00035: val_loss did not improve from 0.30885\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2789 - acc: 0.9143 - val_loss: 0.3178 - val_acc: 0.9231\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9139\n",
      "Epoch 00036: val_loss improved from 0.30885 to 0.30256, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/036-0.3026.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2746 - acc: 0.9139 - val_loss: 0.3026 - val_acc: 0.9199\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9183\n",
      "Epoch 00037: val_loss improved from 0.30256 to 0.29805, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/037-0.2980.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2664 - acc: 0.9183 - val_loss: 0.2980 - val_acc: 0.9262\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9159\n",
      "Epoch 00038: val_loss improved from 0.29805 to 0.28445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/038-0.2845.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2659 - acc: 0.9159 - val_loss: 0.2845 - val_acc: 0.9262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9194\n",
      "Epoch 00039: val_loss did not improve from 0.28445\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2578 - acc: 0.9194 - val_loss: 0.3092 - val_acc: 0.9199\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9218\n",
      "Epoch 00040: val_loss did not improve from 0.28445\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2485 - acc: 0.9218 - val_loss: 0.3010 - val_acc: 0.9229\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9237\n",
      "Epoch 00041: val_loss did not improve from 0.28445\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2429 - acc: 0.9238 - val_loss: 0.3168 - val_acc: 0.9222\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9265\n",
      "Epoch 00042: val_loss did not improve from 0.28445\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2363 - acc: 0.9265 - val_loss: 0.2881 - val_acc: 0.9280\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9269\n",
      "Epoch 00043: val_loss improved from 0.28445 to 0.28236, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/043-0.2824.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2330 - acc: 0.9269 - val_loss: 0.2824 - val_acc: 0.9304\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9297\n",
      "Epoch 00044: val_loss improved from 0.28236 to 0.27974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/044-0.2797.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2256 - acc: 0.9297 - val_loss: 0.2797 - val_acc: 0.9255\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9311\n",
      "Epoch 00045: val_loss improved from 0.27974 to 0.27883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/045-0.2788.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2206 - acc: 0.9312 - val_loss: 0.2788 - val_acc: 0.9301\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9321\n",
      "Epoch 00046: val_loss did not improve from 0.27883\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2158 - acc: 0.9320 - val_loss: 0.2854 - val_acc: 0.9266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9320\n",
      "Epoch 00047: val_loss did not improve from 0.27883\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2128 - acc: 0.9320 - val_loss: 0.2964 - val_acc: 0.9297\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9304\n",
      "Epoch 00048: val_loss did not improve from 0.27883\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2145 - acc: 0.9304 - val_loss: 0.2814 - val_acc: 0.9338\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9357\n",
      "Epoch 00049: val_loss did not improve from 0.27883\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2021 - acc: 0.9357 - val_loss: 0.3127 - val_acc: 0.9234\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9346\n",
      "Epoch 00050: val_loss improved from 0.27883 to 0.27123, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/050-0.2712.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2039 - acc: 0.9347 - val_loss: 0.2712 - val_acc: 0.9301\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9379\n",
      "Epoch 00051: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1961 - acc: 0.9379 - val_loss: 0.2968 - val_acc: 0.9297\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9385\n",
      "Epoch 00052: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1938 - acc: 0.9385 - val_loss: 0.2872 - val_acc: 0.9294\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9379\n",
      "Epoch 00053: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1901 - acc: 0.9379 - val_loss: 0.2763 - val_acc: 0.9324\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9382\n",
      "Epoch 00054: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1934 - acc: 0.9382 - val_loss: 0.2844 - val_acc: 0.9301\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9396\n",
      "Epoch 00055: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1851 - acc: 0.9396 - val_loss: 0.2972 - val_acc: 0.9292\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9431\n",
      "Epoch 00056: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1777 - acc: 0.9431 - val_loss: 0.2821 - val_acc: 0.9334\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9412\n",
      "Epoch 00057: val_loss did not improve from 0.27123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1828 - acc: 0.9412 - val_loss: 0.2773 - val_acc: 0.9352\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9441\n",
      "Epoch 00058: val_loss improved from 0.27123 to 0.25766, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/058-0.2577.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1715 - acc: 0.9441 - val_loss: 0.2577 - val_acc: 0.9334\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9443\n",
      "Epoch 00059: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1695 - acc: 0.9443 - val_loss: 0.2727 - val_acc: 0.9338\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9459\n",
      "Epoch 00060: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1658 - acc: 0.9459 - val_loss: 0.3004 - val_acc: 0.9292\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9455\n",
      "Epoch 00061: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1691 - acc: 0.9456 - val_loss: 0.2931 - val_acc: 0.9266\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9468\n",
      "Epoch 00062: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1618 - acc: 0.9469 - val_loss: 0.3166 - val_acc: 0.9276\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9479\n",
      "Epoch 00063: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1610 - acc: 0.9479 - val_loss: 0.2701 - val_acc: 0.9371\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9471\n",
      "Epoch 00064: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1614 - acc: 0.9471 - val_loss: 0.2662 - val_acc: 0.9362\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9473\n",
      "Epoch 00065: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1589 - acc: 0.9473 - val_loss: 0.2626 - val_acc: 0.9350\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9499\n",
      "Epoch 00066: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1511 - acc: 0.9499 - val_loss: 0.3140 - val_acc: 0.9299\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9485\n",
      "Epoch 00067: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1547 - acc: 0.9485 - val_loss: 0.2936 - val_acc: 0.9308\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9501\n",
      "Epoch 00068: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1531 - acc: 0.9501 - val_loss: 0.2682 - val_acc: 0.9313\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9517\n",
      "Epoch 00069: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1469 - acc: 0.9517 - val_loss: 0.2924 - val_acc: 0.9387\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9526\n",
      "Epoch 00070: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1429 - acc: 0.9526 - val_loss: 0.2833 - val_acc: 0.9364\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9520\n",
      "Epoch 00071: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1433 - acc: 0.9520 - val_loss: 0.3151 - val_acc: 0.9317\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9548\n",
      "Epoch 00072: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1376 - acc: 0.9548 - val_loss: 0.2971 - val_acc: 0.9322\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9537\n",
      "Epoch 00073: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1368 - acc: 0.9536 - val_loss: 0.2683 - val_acc: 0.9390\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9540\n",
      "Epoch 00074: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1376 - acc: 0.9540 - val_loss: 0.3123 - val_acc: 0.9373\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9520\n",
      "Epoch 00075: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1428 - acc: 0.9520 - val_loss: 0.2680 - val_acc: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9567\n",
      "Epoch 00076: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1301 - acc: 0.9567 - val_loss: 0.2923 - val_acc: 0.9331\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9562\n",
      "Epoch 00077: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1355 - acc: 0.9562 - val_loss: 0.3144 - val_acc: 0.9369\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9576\n",
      "Epoch 00078: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1268 - acc: 0.9576 - val_loss: 0.2824 - val_acc: 0.9387\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9571\n",
      "Epoch 00079: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1282 - acc: 0.9571 - val_loss: 0.2812 - val_acc: 0.9331\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9590\n",
      "Epoch 00080: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1268 - acc: 0.9590 - val_loss: 0.2789 - val_acc: 0.9343\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9588\n",
      "Epoch 00081: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1253 - acc: 0.9588 - val_loss: 0.3231 - val_acc: 0.9345\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9576\n",
      "Epoch 00082: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1279 - acc: 0.9576 - val_loss: 0.2981 - val_acc: 0.9338\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9599\n",
      "Epoch 00083: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1204 - acc: 0.9599 - val_loss: 0.3295 - val_acc: 0.9320\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9599\n",
      "Epoch 00084: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1182 - acc: 0.9599 - val_loss: 0.3306 - val_acc: 0.9378\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9606\n",
      "Epoch 00085: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1198 - acc: 0.9606 - val_loss: 0.3001 - val_acc: 0.9376\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9610\n",
      "Epoch 00086: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1195 - acc: 0.9610 - val_loss: 0.2869 - val_acc: 0.9364\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9610\n",
      "Epoch 00087: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1183 - acc: 0.9610 - val_loss: 0.2806 - val_acc: 0.9355\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9633\n",
      "Epoch 00088: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1117 - acc: 0.9633 - val_loss: 0.3047 - val_acc: 0.9378\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9623\n",
      "Epoch 00089: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1136 - acc: 0.9623 - val_loss: 0.3051 - val_acc: 0.9348\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9631\n",
      "Epoch 00090: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1122 - acc: 0.9631 - val_loss: 0.3061 - val_acc: 0.9394\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9642\n",
      "Epoch 00091: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1101 - acc: 0.9641 - val_loss: 0.2789 - val_acc: 0.9385\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9641\n",
      "Epoch 00092: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1091 - acc: 0.9641 - val_loss: 0.2950 - val_acc: 0.9362\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9653\n",
      "Epoch 00093: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1054 - acc: 0.9653 - val_loss: 0.3348 - val_acc: 0.9338\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9641\n",
      "Epoch 00094: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1100 - acc: 0.9641 - val_loss: 0.3142 - val_acc: 0.9359\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9637\n",
      "Epoch 00095: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1116 - acc: 0.9637 - val_loss: 0.3058 - val_acc: 0.9352\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9664\n",
      "Epoch 00096: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1021 - acc: 0.9664 - val_loss: 0.2998 - val_acc: 0.9383\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9640\n",
      "Epoch 00097: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1087 - acc: 0.9640 - val_loss: 0.2844 - val_acc: 0.9397\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9669\n",
      "Epoch 00098: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0992 - acc: 0.9669 - val_loss: 0.3154 - val_acc: 0.9336\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9664\n",
      "Epoch 00099: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1026 - acc: 0.9664 - val_loss: 0.2890 - val_acc: 0.9427\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9684\n",
      "Epoch 00100: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0944 - acc: 0.9684 - val_loss: 0.3029 - val_acc: 0.9364\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9668\n",
      "Epoch 00101: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1002 - acc: 0.9668 - val_loss: 0.3138 - val_acc: 0.9399\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9667\n",
      "Epoch 00102: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0991 - acc: 0.9667 - val_loss: 0.3179 - val_acc: 0.9350\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9679\n",
      "Epoch 00103: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0985 - acc: 0.9679 - val_loss: 0.3234 - val_acc: 0.9352\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9664\n",
      "Epoch 00104: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1010 - acc: 0.9664 - val_loss: 0.2896 - val_acc: 0.9387\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9685\n",
      "Epoch 00105: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0944 - acc: 0.9685 - val_loss: 0.2650 - val_acc: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9686\n",
      "Epoch 00106: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0928 - acc: 0.9686 - val_loss: 0.2753 - val_acc: 0.9394\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9695\n",
      "Epoch 00107: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0945 - acc: 0.9695 - val_loss: 0.3468 - val_acc: 0.9313\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9692\n",
      "Epoch 00108: val_loss did not improve from 0.25766\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0925 - acc: 0.9691 - val_loss: 0.3218 - val_acc: 0.9348\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 953us/sample - loss: 0.3132 - acc: 0.9140\n",
      "Loss: 0.3132208975676064 Accuracy: 0.9140187\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6841 - acc: 0.0955\n",
      "Epoch 00001: val_loss improved from inf to 2.46401, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/001-2.4640.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 2.6840 - acc: 0.0955 - val_loss: 2.4640 - val_acc: 0.1992\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0600 - acc: 0.3188\n",
      "Epoch 00002: val_loss improved from 2.46401 to 1.46965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/002-1.4696.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.0600 - acc: 0.3188 - val_loss: 1.4696 - val_acc: 0.5365\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5950 - acc: 0.4651\n",
      "Epoch 00003: val_loss improved from 1.46965 to 1.25329, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/003-1.2533.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.5950 - acc: 0.4651 - val_loss: 1.2533 - val_acc: 0.6087\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4049 - acc: 0.5299\n",
      "Epoch 00004: val_loss improved from 1.25329 to 1.09525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/004-1.0952.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.4048 - acc: 0.5300 - val_loss: 1.0952 - val_acc: 0.6667\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2590 - acc: 0.5793\n",
      "Epoch 00005: val_loss improved from 1.09525 to 1.04739, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/005-1.0474.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2591 - acc: 0.5792 - val_loss: 1.0474 - val_acc: 0.6883\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1438 - acc: 0.6255\n",
      "Epoch 00006: val_loss improved from 1.04739 to 0.86907, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/006-0.8691.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1437 - acc: 0.6255 - val_loss: 0.8691 - val_acc: 0.7349\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0350 - acc: 0.6647\n",
      "Epoch 00007: val_loss improved from 0.86907 to 0.78417, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/007-0.7842.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0349 - acc: 0.6647 - val_loss: 0.7842 - val_acc: 0.7706\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.7007\n",
      "Epoch 00008: val_loss improved from 0.78417 to 0.67500, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/008-0.6750.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9375 - acc: 0.7006 - val_loss: 0.6750 - val_acc: 0.8027\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8336 - acc: 0.7367\n",
      "Epoch 00009: val_loss improved from 0.67500 to 0.60248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/009-0.6025.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8336 - acc: 0.7367 - val_loss: 0.6025 - val_acc: 0.8258\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7561 - acc: 0.7646\n",
      "Epoch 00010: val_loss improved from 0.60248 to 0.54098, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/010-0.5410.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7560 - acc: 0.7646 - val_loss: 0.5410 - val_acc: 0.8418\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6960 - acc: 0.7815\n",
      "Epoch 00011: val_loss improved from 0.54098 to 0.49381, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/011-0.4938.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6960 - acc: 0.7815 - val_loss: 0.4938 - val_acc: 0.8633\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6260 - acc: 0.8056\n",
      "Epoch 00012: val_loss improved from 0.49381 to 0.42932, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/012-0.4293.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6259 - acc: 0.8057 - val_loss: 0.4293 - val_acc: 0.8786\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.8203\n",
      "Epoch 00013: val_loss improved from 0.42932 to 0.37778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/013-0.3778.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5802 - acc: 0.8203 - val_loss: 0.3778 - val_acc: 0.8942\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8337\n",
      "Epoch 00014: val_loss improved from 0.37778 to 0.35846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/014-0.3585.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5417 - acc: 0.8337 - val_loss: 0.3585 - val_acc: 0.9001\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8433\n",
      "Epoch 00015: val_loss did not improve from 0.35846\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5086 - acc: 0.8433 - val_loss: 0.3828 - val_acc: 0.8917\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4740 - acc: 0.8553\n",
      "Epoch 00016: val_loss improved from 0.35846 to 0.33400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/016-0.3340.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4739 - acc: 0.8553 - val_loss: 0.3340 - val_acc: 0.9089\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8611\n",
      "Epoch 00017: val_loss improved from 0.33400 to 0.31667, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/017-0.3167.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4546 - acc: 0.8611 - val_loss: 0.3167 - val_acc: 0.9092\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8687\n",
      "Epoch 00018: val_loss did not improve from 0.31667\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4317 - acc: 0.8687 - val_loss: 0.3188 - val_acc: 0.9064\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8743\n",
      "Epoch 00019: val_loss improved from 0.31667 to 0.28290, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/019-0.2829.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4072 - acc: 0.8743 - val_loss: 0.2829 - val_acc: 0.9201\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8814\n",
      "Epoch 00020: val_loss did not improve from 0.28290\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3874 - acc: 0.8813 - val_loss: 0.2942 - val_acc: 0.9133\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8836\n",
      "Epoch 00021: val_loss improved from 0.28290 to 0.26551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/021-0.2655.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3744 - acc: 0.8837 - val_loss: 0.2655 - val_acc: 0.9248\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8898\n",
      "Epoch 00022: val_loss improved from 0.26551 to 0.25269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/022-0.2527.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3620 - acc: 0.8898 - val_loss: 0.2527 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8939\n",
      "Epoch 00023: val_loss did not improve from 0.25269\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3492 - acc: 0.8939 - val_loss: 0.2596 - val_acc: 0.9257\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8992\n",
      "Epoch 00024: val_loss did not improve from 0.25269\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3302 - acc: 0.8991 - val_loss: 0.2597 - val_acc: 0.9224\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9040\n",
      "Epoch 00025: val_loss improved from 0.25269 to 0.23574, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/025-0.2357.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3178 - acc: 0.9040 - val_loss: 0.2357 - val_acc: 0.9345\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9051\n",
      "Epoch 00026: val_loss improved from 0.23574 to 0.22427, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/026-0.2243.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3120 - acc: 0.9051 - val_loss: 0.2243 - val_acc: 0.9383\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9079\n",
      "Epoch 00027: val_loss did not improve from 0.22427\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3024 - acc: 0.9079 - val_loss: 0.2389 - val_acc: 0.9362\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9101\n",
      "Epoch 00028: val_loss did not improve from 0.22427\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2887 - acc: 0.9101 - val_loss: 0.2391 - val_acc: 0.9304\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9150\n",
      "Epoch 00029: val_loss improved from 0.22427 to 0.19724, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/029-0.1972.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2753 - acc: 0.9150 - val_loss: 0.1972 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9146\n",
      "Epoch 00030: val_loss did not improve from 0.19724\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2756 - acc: 0.9147 - val_loss: 0.2350 - val_acc: 0.9283\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9162\n",
      "Epoch 00031: val_loss did not improve from 0.19724\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2692 - acc: 0.9162 - val_loss: 0.2331 - val_acc: 0.9329\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9215\n",
      "Epoch 00032: val_loss improved from 0.19724 to 0.19472, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/032-0.1947.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2555 - acc: 0.9215 - val_loss: 0.1947 - val_acc: 0.9497\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9234\n",
      "Epoch 00033: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2503 - acc: 0.9234 - val_loss: 0.1961 - val_acc: 0.9460\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9248\n",
      "Epoch 00034: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2416 - acc: 0.9248 - val_loss: 0.2022 - val_acc: 0.9441\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9250\n",
      "Epoch 00035: val_loss improved from 0.19472 to 0.19311, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/035-0.1931.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2432 - acc: 0.9250 - val_loss: 0.1931 - val_acc: 0.9464\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9274\n",
      "Epoch 00036: val_loss did not improve from 0.19311\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2342 - acc: 0.9274 - val_loss: 0.2058 - val_acc: 0.9420\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9289\n",
      "Epoch 00037: val_loss did not improve from 0.19311\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2270 - acc: 0.9289 - val_loss: 0.1943 - val_acc: 0.9436\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9311\n",
      "Epoch 00038: val_loss improved from 0.19311 to 0.18460, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/038-0.1846.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2166 - acc: 0.9312 - val_loss: 0.1846 - val_acc: 0.9490\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9326\n",
      "Epoch 00039: val_loss improved from 0.18460 to 0.18158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/039-0.1816.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2142 - acc: 0.9326 - val_loss: 0.1816 - val_acc: 0.9485\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9359\n",
      "Epoch 00040: val_loss did not improve from 0.18158\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2045 - acc: 0.9359 - val_loss: 0.2042 - val_acc: 0.9455\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9360\n",
      "Epoch 00041: val_loss improved from 0.18158 to 0.17412, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/041-0.1741.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2053 - acc: 0.9360 - val_loss: 0.1741 - val_acc: 0.9506\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9368\n",
      "Epoch 00042: val_loss did not improve from 0.17412\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1997 - acc: 0.9368 - val_loss: 0.1837 - val_acc: 0.9499\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9374\n",
      "Epoch 00043: val_loss improved from 0.17412 to 0.17170, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/043-0.1717.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1968 - acc: 0.9374 - val_loss: 0.1717 - val_acc: 0.9515\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9392\n",
      "Epoch 00044: val_loss did not improve from 0.17170\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1927 - acc: 0.9391 - val_loss: 0.2278 - val_acc: 0.9380\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9388\n",
      "Epoch 00045: val_loss improved from 0.17170 to 0.16973, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/045-0.1697.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1917 - acc: 0.9388 - val_loss: 0.1697 - val_acc: 0.9520\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9427\n",
      "Epoch 00046: val_loss did not improve from 0.16973\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1798 - acc: 0.9426 - val_loss: 0.1821 - val_acc: 0.9499\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9418\n",
      "Epoch 00047: val_loss did not improve from 0.16973\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1794 - acc: 0.9418 - val_loss: 0.1775 - val_acc: 0.9527\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9438\n",
      "Epoch 00048: val_loss did not improve from 0.16973\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1781 - acc: 0.9438 - val_loss: 0.1723 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9445\n",
      "Epoch 00049: val_loss improved from 0.16973 to 0.16293, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/049-0.1629.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1749 - acc: 0.9445 - val_loss: 0.1629 - val_acc: 0.9567\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9461\n",
      "Epoch 00050: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1677 - acc: 0.9461 - val_loss: 0.1691 - val_acc: 0.9541\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9481\n",
      "Epoch 00051: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1651 - acc: 0.9481 - val_loss: 0.1846 - val_acc: 0.9539\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9460\n",
      "Epoch 00052: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1686 - acc: 0.9460 - val_loss: 0.1756 - val_acc: 0.9546\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9501\n",
      "Epoch 00053: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1542 - acc: 0.9501 - val_loss: 0.1692 - val_acc: 0.9518\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9513\n",
      "Epoch 00054: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1544 - acc: 0.9513 - val_loss: 0.1929 - val_acc: 0.9541\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9493\n",
      "Epoch 00055: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1558 - acc: 0.9493 - val_loss: 0.1652 - val_acc: 0.9560\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9515\n",
      "Epoch 00056: val_loss did not improve from 0.16293\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1512 - acc: 0.9515 - val_loss: 0.1801 - val_acc: 0.9509\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9531\n",
      "Epoch 00057: val_loss improved from 0.16293 to 0.15860, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/057-0.1586.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1465 - acc: 0.9531 - val_loss: 0.1586 - val_acc: 0.9571\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9521\n",
      "Epoch 00058: val_loss improved from 0.15860 to 0.15767, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/058-0.1577.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1487 - acc: 0.9521 - val_loss: 0.1577 - val_acc: 0.9546\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9538\n",
      "Epoch 00059: val_loss did not improve from 0.15767\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1416 - acc: 0.9538 - val_loss: 0.1767 - val_acc: 0.9515\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9543\n",
      "Epoch 00060: val_loss improved from 0.15767 to 0.15646, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/060-0.1565.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1442 - acc: 0.9544 - val_loss: 0.1565 - val_acc: 0.9581\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9548\n",
      "Epoch 00061: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1389 - acc: 0.9548 - val_loss: 0.1733 - val_acc: 0.9581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9566\n",
      "Epoch 00062: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1334 - acc: 0.9566 - val_loss: 0.1722 - val_acc: 0.9576\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9563\n",
      "Epoch 00063: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1319 - acc: 0.9563 - val_loss: 0.1623 - val_acc: 0.9555\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9588\n",
      "Epoch 00064: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1280 - acc: 0.9588 - val_loss: 0.1787 - val_acc: 0.9553\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9581\n",
      "Epoch 00065: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1297 - acc: 0.9580 - val_loss: 0.1765 - val_acc: 0.9562\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9590\n",
      "Epoch 00066: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1259 - acc: 0.9590 - val_loss: 0.1697 - val_acc: 0.9536\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9590\n",
      "Epoch 00067: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1245 - acc: 0.9591 - val_loss: 0.1707 - val_acc: 0.9597\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9613\n",
      "Epoch 00068: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1180 - acc: 0.9613 - val_loss: 0.1775 - val_acc: 0.9548\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9600\n",
      "Epoch 00069: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1211 - acc: 0.9600 - val_loss: 0.1803 - val_acc: 0.9560\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9620\n",
      "Epoch 00070: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1180 - acc: 0.9620 - val_loss: 0.1586 - val_acc: 0.9569\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9632\n",
      "Epoch 00071: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1143 - acc: 0.9632 - val_loss: 0.1685 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9632\n",
      "Epoch 00072: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1119 - acc: 0.9632 - val_loss: 0.1619 - val_acc: 0.9583\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9627\n",
      "Epoch 00073: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1113 - acc: 0.9627 - val_loss: 0.1700 - val_acc: 0.9595\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9652\n",
      "Epoch 00074: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1042 - acc: 0.9652 - val_loss: 0.1832 - val_acc: 0.9571\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9658\n",
      "Epoch 00075: val_loss improved from 0.15646 to 0.14820, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/075-0.1482.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1062 - acc: 0.9658 - val_loss: 0.1482 - val_acc: 0.9618\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9651\n",
      "Epoch 00076: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1098 - acc: 0.9651 - val_loss: 0.1651 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9668\n",
      "Epoch 00077: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1042 - acc: 0.9669 - val_loss: 0.1839 - val_acc: 0.9592\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9657\n",
      "Epoch 00078: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1037 - acc: 0.9657 - val_loss: 0.1631 - val_acc: 0.9609\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9674\n",
      "Epoch 00079: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1007 - acc: 0.9674 - val_loss: 0.1555 - val_acc: 0.9632\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9670\n",
      "Epoch 00080: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0980 - acc: 0.9670 - val_loss: 0.1665 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9669\n",
      "Epoch 00081: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0981 - acc: 0.9669 - val_loss: 0.1508 - val_acc: 0.9616\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9679\n",
      "Epoch 00082: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0963 - acc: 0.9679 - val_loss: 0.1505 - val_acc: 0.9616\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9695\n",
      "Epoch 00083: val_loss improved from 0.14820 to 0.14705, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/083-0.1470.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0925 - acc: 0.9695 - val_loss: 0.1470 - val_acc: 0.9625\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9702\n",
      "Epoch 00084: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0938 - acc: 0.9702 - val_loss: 0.1592 - val_acc: 0.9630\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9688\n",
      "Epoch 00085: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0940 - acc: 0.9688 - val_loss: 0.1614 - val_acc: 0.9625\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9710\n",
      "Epoch 00086: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0880 - acc: 0.9710 - val_loss: 0.1589 - val_acc: 0.9648\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9700\n",
      "Epoch 00087: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0894 - acc: 0.9700 - val_loss: 0.1564 - val_acc: 0.9606\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9695\n",
      "Epoch 00088: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0910 - acc: 0.9695 - val_loss: 0.1527 - val_acc: 0.9644\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9708\n",
      "Epoch 00089: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0870 - acc: 0.9707 - val_loss: 0.1711 - val_acc: 0.9630\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9712\n",
      "Epoch 00090: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0893 - acc: 0.9711 - val_loss: 0.1556 - val_acc: 0.9592\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9702\n",
      "Epoch 00091: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0895 - acc: 0.9702 - val_loss: 0.1620 - val_acc: 0.9648\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9726\n",
      "Epoch 00092: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0817 - acc: 0.9726 - val_loss: 0.1639 - val_acc: 0.9662\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9739\n",
      "Epoch 00093: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0782 - acc: 0.9739 - val_loss: 0.1645 - val_acc: 0.9623\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9724\n",
      "Epoch 00094: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0813 - acc: 0.9724 - val_loss: 0.1634 - val_acc: 0.9655\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9711\n",
      "Epoch 00095: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0852 - acc: 0.9711 - val_loss: 0.1602 - val_acc: 0.9623\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9746\n",
      "Epoch 00096: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0770 - acc: 0.9746 - val_loss: 0.1553 - val_acc: 0.9639\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9743\n",
      "Epoch 00097: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0781 - acc: 0.9744 - val_loss: 0.1789 - val_acc: 0.9588\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9743\n",
      "Epoch 00098: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0773 - acc: 0.9744 - val_loss: 0.1694 - val_acc: 0.9641\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9747\n",
      "Epoch 00099: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0752 - acc: 0.9747 - val_loss: 0.1673 - val_acc: 0.9620\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9754\n",
      "Epoch 00100: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0778 - acc: 0.9754 - val_loss: 0.1518 - val_acc: 0.9646\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9749\n",
      "Epoch 00101: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0747 - acc: 0.9749 - val_loss: 0.1885 - val_acc: 0.9623\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9747\n",
      "Epoch 00102: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0767 - acc: 0.9747 - val_loss: 0.1673 - val_acc: 0.9637\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9748\n",
      "Epoch 00103: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0751 - acc: 0.9748 - val_loss: 0.1793 - val_acc: 0.9613\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9756\n",
      "Epoch 00104: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0725 - acc: 0.9756 - val_loss: 0.1894 - val_acc: 0.9590\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9761\n",
      "Epoch 00105: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0704 - acc: 0.9761 - val_loss: 0.1634 - val_acc: 0.9674\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9772\n",
      "Epoch 00106: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0667 - acc: 0.9772 - val_loss: 0.1526 - val_acc: 0.9639\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9776\n",
      "Epoch 00107: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0671 - acc: 0.9776 - val_loss: 0.1590 - val_acc: 0.9662\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9773\n",
      "Epoch 00108: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0674 - acc: 0.9773 - val_loss: 0.1819 - val_acc: 0.9620\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9770\n",
      "Epoch 00109: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0694 - acc: 0.9770 - val_loss: 0.1681 - val_acc: 0.9625\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9789\n",
      "Epoch 00110: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0617 - acc: 0.9789 - val_loss: 0.1828 - val_acc: 0.9634\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9784\n",
      "Epoch 00111: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0654 - acc: 0.9784 - val_loss: 0.1776 - val_acc: 0.9655\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9766\n",
      "Epoch 00112: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0678 - acc: 0.9766 - val_loss: 0.1736 - val_acc: 0.9609\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9807\n",
      "Epoch 00113: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0615 - acc: 0.9807 - val_loss: 0.1806 - val_acc: 0.9623\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9801\n",
      "Epoch 00114: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0601 - acc: 0.9801 - val_loss: 0.1590 - val_acc: 0.9639\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9783\n",
      "Epoch 00115: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0660 - acc: 0.9783 - val_loss: 0.1869 - val_acc: 0.9595\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9792\n",
      "Epoch 00116: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0622 - acc: 0.9792 - val_loss: 0.1860 - val_acc: 0.9637\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9807\n",
      "Epoch 00117: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0605 - acc: 0.9807 - val_loss: 0.1861 - val_acc: 0.9630\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9802\n",
      "Epoch 00118: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0618 - acc: 0.9802 - val_loss: 0.1807 - val_acc: 0.9630\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9804\n",
      "Epoch 00119: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0608 - acc: 0.9804 - val_loss: 0.1642 - val_acc: 0.9620\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9819\n",
      "Epoch 00120: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0551 - acc: 0.9819 - val_loss: 0.1862 - val_acc: 0.9623\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9793\n",
      "Epoch 00121: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0616 - acc: 0.9793 - val_loss: 0.1615 - val_acc: 0.9644\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9810\n",
      "Epoch 00122: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0570 - acc: 0.9810 - val_loss: 0.1861 - val_acc: 0.9641\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9811\n",
      "Epoch 00123: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0569 - acc: 0.9811 - val_loss: 0.1733 - val_acc: 0.9648\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9805\n",
      "Epoch 00124: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0574 - acc: 0.9805 - val_loss: 0.1656 - val_acc: 0.9651\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9820\n",
      "Epoch 00125: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0537 - acc: 0.9820 - val_loss: 0.1670 - val_acc: 0.9627\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9812\n",
      "Epoch 00126: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0550 - acc: 0.9813 - val_loss: 0.1780 - val_acc: 0.9641\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9829\n",
      "Epoch 00127: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0513 - acc: 0.9829 - val_loss: 0.1859 - val_acc: 0.9667\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9799\n",
      "Epoch 00128: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0594 - acc: 0.9799 - val_loss: 0.2055 - val_acc: 0.9595\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9830\n",
      "Epoch 00129: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0514 - acc: 0.9830 - val_loss: 0.1755 - val_acc: 0.9623\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9826\n",
      "Epoch 00130: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0522 - acc: 0.9826 - val_loss: 0.1805 - val_acc: 0.9665\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9831\n",
      "Epoch 00131: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0516 - acc: 0.9831 - val_loss: 0.1726 - val_acc: 0.9641\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9820\n",
      "Epoch 00132: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0529 - acc: 0.9820 - val_loss: 0.1846 - val_acc: 0.9655\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9841\n",
      "Epoch 00133: val_loss did not improve from 0.14705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0486 - acc: 0.9841 - val_loss: 0.1850 - val_acc: 0.9672\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 986us/sample - loss: 0.1763 - acc: 0.9510\n",
      "Loss: 0.17631158992369597 Accuracy: 0.9509865\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6572 - acc: 0.1121\n",
      "Epoch 00001: val_loss improved from inf to 2.24626, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/001-2.2463.hdf5\n",
      "36805/36805 [==============================] - 97s 3ms/sample - loss: 2.6571 - acc: 0.1121 - val_loss: 2.2463 - val_acc: 0.3182\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9518 - acc: 0.3585\n",
      "Epoch 00002: val_loss improved from 2.24626 to 1.29374, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/002-1.2937.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.9518 - acc: 0.3585 - val_loss: 1.2937 - val_acc: 0.6007\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4153 - acc: 0.5306\n",
      "Epoch 00003: val_loss improved from 1.29374 to 0.96358, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/003-0.9636.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4153 - acc: 0.5306 - val_loss: 0.9636 - val_acc: 0.6976\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1664 - acc: 0.6154\n",
      "Epoch 00004: val_loss improved from 0.96358 to 0.79202, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/004-0.7920.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.1664 - acc: 0.6154 - val_loss: 0.7920 - val_acc: 0.7589\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9958 - acc: 0.6721\n",
      "Epoch 00005: val_loss did not improve from 0.79202\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9960 - acc: 0.6721 - val_loss: 0.7975 - val_acc: 0.7461\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8826 - acc: 0.7120\n",
      "Epoch 00006: val_loss improved from 0.79202 to 0.57756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/006-0.5776.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8826 - acc: 0.7119 - val_loss: 0.5776 - val_acc: 0.8160\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7472\n",
      "Epoch 00007: val_loss improved from 0.57756 to 0.54945, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/007-0.5494.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7780 - acc: 0.7472 - val_loss: 0.5494 - val_acc: 0.8258\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7729\n",
      "Epoch 00008: val_loss improved from 0.54945 to 0.44641, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/008-0.4464.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7122 - acc: 0.7729 - val_loss: 0.4464 - val_acc: 0.8665\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6303 - acc: 0.7964\n",
      "Epoch 00009: val_loss improved from 0.44641 to 0.39410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/009-0.3941.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6302 - acc: 0.7964 - val_loss: 0.3941 - val_acc: 0.8744\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.8186\n",
      "Epoch 00010: val_loss improved from 0.39410 to 0.32968, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/010-0.3297.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5656 - acc: 0.8186 - val_loss: 0.3297 - val_acc: 0.9015\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8333\n",
      "Epoch 00011: val_loss improved from 0.32968 to 0.32475, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/011-0.3248.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5317 - acc: 0.8333 - val_loss: 0.3248 - val_acc: 0.9026\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8446\n",
      "Epoch 00012: val_loss improved from 0.32475 to 0.29615, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/012-0.2961.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4828 - acc: 0.8445 - val_loss: 0.2961 - val_acc: 0.9122\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8616\n",
      "Epoch 00013: val_loss improved from 0.29615 to 0.27542, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/013-0.2754.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4459 - acc: 0.8616 - val_loss: 0.2754 - val_acc: 0.9143\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8689\n",
      "Epoch 00014: val_loss improved from 0.27542 to 0.24023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/014-0.2402.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4137 - acc: 0.8689 - val_loss: 0.2402 - val_acc: 0.9271\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8769\n",
      "Epoch 00015: val_loss did not improve from 0.24023\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3916 - acc: 0.8769 - val_loss: 0.2483 - val_acc: 0.9224\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8844\n",
      "Epoch 00016: val_loss improved from 0.24023 to 0.23590, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/016-0.2359.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3713 - acc: 0.8844 - val_loss: 0.2359 - val_acc: 0.9308\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8905\n",
      "Epoch 00017: val_loss improved from 0.23590 to 0.22028, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/017-0.2203.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3493 - acc: 0.8905 - val_loss: 0.2203 - val_acc: 0.9348\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8985\n",
      "Epoch 00018: val_loss improved from 0.22028 to 0.18558, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/018-0.1856.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3275 - acc: 0.8985 - val_loss: 0.1856 - val_acc: 0.9446\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9032\n",
      "Epoch 00019: val_loss improved from 0.18558 to 0.18034, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/019-0.1803.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3136 - acc: 0.9032 - val_loss: 0.1803 - val_acc: 0.9488\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9062\n",
      "Epoch 00020: val_loss did not improve from 0.18034\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2943 - acc: 0.9062 - val_loss: 0.2269 - val_acc: 0.9324\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9123\n",
      "Epoch 00021: val_loss did not improve from 0.18034\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2819 - acc: 0.9123 - val_loss: 0.1819 - val_acc: 0.9499\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9149\n",
      "Epoch 00022: val_loss improved from 0.18034 to 0.17801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/022-0.1780.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2731 - acc: 0.9149 - val_loss: 0.1780 - val_acc: 0.9492\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9205\n",
      "Epoch 00023: val_loss did not improve from 0.17801\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2544 - acc: 0.9205 - val_loss: 0.1866 - val_acc: 0.9434\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9238\n",
      "Epoch 00024: val_loss improved from 0.17801 to 0.17220, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/024-0.1722.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2477 - acc: 0.9238 - val_loss: 0.1722 - val_acc: 0.9476\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9263\n",
      "Epoch 00025: val_loss improved from 0.17220 to 0.16398, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/025-0.1640.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2372 - acc: 0.9263 - val_loss: 0.1640 - val_acc: 0.9550\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9294\n",
      "Epoch 00026: val_loss did not improve from 0.16398\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2247 - acc: 0.9294 - val_loss: 0.1650 - val_acc: 0.9513\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9325\n",
      "Epoch 00027: val_loss improved from 0.16398 to 0.14520, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/027-0.1452.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2180 - acc: 0.9325 - val_loss: 0.1452 - val_acc: 0.9571\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9356\n",
      "Epoch 00028: val_loss did not improve from 0.14520\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2089 - acc: 0.9356 - val_loss: 0.1473 - val_acc: 0.9567\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9358\n",
      "Epoch 00029: val_loss did not improve from 0.14520\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2034 - acc: 0.9358 - val_loss: 0.1457 - val_acc: 0.9578\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9386\n",
      "Epoch 00030: val_loss did not improve from 0.14520\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1980 - acc: 0.9386 - val_loss: 0.1570 - val_acc: 0.9541\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9421\n",
      "Epoch 00031: val_loss did not improve from 0.14520\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1895 - acc: 0.9421 - val_loss: 0.1528 - val_acc: 0.9534\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9453\n",
      "Epoch 00032: val_loss improved from 0.14520 to 0.13079, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/032-0.1308.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1751 - acc: 0.9453 - val_loss: 0.1308 - val_acc: 0.9606\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9453\n",
      "Epoch 00033: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1768 - acc: 0.9453 - val_loss: 0.1359 - val_acc: 0.9590\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9466\n",
      "Epoch 00034: val_loss improved from 0.13079 to 0.11754, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/034-0.1175.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1680 - acc: 0.9466 - val_loss: 0.1175 - val_acc: 0.9655\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9476\n",
      "Epoch 00035: val_loss did not improve from 0.11754\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1671 - acc: 0.9476 - val_loss: 0.1314 - val_acc: 0.9646\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9492\n",
      "Epoch 00036: val_loss did not improve from 0.11754\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1629 - acc: 0.9492 - val_loss: 0.1261 - val_acc: 0.9651\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9520\n",
      "Epoch 00037: val_loss improved from 0.11754 to 0.11166, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/037-0.1117.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1502 - acc: 0.9520 - val_loss: 0.1117 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9521\n",
      "Epoch 00038: val_loss did not improve from 0.11166\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1502 - acc: 0.9521 - val_loss: 0.1210 - val_acc: 0.9644\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9535\n",
      "Epoch 00039: val_loss did not improve from 0.11166\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1459 - acc: 0.9535 - val_loss: 0.1238 - val_acc: 0.9665\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9565\n",
      "Epoch 00040: val_loss improved from 0.11166 to 0.10934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/040-0.1093.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1397 - acc: 0.9565 - val_loss: 0.1093 - val_acc: 0.9658\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9567\n",
      "Epoch 00041: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1354 - acc: 0.9567 - val_loss: 0.1202 - val_acc: 0.9637\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9567\n",
      "Epoch 00042: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1324 - acc: 0.9567 - val_loss: 0.1134 - val_acc: 0.9679\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9582\n",
      "Epoch 00043: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1306 - acc: 0.9582 - val_loss: 0.1120 - val_acc: 0.9674\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9608\n",
      "Epoch 00044: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1242 - acc: 0.9608 - val_loss: 0.1110 - val_acc: 0.9683\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9604\n",
      "Epoch 00045: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1218 - acc: 0.9603 - val_loss: 0.1103 - val_acc: 0.9688\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9584\n",
      "Epoch 00046: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1283 - acc: 0.9584 - val_loss: 0.1241 - val_acc: 0.9660\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9617\n",
      "Epoch 00047: val_loss did not improve from 0.10934\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1166 - acc: 0.9617 - val_loss: 0.1164 - val_acc: 0.9667\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9638\n",
      "Epoch 00048: val_loss improved from 0.10934 to 0.10896, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/048-0.1090.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1112 - acc: 0.9637 - val_loss: 0.1090 - val_acc: 0.9693\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9643\n",
      "Epoch 00049: val_loss improved from 0.10896 to 0.10324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/049-0.1032.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1085 - acc: 0.9643 - val_loss: 0.1032 - val_acc: 0.9695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9654\n",
      "Epoch 00050: val_loss did not improve from 0.10324\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1061 - acc: 0.9654 - val_loss: 0.1077 - val_acc: 0.9693\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9665\n",
      "Epoch 00051: val_loss did not improve from 0.10324\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1038 - acc: 0.9665 - val_loss: 0.1215 - val_acc: 0.9686\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9664\n",
      "Epoch 00052: val_loss did not improve from 0.10324\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1035 - acc: 0.9664 - val_loss: 0.1127 - val_acc: 0.9665\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9688\n",
      "Epoch 00053: val_loss improved from 0.10324 to 0.10257, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/053-0.1026.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1007 - acc: 0.9688 - val_loss: 0.1026 - val_acc: 0.9669\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9684\n",
      "Epoch 00054: val_loss did not improve from 0.10257\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0975 - acc: 0.9684 - val_loss: 0.1178 - val_acc: 0.9660\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9699\n",
      "Epoch 00055: val_loss did not improve from 0.10257\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0940 - acc: 0.9699 - val_loss: 0.1119 - val_acc: 0.9718\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9674\n",
      "Epoch 00056: val_loss did not improve from 0.10257\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0985 - acc: 0.9674 - val_loss: 0.1055 - val_acc: 0.9686\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9708\n",
      "Epoch 00057: val_loss improved from 0.10257 to 0.10213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/057-0.1021.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0890 - acc: 0.9708 - val_loss: 0.1021 - val_acc: 0.9700\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9715\n",
      "Epoch 00058: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0903 - acc: 0.9715 - val_loss: 0.1102 - val_acc: 0.9686\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9723\n",
      "Epoch 00059: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0846 - acc: 0.9723 - val_loss: 0.1116 - val_acc: 0.9688\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9733\n",
      "Epoch 00060: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0832 - acc: 0.9733 - val_loss: 0.1095 - val_acc: 0.9700\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9721\n",
      "Epoch 00061: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0836 - acc: 0.9721 - val_loss: 0.1134 - val_acc: 0.9704\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9705\n",
      "Epoch 00062: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0897 - acc: 0.9705 - val_loss: 0.1043 - val_acc: 0.9734\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9740\n",
      "Epoch 00063: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0777 - acc: 0.9741 - val_loss: 0.1022 - val_acc: 0.9725\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9742\n",
      "Epoch 00064: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0781 - acc: 0.9742 - val_loss: 0.1140 - val_acc: 0.9686\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9746\n",
      "Epoch 00065: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0767 - acc: 0.9746 - val_loss: 0.1432 - val_acc: 0.9665\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9757\n",
      "Epoch 00066: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0750 - acc: 0.9757 - val_loss: 0.1051 - val_acc: 0.9697\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9762\n",
      "Epoch 00067: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0729 - acc: 0.9762 - val_loss: 0.1165 - val_acc: 0.9688\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9763\n",
      "Epoch 00068: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0712 - acc: 0.9763 - val_loss: 0.1094 - val_acc: 0.9709\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9772\n",
      "Epoch 00069: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0707 - acc: 0.9771 - val_loss: 0.1158 - val_acc: 0.9693\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9738\n",
      "Epoch 00070: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0812 - acc: 0.9738 - val_loss: 0.1133 - val_acc: 0.9697\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9775\n",
      "Epoch 00071: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0696 - acc: 0.9775 - val_loss: 0.1269 - val_acc: 0.9681\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9790\n",
      "Epoch 00072: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0644 - acc: 0.9790 - val_loss: 0.1127 - val_acc: 0.9702\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9795\n",
      "Epoch 00073: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0642 - acc: 0.9795 - val_loss: 0.1139 - val_acc: 0.9700\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9791\n",
      "Epoch 00074: val_loss did not improve from 0.10213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0651 - acc: 0.9791 - val_loss: 0.1182 - val_acc: 0.9686\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9782\n",
      "Epoch 00075: val_loss improved from 0.10213 to 0.09761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/075-0.0976.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0666 - acc: 0.9782 - val_loss: 0.0976 - val_acc: 0.9723\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9802\n",
      "Epoch 00076: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0604 - acc: 0.9802 - val_loss: 0.1183 - val_acc: 0.9672\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9798\n",
      "Epoch 00077: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0602 - acc: 0.9798 - val_loss: 0.1258 - val_acc: 0.9690\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9802\n",
      "Epoch 00078: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0587 - acc: 0.9802 - val_loss: 0.1227 - val_acc: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9803\n",
      "Epoch 00079: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0586 - acc: 0.9803 - val_loss: 0.1042 - val_acc: 0.9725\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9804\n",
      "Epoch 00080: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0612 - acc: 0.9804 - val_loss: 0.1198 - val_acc: 0.9690\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9818\n",
      "Epoch 00081: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0554 - acc: 0.9818 - val_loss: 0.1244 - val_acc: 0.9741\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9821\n",
      "Epoch 00082: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0548 - acc: 0.9821 - val_loss: 0.1158 - val_acc: 0.9706\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9822\n",
      "Epoch 00083: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0535 - acc: 0.9822 - val_loss: 0.1327 - val_acc: 0.9720\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9811\n",
      "Epoch 00084: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0565 - acc: 0.9811 - val_loss: 0.1078 - val_acc: 0.9716\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9820\n",
      "Epoch 00085: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0553 - acc: 0.9820 - val_loss: 0.1160 - val_acc: 0.9720\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9823\n",
      "Epoch 00086: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0527 - acc: 0.9822 - val_loss: 0.1037 - val_acc: 0.9741\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9826\n",
      "Epoch 00087: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0510 - acc: 0.9826 - val_loss: 0.1218 - val_acc: 0.9706\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9837\n",
      "Epoch 00088: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0508 - acc: 0.9837 - val_loss: 0.1190 - val_acc: 0.9718\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9829\n",
      "Epoch 00089: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0512 - acc: 0.9829 - val_loss: 0.1229 - val_acc: 0.9697\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9838\n",
      "Epoch 00090: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0492 - acc: 0.9838 - val_loss: 0.1218 - val_acc: 0.9709\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9859\n",
      "Epoch 00091: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0444 - acc: 0.9858 - val_loss: 0.1249 - val_acc: 0.9720\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9836\n",
      "Epoch 00092: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0500 - acc: 0.9836 - val_loss: 0.1204 - val_acc: 0.9732\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9851\n",
      "Epoch 00093: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0450 - acc: 0.9851 - val_loss: 0.1138 - val_acc: 0.9723\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9857\n",
      "Epoch 00094: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0429 - acc: 0.9857 - val_loss: 0.1189 - val_acc: 0.9723\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9843\n",
      "Epoch 00095: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0484 - acc: 0.9843 - val_loss: 0.1639 - val_acc: 0.9667\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9842\n",
      "Epoch 00096: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0486 - acc: 0.9842 - val_loss: 0.1278 - val_acc: 0.9679\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9849\n",
      "Epoch 00097: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0447 - acc: 0.9849 - val_loss: 0.1447 - val_acc: 0.9688\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9853\n",
      "Epoch 00098: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0449 - acc: 0.9853 - val_loss: 0.1317 - val_acc: 0.9674\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9847\n",
      "Epoch 00099: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0474 - acc: 0.9847 - val_loss: 0.1275 - val_acc: 0.9695\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9866\n",
      "Epoch 00100: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0421 - acc: 0.9866 - val_loss: 0.1307 - val_acc: 0.9725\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9864\n",
      "Epoch 00101: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0414 - acc: 0.9864 - val_loss: 0.1411 - val_acc: 0.9739\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9866\n",
      "Epoch 00102: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0413 - acc: 0.9866 - val_loss: 0.1316 - val_acc: 0.9711\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9876\n",
      "Epoch 00103: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0390 - acc: 0.9876 - val_loss: 0.1195 - val_acc: 0.9702\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9873\n",
      "Epoch 00104: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0393 - acc: 0.9873 - val_loss: 0.1414 - val_acc: 0.9702\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9866\n",
      "Epoch 00105: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0422 - acc: 0.9866 - val_loss: 0.1289 - val_acc: 0.9709\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9870\n",
      "Epoch 00106: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0379 - acc: 0.9870 - val_loss: 0.1310 - val_acc: 0.9704\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9877\n",
      "Epoch 00107: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0384 - acc: 0.9877 - val_loss: 0.1201 - val_acc: 0.9727\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9876\n",
      "Epoch 00108: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0395 - acc: 0.9876 - val_loss: 0.1290 - val_acc: 0.9706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9868\n",
      "Epoch 00109: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0424 - acc: 0.9868 - val_loss: 0.1257 - val_acc: 0.9730\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9881\n",
      "Epoch 00110: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0362 - acc: 0.9881 - val_loss: 0.1330 - val_acc: 0.9725\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9891\n",
      "Epoch 00111: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0349 - acc: 0.9891 - val_loss: 0.1242 - val_acc: 0.9737\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9889\n",
      "Epoch 00112: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0341 - acc: 0.9889 - val_loss: 0.1463 - val_acc: 0.9627\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9871\n",
      "Epoch 00113: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0393 - acc: 0.9871 - val_loss: 0.1299 - val_acc: 0.9702\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9889\n",
      "Epoch 00114: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0341 - acc: 0.9889 - val_loss: 0.1375 - val_acc: 0.9718\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9889\n",
      "Epoch 00115: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0340 - acc: 0.9889 - val_loss: 0.1461 - val_acc: 0.9711\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9895\n",
      "Epoch 00116: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0317 - acc: 0.9895 - val_loss: 0.1351 - val_acc: 0.9709\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887\n",
      "Epoch 00117: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0340 - acc: 0.9887 - val_loss: 0.1376 - val_acc: 0.9727\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9883\n",
      "Epoch 00118: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0354 - acc: 0.9883 - val_loss: 0.1455 - val_acc: 0.9720\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9893\n",
      "Epoch 00119: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0328 - acc: 0.9893 - val_loss: 0.1386 - val_acc: 0.9711\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9893\n",
      "Epoch 00120: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0317 - acc: 0.9893 - val_loss: 0.1293 - val_acc: 0.9727\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9905\n",
      "Epoch 00121: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0297 - acc: 0.9905 - val_loss: 0.1425 - val_acc: 0.9725\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9899\n",
      "Epoch 00122: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0325 - acc: 0.9899 - val_loss: 0.1430 - val_acc: 0.9702\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9901\n",
      "Epoch 00123: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0313 - acc: 0.9901 - val_loss: 0.1450 - val_acc: 0.9700\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9897\n",
      "Epoch 00124: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0312 - acc: 0.9897 - val_loss: 0.1367 - val_acc: 0.9695\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9898\n",
      "Epoch 00125: val_loss did not improve from 0.09761\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0324 - acc: 0.9897 - val_loss: 0.1289 - val_acc: 0.9725\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1470 - acc: 0.9616\n",
      "Loss: 0.14699696631142764 Accuracy: 0.9615784\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3951 - acc: 0.2141\n",
      "Epoch 00001: val_loss improved from inf to 1.65410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/001-1.6541.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.3951 - acc: 0.2141 - val_loss: 1.6541 - val_acc: 0.4596\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4471 - acc: 0.5235\n",
      "Epoch 00002: val_loss improved from 1.65410 to 0.91895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/002-0.9190.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.4469 - acc: 0.5235 - val_loss: 0.9190 - val_acc: 0.7098\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0290 - acc: 0.6644\n",
      "Epoch 00003: val_loss improved from 0.91895 to 0.68361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/003-0.6836.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.0291 - acc: 0.6644 - val_loss: 0.6836 - val_acc: 0.7939\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8507 - acc: 0.7263\n",
      "Epoch 00004: val_loss improved from 0.68361 to 0.57283, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/004-0.5728.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.8507 - acc: 0.7263 - val_loss: 0.5728 - val_acc: 0.8199\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7289 - acc: 0.7673\n",
      "Epoch 00005: val_loss improved from 0.57283 to 0.47401, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/005-0.4740.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7289 - acc: 0.7673 - val_loss: 0.4740 - val_acc: 0.8442\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.7957\n",
      "Epoch 00006: val_loss improved from 0.47401 to 0.42426, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/006-0.4243.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.6350 - acc: 0.7956 - val_loss: 0.4243 - val_acc: 0.8656\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.8230\n",
      "Epoch 00007: val_loss improved from 0.42426 to 0.34743, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/007-0.3474.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.5601 - acc: 0.8230 - val_loss: 0.3474 - val_acc: 0.8859\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8430\n",
      "Epoch 00008: val_loss improved from 0.34743 to 0.29615, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/008-0.2961.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4981 - acc: 0.8431 - val_loss: 0.2961 - val_acc: 0.9078\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8638\n",
      "Epoch 00009: val_loss improved from 0.29615 to 0.29270, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/009-0.2927.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4369 - acc: 0.8638 - val_loss: 0.2927 - val_acc: 0.9103\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8778\n",
      "Epoch 00010: val_loss improved from 0.29270 to 0.27014, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/010-0.2701.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3918 - acc: 0.8778 - val_loss: 0.2701 - val_acc: 0.9194\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8924\n",
      "Epoch 00011: val_loss improved from 0.27014 to 0.22744, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/011-0.2274.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3473 - acc: 0.8924 - val_loss: 0.2274 - val_acc: 0.9287\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8989\n",
      "Epoch 00012: val_loss improved from 0.22744 to 0.20430, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/012-0.2043.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3279 - acc: 0.8989 - val_loss: 0.2043 - val_acc: 0.9366\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9068\n",
      "Epoch 00013: val_loss did not improve from 0.20430\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3060 - acc: 0.9068 - val_loss: 0.2545 - val_acc: 0.9187\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9153\n",
      "Epoch 00014: val_loss improved from 0.20430 to 0.17295, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/014-0.1729.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2719 - acc: 0.9153 - val_loss: 0.1729 - val_acc: 0.9476\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9221\n",
      "Epoch 00015: val_loss improved from 0.17295 to 0.16384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/015-0.1638.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2528 - acc: 0.9221 - val_loss: 0.1638 - val_acc: 0.9492\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9259\n",
      "Epoch 00016: val_loss did not improve from 0.16384\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2411 - acc: 0.9259 - val_loss: 0.2309 - val_acc: 0.9308\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9285\n",
      "Epoch 00017: val_loss did not improve from 0.16384\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2336 - acc: 0.9285 - val_loss: 0.1704 - val_acc: 0.9497\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9339\n",
      "Epoch 00018: val_loss improved from 0.16384 to 0.14255, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/018-0.1426.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2114 - acc: 0.9339 - val_loss: 0.1426 - val_acc: 0.9590\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9391\n",
      "Epoch 00019: val_loss did not improve from 0.14255\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1949 - acc: 0.9391 - val_loss: 0.1492 - val_acc: 0.9546\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9394\n",
      "Epoch 00020: val_loss improved from 0.14255 to 0.14093, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/020-0.1409.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1937 - acc: 0.9394 - val_loss: 0.1409 - val_acc: 0.9562\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9446\n",
      "Epoch 00021: val_loss improved from 0.14093 to 0.13241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/021-0.1324.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1801 - acc: 0.9446 - val_loss: 0.1324 - val_acc: 0.9604\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9470\n",
      "Epoch 00022: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1691 - acc: 0.9470 - val_loss: 0.1578 - val_acc: 0.9515\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9496\n",
      "Epoch 00023: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1622 - acc: 0.9496 - val_loss: 0.1344 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9524\n",
      "Epoch 00024: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1506 - acc: 0.9524 - val_loss: 0.1422 - val_acc: 0.9599\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9536\n",
      "Epoch 00025: val_loss improved from 0.13241 to 0.12905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/025-0.1290.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1466 - acc: 0.9536 - val_loss: 0.1290 - val_acc: 0.9634\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9572\n",
      "Epoch 00026: val_loss did not improve from 0.12905\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1369 - acc: 0.9572 - val_loss: 0.1391 - val_acc: 0.9583\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9592\n",
      "Epoch 00027: val_loss did not improve from 0.12905\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1274 - acc: 0.9592 - val_loss: 0.1343 - val_acc: 0.9599\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9588\n",
      "Epoch 00028: val_loss improved from 0.12905 to 0.11604, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/028-0.1160.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1295 - acc: 0.9588 - val_loss: 0.1160 - val_acc: 0.9662\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9623\n",
      "Epoch 00029: val_loss did not improve from 0.11604\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1189 - acc: 0.9623 - val_loss: 0.1184 - val_acc: 0.9674\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9644\n",
      "Epoch 00030: val_loss did not improve from 0.11604\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1131 - acc: 0.9644 - val_loss: 0.1254 - val_acc: 0.9653\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9653\n",
      "Epoch 00031: val_loss did not improve from 0.11604\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1082 - acc: 0.9653 - val_loss: 0.1190 - val_acc: 0.9595\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9653\n",
      "Epoch 00032: val_loss improved from 0.11604 to 0.11102, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/032-0.1110.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1081 - acc: 0.9653 - val_loss: 0.1110 - val_acc: 0.9662\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9664\n",
      "Epoch 00033: val_loss did not improve from 0.11102\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1051 - acc: 0.9664 - val_loss: 0.1193 - val_acc: 0.9625\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9686\n",
      "Epoch 00034: val_loss improved from 0.11102 to 0.11015, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/034-0.1101.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0991 - acc: 0.9686 - val_loss: 0.1101 - val_acc: 0.9681\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9701\n",
      "Epoch 00035: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0899 - acc: 0.9701 - val_loss: 0.1260 - val_acc: 0.9604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9699\n",
      "Epoch 00036: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0924 - acc: 0.9699 - val_loss: 0.1228 - val_acc: 0.9653\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9704\n",
      "Epoch 00037: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0898 - acc: 0.9704 - val_loss: 0.1375 - val_acc: 0.9644\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9742\n",
      "Epoch 00038: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0834 - acc: 0.9742 - val_loss: 0.1226 - val_acc: 0.9648\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9740\n",
      "Epoch 00039: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0797 - acc: 0.9740 - val_loss: 0.1111 - val_acc: 0.9676\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9755\n",
      "Epoch 00040: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0764 - acc: 0.9755 - val_loss: 0.1262 - val_acc: 0.9639\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9737\n",
      "Epoch 00041: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0800 - acc: 0.9737 - val_loss: 0.1124 - val_acc: 0.9702\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9772\n",
      "Epoch 00042: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0695 - acc: 0.9772 - val_loss: 0.1170 - val_acc: 0.9669\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9772\n",
      "Epoch 00043: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0714 - acc: 0.9772 - val_loss: 0.1306 - val_acc: 0.9674\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9758\n",
      "Epoch 00044: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0750 - acc: 0.9758 - val_loss: 0.1316 - val_acc: 0.9639\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9786\n",
      "Epoch 00045: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0653 - acc: 0.9786 - val_loss: 0.1292 - val_acc: 0.9697\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9779\n",
      "Epoch 00046: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0662 - acc: 0.9779 - val_loss: 0.1312 - val_acc: 0.9686\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9796\n",
      "Epoch 00047: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0613 - acc: 0.9796 - val_loss: 0.1380 - val_acc: 0.9672\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9804\n",
      "Epoch 00048: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0595 - acc: 0.9804 - val_loss: 0.1251 - val_acc: 0.9690\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9808\n",
      "Epoch 00049: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0562 - acc: 0.9808 - val_loss: 0.1400 - val_acc: 0.9648\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9807\n",
      "Epoch 00050: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0602 - acc: 0.9807 - val_loss: 0.1327 - val_acc: 0.9634\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9812\n",
      "Epoch 00051: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0562 - acc: 0.9813 - val_loss: 0.1278 - val_acc: 0.9690\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9817\n",
      "Epoch 00052: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0547 - acc: 0.9817 - val_loss: 0.1130 - val_acc: 0.9725\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9825\n",
      "Epoch 00053: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0544 - acc: 0.9825 - val_loss: 0.1331 - val_acc: 0.9655\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9826\n",
      "Epoch 00054: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0512 - acc: 0.9826 - val_loss: 0.1581 - val_acc: 0.9653\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9844\n",
      "Epoch 00055: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0485 - acc: 0.9844 - val_loss: 0.1591 - val_acc: 0.9674\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9845\n",
      "Epoch 00056: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0491 - acc: 0.9845 - val_loss: 0.1328 - val_acc: 0.9686\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9839\n",
      "Epoch 00057: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0519 - acc: 0.9839 - val_loss: 0.1326 - val_acc: 0.9702\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9845\n",
      "Epoch 00058: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0462 - acc: 0.9845 - val_loss: 0.1380 - val_acc: 0.9697\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9826\n",
      "Epoch 00059: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0525 - acc: 0.9826 - val_loss: 0.1322 - val_acc: 0.9672\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9857\n",
      "Epoch 00060: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0453 - acc: 0.9857 - val_loss: 0.1639 - val_acc: 0.9651\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9863\n",
      "Epoch 00061: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0396 - acc: 0.9863 - val_loss: 0.1445 - val_acc: 0.9681\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9857\n",
      "Epoch 00062: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0429 - acc: 0.9857 - val_loss: 0.1548 - val_acc: 0.9683\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9864\n",
      "Epoch 00063: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0419 - acc: 0.9864 - val_loss: 0.1566 - val_acc: 0.9648\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9865\n",
      "Epoch 00064: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0421 - acc: 0.9866 - val_loss: 0.1549 - val_acc: 0.9646\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9863\n",
      "Epoch 00065: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0422 - acc: 0.9863 - val_loss: 0.1441 - val_acc: 0.9690\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9871\n",
      "Epoch 00066: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0410 - acc: 0.9871 - val_loss: 0.1322 - val_acc: 0.9723\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9873\n",
      "Epoch 00067: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0402 - acc: 0.9873 - val_loss: 0.1914 - val_acc: 0.9602\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9883\n",
      "Epoch 00068: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0370 - acc: 0.9883 - val_loss: 0.1420 - val_acc: 0.9686\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9885\n",
      "Epoch 00069: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0348 - acc: 0.9885 - val_loss: 0.1458 - val_acc: 0.9697\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9878\n",
      "Epoch 00070: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0389 - acc: 0.9878 - val_loss: 0.1791 - val_acc: 0.9606\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9885\n",
      "Epoch 00071: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0364 - acc: 0.9885 - val_loss: 0.1590 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9881\n",
      "Epoch 00072: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0361 - acc: 0.9881 - val_loss: 0.1420 - val_acc: 0.9665\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9890\n",
      "Epoch 00073: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0341 - acc: 0.9891 - val_loss: 0.1541 - val_acc: 0.9697\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9882\n",
      "Epoch 00074: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0363 - acc: 0.9882 - val_loss: 0.1529 - val_acc: 0.9697\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9894\n",
      "Epoch 00075: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0325 - acc: 0.9894 - val_loss: 0.1553 - val_acc: 0.9686\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9909\n",
      "Epoch 00076: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0296 - acc: 0.9909 - val_loss: 0.2191 - val_acc: 0.9625\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9898\n",
      "Epoch 00077: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0333 - acc: 0.9898 - val_loss: 0.1611 - val_acc: 0.9679\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9894\n",
      "Epoch 00078: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0350 - acc: 0.9894 - val_loss: 0.1422 - val_acc: 0.9683\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9904\n",
      "Epoch 00079: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0304 - acc: 0.9904 - val_loss: 0.1462 - val_acc: 0.9709\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9902\n",
      "Epoch 00080: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0300 - acc: 0.9902 - val_loss: 0.1517 - val_acc: 0.9711\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9900\n",
      "Epoch 00081: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0313 - acc: 0.9900 - val_loss: 0.1431 - val_acc: 0.9739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 00082: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0265 - acc: 0.9914 - val_loss: 0.1608 - val_acc: 0.9693\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9909\n",
      "Epoch 00083: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0290 - acc: 0.9909 - val_loss: 0.1541 - val_acc: 0.9718\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9905\n",
      "Epoch 00084: val_loss did not improve from 0.11015\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0294 - acc: 0.9905 - val_loss: 0.1548 - val_acc: 0.9704\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1638 - acc: 0.9568\n",
      "Loss: 0.16382623730809548 Accuracy: 0.95680165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    model_name = '1D_CNN_custom_conv_3_VGG_DO_075_DO_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "    \n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 540us/sample - loss: 2.0562 - acc: 0.3641\n",
      "Loss: 2.0561687808913236 Accuracy: 0.36407062\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 853us/sample - loss: 1.7243 - acc: 0.4665\n",
      "Loss: 1.724294582061926 Accuracy: 0.46645898\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 971us/sample - loss: 1.3509 - acc: 0.5850\n",
      "Loss: 1.3508993964576523 Accuracy: 0.5850467\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8876 - acc: 0.7697\n",
      "Loss: 0.8876171379569658 Accuracy: 0.7696781\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.6308 - acc: 0.8706\n",
      "Loss: 0.6307582460088522 Accuracy: 0.8706127\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3132 - acc: 0.9140\n",
      "Loss: 0.3132208975676064 Accuracy: 0.9140187\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1763 - acc: 0.9510\n",
      "Loss: 0.17631158992369597 Accuracy: 0.9509865\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_26 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1470 - acc: 0.9616\n",
      "Loss: 0.14699696631142764 Accuracy: 0.9615784\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1638 - acc: 0.9568\n",
      "Loss: 0.16382623730809548 Accuracy: 0.95680165\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6326 - acc: 0.6648\n",
      "Loss: 1.632649170225902 Accuracy: 0.6647975\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9869 - acc: 0.7915\n",
      "Loss: 0.9868532168407183 Accuracy: 0.79148495\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6370 - acc: 0.8694\n",
      "Loss: 0.6370343017367683 Accuracy: 0.8693666\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3734 - acc: 0.9186\n",
      "Loss: 0.37344957900041115 Accuracy: 0.91858774\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1993 - acc: 0.9537\n",
      "Loss: 0.1992640325052012 Accuracy: 0.9536864\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_26 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1853 - acc: 0.9620\n",
      "Loss: 0.18526855402912512 Accuracy: 0.96199375\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2036 - acc: 0.9593\n",
      "Loss: 0.20364515322298313 Accuracy: 0.9592939\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
