{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3975 - acc: 0.2426\n",
      "Epoch 00001: val_loss improved from inf to 2.20118, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv_checkpoint/001-2.2012.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.3975 - acc: 0.2427 - val_loss: 2.2012 - val_acc: 0.3380\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9784 - acc: 0.4061\n",
      "Epoch 00002: val_loss improved from 2.20118 to 2.06752, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv_checkpoint/002-2.0675.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.9784 - acc: 0.4061 - val_loss: 2.0675 - val_acc: 0.3482\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6591 - acc: 0.5097\n",
      "Epoch 00003: val_loss improved from 2.06752 to 2.03874, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv_checkpoint/003-2.0387.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.6590 - acc: 0.5097 - val_loss: 2.0387 - val_acc: 0.3739\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3915 - acc: 0.5930\n",
      "Epoch 00004: val_loss improved from 2.03874 to 2.01305, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv_checkpoint/004-2.0130.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.3915 - acc: 0.5930 - val_loss: 2.0130 - val_acc: 0.3899\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1708 - acc: 0.6597\n",
      "Epoch 00005: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1708 - acc: 0.6597 - val_loss: 2.0806 - val_acc: 0.3869\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9968 - acc: 0.7123\n",
      "Epoch 00006: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9968 - acc: 0.7123 - val_loss: 2.1341 - val_acc: 0.3816\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7617\n",
      "Epoch 00007: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8439 - acc: 0.7617 - val_loss: 2.1965 - val_acc: 0.3708\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.8027\n",
      "Epoch 00008: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7156 - acc: 0.8027 - val_loss: 2.2559 - val_acc: 0.3713\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.8302\n",
      "Epoch 00009: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6125 - acc: 0.8301 - val_loss: 2.3689 - val_acc: 0.3629\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8608\n",
      "Epoch 00010: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5194 - acc: 0.8608 - val_loss: 2.4204 - val_acc: 0.3648\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8808\n",
      "Epoch 00011: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4465 - acc: 0.8808 - val_loss: 2.5258 - val_acc: 0.3774\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8973\n",
      "Epoch 00012: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3870 - acc: 0.8972 - val_loss: 2.6357 - val_acc: 0.3680\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3393 - acc: 0.9115\n",
      "Epoch 00013: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3393 - acc: 0.9116 - val_loss: 2.6977 - val_acc: 0.3823\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9246\n",
      "Epoch 00014: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2962 - acc: 0.9246 - val_loss: 2.7901 - val_acc: 0.3711\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9337\n",
      "Epoch 00015: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2573 - acc: 0.9337 - val_loss: 2.8975 - val_acc: 0.3706\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9429\n",
      "Epoch 00016: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2301 - acc: 0.9429 - val_loss: 2.9536 - val_acc: 0.3678\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9484\n",
      "Epoch 00017: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2082 - acc: 0.9484 - val_loss: 3.0622 - val_acc: 0.3725\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9543\n",
      "Epoch 00018: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1840 - acc: 0.9543 - val_loss: 3.1085 - val_acc: 0.3673\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9600\n",
      "Epoch 00019: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1680 - acc: 0.9600 - val_loss: 3.2052 - val_acc: 0.3713\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9642\n",
      "Epoch 00020: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1480 - acc: 0.9642 - val_loss: 3.3892 - val_acc: 0.3685\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9677\n",
      "Epoch 00021: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1382 - acc: 0.9677 - val_loss: 3.3281 - val_acc: 0.3711\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9706\n",
      "Epoch 00022: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1257 - acc: 0.9706 - val_loss: 3.3998 - val_acc: 0.3725\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9739\n",
      "Epoch 00023: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1193 - acc: 0.9739 - val_loss: 3.4784 - val_acc: 0.3722\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9764\n",
      "Epoch 00024: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1080 - acc: 0.9764 - val_loss: 3.5463 - val_acc: 0.3699\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9776\n",
      "Epoch 00025: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1006 - acc: 0.9776 - val_loss: 3.6344 - val_acc: 0.3713\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9775\n",
      "Epoch 00026: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0997 - acc: 0.9775 - val_loss: 3.6548 - val_acc: 0.3750\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9802\n",
      "Epoch 00027: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0912 - acc: 0.9802 - val_loss: 3.7802 - val_acc: 0.3739\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9807\n",
      "Epoch 00028: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0867 - acc: 0.9807 - val_loss: 3.7958 - val_acc: 0.3776\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9825\n",
      "Epoch 00029: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0828 - acc: 0.9825 - val_loss: 3.8466 - val_acc: 0.3741\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9838\n",
      "Epoch 00030: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0750 - acc: 0.9838 - val_loss: 3.8884 - val_acc: 0.3753\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9845\n",
      "Epoch 00031: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0749 - acc: 0.9845 - val_loss: 3.8718 - val_acc: 0.3774\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9846\n",
      "Epoch 00032: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0726 - acc: 0.9846 - val_loss: 3.9335 - val_acc: 0.3804\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9858\n",
      "Epoch 00033: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0675 - acc: 0.9858 - val_loss: 3.9048 - val_acc: 0.3734\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9851\n",
      "Epoch 00034: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0675 - acc: 0.9851 - val_loss: 4.0014 - val_acc: 0.3701\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9867\n",
      "Epoch 00035: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0631 - acc: 0.9867 - val_loss: 4.0008 - val_acc: 0.3834\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9873\n",
      "Epoch 00036: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0606 - acc: 0.9873 - val_loss: 4.0895 - val_acc: 0.3755\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9868\n",
      "Epoch 00037: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0607 - acc: 0.9868 - val_loss: 4.0933 - val_acc: 0.3823\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9872\n",
      "Epoch 00038: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0601 - acc: 0.9872 - val_loss: 4.1319 - val_acc: 0.3799\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9883\n",
      "Epoch 00039: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0579 - acc: 0.9883 - val_loss: 4.0827 - val_acc: 0.3776\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9895\n",
      "Epoch 00040: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0520 - acc: 0.9895 - val_loss: 4.1232 - val_acc: 0.3823\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9898\n",
      "Epoch 00041: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0513 - acc: 0.9898 - val_loss: 4.2117 - val_acc: 0.3841\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9887\n",
      "Epoch 00042: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0524 - acc: 0.9887 - val_loss: 4.2813 - val_acc: 0.3748\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9889\n",
      "Epoch 00043: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0503 - acc: 0.9889 - val_loss: 4.2325 - val_acc: 0.3811\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9896\n",
      "Epoch 00044: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0497 - acc: 0.9896 - val_loss: 4.3182 - val_acc: 0.3792\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9909\n",
      "Epoch 00045: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0445 - acc: 0.9909 - val_loss: 4.4416 - val_acc: 0.3743\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9911\n",
      "Epoch 00046: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0462 - acc: 0.9911 - val_loss: 4.2926 - val_acc: 0.3818\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9916\n",
      "Epoch 00047: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0431 - acc: 0.9916 - val_loss: 4.3818 - val_acc: 0.3748\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9905\n",
      "Epoch 00048: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0460 - acc: 0.9905 - val_loss: 4.4467 - val_acc: 0.3820\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9904\n",
      "Epoch 00049: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0464 - acc: 0.9904 - val_loss: 4.4019 - val_acc: 0.3764\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9917\n",
      "Epoch 00050: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0425 - acc: 0.9917 - val_loss: 4.4397 - val_acc: 0.3760\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9918\n",
      "Epoch 00051: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0418 - acc: 0.9918 - val_loss: 4.4877 - val_acc: 0.3836\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9920\n",
      "Epoch 00052: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0412 - acc: 0.9920 - val_loss: 4.5321 - val_acc: 0.3760\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9896\n",
      "Epoch 00053: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0457 - acc: 0.9896 - val_loss: 4.4761 - val_acc: 0.3843\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9910\n",
      "Epoch 00054: val_loss did not improve from 2.01305\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0426 - acc: 0.9910 - val_loss: 4.4838 - val_acc: 0.3774\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 490us/sample - loss: 2.0562 - acc: 0.3641\n",
      "Loss: 2.0561687808913236 Accuracy: 0.36407062\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2985 - acc: 0.2742\n",
      "Epoch 00001: val_loss improved from inf to 2.00837, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv_checkpoint/001-2.0084.hdf5\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 2.2985 - acc: 0.2743 - val_loss: 2.0084 - val_acc: 0.3860\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8164 - acc: 0.4527\n",
      "Epoch 00002: val_loss improved from 2.00837 to 1.75839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv_checkpoint/002-1.7584.hdf5\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 1.8164 - acc: 0.4527 - val_loss: 1.7584 - val_acc: 0.4500\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5527 - acc: 0.5352\n",
      "Epoch 00003: val_loss improved from 1.75839 to 1.66237, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv_checkpoint/003-1.6624.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.5527 - acc: 0.5352 - val_loss: 1.6624 - val_acc: 0.4899\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3647 - acc: 0.5917\n",
      "Epoch 00004: val_loss improved from 1.66237 to 1.66007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv_checkpoint/004-1.6601.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.3646 - acc: 0.5917 - val_loss: 1.6601 - val_acc: 0.4859\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2092 - acc: 0.6402\n",
      "Epoch 00005: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.2093 - acc: 0.6402 - val_loss: 1.6775 - val_acc: 0.4894\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6781\n",
      "Epoch 00006: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.0698 - acc: 0.6781 - val_loss: 1.7127 - val_acc: 0.4901\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.7138\n",
      "Epoch 00007: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.9526 - acc: 0.7138 - val_loss: 1.7407 - val_acc: 0.4871\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8416 - acc: 0.7434\n",
      "Epoch 00008: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.8416 - acc: 0.7434 - val_loss: 1.8282 - val_acc: 0.4847\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7519 - acc: 0.7695\n",
      "Epoch 00009: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.7519 - acc: 0.7695 - val_loss: 1.8208 - val_acc: 0.4938\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.7974\n",
      "Epoch 00010: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.6539 - acc: 0.7974 - val_loss: 1.8870 - val_acc: 0.4992\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8159\n",
      "Epoch 00011: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5952 - acc: 0.8159 - val_loss: 1.9510 - val_acc: 0.4922\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8298\n",
      "Epoch 00012: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5406 - acc: 0.8298 - val_loss: 2.0030 - val_acc: 0.4955\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.8489\n",
      "Epoch 00013: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4831 - acc: 0.8490 - val_loss: 2.0817 - val_acc: 0.4864\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8587\n",
      "Epoch 00014: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4485 - acc: 0.8586 - val_loss: 2.1145 - val_acc: 0.4878\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8720\n",
      "Epoch 00015: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4025 - acc: 0.8720 - val_loss: 2.1972 - val_acc: 0.4913\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8828\n",
      "Epoch 00016: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3677 - acc: 0.8828 - val_loss: 2.2767 - val_acc: 0.4915\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8925\n",
      "Epoch 00017: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3416 - acc: 0.8925 - val_loss: 2.2818 - val_acc: 0.4943\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8977\n",
      "Epoch 00018: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3263 - acc: 0.8977 - val_loss: 2.3445 - val_acc: 0.4878\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9083\n",
      "Epoch 00019: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2908 - acc: 0.9083 - val_loss: 2.4078 - val_acc: 0.4878\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9145\n",
      "Epoch 00020: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2706 - acc: 0.9145 - val_loss: 2.5317 - val_acc: 0.4943\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9200\n",
      "Epoch 00021: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2564 - acc: 0.9200 - val_loss: 2.4660 - val_acc: 0.5020\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9241\n",
      "Epoch 00022: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2456 - acc: 0.9241 - val_loss: 2.5480 - val_acc: 0.4978\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9305\n",
      "Epoch 00023: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2245 - acc: 0.9305 - val_loss: 2.5481 - val_acc: 0.5073\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9324\n",
      "Epoch 00024: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2211 - acc: 0.9324 - val_loss: 2.6126 - val_acc: 0.4962\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9353\n",
      "Epoch 00025: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2105 - acc: 0.9353 - val_loss: 2.6348 - val_acc: 0.5027\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9390\n",
      "Epoch 00026: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2007 - acc: 0.9389 - val_loss: 2.6456 - val_acc: 0.5076\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9411\n",
      "Epoch 00027: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1931 - acc: 0.9411 - val_loss: 2.6617 - val_acc: 0.5087\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9454\n",
      "Epoch 00028: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1807 - acc: 0.9454 - val_loss: 2.7251 - val_acc: 0.5001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9453\n",
      "Epoch 00029: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1784 - acc: 0.9453 - val_loss: 2.7311 - val_acc: 0.4980\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9493\n",
      "Epoch 00030: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1671 - acc: 0.9493 - val_loss: 2.8130 - val_acc: 0.5034\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9530\n",
      "Epoch 00031: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1579 - acc: 0.9530 - val_loss: 2.7672 - val_acc: 0.5113\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9536\n",
      "Epoch 00032: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1541 - acc: 0.9536 - val_loss: 2.7768 - val_acc: 0.5134\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9573\n",
      "Epoch 00033: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1466 - acc: 0.9573 - val_loss: 2.8095 - val_acc: 0.5183\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9561\n",
      "Epoch 00034: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1484 - acc: 0.9561 - val_loss: 2.8108 - val_acc: 0.5127\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9590\n",
      "Epoch 00035: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1403 - acc: 0.9590 - val_loss: 2.8261 - val_acc: 0.5094\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9608\n",
      "Epoch 00036: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1363 - acc: 0.9608 - val_loss: 2.8680 - val_acc: 0.5115\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9602\n",
      "Epoch 00037: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1370 - acc: 0.9602 - val_loss: 2.8374 - val_acc: 0.5090\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9612\n",
      "Epoch 00038: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1307 - acc: 0.9612 - val_loss: 2.9884 - val_acc: 0.5055\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9627\n",
      "Epoch 00039: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1314 - acc: 0.9627 - val_loss: 2.9725 - val_acc: 0.5062\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9640\n",
      "Epoch 00040: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1235 - acc: 0.9639 - val_loss: 2.8735 - val_acc: 0.5150\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9636\n",
      "Epoch 00041: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1206 - acc: 0.9636 - val_loss: 2.9007 - val_acc: 0.5190\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9658\n",
      "Epoch 00042: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1180 - acc: 0.9658 - val_loss: 2.8966 - val_acc: 0.5153\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9675\n",
      "Epoch 00043: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1161 - acc: 0.9675 - val_loss: 2.9577 - val_acc: 0.5181\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9668\n",
      "Epoch 00044: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1137 - acc: 0.9668 - val_loss: 2.9645 - val_acc: 0.5129\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9700\n",
      "Epoch 00045: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1084 - acc: 0.9700 - val_loss: 3.0289 - val_acc: 0.5132\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9684\n",
      "Epoch 00046: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1072 - acc: 0.9684 - val_loss: 3.0219 - val_acc: 0.5218\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9686\n",
      "Epoch 00047: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1113 - acc: 0.9686 - val_loss: 3.0155 - val_acc: 0.5262\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9698\n",
      "Epoch 00048: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1040 - acc: 0.9698 - val_loss: 3.0749 - val_acc: 0.5143\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9720\n",
      "Epoch 00049: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1010 - acc: 0.9720 - val_loss: 2.9953 - val_acc: 0.5262\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9724\n",
      "Epoch 00050: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0978 - acc: 0.9724 - val_loss: 3.0861 - val_acc: 0.5153\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9708\n",
      "Epoch 00051: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1015 - acc: 0.9708 - val_loss: 3.0213 - val_acc: 0.5127\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9732\n",
      "Epoch 00052: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0973 - acc: 0.9732 - val_loss: 2.9909 - val_acc: 0.5218\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9725\n",
      "Epoch 00053: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0983 - acc: 0.9725 - val_loss: 2.9702 - val_acc: 0.5248\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9752\n",
      "Epoch 00054: val_loss did not improve from 1.66007\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0902 - acc: 0.9752 - val_loss: 2.9903 - val_acc: 0.5253\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 782us/sample - loss: 1.7243 - acc: 0.4665\n",
      "Loss: 1.724294582061926 Accuracy: 0.46645898\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2363 - acc: 0.2932\n",
      "Epoch 00001: val_loss improved from inf to 1.80666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/001-1.8067.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 2.2364 - acc: 0.2932 - val_loss: 1.8067 - val_acc: 0.4596\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6579 - acc: 0.4917\n",
      "Epoch 00002: val_loss improved from 1.80666 to 1.51766, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/002-1.5177.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.6579 - acc: 0.4916 - val_loss: 1.5177 - val_acc: 0.5236\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4140 - acc: 0.5629\n",
      "Epoch 00003: val_loss improved from 1.51766 to 1.42854, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/003-1.4285.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.4141 - acc: 0.5629 - val_loss: 1.4285 - val_acc: 0.5628\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.6082\n",
      "Epoch 00004: val_loss improved from 1.42854 to 1.37689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/004-1.3769.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.2639 - acc: 0.6082 - val_loss: 1.3769 - val_acc: 0.5726\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1447 - acc: 0.6481\n",
      "Epoch 00005: val_loss improved from 1.37689 to 1.32421, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/005-1.3242.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1448 - acc: 0.6481 - val_loss: 1.3242 - val_acc: 0.5984\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0521 - acc: 0.6743\n",
      "Epoch 00006: val_loss improved from 1.32421 to 1.29429, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/006-1.2943.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0521 - acc: 0.6743 - val_loss: 1.2943 - val_acc: 0.6082\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.6972\n",
      "Epoch 00007: val_loss improved from 1.29429 to 1.26386, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv_checkpoint/007-1.2639.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9721 - acc: 0.6972 - val_loss: 1.2639 - val_acc: 0.6217\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8925 - acc: 0.7246\n",
      "Epoch 00008: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8924 - acc: 0.7246 - val_loss: 1.3043 - val_acc: 0.6168\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.7420\n",
      "Epoch 00009: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8271 - acc: 0.7420 - val_loss: 1.2675 - val_acc: 0.6282\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.7579\n",
      "Epoch 00010: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7699 - acc: 0.7579 - val_loss: 1.2648 - val_acc: 0.6375\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7781\n",
      "Epoch 00011: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7095 - acc: 0.7782 - val_loss: 1.2748 - val_acc: 0.6348\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6633 - acc: 0.7893\n",
      "Epoch 00012: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6633 - acc: 0.7893 - val_loss: 1.2946 - val_acc: 0.6375\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.8002\n",
      "Epoch 00013: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6228 - acc: 0.8002 - val_loss: 1.2922 - val_acc: 0.6389\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8161\n",
      "Epoch 00014: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5812 - acc: 0.8162 - val_loss: 1.3179 - val_acc: 0.6362\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8266\n",
      "Epoch 00015: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5476 - acc: 0.8266 - val_loss: 1.3310 - val_acc: 0.6403\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8383\n",
      "Epoch 00016: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5106 - acc: 0.8383 - val_loss: 1.3261 - val_acc: 0.6483\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8470\n",
      "Epoch 00017: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4769 - acc: 0.8470 - val_loss: 1.3442 - val_acc: 0.6504\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8535\n",
      "Epoch 00018: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4527 - acc: 0.8535 - val_loss: 1.3868 - val_acc: 0.6427\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.8601\n",
      "Epoch 00019: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4268 - acc: 0.8601 - val_loss: 1.3754 - val_acc: 0.6480\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8721\n",
      "Epoch 00020: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4008 - acc: 0.8722 - val_loss: 1.3757 - val_acc: 0.6469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8758\n",
      "Epoch 00021: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3791 - acc: 0.8758 - val_loss: 1.3878 - val_acc: 0.6520\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8842\n",
      "Epoch 00022: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3625 - acc: 0.8842 - val_loss: 1.4759 - val_acc: 0.6469\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8880\n",
      "Epoch 00023: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3493 - acc: 0.8880 - val_loss: 1.3817 - val_acc: 0.6611\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8936\n",
      "Epoch 00024: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3313 - acc: 0.8937 - val_loss: 1.3988 - val_acc: 0.6667\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.8973\n",
      "Epoch 00025: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3151 - acc: 0.8973 - val_loss: 1.4780 - val_acc: 0.6622\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9014\n",
      "Epoch 00026: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3052 - acc: 0.9014 - val_loss: 1.4288 - val_acc: 0.6613\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9059\n",
      "Epoch 00027: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2947 - acc: 0.9059 - val_loss: 1.4203 - val_acc: 0.6697\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9087\n",
      "Epoch 00028: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2794 - acc: 0.9087 - val_loss: 1.4751 - val_acc: 0.6676\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9105\n",
      "Epoch 00029: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2718 - acc: 0.9105 - val_loss: 1.4803 - val_acc: 0.6713\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9179\n",
      "Epoch 00030: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2607 - acc: 0.9179 - val_loss: 1.4921 - val_acc: 0.6669\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9189\n",
      "Epoch 00031: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2513 - acc: 0.9189 - val_loss: 1.4434 - val_acc: 0.6674\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9208\n",
      "Epoch 00032: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2501 - acc: 0.9208 - val_loss: 1.4941 - val_acc: 0.6720\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9235\n",
      "Epoch 00033: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2386 - acc: 0.9235 - val_loss: 1.4935 - val_acc: 0.6706\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9271\n",
      "Epoch 00034: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2313 - acc: 0.9272 - val_loss: 1.5076 - val_acc: 0.6746\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9291\n",
      "Epoch 00035: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2235 - acc: 0.9291 - val_loss: 1.4899 - val_acc: 0.6695\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9326\n",
      "Epoch 00036: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2138 - acc: 0.9326 - val_loss: 1.5004 - val_acc: 0.6734\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9302\n",
      "Epoch 00037: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2171 - acc: 0.9303 - val_loss: 1.4991 - val_acc: 0.6727\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9357\n",
      "Epoch 00038: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2034 - acc: 0.9357 - val_loss: 1.5673 - val_acc: 0.6755\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9377\n",
      "Epoch 00039: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1942 - acc: 0.9377 - val_loss: 1.5578 - val_acc: 0.6639\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9389\n",
      "Epoch 00040: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1924 - acc: 0.9389 - val_loss: 1.5343 - val_acc: 0.6767\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9384\n",
      "Epoch 00041: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1921 - acc: 0.9384 - val_loss: 1.5413 - val_acc: 0.6832\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9420\n",
      "Epoch 00042: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1862 - acc: 0.9420 - val_loss: 1.5349 - val_acc: 0.6725\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9427\n",
      "Epoch 00043: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1830 - acc: 0.9428 - val_loss: 1.5546 - val_acc: 0.6739\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9418\n",
      "Epoch 00044: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1837 - acc: 0.9419 - val_loss: 1.5320 - val_acc: 0.6855\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9467\n",
      "Epoch 00045: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1713 - acc: 0.9467 - val_loss: 1.4756 - val_acc: 0.6916\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9464\n",
      "Epoch 00046: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1696 - acc: 0.9464 - val_loss: 1.5186 - val_acc: 0.6869\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9486\n",
      "Epoch 00047: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1653 - acc: 0.9486 - val_loss: 1.6014 - val_acc: 0.6937\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9499\n",
      "Epoch 00048: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1614 - acc: 0.9499 - val_loss: 1.5656 - val_acc: 0.6888\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9473\n",
      "Epoch 00049: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1700 - acc: 0.9473 - val_loss: 1.5519 - val_acc: 0.6816\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9525\n",
      "Epoch 00050: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1548 - acc: 0.9525 - val_loss: 1.5482 - val_acc: 0.6841\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9517\n",
      "Epoch 00051: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1536 - acc: 0.9517 - val_loss: 1.6071 - val_acc: 0.6830\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9539\n",
      "Epoch 00052: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1498 - acc: 0.9539 - val_loss: 1.6134 - val_acc: 0.6774\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9527\n",
      "Epoch 00053: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1503 - acc: 0.9528 - val_loss: 1.5592 - val_acc: 0.6876\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9546\n",
      "Epoch 00054: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1485 - acc: 0.9546 - val_loss: 1.5665 - val_acc: 0.6900\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9550\n",
      "Epoch 00055: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1452 - acc: 0.9550 - val_loss: 1.5772 - val_acc: 0.6916\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9548\n",
      "Epoch 00056: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1440 - acc: 0.9548 - val_loss: 1.5810 - val_acc: 0.7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9530\n",
      "Epoch 00057: val_loss did not improve from 1.26386\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1507 - acc: 0.9530 - val_loss: 1.5136 - val_acc: 0.6974\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 907us/sample - loss: 1.3509 - acc: 0.5850\n",
      "Loss: 1.3508993964576523 Accuracy: 0.5850467\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3290 - acc: 0.2443\n",
      "Epoch 00001: val_loss improved from inf to 1.79575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/001-1.7957.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.3289 - acc: 0.2444 - val_loss: 1.7957 - val_acc: 0.4060\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6060 - acc: 0.4776\n",
      "Epoch 00002: val_loss improved from 1.79575 to 1.40597, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/002-1.4060.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.6060 - acc: 0.4776 - val_loss: 1.4060 - val_acc: 0.5546\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4054 - acc: 0.5498\n",
      "Epoch 00003: val_loss improved from 1.40597 to 1.28381, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/003-1.2838.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.4053 - acc: 0.5498 - val_loss: 1.2838 - val_acc: 0.5970\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2490 - acc: 0.6071\n",
      "Epoch 00004: val_loss improved from 1.28381 to 1.14221, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/004-1.1422.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2489 - acc: 0.6071 - val_loss: 1.1422 - val_acc: 0.6469\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1336 - acc: 0.6450\n",
      "Epoch 00005: val_loss improved from 1.14221 to 1.07273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/005-1.0727.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1337 - acc: 0.6450 - val_loss: 1.0727 - val_acc: 0.6681\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0385 - acc: 0.6795\n",
      "Epoch 00006: val_loss improved from 1.07273 to 1.01488, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/006-1.0149.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0385 - acc: 0.6795 - val_loss: 1.0149 - val_acc: 0.6881\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9573 - acc: 0.7015\n",
      "Epoch 00007: val_loss improved from 1.01488 to 0.97189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/007-0.9719.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9574 - acc: 0.7015 - val_loss: 0.9719 - val_acc: 0.7091\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8941 - acc: 0.7248\n",
      "Epoch 00008: val_loss improved from 0.97189 to 0.91921, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/008-0.9192.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8941 - acc: 0.7248 - val_loss: 0.9192 - val_acc: 0.7263\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.7429\n",
      "Epoch 00009: val_loss did not improve from 0.91921\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8401 - acc: 0.7428 - val_loss: 0.9574 - val_acc: 0.7123\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7596\n",
      "Epoch 00010: val_loss improved from 0.91921 to 0.87045, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/010-0.8704.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7839 - acc: 0.7596 - val_loss: 0.8704 - val_acc: 0.7314\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7351 - acc: 0.7733\n",
      "Epoch 00011: val_loss improved from 0.87045 to 0.83814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/011-0.8381.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7350 - acc: 0.7733 - val_loss: 0.8381 - val_acc: 0.7503\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.7854\n",
      "Epoch 00012: val_loss did not improve from 0.83814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6915 - acc: 0.7854 - val_loss: 0.8459 - val_acc: 0.7431\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6601 - acc: 0.7968\n",
      "Epoch 00013: val_loss improved from 0.83814 to 0.81265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/013-0.8126.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6602 - acc: 0.7968 - val_loss: 0.8126 - val_acc: 0.7608\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6166 - acc: 0.8081\n",
      "Epoch 00014: val_loss did not improve from 0.81265\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6166 - acc: 0.8082 - val_loss: 0.8217 - val_acc: 0.7603\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5819 - acc: 0.8177\n",
      "Epoch 00015: val_loss did not improve from 0.81265\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5820 - acc: 0.8177 - val_loss: 0.8281 - val_acc: 0.7598\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8252\n",
      "Epoch 00016: val_loss did not improve from 0.81265\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5584 - acc: 0.8252 - val_loss: 0.8134 - val_acc: 0.7582\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.8315\n",
      "Epoch 00017: val_loss improved from 0.81265 to 0.80153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/017-0.8015.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5384 - acc: 0.8315 - val_loss: 0.8015 - val_acc: 0.7666\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8403\n",
      "Epoch 00018: val_loss improved from 0.80153 to 0.78730, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/018-0.7873.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5135 - acc: 0.8403 - val_loss: 0.7873 - val_acc: 0.7741\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8456\n",
      "Epoch 00019: val_loss improved from 0.78730 to 0.78400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/019-0.7840.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4882 - acc: 0.8456 - val_loss: 0.7840 - val_acc: 0.7759\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8495\n",
      "Epoch 00020: val_loss improved from 0.78400 to 0.78091, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/020-0.7809.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4720 - acc: 0.8495 - val_loss: 0.7809 - val_acc: 0.7780\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8569\n",
      "Epoch 00021: val_loss did not improve from 0.78091\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4442 - acc: 0.8569 - val_loss: 0.7874 - val_acc: 0.7822\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.8601\n",
      "Epoch 00022: val_loss did not improve from 0.78091\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4355 - acc: 0.8600 - val_loss: 0.8610 - val_acc: 0.7703\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8652\n",
      "Epoch 00023: val_loss improved from 0.78091 to 0.77832, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/023-0.7783.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4171 - acc: 0.8653 - val_loss: 0.7783 - val_acc: 0.7873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8692\n",
      "Epoch 00024: val_loss did not improve from 0.77832\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4029 - acc: 0.8692 - val_loss: 0.8028 - val_acc: 0.7787\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8725\n",
      "Epoch 00025: val_loss did not improve from 0.77832\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3905 - acc: 0.8725 - val_loss: 0.7788 - val_acc: 0.7845\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8763\n",
      "Epoch 00026: val_loss did not improve from 0.77832\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3820 - acc: 0.8763 - val_loss: 0.7850 - val_acc: 0.7822\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8815\n",
      "Epoch 00027: val_loss improved from 0.77832 to 0.77662, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/027-0.7766.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3663 - acc: 0.8815 - val_loss: 0.7766 - val_acc: 0.7864\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8815\n",
      "Epoch 00028: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3610 - acc: 0.8815 - val_loss: 0.8471 - val_acc: 0.7834\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8868\n",
      "Epoch 00029: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3456 - acc: 0.8868 - val_loss: 0.7896 - val_acc: 0.7906\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8863\n",
      "Epoch 00030: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3435 - acc: 0.8863 - val_loss: 0.8324 - val_acc: 0.7817\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.8904\n",
      "Epoch 00031: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3360 - acc: 0.8904 - val_loss: 0.8705 - val_acc: 0.7824\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8926\n",
      "Epoch 00032: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3250 - acc: 0.8926 - val_loss: 0.8107 - val_acc: 0.7906\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.8958\n",
      "Epoch 00033: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3111 - acc: 0.8957 - val_loss: 0.7780 - val_acc: 0.7950\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8979\n",
      "Epoch 00034: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3107 - acc: 0.8978 - val_loss: 0.8055 - val_acc: 0.7969\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8977\n",
      "Epoch 00035: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3046 - acc: 0.8977 - val_loss: 0.8335 - val_acc: 0.7945\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.8993\n",
      "Epoch 00036: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2987 - acc: 0.8994 - val_loss: 0.7863 - val_acc: 0.7990\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9041\n",
      "Epoch 00037: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2895 - acc: 0.9041 - val_loss: 0.8017 - val_acc: 0.7920\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9047\n",
      "Epoch 00038: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2842 - acc: 0.9047 - val_loss: 0.8026 - val_acc: 0.8006\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9066\n",
      "Epoch 00039: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2830 - acc: 0.9066 - val_loss: 0.8239 - val_acc: 0.7911\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9087\n",
      "Epoch 00040: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2760 - acc: 0.9087 - val_loss: 0.8012 - val_acc: 0.7999\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9089\n",
      "Epoch 00041: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2717 - acc: 0.9088 - val_loss: 0.8187 - val_acc: 0.8027\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9093\n",
      "Epoch 00042: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2696 - acc: 0.9093 - val_loss: 0.8228 - val_acc: 0.8046\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9086\n",
      "Epoch 00043: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2714 - acc: 0.9086 - val_loss: 0.8831 - val_acc: 0.7929\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9142\n",
      "Epoch 00044: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2553 - acc: 0.9141 - val_loss: 0.8259 - val_acc: 0.7980\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9152\n",
      "Epoch 00045: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2570 - acc: 0.9152 - val_loss: 0.8010 - val_acc: 0.7987\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9154\n",
      "Epoch 00046: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2518 - acc: 0.9154 - val_loss: 0.7828 - val_acc: 0.8055\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9158\n",
      "Epoch 00047: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2529 - acc: 0.9158 - val_loss: 0.8167 - val_acc: 0.7994\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9178\n",
      "Epoch 00048: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2469 - acc: 0.9178 - val_loss: 0.8100 - val_acc: 0.8055\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9177\n",
      "Epoch 00049: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2451 - acc: 0.9176 - val_loss: 0.8162 - val_acc: 0.8092\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9215\n",
      "Epoch 00050: val_loss did not improve from 0.77662\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2396 - acc: 0.9215 - val_loss: 0.8023 - val_acc: 0.8069\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9204\n",
      "Epoch 00051: val_loss improved from 0.77662 to 0.76814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv_checkpoint/051-0.7681.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2358 - acc: 0.9204 - val_loss: 0.7681 - val_acc: 0.8015\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9221\n",
      "Epoch 00052: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2323 - acc: 0.9221 - val_loss: 0.7967 - val_acc: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9230\n",
      "Epoch 00053: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2323 - acc: 0.9229 - val_loss: 0.8038 - val_acc: 0.8064\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9267\n",
      "Epoch 00054: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2220 - acc: 0.9266 - val_loss: 0.7840 - val_acc: 0.8097\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9274\n",
      "Epoch 00055: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2209 - acc: 0.9274 - val_loss: 0.7793 - val_acc: 0.8162\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9290\n",
      "Epoch 00056: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2146 - acc: 0.9290 - val_loss: 0.8637 - val_acc: 0.8050\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9273\n",
      "Epoch 00057: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2207 - acc: 0.9273 - val_loss: 0.8264 - val_acc: 0.8116\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9280\n",
      "Epoch 00058: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2136 - acc: 0.9280 - val_loss: 0.8074 - val_acc: 0.8097\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9286\n",
      "Epoch 00059: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2121 - acc: 0.9286 - val_loss: 0.8463 - val_acc: 0.8060\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9308\n",
      "Epoch 00060: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2064 - acc: 0.9308 - val_loss: 0.8432 - val_acc: 0.8113\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9314\n",
      "Epoch 00061: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2053 - acc: 0.9314 - val_loss: 0.7899 - val_acc: 0.8195\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9326\n",
      "Epoch 00062: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2026 - acc: 0.9326 - val_loss: 0.7891 - val_acc: 0.8162\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9319\n",
      "Epoch 00063: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1992 - acc: 0.9319 - val_loss: 0.7799 - val_acc: 0.8162\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9348\n",
      "Epoch 00064: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1965 - acc: 0.9347 - val_loss: 0.8134 - val_acc: 0.8085\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9349\n",
      "Epoch 00065: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1981 - acc: 0.9349 - val_loss: 0.7914 - val_acc: 0.8139\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9334\n",
      "Epoch 00066: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1972 - acc: 0.9334 - val_loss: 0.8486 - val_acc: 0.8104\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9345\n",
      "Epoch 00067: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1954 - acc: 0.9345 - val_loss: 0.8028 - val_acc: 0.8153\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9324\n",
      "Epoch 00068: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2022 - acc: 0.9324 - val_loss: 0.7964 - val_acc: 0.8157\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9358\n",
      "Epoch 00069: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1929 - acc: 0.9358 - val_loss: 0.7879 - val_acc: 0.8244\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9374\n",
      "Epoch 00070: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1912 - acc: 0.9374 - val_loss: 0.7942 - val_acc: 0.8192\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9361\n",
      "Epoch 00071: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1940 - acc: 0.9361 - val_loss: 0.7856 - val_acc: 0.8155\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9368\n",
      "Epoch 00072: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1900 - acc: 0.9368 - val_loss: 0.7945 - val_acc: 0.8183\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9405\n",
      "Epoch 00073: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1812 - acc: 0.9405 - val_loss: 0.8338 - val_acc: 0.8120\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9387\n",
      "Epoch 00074: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1801 - acc: 0.9387 - val_loss: 0.8210 - val_acc: 0.8130\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9406\n",
      "Epoch 00075: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1811 - acc: 0.9406 - val_loss: 0.8149 - val_acc: 0.8153\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9402\n",
      "Epoch 00076: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1812 - acc: 0.9402 - val_loss: 0.8171 - val_acc: 0.8197\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9406\n",
      "Epoch 00077: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1785 - acc: 0.9406 - val_loss: 0.8202 - val_acc: 0.8199\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9425\n",
      "Epoch 00078: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1761 - acc: 0.9425 - val_loss: 0.7978 - val_acc: 0.8246\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9390\n",
      "Epoch 00079: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1832 - acc: 0.9391 - val_loss: 0.8040 - val_acc: 0.8197\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9449\n",
      "Epoch 00080: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1728 - acc: 0.9449 - val_loss: 0.8042 - val_acc: 0.8241\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9425\n",
      "Epoch 00081: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1761 - acc: 0.9425 - val_loss: 0.8381 - val_acc: 0.8178\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9423\n",
      "Epoch 00082: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1751 - acc: 0.9423 - val_loss: 0.8536 - val_acc: 0.8118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9440\n",
      "Epoch 00083: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1663 - acc: 0.9440 - val_loss: 0.8166 - val_acc: 0.8185\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9454\n",
      "Epoch 00084: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1680 - acc: 0.9454 - val_loss: 0.7969 - val_acc: 0.8220\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9431\n",
      "Epoch 00085: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1712 - acc: 0.9431 - val_loss: 0.7896 - val_acc: 0.8272\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9443\n",
      "Epoch 00086: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1657 - acc: 0.9443 - val_loss: 0.7940 - val_acc: 0.8248\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9465\n",
      "Epoch 00087: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1655 - acc: 0.9466 - val_loss: 0.8037 - val_acc: 0.8195\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9461\n",
      "Epoch 00088: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1623 - acc: 0.9460 - val_loss: 0.8198 - val_acc: 0.8230\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9440\n",
      "Epoch 00089: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1714 - acc: 0.9440 - val_loss: 0.8525 - val_acc: 0.8192\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9445\n",
      "Epoch 00090: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1675 - acc: 0.9445 - val_loss: 0.8076 - val_acc: 0.8195\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9475\n",
      "Epoch 00091: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1583 - acc: 0.9475 - val_loss: 0.8079 - val_acc: 0.8234\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9461\n",
      "Epoch 00092: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1655 - acc: 0.9461 - val_loss: 0.8292 - val_acc: 0.8213\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9463\n",
      "Epoch 00093: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1640 - acc: 0.9463 - val_loss: 0.7827 - val_acc: 0.8176\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9488\n",
      "Epoch 00094: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1552 - acc: 0.9488 - val_loss: 0.8147 - val_acc: 0.8225\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9487\n",
      "Epoch 00095: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1601 - acc: 0.9487 - val_loss: 0.8370 - val_acc: 0.8199\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9498\n",
      "Epoch 00096: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1538 - acc: 0.9498 - val_loss: 0.8376 - val_acc: 0.8248\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9460\n",
      "Epoch 00097: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1640 - acc: 0.9460 - val_loss: 0.7914 - val_acc: 0.8290\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9454\n",
      "Epoch 00098: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1629 - acc: 0.9454 - val_loss: 0.8220 - val_acc: 0.8262\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9500\n",
      "Epoch 00099: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1506 - acc: 0.9500 - val_loss: 0.8280 - val_acc: 0.8265\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9487\n",
      "Epoch 00100: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1550 - acc: 0.9487 - val_loss: 0.8089 - val_acc: 0.8330\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9523\n",
      "Epoch 00101: val_loss did not improve from 0.76814\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1487 - acc: 0.9523 - val_loss: 0.8675 - val_acc: 0.8246\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 939us/sample - loss: 0.8876 - acc: 0.7697\n",
      "Loss: 0.8876171379569658 Accuracy: 0.7696781\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3772 - acc: 0.2232\n",
      "Epoch 00001: val_loss improved from inf to 1.65152, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/001-1.6515.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 2.3772 - acc: 0.2232 - val_loss: 1.6515 - val_acc: 0.4582\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5650 - acc: 0.4908\n",
      "Epoch 00002: val_loss improved from 1.65152 to 1.35204, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/002-1.3520.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5652 - acc: 0.4907 - val_loss: 1.3520 - val_acc: 0.5702\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3886 - acc: 0.5496\n",
      "Epoch 00003: val_loss improved from 1.35204 to 1.28828, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/003-1.2883.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3886 - acc: 0.5496 - val_loss: 1.2883 - val_acc: 0.5993\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2642 - acc: 0.5936\n",
      "Epoch 00004: val_loss improved from 1.28828 to 1.12717, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/004-1.1272.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2642 - acc: 0.5936 - val_loss: 1.1272 - val_acc: 0.6483\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1449 - acc: 0.6393\n",
      "Epoch 00005: val_loss improved from 1.12717 to 1.05421, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/005-1.0542.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1448 - acc: 0.6393 - val_loss: 1.0542 - val_acc: 0.6669\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0306 - acc: 0.6799\n",
      "Epoch 00006: val_loss improved from 1.05421 to 1.01254, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/006-1.0125.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0306 - acc: 0.6799 - val_loss: 1.0125 - val_acc: 0.7142\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9394 - acc: 0.7126\n",
      "Epoch 00007: val_loss improved from 1.01254 to 0.91058, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/007-0.9106.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9396 - acc: 0.7125 - val_loss: 0.9106 - val_acc: 0.7140\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8625 - acc: 0.7346\n",
      "Epoch 00008: val_loss improved from 0.91058 to 0.80543, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/008-0.8054.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8626 - acc: 0.7346 - val_loss: 0.8054 - val_acc: 0.7654\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8000 - acc: 0.7525\n",
      "Epoch 00009: val_loss did not improve from 0.80543\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7999 - acc: 0.7525 - val_loss: 0.8126 - val_acc: 0.7512\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.7733\n",
      "Epoch 00010: val_loss improved from 0.80543 to 0.71683, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/010-0.7168.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7473 - acc: 0.7733 - val_loss: 0.7168 - val_acc: 0.7890\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7064 - acc: 0.7867\n",
      "Epoch 00011: val_loss improved from 0.71683 to 0.70495, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/011-0.7049.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7067 - acc: 0.7867 - val_loss: 0.7049 - val_acc: 0.7964\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.7997\n",
      "Epoch 00012: val_loss did not improve from 0.70495\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6670 - acc: 0.7996 - val_loss: 0.7095 - val_acc: 0.8015\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.8053\n",
      "Epoch 00013: val_loss improved from 0.70495 to 0.65770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/013-0.6577.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6390 - acc: 0.8053 - val_loss: 0.6577 - val_acc: 0.8139\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.8165\n",
      "Epoch 00014: val_loss improved from 0.65770 to 0.64458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/014-0.6446.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6041 - acc: 0.8165 - val_loss: 0.6446 - val_acc: 0.8167\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.8279\n",
      "Epoch 00015: val_loss did not improve from 0.64458\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5710 - acc: 0.8279 - val_loss: 0.6477 - val_acc: 0.8104\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8335\n",
      "Epoch 00016: val_loss improved from 0.64458 to 0.62443, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/016-0.6244.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5456 - acc: 0.8336 - val_loss: 0.6244 - val_acc: 0.8223\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.8428\n",
      "Epoch 00017: val_loss improved from 0.62443 to 0.58792, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/017-0.5879.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5241 - acc: 0.8427 - val_loss: 0.5879 - val_acc: 0.8379\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.8475\n",
      "Epoch 00018: val_loss did not improve from 0.58792\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5034 - acc: 0.8475 - val_loss: 0.6068 - val_acc: 0.8341\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8536\n",
      "Epoch 00019: val_loss did not improve from 0.58792\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4797 - acc: 0.8536 - val_loss: 0.6052 - val_acc: 0.8281\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.8581\n",
      "Epoch 00020: val_loss did not improve from 0.58792\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4653 - acc: 0.8581 - val_loss: 0.6195 - val_acc: 0.8192\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8617\n",
      "Epoch 00021: val_loss improved from 0.58792 to 0.58754, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/021-0.5875.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4481 - acc: 0.8618 - val_loss: 0.5875 - val_acc: 0.8346\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8682\n",
      "Epoch 00022: val_loss did not improve from 0.58754\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4264 - acc: 0.8682 - val_loss: 0.6000 - val_acc: 0.8297\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8720\n",
      "Epoch 00023: val_loss improved from 0.58754 to 0.54274, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/023-0.5427.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4158 - acc: 0.8720 - val_loss: 0.5427 - val_acc: 0.8505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8789\n",
      "Epoch 00024: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3912 - acc: 0.8789 - val_loss: 0.6195 - val_acc: 0.8262\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8805\n",
      "Epoch 00025: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3820 - acc: 0.8805 - val_loss: 0.6001 - val_acc: 0.8379\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8848\n",
      "Epoch 00026: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3700 - acc: 0.8848 - val_loss: 0.5498 - val_acc: 0.8502\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8875\n",
      "Epoch 00027: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3610 - acc: 0.8875 - val_loss: 0.5729 - val_acc: 0.8467\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8939\n",
      "Epoch 00028: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3409 - acc: 0.8939 - val_loss: 0.5875 - val_acc: 0.8498\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8952\n",
      "Epoch 00029: val_loss did not improve from 0.54274\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3343 - acc: 0.8952 - val_loss: 0.5542 - val_acc: 0.8528\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8985\n",
      "Epoch 00030: val_loss improved from 0.54274 to 0.53407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/030-0.5341.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3257 - acc: 0.8985 - val_loss: 0.5341 - val_acc: 0.8553\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9013\n",
      "Epoch 00031: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3179 - acc: 0.9013 - val_loss: 0.5630 - val_acc: 0.8553\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9016\n",
      "Epoch 00032: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3097 - acc: 0.9016 - val_loss: 0.5671 - val_acc: 0.8516\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9055\n",
      "Epoch 00033: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2968 - acc: 0.9054 - val_loss: 0.5640 - val_acc: 0.8500\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9046\n",
      "Epoch 00034: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2984 - acc: 0.9047 - val_loss: 0.5564 - val_acc: 0.8595\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9104\n",
      "Epoch 00035: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2857 - acc: 0.9104 - val_loss: 0.5395 - val_acc: 0.8607\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9110\n",
      "Epoch 00036: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2734 - acc: 0.9110 - val_loss: 0.5404 - val_acc: 0.8579\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9143\n",
      "Epoch 00037: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2634 - acc: 0.9143 - val_loss: 0.5487 - val_acc: 0.8586\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9157\n",
      "Epoch 00038: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2618 - acc: 0.9157 - val_loss: 0.5597 - val_acc: 0.8579\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9175\n",
      "Epoch 00039: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2560 - acc: 0.9175 - val_loss: 0.5541 - val_acc: 0.8637\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9208\n",
      "Epoch 00040: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2472 - acc: 0.9208 - val_loss: 0.5430 - val_acc: 0.8598\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9193\n",
      "Epoch 00041: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2442 - acc: 0.9193 - val_loss: 0.5644 - val_acc: 0.8537\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9229\n",
      "Epoch 00042: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2425 - acc: 0.9229 - val_loss: 0.5902 - val_acc: 0.8488\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9256\n",
      "Epoch 00043: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2309 - acc: 0.9256 - val_loss: 0.5530 - val_acc: 0.8668\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9236\n",
      "Epoch 00044: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2369 - acc: 0.9236 - val_loss: 0.5415 - val_acc: 0.8628\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9241\n",
      "Epoch 00045: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2261 - acc: 0.9241 - val_loss: 0.5617 - val_acc: 0.8654\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9332\n",
      "Epoch 00046: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2098 - acc: 0.9332 - val_loss: 0.5571 - val_acc: 0.8682\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9286\n",
      "Epoch 00047: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2139 - acc: 0.9286 - val_loss: 0.5644 - val_acc: 0.8672\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9316\n",
      "Epoch 00048: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2118 - acc: 0.9316 - val_loss: 0.5577 - val_acc: 0.8668\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9343\n",
      "Epoch 00049: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2080 - acc: 0.9343 - val_loss: 0.5522 - val_acc: 0.8696\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9364\n",
      "Epoch 00050: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1987 - acc: 0.9364 - val_loss: 0.5480 - val_acc: 0.8649\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9355\n",
      "Epoch 00051: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1975 - acc: 0.9356 - val_loss: 0.5620 - val_acc: 0.8642\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9367\n",
      "Epoch 00052: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1957 - acc: 0.9367 - val_loss: 0.5393 - val_acc: 0.8742\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9353\n",
      "Epoch 00053: val_loss did not improve from 0.53407\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1952 - acc: 0.9353 - val_loss: 0.5628 - val_acc: 0.8658\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9390\n",
      "Epoch 00054: val_loss improved from 0.53407 to 0.53248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/054-0.5325.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1892 - acc: 0.9390 - val_loss: 0.5325 - val_acc: 0.8684\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9400\n",
      "Epoch 00055: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1866 - acc: 0.9400 - val_loss: 0.5706 - val_acc: 0.8630\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9417\n",
      "Epoch 00056: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1802 - acc: 0.9417 - val_loss: 0.5721 - val_acc: 0.8635\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9416\n",
      "Epoch 00057: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1827 - acc: 0.9416 - val_loss: 0.5530 - val_acc: 0.8705\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9400\n",
      "Epoch 00058: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1811 - acc: 0.9400 - val_loss: 0.5405 - val_acc: 0.8707\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9429\n",
      "Epoch 00059: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1779 - acc: 0.9429 - val_loss: 0.5765 - val_acc: 0.8714\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9444\n",
      "Epoch 00060: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1739 - acc: 0.9444 - val_loss: 0.5981 - val_acc: 0.8581\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9443\n",
      "Epoch 00061: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1688 - acc: 0.9443 - val_loss: 0.5547 - val_acc: 0.8735\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9428\n",
      "Epoch 00062: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1745 - acc: 0.9428 - val_loss: 0.5676 - val_acc: 0.8665\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9456\n",
      "Epoch 00063: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1679 - acc: 0.9456 - val_loss: 0.5434 - val_acc: 0.8775\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9460\n",
      "Epoch 00064: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1630 - acc: 0.9460 - val_loss: 0.5674 - val_acc: 0.8740\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9462\n",
      "Epoch 00065: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1643 - acc: 0.9462 - val_loss: 0.5633 - val_acc: 0.8758\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9515\n",
      "Epoch 00066: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1535 - acc: 0.9515 - val_loss: 0.6018 - val_acc: 0.8712\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9490\n",
      "Epoch 00067: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1606 - acc: 0.9490 - val_loss: 0.5598 - val_acc: 0.8733\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9493\n",
      "Epoch 00068: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1551 - acc: 0.9493 - val_loss: 0.5850 - val_acc: 0.8751\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9508\n",
      "Epoch 00069: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1521 - acc: 0.9508 - val_loss: 0.5647 - val_acc: 0.8775\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9508\n",
      "Epoch 00070: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1527 - acc: 0.9508 - val_loss: 0.5810 - val_acc: 0.8770\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9510\n",
      "Epoch 00071: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1488 - acc: 0.9510 - val_loss: 0.5988 - val_acc: 0.8737\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9508\n",
      "Epoch 00072: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1520 - acc: 0.9508 - val_loss: 0.5470 - val_acc: 0.8810\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9528\n",
      "Epoch 00073: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1468 - acc: 0.9528 - val_loss: 0.5576 - val_acc: 0.8817\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9535\n",
      "Epoch 00074: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1450 - acc: 0.9535 - val_loss: 0.5462 - val_acc: 0.8751\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9524\n",
      "Epoch 00075: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1462 - acc: 0.9524 - val_loss: 0.5420 - val_acc: 0.8786\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9530\n",
      "Epoch 00076: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1428 - acc: 0.9530 - val_loss: 0.5791 - val_acc: 0.8758\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9536\n",
      "Epoch 00077: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1419 - acc: 0.9536 - val_loss: 0.5562 - val_acc: 0.8779\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9538\n",
      "Epoch 00078: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1408 - acc: 0.9538 - val_loss: 0.5414 - val_acc: 0.8842\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9560\n",
      "Epoch 00079: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1387 - acc: 0.9560 - val_loss: 0.6026 - val_acc: 0.8668\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9551\n",
      "Epoch 00080: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1391 - acc: 0.9551 - val_loss: 0.5464 - val_acc: 0.8803\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9555\n",
      "Epoch 00081: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1380 - acc: 0.9555 - val_loss: 0.5483 - val_acc: 0.8826\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9584\n",
      "Epoch 00082: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1311 - acc: 0.9583 - val_loss: 0.5540 - val_acc: 0.8798\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9558\n",
      "Epoch 00083: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1357 - acc: 0.9558 - val_loss: 0.5566 - val_acc: 0.8803\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9577\n",
      "Epoch 00084: val_loss did not improve from 0.53248\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1309 - acc: 0.9578 - val_loss: 0.5796 - val_acc: 0.8810\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9595\n",
      "Epoch 00085: val_loss improved from 0.53248 to 0.52952, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/085-0.5295.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1248 - acc: 0.9595 - val_loss: 0.5295 - val_acc: 0.8840\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9584\n",
      "Epoch 00086: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1318 - acc: 0.9584 - val_loss: 0.5485 - val_acc: 0.8803\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9585\n",
      "Epoch 00087: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1296 - acc: 0.9585 - val_loss: 0.5760 - val_acc: 0.8814\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9610\n",
      "Epoch 00088: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1241 - acc: 0.9610 - val_loss: 0.5439 - val_acc: 0.8784\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9602\n",
      "Epoch 00089: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1221 - acc: 0.9602 - val_loss: 0.5330 - val_acc: 0.8856\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9594\n",
      "Epoch 00090: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1239 - acc: 0.9594 - val_loss: 0.5685 - val_acc: 0.8831\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9606\n",
      "Epoch 00091: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1232 - acc: 0.9606 - val_loss: 0.5555 - val_acc: 0.8838\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9611\n",
      "Epoch 00092: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1186 - acc: 0.9611 - val_loss: 0.5554 - val_acc: 0.8796\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9634\n",
      "Epoch 00093: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1172 - acc: 0.9634 - val_loss: 0.5851 - val_acc: 0.8796\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9598\n",
      "Epoch 00094: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1241 - acc: 0.9598 - val_loss: 0.5439 - val_acc: 0.8775\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9618\n",
      "Epoch 00095: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1166 - acc: 0.9618 - val_loss: 0.5836 - val_acc: 0.8840\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9636\n",
      "Epoch 00096: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1147 - acc: 0.9636 - val_loss: 0.5896 - val_acc: 0.8810\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9619\n",
      "Epoch 00097: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1195 - acc: 0.9619 - val_loss: 0.5758 - val_acc: 0.8856\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9627\n",
      "Epoch 00098: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1166 - acc: 0.9627 - val_loss: 0.5613 - val_acc: 0.8849\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9609\n",
      "Epoch 00099: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1188 - acc: 0.9609 - val_loss: 0.5480 - val_acc: 0.8915\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9629\n",
      "Epoch 00100: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1144 - acc: 0.9629 - val_loss: 0.5458 - val_acc: 0.8863\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9649\n",
      "Epoch 00101: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1114 - acc: 0.9650 - val_loss: 0.5802 - val_acc: 0.8873\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9637\n",
      "Epoch 00102: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1138 - acc: 0.9636 - val_loss: 0.5691 - val_acc: 0.8880\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9618\n",
      "Epoch 00103: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1184 - acc: 0.9618 - val_loss: 0.5551 - val_acc: 0.8908\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9633\n",
      "Epoch 00104: val_loss did not improve from 0.52952\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1165 - acc: 0.9633 - val_loss: 0.5409 - val_acc: 0.8868\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9646\n",
      "Epoch 00105: val_loss improved from 0.52952 to 0.52356, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/105-0.5236.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1125 - acc: 0.9646 - val_loss: 0.5236 - val_acc: 0.8863\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9654\n",
      "Epoch 00106: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1077 - acc: 0.9653 - val_loss: 0.5568 - val_acc: 0.8842\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9642\n",
      "Epoch 00107: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1148 - acc: 0.9642 - val_loss: 0.5526 - val_acc: 0.8856\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9641\n",
      "Epoch 00108: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1119 - acc: 0.9641 - val_loss: 0.5482 - val_acc: 0.8912\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9668\n",
      "Epoch 00109: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1045 - acc: 0.9668 - val_loss: 0.5570 - val_acc: 0.8910\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9676\n",
      "Epoch 00110: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1033 - acc: 0.9676 - val_loss: 0.5850 - val_acc: 0.8919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9649\n",
      "Epoch 00111: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1087 - acc: 0.9650 - val_loss: 0.5404 - val_acc: 0.8921\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9663\n",
      "Epoch 00112: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1059 - acc: 0.9663 - val_loss: 0.5617 - val_acc: 0.8891\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9677\n",
      "Epoch 00113: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1030 - acc: 0.9677 - val_loss: 0.5476 - val_acc: 0.8868\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9672\n",
      "Epoch 00114: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1025 - acc: 0.9672 - val_loss: 0.5774 - val_acc: 0.8910\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9651\n",
      "Epoch 00115: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1112 - acc: 0.9651 - val_loss: 0.5739 - val_acc: 0.8870\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9684\n",
      "Epoch 00116: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1012 - acc: 0.9684 - val_loss: 0.5629 - val_acc: 0.8901\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9682\n",
      "Epoch 00117: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0983 - acc: 0.9682 - val_loss: 0.5683 - val_acc: 0.8887\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9695\n",
      "Epoch 00118: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0989 - acc: 0.9695 - val_loss: 0.5548 - val_acc: 0.8845\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9693\n",
      "Epoch 00119: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1009 - acc: 0.9693 - val_loss: 0.5722 - val_acc: 0.8859\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9694\n",
      "Epoch 00120: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0954 - acc: 0.9694 - val_loss: 0.5496 - val_acc: 0.8880\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9673\n",
      "Epoch 00121: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1026 - acc: 0.9673 - val_loss: 0.5473 - val_acc: 0.8973\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9687\n",
      "Epoch 00122: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0985 - acc: 0.9687 - val_loss: 0.5462 - val_acc: 0.8942\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9683\n",
      "Epoch 00123: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1007 - acc: 0.9683 - val_loss: 0.5592 - val_acc: 0.8931\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9699\n",
      "Epoch 00124: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0957 - acc: 0.9699 - val_loss: 0.5430 - val_acc: 0.8935\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9699\n",
      "Epoch 00125: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0951 - acc: 0.9699 - val_loss: 0.5678 - val_acc: 0.8933\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9697\n",
      "Epoch 00126: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0957 - acc: 0.9697 - val_loss: 0.5359 - val_acc: 0.8947\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9712\n",
      "Epoch 00127: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0942 - acc: 0.9712 - val_loss: 0.5351 - val_acc: 0.8952\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9704\n",
      "Epoch 00128: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0921 - acc: 0.9704 - val_loss: 0.5644 - val_acc: 0.8945\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9723\n",
      "Epoch 00129: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0890 - acc: 0.9723 - val_loss: 0.5548 - val_acc: 0.8924\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9697\n",
      "Epoch 00130: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0936 - acc: 0.9697 - val_loss: 0.5594 - val_acc: 0.8949\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9717\n",
      "Epoch 00131: val_loss did not improve from 0.52356\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0933 - acc: 0.9717 - val_loss: 0.5630 - val_acc: 0.8952\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9706\n",
      "Epoch 00132: val_loss improved from 0.52356 to 0.52088, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/132-0.5209.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0907 - acc: 0.9706 - val_loss: 0.5209 - val_acc: 0.8940\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9724\n",
      "Epoch 00133: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0885 - acc: 0.9724 - val_loss: 0.5478 - val_acc: 0.8905\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9711\n",
      "Epoch 00134: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0938 - acc: 0.9710 - val_loss: 0.6205 - val_acc: 0.8789\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9695\n",
      "Epoch 00135: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0972 - acc: 0.9695 - val_loss: 0.5280 - val_acc: 0.8931\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9724\n",
      "Epoch 00136: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0880 - acc: 0.9724 - val_loss: 0.5399 - val_acc: 0.8905\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9716\n",
      "Epoch 00137: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0904 - acc: 0.9716 - val_loss: 0.5372 - val_acc: 0.8935\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9719\n",
      "Epoch 00138: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0892 - acc: 0.9719 - val_loss: 0.5363 - val_acc: 0.8901\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9736\n",
      "Epoch 00139: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0871 - acc: 0.9736 - val_loss: 0.5684 - val_acc: 0.8905\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9735\n",
      "Epoch 00140: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0866 - acc: 0.9735 - val_loss: 0.5761 - val_acc: 0.8868\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9726\n",
      "Epoch 00141: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0880 - acc: 0.9726 - val_loss: 0.5520 - val_acc: 0.8894\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9716\n",
      "Epoch 00142: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0887 - acc: 0.9716 - val_loss: 0.5614 - val_acc: 0.8959\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9731\n",
      "Epoch 00143: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0857 - acc: 0.9731 - val_loss: 0.5912 - val_acc: 0.8931\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9729\n",
      "Epoch 00144: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0850 - acc: 0.9729 - val_loss: 0.5709 - val_acc: 0.8926\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9735\n",
      "Epoch 00145: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0884 - acc: 0.9735 - val_loss: 0.5421 - val_acc: 0.8898\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9735\n",
      "Epoch 00146: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0834 - acc: 0.9735 - val_loss: 0.5496 - val_acc: 0.8963\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9733\n",
      "Epoch 00147: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0835 - acc: 0.9733 - val_loss: 0.5867 - val_acc: 0.8947\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9731\n",
      "Epoch 00148: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0886 - acc: 0.9731 - val_loss: 0.5679 - val_acc: 0.8912\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9736\n",
      "Epoch 00149: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0846 - acc: 0.9736 - val_loss: 0.5413 - val_acc: 0.8926\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9746\n",
      "Epoch 00150: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0811 - acc: 0.9747 - val_loss: 0.5754 - val_acc: 0.8973\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9745\n",
      "Epoch 00151: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0807 - acc: 0.9745 - val_loss: 0.5340 - val_acc: 0.8954\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9734\n",
      "Epoch 00152: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0828 - acc: 0.9734 - val_loss: 0.5675 - val_acc: 0.8968\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9752\n",
      "Epoch 00153: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0808 - acc: 0.9752 - val_loss: 0.5312 - val_acc: 0.8949\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9746\n",
      "Epoch 00154: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0852 - acc: 0.9746 - val_loss: 0.5262 - val_acc: 0.8956\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9762\n",
      "Epoch 00155: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0774 - acc: 0.9762 - val_loss: 0.5836 - val_acc: 0.8968\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9758\n",
      "Epoch 00156: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0777 - acc: 0.9758 - val_loss: 0.5650 - val_acc: 0.8928\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9757\n",
      "Epoch 00157: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0796 - acc: 0.9757 - val_loss: 0.5440 - val_acc: 0.8966\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9738\n",
      "Epoch 00158: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0831 - acc: 0.9738 - val_loss: 0.5245 - val_acc: 0.8949\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9756\n",
      "Epoch 00159: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0797 - acc: 0.9756 - val_loss: 0.5683 - val_acc: 0.8921\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9760\n",
      "Epoch 00160: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0772 - acc: 0.9760 - val_loss: 0.5693 - val_acc: 0.8884\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9756\n",
      "Epoch 00161: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0804 - acc: 0.9756 - val_loss: 0.5219 - val_acc: 0.9012\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9759\n",
      "Epoch 00162: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0802 - acc: 0.9759 - val_loss: 0.5483 - val_acc: 0.8977\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9772\n",
      "Epoch 00163: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0735 - acc: 0.9772 - val_loss: 0.5756 - val_acc: 0.8961\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9756\n",
      "Epoch 00164: val_loss did not improve from 0.52088\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0788 - acc: 0.9756 - val_loss: 0.5610 - val_acc: 0.8915\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9732\n",
      "Epoch 00165: val_loss improved from 0.52088 to 0.51156, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv_checkpoint/165-0.5116.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0894 - acc: 0.9732 - val_loss: 0.5116 - val_acc: 0.9022\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9761\n",
      "Epoch 00166: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0782 - acc: 0.9761 - val_loss: 0.5558 - val_acc: 0.8942\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9758\n",
      "Epoch 00167: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0784 - acc: 0.9758 - val_loss: 0.5315 - val_acc: 0.8924\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9765\n",
      "Epoch 00168: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0771 - acc: 0.9765 - val_loss: 0.5425 - val_acc: 0.8931\n",
      "Epoch 169/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9763\n",
      "Epoch 00169: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0759 - acc: 0.9763 - val_loss: 0.5469 - val_acc: 0.8980\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9766\n",
      "Epoch 00170: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0763 - acc: 0.9766 - val_loss: 0.5745 - val_acc: 0.8945\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9751\n",
      "Epoch 00171: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0807 - acc: 0.9751 - val_loss: 0.5252 - val_acc: 0.9010\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9776\n",
      "Epoch 00172: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0717 - acc: 0.9776 - val_loss: 0.5317 - val_acc: 0.9029\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9776\n",
      "Epoch 00173: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0731 - acc: 0.9776 - val_loss: 0.5418 - val_acc: 0.9012\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9774\n",
      "Epoch 00174: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0748 - acc: 0.9774 - val_loss: 0.5652 - val_acc: 0.9005\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9771\n",
      "Epoch 00175: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0751 - acc: 0.9771 - val_loss: 0.5409 - val_acc: 0.8994\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9769\n",
      "Epoch 00176: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0763 - acc: 0.9769 - val_loss: 0.5811 - val_acc: 0.8987\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9771\n",
      "Epoch 00177: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0757 - acc: 0.9771 - val_loss: 0.5729 - val_acc: 0.8961\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9795\n",
      "Epoch 00178: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0689 - acc: 0.9795 - val_loss: 0.5619 - val_acc: 0.9017\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9777\n",
      "Epoch 00179: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0723 - acc: 0.9777 - val_loss: 0.5754 - val_acc: 0.8859\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9780\n",
      "Epoch 00180: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0749 - acc: 0.9780 - val_loss: 0.5750 - val_acc: 0.9012\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9780\n",
      "Epoch 00181: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0722 - acc: 0.9780 - val_loss: 0.5835 - val_acc: 0.8973\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9780\n",
      "Epoch 00182: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0746 - acc: 0.9780 - val_loss: 0.5490 - val_acc: 0.8968\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9791\n",
      "Epoch 00183: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0706 - acc: 0.9791 - val_loss: 0.5675 - val_acc: 0.8975\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9765\n",
      "Epoch 00184: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0741 - acc: 0.9765 - val_loss: 0.5742 - val_acc: 0.8917\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9786\n",
      "Epoch 00185: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0717 - acc: 0.9786 - val_loss: 0.5371 - val_acc: 0.8954\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9781\n",
      "Epoch 00186: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0719 - acc: 0.9780 - val_loss: 0.5689 - val_acc: 0.8947\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9783\n",
      "Epoch 00187: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0725 - acc: 0.9783 - val_loss: 0.5323 - val_acc: 0.8980\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9801\n",
      "Epoch 00188: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0690 - acc: 0.9801 - val_loss: 0.5426 - val_acc: 0.9010\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9788\n",
      "Epoch 00189: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0707 - acc: 0.9788 - val_loss: 0.5517 - val_acc: 0.8959\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9808\n",
      "Epoch 00190: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0648 - acc: 0.9808 - val_loss: 0.5798 - val_acc: 0.8954\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9776\n",
      "Epoch 00191: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0747 - acc: 0.9776 - val_loss: 0.5336 - val_acc: 0.8966\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9789\n",
      "Epoch 00192: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0693 - acc: 0.9789 - val_loss: 0.5641 - val_acc: 0.8949\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9786\n",
      "Epoch 00193: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0703 - acc: 0.9786 - val_loss: 0.5516 - val_acc: 0.8966\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9789\n",
      "Epoch 00194: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0711 - acc: 0.9789 - val_loss: 0.5329 - val_acc: 0.9038\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9789\n",
      "Epoch 00195: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0708 - acc: 0.9789 - val_loss: 0.5774 - val_acc: 0.8984\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9788\n",
      "Epoch 00196: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0685 - acc: 0.9788 - val_loss: 0.5203 - val_acc: 0.9001\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9787\n",
      "Epoch 00197: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0693 - acc: 0.9787 - val_loss: 0.5652 - val_acc: 0.8966\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9792\n",
      "Epoch 00198: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0691 - acc: 0.9792 - val_loss: 0.5445 - val_acc: 0.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9800\n",
      "Epoch 00199: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0673 - acc: 0.9800 - val_loss: 0.5693 - val_acc: 0.9019\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9803\n",
      "Epoch 00200: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0655 - acc: 0.9803 - val_loss: 0.5479 - val_acc: 0.8959\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9812\n",
      "Epoch 00201: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0637 - acc: 0.9812 - val_loss: 0.5669 - val_acc: 0.8991\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9809\n",
      "Epoch 00202: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0649 - acc: 0.9809 - val_loss: 0.5520 - val_acc: 0.9033\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9801\n",
      "Epoch 00203: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0686 - acc: 0.9801 - val_loss: 0.5738 - val_acc: 0.8987\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9787\n",
      "Epoch 00204: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0691 - acc: 0.9787 - val_loss: 0.5526 - val_acc: 0.9019\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9806\n",
      "Epoch 00205: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0655 - acc: 0.9806 - val_loss: 0.5345 - val_acc: 0.9017\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9797\n",
      "Epoch 00206: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0676 - acc: 0.9797 - val_loss: 0.5294 - val_acc: 0.9010\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9808\n",
      "Epoch 00207: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0654 - acc: 0.9808 - val_loss: 0.5694 - val_acc: 0.8977\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9786\n",
      "Epoch 00208: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0705 - acc: 0.9786 - val_loss: 0.5516 - val_acc: 0.9036\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9812\n",
      "Epoch 00209: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0642 - acc: 0.9812 - val_loss: 0.5539 - val_acc: 0.8970\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9801\n",
      "Epoch 00210: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0660 - acc: 0.9801 - val_loss: 0.5477 - val_acc: 0.9017\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9807\n",
      "Epoch 00211: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0637 - acc: 0.9807 - val_loss: 0.5427 - val_acc: 0.9045\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9820\n",
      "Epoch 00212: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0635 - acc: 0.9820 - val_loss: 0.5453 - val_acc: 0.8987\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9789\n",
      "Epoch 00213: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0668 - acc: 0.9789 - val_loss: 0.5540 - val_acc: 0.9038\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9804\n",
      "Epoch 00214: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0656 - acc: 0.9803 - val_loss: 0.5177 - val_acc: 0.9026\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9806\n",
      "Epoch 00215: val_loss did not improve from 0.51156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0633 - acc: 0.9806 - val_loss: 0.5610 - val_acc: 0.8938\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 976us/sample - loss: 0.6308 - acc: 0.8706\n",
      "Loss: 0.6307582460088522 Accuracy: 0.8706127\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4687 - acc: 0.1772\n",
      "Epoch 00001: val_loss improved from inf to 1.81634, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/001-1.8163.hdf5\n",
      "36805/36805 [==============================] - 96s 3ms/sample - loss: 2.4685 - acc: 0.1773 - val_loss: 1.8163 - val_acc: 0.4253\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6874 - acc: 0.4425\n",
      "Epoch 00002: val_loss improved from 1.81634 to 1.36578, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/002-1.3658.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.6874 - acc: 0.4425 - val_loss: 1.3658 - val_acc: 0.5700\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4490 - acc: 0.5241\n",
      "Epoch 00003: val_loss improved from 1.36578 to 1.22121, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/003-1.2212.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4489 - acc: 0.5241 - val_loss: 1.2212 - val_acc: 0.6299\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3215 - acc: 0.5655\n",
      "Epoch 00004: val_loss improved from 1.22121 to 1.15121, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/004-1.1512.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3215 - acc: 0.5655 - val_loss: 1.1512 - val_acc: 0.6594\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2334 - acc: 0.6012\n",
      "Epoch 00005: val_loss improved from 1.15121 to 1.04239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/005-1.0424.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2333 - acc: 0.6012 - val_loss: 1.0424 - val_acc: 0.6769\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1333 - acc: 0.6393\n",
      "Epoch 00006: val_loss improved from 1.04239 to 0.96345, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/006-0.9635.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1334 - acc: 0.6394 - val_loss: 0.9635 - val_acc: 0.7028\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0445 - acc: 0.6726\n",
      "Epoch 00007: val_loss improved from 0.96345 to 0.87675, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/007-0.8768.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0446 - acc: 0.6726 - val_loss: 0.8768 - val_acc: 0.7379\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9572 - acc: 0.7041\n",
      "Epoch 00008: val_loss improved from 0.87675 to 0.83012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/008-0.8301.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9572 - acc: 0.7041 - val_loss: 0.8301 - val_acc: 0.7603\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8723 - acc: 0.7323\n",
      "Epoch 00009: val_loss improved from 0.83012 to 0.71800, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/009-0.7180.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8723 - acc: 0.7323 - val_loss: 0.7180 - val_acc: 0.7892\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7593\n",
      "Epoch 00010: val_loss improved from 0.71800 to 0.65280, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/010-0.6528.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7929 - acc: 0.7593 - val_loss: 0.6528 - val_acc: 0.8088\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7785\n",
      "Epoch 00011: val_loss improved from 0.65280 to 0.58918, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/011-0.5892.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7319 - acc: 0.7785 - val_loss: 0.5892 - val_acc: 0.8290\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.7942\n",
      "Epoch 00012: val_loss improved from 0.58918 to 0.56738, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/012-0.5674.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6821 - acc: 0.7942 - val_loss: 0.5674 - val_acc: 0.8414\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8112\n",
      "Epoch 00013: val_loss improved from 0.56738 to 0.51532, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/013-0.5153.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6378 - acc: 0.8112 - val_loss: 0.5153 - val_acc: 0.8553\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8224\n",
      "Epoch 00014: val_loss did not improve from 0.51532\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6010 - acc: 0.8224 - val_loss: 0.6591 - val_acc: 0.8125\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8292\n",
      "Epoch 00015: val_loss improved from 0.51532 to 0.45719, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/015-0.4572.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5748 - acc: 0.8292 - val_loss: 0.4572 - val_acc: 0.8744\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8430\n",
      "Epoch 00016: val_loss did not improve from 0.45719\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5320 - acc: 0.8430 - val_loss: 0.4620 - val_acc: 0.8693\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8491\n",
      "Epoch 00017: val_loss improved from 0.45719 to 0.43652, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/017-0.4365.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5089 - acc: 0.8491 - val_loss: 0.4365 - val_acc: 0.8840\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8574\n",
      "Epoch 00018: val_loss did not improve from 0.43652\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4816 - acc: 0.8575 - val_loss: 0.4372 - val_acc: 0.8805\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.8615\n",
      "Epoch 00019: val_loss improved from 0.43652 to 0.40528, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/019-0.4053.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4612 - acc: 0.8615 - val_loss: 0.4053 - val_acc: 0.8887\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.8669\n",
      "Epoch 00020: val_loss improved from 0.40528 to 0.39795, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/020-0.3979.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4449 - acc: 0.8669 - val_loss: 0.3979 - val_acc: 0.8961\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8719\n",
      "Epoch 00021: val_loss improved from 0.39795 to 0.38465, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/021-0.3847.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4258 - acc: 0.8719 - val_loss: 0.3847 - val_acc: 0.8933\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8785\n",
      "Epoch 00022: val_loss improved from 0.38465 to 0.34662, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/022-0.3466.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4006 - acc: 0.8785 - val_loss: 0.3466 - val_acc: 0.8998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8804\n",
      "Epoch 00023: val_loss improved from 0.34662 to 0.34633, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/023-0.3463.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3935 - acc: 0.8805 - val_loss: 0.3463 - val_acc: 0.9071\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8859\n",
      "Epoch 00024: val_loss did not improve from 0.34633\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3777 - acc: 0.8859 - val_loss: 0.3479 - val_acc: 0.9066\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8896\n",
      "Epoch 00025: val_loss improved from 0.34633 to 0.33334, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/025-0.3333.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3704 - acc: 0.8896 - val_loss: 0.3333 - val_acc: 0.9092\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8935\n",
      "Epoch 00026: val_loss improved from 0.33334 to 0.32449, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/026-0.3245.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3539 - acc: 0.8934 - val_loss: 0.3245 - val_acc: 0.9110\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8938\n",
      "Epoch 00027: val_loss improved from 0.32449 to 0.32013, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/027-0.3201.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3475 - acc: 0.8938 - val_loss: 0.3201 - val_acc: 0.9124\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8970\n",
      "Epoch 00028: val_loss improved from 0.32013 to 0.32008, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/028-0.3201.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3388 - acc: 0.8970 - val_loss: 0.3201 - val_acc: 0.9108\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9042\n",
      "Epoch 00029: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3218 - acc: 0.9043 - val_loss: 0.3222 - val_acc: 0.9101\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9028\n",
      "Epoch 00030: val_loss improved from 0.32008 to 0.30950, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/030-0.3095.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3172 - acc: 0.9028 - val_loss: 0.3095 - val_acc: 0.9182\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9064\n",
      "Epoch 00031: val_loss did not improve from 0.30950\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3026 - acc: 0.9064 - val_loss: 0.3186 - val_acc: 0.9113\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9074\n",
      "Epoch 00032: val_loss improved from 0.30950 to 0.29643, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/032-0.2964.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3014 - acc: 0.9074 - val_loss: 0.2964 - val_acc: 0.9182\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9102\n",
      "Epoch 00033: val_loss did not improve from 0.29643\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2885 - acc: 0.9102 - val_loss: 0.3044 - val_acc: 0.9171\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9085\n",
      "Epoch 00034: val_loss did not improve from 0.29643\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2899 - acc: 0.9085 - val_loss: 0.3082 - val_acc: 0.9196\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9150\n",
      "Epoch 00035: val_loss did not improve from 0.29643\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2766 - acc: 0.9150 - val_loss: 0.3017 - val_acc: 0.9199\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9150\n",
      "Epoch 00036: val_loss improved from 0.29643 to 0.29097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/036-0.2910.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2702 - acc: 0.9150 - val_loss: 0.2910 - val_acc: 0.9224\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9187\n",
      "Epoch 00037: val_loss improved from 0.29097 to 0.28463, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/037-0.2846.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2610 - acc: 0.9188 - val_loss: 0.2846 - val_acc: 0.9257\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9200\n",
      "Epoch 00038: val_loss did not improve from 0.28463\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2555 - acc: 0.9200 - val_loss: 0.2853 - val_acc: 0.9210\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9202\n",
      "Epoch 00039: val_loss improved from 0.28463 to 0.28054, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/039-0.2805.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2545 - acc: 0.9202 - val_loss: 0.2805 - val_acc: 0.9231\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9232\n",
      "Epoch 00040: val_loss did not improve from 0.28054\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2439 - acc: 0.9232 - val_loss: 0.2850 - val_acc: 0.9227\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9253\n",
      "Epoch 00041: val_loss did not improve from 0.28054\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2429 - acc: 0.9253 - val_loss: 0.2842 - val_acc: 0.9224\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9249\n",
      "Epoch 00042: val_loss improved from 0.28054 to 0.27895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/042-0.2789.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2396 - acc: 0.9250 - val_loss: 0.2789 - val_acc: 0.9276\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9263\n",
      "Epoch 00043: val_loss improved from 0.27895 to 0.27818, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/043-0.2782.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2326 - acc: 0.9263 - val_loss: 0.2782 - val_acc: 0.9255\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9281\n",
      "Epoch 00044: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2237 - acc: 0.9281 - val_loss: 0.2785 - val_acc: 0.9255\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9282\n",
      "Epoch 00045: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2258 - acc: 0.9282 - val_loss: 0.3015 - val_acc: 0.9276\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9301\n",
      "Epoch 00046: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2208 - acc: 0.9301 - val_loss: 0.2914 - val_acc: 0.9278\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9312\n",
      "Epoch 00047: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2174 - acc: 0.9313 - val_loss: 0.2810 - val_acc: 0.9238\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9327\n",
      "Epoch 00048: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2095 - acc: 0.9327 - val_loss: 0.2800 - val_acc: 0.9257\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9332\n",
      "Epoch 00049: val_loss did not improve from 0.27818\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2088 - acc: 0.9331 - val_loss: 0.2806 - val_acc: 0.9331\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9337\n",
      "Epoch 00050: val_loss improved from 0.27818 to 0.27041, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/050-0.2704.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2019 - acc: 0.9337 - val_loss: 0.2704 - val_acc: 0.9320\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9345\n",
      "Epoch 00051: val_loss did not improve from 0.27041\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2035 - acc: 0.9345 - val_loss: 0.2712 - val_acc: 0.9322\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9378\n",
      "Epoch 00052: val_loss improved from 0.27041 to 0.26925, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/052-0.2692.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1949 - acc: 0.9378 - val_loss: 0.2692 - val_acc: 0.9311\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9376\n",
      "Epoch 00053: val_loss did not improve from 0.26925\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1880 - acc: 0.9376 - val_loss: 0.3039 - val_acc: 0.9187\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9386\n",
      "Epoch 00054: val_loss did not improve from 0.26925\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1897 - acc: 0.9386 - val_loss: 0.3003 - val_acc: 0.9287\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9408\n",
      "Epoch 00055: val_loss improved from 0.26925 to 0.26021, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/055-0.2602.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1840 - acc: 0.9408 - val_loss: 0.2602 - val_acc: 0.9301\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9414\n",
      "Epoch 00056: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1819 - acc: 0.9414 - val_loss: 0.2798 - val_acc: 0.9331\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9426\n",
      "Epoch 00057: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1772 - acc: 0.9426 - val_loss: 0.2674 - val_acc: 0.9338\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9435\n",
      "Epoch 00058: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1760 - acc: 0.9435 - val_loss: 0.2625 - val_acc: 0.9320\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9435\n",
      "Epoch 00059: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1723 - acc: 0.9435 - val_loss: 0.3313 - val_acc: 0.9238\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9418\n",
      "Epoch 00060: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1744 - acc: 0.9418 - val_loss: 0.2842 - val_acc: 0.9315\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9474\n",
      "Epoch 00061: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1640 - acc: 0.9474 - val_loss: 0.2829 - val_acc: 0.9329\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9481\n",
      "Epoch 00062: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1593 - acc: 0.9481 - val_loss: 0.2834 - val_acc: 0.9299\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9475\n",
      "Epoch 00063: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1609 - acc: 0.9475 - val_loss: 0.2784 - val_acc: 0.9315\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9499\n",
      "Epoch 00064: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1538 - acc: 0.9499 - val_loss: 0.3044 - val_acc: 0.9264\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9468\n",
      "Epoch 00065: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1624 - acc: 0.9468 - val_loss: 0.3244 - val_acc: 0.9222\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9490\n",
      "Epoch 00066: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1558 - acc: 0.9490 - val_loss: 0.2896 - val_acc: 0.9313\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9494\n",
      "Epoch 00067: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1527 - acc: 0.9494 - val_loss: 0.2833 - val_acc: 0.9297\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9513\n",
      "Epoch 00068: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1483 - acc: 0.9513 - val_loss: 0.3046 - val_acc: 0.9262\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9516\n",
      "Epoch 00069: val_loss did not improve from 0.26021\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1472 - acc: 0.9516 - val_loss: 0.3109 - val_acc: 0.9327\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9514\n",
      "Epoch 00070: val_loss improved from 0.26021 to 0.25932, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/070-0.2593.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1482 - acc: 0.9514 - val_loss: 0.2593 - val_acc: 0.9364\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9539\n",
      "Epoch 00071: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1418 - acc: 0.9539 - val_loss: 0.2793 - val_acc: 0.9362\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9527\n",
      "Epoch 00072: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1417 - acc: 0.9526 - val_loss: 0.2778 - val_acc: 0.9343\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9534\n",
      "Epoch 00073: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1394 - acc: 0.9534 - val_loss: 0.2794 - val_acc: 0.9357\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9536\n",
      "Epoch 00074: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1374 - acc: 0.9536 - val_loss: 0.2858 - val_acc: 0.9315\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9557\n",
      "Epoch 00075: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1348 - acc: 0.9557 - val_loss: 0.2746 - val_acc: 0.9383\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9527\n",
      "Epoch 00076: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1406 - acc: 0.9527 - val_loss: 0.3108 - val_acc: 0.9336\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9568\n",
      "Epoch 00077: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1295 - acc: 0.9568 - val_loss: 0.2615 - val_acc: 0.9350\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9565\n",
      "Epoch 00078: val_loss did not improve from 0.25932\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1297 - acc: 0.9566 - val_loss: 0.2849 - val_acc: 0.9336\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9573\n",
      "Epoch 00079: val_loss improved from 0.25932 to 0.25898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv_checkpoint/079-0.2590.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1298 - acc: 0.9573 - val_loss: 0.2590 - val_acc: 0.9345\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9583\n",
      "Epoch 00080: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1252 - acc: 0.9582 - val_loss: 0.2807 - val_acc: 0.9308\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9588\n",
      "Epoch 00081: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1230 - acc: 0.9588 - val_loss: 0.2875 - val_acc: 0.9306\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9582\n",
      "Epoch 00082: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1240 - acc: 0.9582 - val_loss: 0.3118 - val_acc: 0.9343\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9599\n",
      "Epoch 00083: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1211 - acc: 0.9599 - val_loss: 0.2977 - val_acc: 0.9345\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9591\n",
      "Epoch 00084: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1246 - acc: 0.9591 - val_loss: 0.2823 - val_acc: 0.9334\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9603\n",
      "Epoch 00085: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1186 - acc: 0.9603 - val_loss: 0.2899 - val_acc: 0.9343\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9598\n",
      "Epoch 00086: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1213 - acc: 0.9598 - val_loss: 0.2962 - val_acc: 0.9331\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9620\n",
      "Epoch 00087: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1137 - acc: 0.9620 - val_loss: 0.2806 - val_acc: 0.9401\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9619\n",
      "Epoch 00088: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1131 - acc: 0.9619 - val_loss: 0.3148 - val_acc: 0.9364\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9584\n",
      "Epoch 00089: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1252 - acc: 0.9584 - val_loss: 0.2801 - val_acc: 0.9378\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9630\n",
      "Epoch 00090: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1133 - acc: 0.9630 - val_loss: 0.2805 - val_acc: 0.9394\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9609\n",
      "Epoch 00091: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1149 - acc: 0.9609 - val_loss: 0.2900 - val_acc: 0.9369\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9618\n",
      "Epoch 00092: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1119 - acc: 0.9618 - val_loss: 0.2773 - val_acc: 0.9385\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9648\n",
      "Epoch 00093: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1052 - acc: 0.9648 - val_loss: 0.2852 - val_acc: 0.9387\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9629\n",
      "Epoch 00094: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1157 - acc: 0.9628 - val_loss: 0.3070 - val_acc: 0.9306\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9633\n",
      "Epoch 00095: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1115 - acc: 0.9632 - val_loss: 0.3110 - val_acc: 0.9359\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9645\n",
      "Epoch 00096: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1033 - acc: 0.9645 - val_loss: 0.3041 - val_acc: 0.9404\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9642\n",
      "Epoch 00097: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1062 - acc: 0.9642 - val_loss: 0.3018 - val_acc: 0.9373\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9661\n",
      "Epoch 00098: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1027 - acc: 0.9660 - val_loss: 0.2838 - val_acc: 0.9345\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9658\n",
      "Epoch 00099: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1038 - acc: 0.9658 - val_loss: 0.3054 - val_acc: 0.9366\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9655\n",
      "Epoch 00100: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1046 - acc: 0.9655 - val_loss: 0.2945 - val_acc: 0.9385\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9666\n",
      "Epoch 00101: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0989 - acc: 0.9666 - val_loss: 0.2923 - val_acc: 0.9392\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9680\n",
      "Epoch 00102: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0992 - acc: 0.9680 - val_loss: 0.3436 - val_acc: 0.9355\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9667\n",
      "Epoch 00103: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1005 - acc: 0.9667 - val_loss: 0.3105 - val_acc: 0.9392\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9687\n",
      "Epoch 00104: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0970 - acc: 0.9687 - val_loss: 0.2913 - val_acc: 0.9390\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9677\n",
      "Epoch 00105: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0969 - acc: 0.9677 - val_loss: 0.3062 - val_acc: 0.9352\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9679\n",
      "Epoch 00106: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0955 - acc: 0.9679 - val_loss: 0.2965 - val_acc: 0.9399\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9677\n",
      "Epoch 00107: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0963 - acc: 0.9677 - val_loss: 0.3111 - val_acc: 0.9404\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9675\n",
      "Epoch 00108: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0962 - acc: 0.9675 - val_loss: 0.2856 - val_acc: 0.9345\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9698\n",
      "Epoch 00109: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0930 - acc: 0.9698 - val_loss: 0.3209 - val_acc: 0.9304\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9693\n",
      "Epoch 00110: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0922 - acc: 0.9693 - val_loss: 0.2831 - val_acc: 0.9385\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9710\n",
      "Epoch 00111: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0889 - acc: 0.9710 - val_loss: 0.3280 - val_acc: 0.9350\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9694\n",
      "Epoch 00112: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0921 - acc: 0.9694 - val_loss: 0.3022 - val_acc: 0.9392\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9689\n",
      "Epoch 00113: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0928 - acc: 0.9689 - val_loss: 0.3114 - val_acc: 0.9369\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9714\n",
      "Epoch 00114: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0859 - acc: 0.9714 - val_loss: 0.3058 - val_acc: 0.9357\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9716\n",
      "Epoch 00115: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0837 - acc: 0.9716 - val_loss: 0.3030 - val_acc: 0.9371\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9702\n",
      "Epoch 00116: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0891 - acc: 0.9702 - val_loss: 0.3237 - val_acc: 0.9369\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9715\n",
      "Epoch 00117: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0853 - acc: 0.9716 - val_loss: 0.2846 - val_acc: 0.9411\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9720\n",
      "Epoch 00118: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0826 - acc: 0.9720 - val_loss: 0.3002 - val_acc: 0.9397\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9727\n",
      "Epoch 00119: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0840 - acc: 0.9727 - val_loss: 0.3061 - val_acc: 0.9392\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9724\n",
      "Epoch 00120: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0819 - acc: 0.9724 - val_loss: 0.2915 - val_acc: 0.9371\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9736\n",
      "Epoch 00121: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0789 - acc: 0.9736 - val_loss: 0.3321 - val_acc: 0.9350\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9725\n",
      "Epoch 00122: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0823 - acc: 0.9725 - val_loss: 0.3120 - val_acc: 0.9406\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9721\n",
      "Epoch 00123: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0848 - acc: 0.9721 - val_loss: 0.3442 - val_acc: 0.9376\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9718\n",
      "Epoch 00124: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0844 - acc: 0.9718 - val_loss: 0.2915 - val_acc: 0.9422\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9734\n",
      "Epoch 00125: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0799 - acc: 0.9734 - val_loss: 0.3181 - val_acc: 0.9415\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9727\n",
      "Epoch 00126: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0817 - acc: 0.9727 - val_loss: 0.2910 - val_acc: 0.9364\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9721\n",
      "Epoch 00127: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0852 - acc: 0.9721 - val_loss: 0.2949 - val_acc: 0.9413\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9729\n",
      "Epoch 00128: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0793 - acc: 0.9729 - val_loss: 0.3214 - val_acc: 0.9394\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9745\n",
      "Epoch 00129: val_loss did not improve from 0.25898\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0750 - acc: 0.9745 - val_loss: 0.3214 - val_acc: 0.9362\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2933 - acc: 0.9205\n",
      "Loss: 0.2933371344567707 Accuracy: 0.9204569\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6475 - acc: 0.1171\n",
      "Epoch 00001: val_loss improved from inf to 2.11950, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/001-2.1195.hdf5\n",
      "36805/36805 [==============================] - 99s 3ms/sample - loss: 2.6475 - acc: 0.1171 - val_loss: 2.1195 - val_acc: 0.3021\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9131 - acc: 0.3582\n",
      "Epoch 00002: val_loss improved from 2.11950 to 1.43833, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/002-1.4383.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.9129 - acc: 0.3582 - val_loss: 1.4383 - val_acc: 0.5409\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5483 - acc: 0.4816\n",
      "Epoch 00003: val_loss improved from 1.43833 to 1.26551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/003-1.2655.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.5483 - acc: 0.4816 - val_loss: 1.2655 - val_acc: 0.6087\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3735 - acc: 0.5414\n",
      "Epoch 00004: val_loss improved from 1.26551 to 1.10771, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/004-1.1077.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.3735 - acc: 0.5414 - val_loss: 1.1077 - val_acc: 0.6720\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2440 - acc: 0.5919\n",
      "Epoch 00005: val_loss improved from 1.10771 to 0.95788, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/005-0.9579.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2439 - acc: 0.5919 - val_loss: 0.9579 - val_acc: 0.7058\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1204 - acc: 0.6369\n",
      "Epoch 00006: val_loss improved from 0.95788 to 0.90967, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/006-0.9097.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1206 - acc: 0.6369 - val_loss: 0.9097 - val_acc: 0.7291\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0134 - acc: 0.6764\n",
      "Epoch 00007: val_loss improved from 0.90967 to 0.75946, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/007-0.7595.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.0135 - acc: 0.6764 - val_loss: 0.7595 - val_acc: 0.7768\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9086 - acc: 0.7115\n",
      "Epoch 00008: val_loss improved from 0.75946 to 0.64449, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/008-0.6445.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9086 - acc: 0.7115 - val_loss: 0.6445 - val_acc: 0.8139\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8084 - acc: 0.7471\n",
      "Epoch 00009: val_loss improved from 0.64449 to 0.57666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/009-0.5767.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8084 - acc: 0.7471 - val_loss: 0.5767 - val_acc: 0.8358\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7367 - acc: 0.7704\n",
      "Epoch 00010: val_loss improved from 0.57666 to 0.53993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/010-0.5399.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7367 - acc: 0.7703 - val_loss: 0.5399 - val_acc: 0.8428\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6693 - acc: 0.7964\n",
      "Epoch 00011: val_loss improved from 0.53993 to 0.46269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/011-0.4627.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6693 - acc: 0.7964 - val_loss: 0.4627 - val_acc: 0.8637\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6161 - acc: 0.8125\n",
      "Epoch 00012: val_loss improved from 0.46269 to 0.40843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/012-0.4084.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6160 - acc: 0.8125 - val_loss: 0.4084 - val_acc: 0.8852\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.8217\n",
      "Epoch 00013: val_loss improved from 0.40843 to 0.40721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/013-0.4072.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5742 - acc: 0.8217 - val_loss: 0.4072 - val_acc: 0.8875\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8370\n",
      "Epoch 00014: val_loss improved from 0.40721 to 0.35037, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/014-0.3504.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5341 - acc: 0.8370 - val_loss: 0.3504 - val_acc: 0.8970\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8457\n",
      "Epoch 00015: val_loss improved from 0.35037 to 0.34869, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/015-0.3487.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5012 - acc: 0.8458 - val_loss: 0.3487 - val_acc: 0.8996\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.8575\n",
      "Epoch 00016: val_loss improved from 0.34869 to 0.30974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/016-0.3097.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4669 - acc: 0.8575 - val_loss: 0.3097 - val_acc: 0.9166\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8649\n",
      "Epoch 00017: val_loss did not improve from 0.30974\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4437 - acc: 0.8650 - val_loss: 0.3260 - val_acc: 0.9089\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8680\n",
      "Epoch 00018: val_loss improved from 0.30974 to 0.29327, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/018-0.2933.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4282 - acc: 0.8681 - val_loss: 0.2933 - val_acc: 0.9229\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8745\n",
      "Epoch 00019: val_loss improved from 0.29327 to 0.27510, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/019-0.2751.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4062 - acc: 0.8745 - val_loss: 0.2751 - val_acc: 0.9243\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8798\n",
      "Epoch 00020: val_loss did not improve from 0.27510\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3904 - acc: 0.8798 - val_loss: 0.3052 - val_acc: 0.9196\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8832\n",
      "Epoch 00021: val_loss improved from 0.27510 to 0.25115, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/021-0.2512.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3749 - acc: 0.8831 - val_loss: 0.2512 - val_acc: 0.9331\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8877\n",
      "Epoch 00022: val_loss did not improve from 0.25115\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3665 - acc: 0.8877 - val_loss: 0.2606 - val_acc: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8932\n",
      "Epoch 00023: val_loss did not improve from 0.25115\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3457 - acc: 0.8932 - val_loss: 0.2533 - val_acc: 0.9308\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8994\n",
      "Epoch 00024: val_loss did not improve from 0.25115\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3294 - acc: 0.8994 - val_loss: 0.2541 - val_acc: 0.9259\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.9002\n",
      "Epoch 00025: val_loss improved from 0.25115 to 0.22180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/025-0.2218.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3243 - acc: 0.9002 - val_loss: 0.2218 - val_acc: 0.9357\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9018\n",
      "Epoch 00026: val_loss improved from 0.22180 to 0.21317, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/026-0.2132.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3141 - acc: 0.9018 - val_loss: 0.2132 - val_acc: 0.9420\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9048\n",
      "Epoch 00027: val_loss did not improve from 0.21317\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3067 - acc: 0.9048 - val_loss: 0.2234 - val_acc: 0.9399\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9085\n",
      "Epoch 00028: val_loss did not improve from 0.21317\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2943 - acc: 0.9085 - val_loss: 0.2343 - val_acc: 0.9343\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9120\n",
      "Epoch 00029: val_loss improved from 0.21317 to 0.20317, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/029-0.2032.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2830 - acc: 0.9120 - val_loss: 0.2032 - val_acc: 0.9478\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9137\n",
      "Epoch 00030: val_loss did not improve from 0.20317\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2783 - acc: 0.9137 - val_loss: 0.2063 - val_acc: 0.9455\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9160\n",
      "Epoch 00031: val_loss improved from 0.20317 to 0.19775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/031-0.1978.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2671 - acc: 0.9160 - val_loss: 0.1978 - val_acc: 0.9432\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9176\n",
      "Epoch 00032: val_loss improved from 0.19775 to 0.19754, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/032-0.1975.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2600 - acc: 0.9176 - val_loss: 0.1975 - val_acc: 0.9478\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9229\n",
      "Epoch 00033: val_loss improved from 0.19754 to 0.19728, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/033-0.1973.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2438 - acc: 0.9229 - val_loss: 0.1973 - val_acc: 0.9460\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9214\n",
      "Epoch 00034: val_loss improved from 0.19728 to 0.18580, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/034-0.1858.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2491 - acc: 0.9215 - val_loss: 0.1858 - val_acc: 0.9506\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9249\n",
      "Epoch 00035: val_loss did not improve from 0.18580\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2397 - acc: 0.9248 - val_loss: 0.1912 - val_acc: 0.9492\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9265\n",
      "Epoch 00036: val_loss did not improve from 0.18580\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2362 - acc: 0.9266 - val_loss: 0.1992 - val_acc: 0.9492\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9277\n",
      "Epoch 00037: val_loss improved from 0.18580 to 0.17906, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/037-0.1791.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2287 - acc: 0.9277 - val_loss: 0.1791 - val_acc: 0.9527\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9301\n",
      "Epoch 00038: val_loss did not improve from 0.17906\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2258 - acc: 0.9301 - val_loss: 0.1795 - val_acc: 0.9499\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9313\n",
      "Epoch 00039: val_loss did not improve from 0.17906\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2215 - acc: 0.9313 - val_loss: 0.1828 - val_acc: 0.9515\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9332\n",
      "Epoch 00040: val_loss improved from 0.17906 to 0.17588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/040-0.1759.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2139 - acc: 0.9332 - val_loss: 0.1759 - val_acc: 0.9548\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9339\n",
      "Epoch 00041: val_loss did not improve from 0.17588\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2064 - acc: 0.9339 - val_loss: 0.1806 - val_acc: 0.9525\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9364\n",
      "Epoch 00042: val_loss did not improve from 0.17588\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2029 - acc: 0.9364 - val_loss: 0.1827 - val_acc: 0.9546\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9378\n",
      "Epoch 00043: val_loss did not improve from 0.17588\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1958 - acc: 0.9378 - val_loss: 0.1856 - val_acc: 0.9574\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9371\n",
      "Epoch 00044: val_loss improved from 0.17588 to 0.16748, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/044-0.1675.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2012 - acc: 0.9371 - val_loss: 0.1675 - val_acc: 0.9543\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9383\n",
      "Epoch 00045: val_loss did not improve from 0.16748\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1944 - acc: 0.9383 - val_loss: 0.1676 - val_acc: 0.9581\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9393\n",
      "Epoch 00046: val_loss improved from 0.16748 to 0.16201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/046-0.1620.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1877 - acc: 0.9393 - val_loss: 0.1620 - val_acc: 0.9527\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9401\n",
      "Epoch 00047: val_loss did not improve from 0.16201\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1860 - acc: 0.9401 - val_loss: 0.1665 - val_acc: 0.9534\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9442\n",
      "Epoch 00048: val_loss did not improve from 0.16201\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1763 - acc: 0.9442 - val_loss: 0.1769 - val_acc: 0.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9418\n",
      "Epoch 00049: val_loss did not improve from 0.16201\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1801 - acc: 0.9418 - val_loss: 0.1687 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9436\n",
      "Epoch 00050: val_loss did not improve from 0.16201\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1775 - acc: 0.9436 - val_loss: 0.1772 - val_acc: 0.9522\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9440\n",
      "Epoch 00051: val_loss did not improve from 0.16201\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1750 - acc: 0.9441 - val_loss: 0.1709 - val_acc: 0.9478\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9473\n",
      "Epoch 00052: val_loss improved from 0.16201 to 0.16091, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/052-0.1609.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1658 - acc: 0.9472 - val_loss: 0.1609 - val_acc: 0.9534\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9467\n",
      "Epoch 00053: val_loss did not improve from 0.16091\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1645 - acc: 0.9467 - val_loss: 0.1662 - val_acc: 0.9546\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9486\n",
      "Epoch 00054: val_loss did not improve from 0.16091\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1623 - acc: 0.9486 - val_loss: 0.1752 - val_acc: 0.9513\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9484\n",
      "Epoch 00055: val_loss improved from 0.16091 to 0.15715, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/055-0.1571.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1608 - acc: 0.9484 - val_loss: 0.1571 - val_acc: 0.9571\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9498\n",
      "Epoch 00056: val_loss improved from 0.15715 to 0.14842, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/056-0.1484.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1577 - acc: 0.9498 - val_loss: 0.1484 - val_acc: 0.9637\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9514\n",
      "Epoch 00057: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1528 - acc: 0.9514 - val_loss: 0.1811 - val_acc: 0.9499\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9509\n",
      "Epoch 00058: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1525 - acc: 0.9509 - val_loss: 0.1550 - val_acc: 0.9576\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9519\n",
      "Epoch 00059: val_loss improved from 0.14842 to 0.14153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv_checkpoint/059-0.1415.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1510 - acc: 0.9519 - val_loss: 0.1415 - val_acc: 0.9588\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9535\n",
      "Epoch 00060: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1440 - acc: 0.9535 - val_loss: 0.2006 - val_acc: 0.9483\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9524\n",
      "Epoch 00061: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1478 - acc: 0.9524 - val_loss: 0.1540 - val_acc: 0.9564\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9539\n",
      "Epoch 00062: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1409 - acc: 0.9539 - val_loss: 0.1673 - val_acc: 0.9548\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9551\n",
      "Epoch 00063: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1399 - acc: 0.9551 - val_loss: 0.1492 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9555\n",
      "Epoch 00064: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1340 - acc: 0.9555 - val_loss: 0.1516 - val_acc: 0.9599\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9563\n",
      "Epoch 00065: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1312 - acc: 0.9563 - val_loss: 0.1520 - val_acc: 0.9585\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9560\n",
      "Epoch 00066: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1298 - acc: 0.9560 - val_loss: 0.1768 - val_acc: 0.9571\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9567\n",
      "Epoch 00067: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1329 - acc: 0.9567 - val_loss: 0.1640 - val_acc: 0.9599\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9588\n",
      "Epoch 00068: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1233 - acc: 0.9588 - val_loss: 0.1488 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9588\n",
      "Epoch 00069: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1251 - acc: 0.9588 - val_loss: 0.1546 - val_acc: 0.9585\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9585\n",
      "Epoch 00070: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1266 - acc: 0.9585 - val_loss: 0.1537 - val_acc: 0.9578\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9585\n",
      "Epoch 00071: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1249 - acc: 0.9585 - val_loss: 0.1594 - val_acc: 0.9557\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9591\n",
      "Epoch 00072: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1240 - acc: 0.9591 - val_loss: 0.1678 - val_acc: 0.9588\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9599\n",
      "Epoch 00073: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1241 - acc: 0.9599 - val_loss: 0.1721 - val_acc: 0.9588\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9629\n",
      "Epoch 00074: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1151 - acc: 0.9629 - val_loss: 0.1663 - val_acc: 0.9595\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9647\n",
      "Epoch 00075: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1098 - acc: 0.9647 - val_loss: 0.1573 - val_acc: 0.9609\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9636\n",
      "Epoch 00076: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1125 - acc: 0.9636 - val_loss: 0.1556 - val_acc: 0.9613\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9642\n",
      "Epoch 00077: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1098 - acc: 0.9642 - val_loss: 0.1639 - val_acc: 0.9585\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9626\n",
      "Epoch 00078: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1128 - acc: 0.9626 - val_loss: 0.1728 - val_acc: 0.9583\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9646\n",
      "Epoch 00079: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1074 - acc: 0.9647 - val_loss: 0.1614 - val_acc: 0.9613\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9652\n",
      "Epoch 00080: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1066 - acc: 0.9652 - val_loss: 0.1492 - val_acc: 0.9602\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9645\n",
      "Epoch 00081: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1089 - acc: 0.9645 - val_loss: 0.1668 - val_acc: 0.9543\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9645\n",
      "Epoch 00082: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1050 - acc: 0.9645 - val_loss: 0.1599 - val_acc: 0.9571\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9671\n",
      "Epoch 00083: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1025 - acc: 0.9671 - val_loss: 0.1644 - val_acc: 0.9618\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9676\n",
      "Epoch 00084: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0989 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9550\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9663\n",
      "Epoch 00085: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1007 - acc: 0.9663 - val_loss: 0.1621 - val_acc: 0.9630\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9671\n",
      "Epoch 00086: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0983 - acc: 0.9671 - val_loss: 0.1536 - val_acc: 0.9627\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9689\n",
      "Epoch 00087: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0927 - acc: 0.9689 - val_loss: 0.1666 - val_acc: 0.9576\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9687\n",
      "Epoch 00088: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0935 - acc: 0.9687 - val_loss: 0.1452 - val_acc: 0.9613\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9692\n",
      "Epoch 00089: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0927 - acc: 0.9692 - val_loss: 0.1522 - val_acc: 0.9639\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9674\n",
      "Epoch 00090: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0929 - acc: 0.9675 - val_loss: 0.1616 - val_acc: 0.9634\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9693\n",
      "Epoch 00091: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0928 - acc: 0.9693 - val_loss: 0.1707 - val_acc: 0.9651\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9689\n",
      "Epoch 00092: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0915 - acc: 0.9689 - val_loss: 0.1458 - val_acc: 0.9639\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9670\n",
      "Epoch 00093: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1012 - acc: 0.9670 - val_loss: 0.1737 - val_acc: 0.9571\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9720\n",
      "Epoch 00094: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0862 - acc: 0.9720 - val_loss: 0.1815 - val_acc: 0.9609\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9715\n",
      "Epoch 00095: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0863 - acc: 0.9715 - val_loss: 0.1635 - val_acc: 0.9616\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9722\n",
      "Epoch 00096: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0834 - acc: 0.9722 - val_loss: 0.1741 - val_acc: 0.9627\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9711\n",
      "Epoch 00097: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0852 - acc: 0.9711 - val_loss: 0.1833 - val_acc: 0.9567\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9718\n",
      "Epoch 00098: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0810 - acc: 0.9719 - val_loss: 0.1574 - val_acc: 0.9620\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9738\n",
      "Epoch 00099: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0795 - acc: 0.9738 - val_loss: 0.1716 - val_acc: 0.9583\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9722\n",
      "Epoch 00100: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0822 - acc: 0.9722 - val_loss: 0.1833 - val_acc: 0.9588\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9729\n",
      "Epoch 00101: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0799 - acc: 0.9729 - val_loss: 0.1690 - val_acc: 0.9606\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9729\n",
      "Epoch 00102: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0800 - acc: 0.9729 - val_loss: 0.1583 - val_acc: 0.9611\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9739\n",
      "Epoch 00103: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0767 - acc: 0.9739 - val_loss: 0.1799 - val_acc: 0.9602\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9735\n",
      "Epoch 00104: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0786 - acc: 0.9735 - val_loss: 0.1578 - val_acc: 0.9632\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9741\n",
      "Epoch 00105: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0762 - acc: 0.9741 - val_loss: 0.1626 - val_acc: 0.9569\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9750\n",
      "Epoch 00106: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0742 - acc: 0.9750 - val_loss: 0.1800 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9759\n",
      "Epoch 00107: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0725 - acc: 0.9759 - val_loss: 0.1635 - val_acc: 0.9595\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9755\n",
      "Epoch 00108: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0731 - acc: 0.9755 - val_loss: 0.1731 - val_acc: 0.9618\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9760\n",
      "Epoch 00109: val_loss did not improve from 0.14153\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0722 - acc: 0.9760 - val_loss: 0.1831 - val_acc: 0.9597\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1740 - acc: 0.9464\n",
      "Loss: 0.1739721952522655 Accuracy: 0.94641745\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6644 - acc: 0.1058\n",
      "Epoch 00001: val_loss improved from inf to 2.32311, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/001-2.3231.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 2.6643 - acc: 0.1058 - val_loss: 2.3231 - val_acc: 0.2432\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0632 - acc: 0.3119\n",
      "Epoch 00002: val_loss improved from 2.32311 to 1.46628, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/002-1.4663.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.0631 - acc: 0.3119 - val_loss: 1.4663 - val_acc: 0.5674\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4747 - acc: 0.5048\n",
      "Epoch 00003: val_loss improved from 1.46628 to 0.98322, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/003-0.9832.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4745 - acc: 0.5048 - val_loss: 0.9832 - val_acc: 0.6951\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1841 - acc: 0.6070\n",
      "Epoch 00004: val_loss improved from 0.98322 to 0.80567, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/004-0.8057.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1841 - acc: 0.6070 - val_loss: 0.8057 - val_acc: 0.7491\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9981 - acc: 0.6745\n",
      "Epoch 00005: val_loss improved from 0.80567 to 0.66554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/005-0.6655.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9982 - acc: 0.6744 - val_loss: 0.6655 - val_acc: 0.7978\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8589 - acc: 0.7207\n",
      "Epoch 00006: val_loss improved from 0.66554 to 0.53479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/006-0.5348.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8588 - acc: 0.7207 - val_loss: 0.5348 - val_acc: 0.8414\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7339 - acc: 0.7639\n",
      "Epoch 00007: val_loss improved from 0.53479 to 0.44827, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/007-0.4483.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7339 - acc: 0.7639 - val_loss: 0.4483 - val_acc: 0.8686\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.7944\n",
      "Epoch 00008: val_loss improved from 0.44827 to 0.36907, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/008-0.3691.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6403 - acc: 0.7944 - val_loss: 0.3691 - val_acc: 0.8966\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8182\n",
      "Epoch 00009: val_loss improved from 0.36907 to 0.31192, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/009-0.3119.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5721 - acc: 0.8182 - val_loss: 0.3119 - val_acc: 0.9101\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8374\n",
      "Epoch 00010: val_loss improved from 0.31192 to 0.30104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/010-0.3010.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5179 - acc: 0.8375 - val_loss: 0.3010 - val_acc: 0.9106\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8504\n",
      "Epoch 00011: val_loss improved from 0.30104 to 0.27482, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/011-0.2748.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4772 - acc: 0.8504 - val_loss: 0.2748 - val_acc: 0.9206\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8665\n",
      "Epoch 00012: val_loss improved from 0.27482 to 0.23630, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/012-0.2363.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4405 - acc: 0.8665 - val_loss: 0.2363 - val_acc: 0.9329\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8748\n",
      "Epoch 00013: val_loss improved from 0.23630 to 0.21721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/013-0.2172.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4071 - acc: 0.8749 - val_loss: 0.2172 - val_acc: 0.9392\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8833\n",
      "Epoch 00014: val_loss improved from 0.21721 to 0.21440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/014-0.2144.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3769 - acc: 0.8833 - val_loss: 0.2144 - val_acc: 0.9397\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8879\n",
      "Epoch 00015: val_loss did not improve from 0.21440\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3616 - acc: 0.8879 - val_loss: 0.2355 - val_acc: 0.9327\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8929\n",
      "Epoch 00016: val_loss improved from 0.21440 to 0.17696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/016-0.1770.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3448 - acc: 0.8929 - val_loss: 0.1770 - val_acc: 0.9495\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9019\n",
      "Epoch 00017: val_loss did not improve from 0.17696\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3225 - acc: 0.9019 - val_loss: 0.1936 - val_acc: 0.9453\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9055\n",
      "Epoch 00018: val_loss did not improve from 0.17696\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3061 - acc: 0.9055 - val_loss: 0.1809 - val_acc: 0.9478\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9090\n",
      "Epoch 00019: val_loss did not improve from 0.17696\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2914 - acc: 0.9090 - val_loss: 0.1832 - val_acc: 0.9485\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9156\n",
      "Epoch 00020: val_loss improved from 0.17696 to 0.16492, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/020-0.1649.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2742 - acc: 0.9156 - val_loss: 0.1649 - val_acc: 0.9529\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9187\n",
      "Epoch 00021: val_loss improved from 0.16492 to 0.16238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/021-0.1624.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2630 - acc: 0.9188 - val_loss: 0.1624 - val_acc: 0.9534\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9219\n",
      "Epoch 00022: val_loss did not improve from 0.16238\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2508 - acc: 0.9219 - val_loss: 0.1738 - val_acc: 0.9502\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9232\n",
      "Epoch 00023: val_loss improved from 0.16238 to 0.13954, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/023-0.1395.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2457 - acc: 0.9232 - val_loss: 0.1395 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9278\n",
      "Epoch 00024: val_loss improved from 0.13954 to 0.13857, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/024-0.1386.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2301 - acc: 0.9278 - val_loss: 0.1386 - val_acc: 0.9590\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9283\n",
      "Epoch 00025: val_loss did not improve from 0.13857\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2277 - acc: 0.9283 - val_loss: 0.1426 - val_acc: 0.9592\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9312\n",
      "Epoch 00026: val_loss did not improve from 0.13857\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2174 - acc: 0.9312 - val_loss: 0.1408 - val_acc: 0.9583\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9342\n",
      "Epoch 00027: val_loss improved from 0.13857 to 0.12451, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/027-0.1245.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2130 - acc: 0.9342 - val_loss: 0.1245 - val_acc: 0.9632\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9370\n",
      "Epoch 00028: val_loss did not improve from 0.12451\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2029 - acc: 0.9370 - val_loss: 0.1586 - val_acc: 0.9525\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9369\n",
      "Epoch 00029: val_loss did not improve from 0.12451\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2011 - acc: 0.9369 - val_loss: 0.1613 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9404\n",
      "Epoch 00030: val_loss improved from 0.12451 to 0.11954, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/030-0.1195.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1878 - acc: 0.9404 - val_loss: 0.1195 - val_acc: 0.9641\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9423\n",
      "Epoch 00031: val_loss did not improve from 0.11954\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1840 - acc: 0.9422 - val_loss: 0.1422 - val_acc: 0.9562\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9433\n",
      "Epoch 00032: val_loss improved from 0.11954 to 0.11798, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/032-0.1180.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1784 - acc: 0.9433 - val_loss: 0.1180 - val_acc: 0.9646\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9446\n",
      "Epoch 00033: val_loss did not improve from 0.11798\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1726 - acc: 0.9446 - val_loss: 0.1189 - val_acc: 0.9637\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9457\n",
      "Epoch 00034: val_loss did not improve from 0.11798\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1712 - acc: 0.9457 - val_loss: 0.1277 - val_acc: 0.9602\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9485\n",
      "Epoch 00035: val_loss did not improve from 0.11798\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1678 - acc: 0.9485 - val_loss: 0.1208 - val_acc: 0.9637\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9498\n",
      "Epoch 00036: val_loss did not improve from 0.11798\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1581 - acc: 0.9498 - val_loss: 0.1231 - val_acc: 0.9632\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9508\n",
      "Epoch 00037: val_loss did not improve from 0.11798\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1551 - acc: 0.9508 - val_loss: 0.1269 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9521\n",
      "Epoch 00038: val_loss improved from 0.11798 to 0.10229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/038-0.1023.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1512 - acc: 0.9522 - val_loss: 0.1023 - val_acc: 0.9690\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9538\n",
      "Epoch 00039: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1451 - acc: 0.9538 - val_loss: 0.1117 - val_acc: 0.9662\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9542\n",
      "Epoch 00040: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1432 - acc: 0.9542 - val_loss: 0.1182 - val_acc: 0.9618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9571\n",
      "Epoch 00041: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1390 - acc: 0.9572 - val_loss: 0.1101 - val_acc: 0.9674\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9572\n",
      "Epoch 00042: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1379 - acc: 0.9572 - val_loss: 0.1097 - val_acc: 0.9693\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9587\n",
      "Epoch 00043: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1285 - acc: 0.9587 - val_loss: 0.1079 - val_acc: 0.9679\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9568\n",
      "Epoch 00044: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1363 - acc: 0.9568 - val_loss: 0.1089 - val_acc: 0.9660\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9592\n",
      "Epoch 00045: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1247 - acc: 0.9592 - val_loss: 0.1121 - val_acc: 0.9665\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9594\n",
      "Epoch 00046: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1249 - acc: 0.9594 - val_loss: 0.1045 - val_acc: 0.9653\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9595\n",
      "Epoch 00047: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1261 - acc: 0.9595 - val_loss: 0.1032 - val_acc: 0.9697\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9628\n",
      "Epoch 00048: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1164 - acc: 0.9628 - val_loss: 0.1115 - val_acc: 0.9674\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9639\n",
      "Epoch 00049: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1126 - acc: 0.9639 - val_loss: 0.1172 - val_acc: 0.9674\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9633\n",
      "Epoch 00050: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1114 - acc: 0.9633 - val_loss: 0.1095 - val_acc: 0.9672\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9636\n",
      "Epoch 00051: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1132 - acc: 0.9636 - val_loss: 0.1134 - val_acc: 0.9641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9649\n",
      "Epoch 00052: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1086 - acc: 0.9650 - val_loss: 0.1163 - val_acc: 0.9644\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9652\n",
      "Epoch 00053: val_loss did not improve from 0.10229\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1039 - acc: 0.9652 - val_loss: 0.1043 - val_acc: 0.9704\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9669\n",
      "Epoch 00054: val_loss improved from 0.10229 to 0.10223, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/054-0.1022.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1020 - acc: 0.9669 - val_loss: 0.1022 - val_acc: 0.9665\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9668\n",
      "Epoch 00055: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1022 - acc: 0.9669 - val_loss: 0.1068 - val_acc: 0.9653\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9676\n",
      "Epoch 00056: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0995 - acc: 0.9675 - val_loss: 0.1134 - val_acc: 0.9697\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9679\n",
      "Epoch 00057: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0964 - acc: 0.9679 - val_loss: 0.1154 - val_acc: 0.9651\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9704\n",
      "Epoch 00058: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0911 - acc: 0.9704 - val_loss: 0.1118 - val_acc: 0.9681\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9710\n",
      "Epoch 00059: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0904 - acc: 0.9710 - val_loss: 0.1072 - val_acc: 0.9713\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9705\n",
      "Epoch 00060: val_loss did not improve from 0.10223\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0900 - acc: 0.9705 - val_loss: 0.1231 - val_acc: 0.9683\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9701\n",
      "Epoch 00061: val_loss improved from 0.10223 to 0.09871, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv_checkpoint/061-0.0987.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0905 - acc: 0.9701 - val_loss: 0.0987 - val_acc: 0.9723\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9714\n",
      "Epoch 00062: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0880 - acc: 0.9714 - val_loss: 0.1035 - val_acc: 0.9686\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9711\n",
      "Epoch 00063: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0867 - acc: 0.9711 - val_loss: 0.1172 - val_acc: 0.9683\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9731\n",
      "Epoch 00064: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0834 - acc: 0.9731 - val_loss: 0.1101 - val_acc: 0.9674\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9726\n",
      "Epoch 00065: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0811 - acc: 0.9726 - val_loss: 0.1103 - val_acc: 0.9709\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9746\n",
      "Epoch 00066: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0769 - acc: 0.9747 - val_loss: 0.1104 - val_acc: 0.9706\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9738\n",
      "Epoch 00067: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0810 - acc: 0.9738 - val_loss: 0.1120 - val_acc: 0.9658\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9755\n",
      "Epoch 00068: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0757 - acc: 0.9755 - val_loss: 0.1197 - val_acc: 0.9672\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9753\n",
      "Epoch 00069: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0732 - acc: 0.9753 - val_loss: 0.1054 - val_acc: 0.9706\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9757\n",
      "Epoch 00070: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0739 - acc: 0.9757 - val_loss: 0.1137 - val_acc: 0.9700\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9752\n",
      "Epoch 00071: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0753 - acc: 0.9752 - val_loss: 0.1093 - val_acc: 0.9704\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9762\n",
      "Epoch 00072: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0738 - acc: 0.9762 - val_loss: 0.1113 - val_acc: 0.9686\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9772\n",
      "Epoch 00073: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0702 - acc: 0.9772 - val_loss: 0.1191 - val_acc: 0.9702\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9763\n",
      "Epoch 00074: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0699 - acc: 0.9763 - val_loss: 0.1200 - val_acc: 0.9697\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9779\n",
      "Epoch 00075: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0685 - acc: 0.9779 - val_loss: 0.1308 - val_acc: 0.9686\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9692\n",
      "Epoch 00076: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0956 - acc: 0.9692 - val_loss: 0.1231 - val_acc: 0.9700\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9784\n",
      "Epoch 00077: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0665 - acc: 0.9784 - val_loss: 0.1276 - val_acc: 0.9702\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9795\n",
      "Epoch 00078: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0621 - acc: 0.9795 - val_loss: 0.1187 - val_acc: 0.9676\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9787\n",
      "Epoch 00079: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0637 - acc: 0.9787 - val_loss: 0.1150 - val_acc: 0.9711\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9786\n",
      "Epoch 00080: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0637 - acc: 0.9786 - val_loss: 0.1195 - val_acc: 0.9718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9797\n",
      "Epoch 00081: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0627 - acc: 0.9797 - val_loss: 0.1089 - val_acc: 0.9702\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9799\n",
      "Epoch 00082: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0617 - acc: 0.9799 - val_loss: 0.1111 - val_acc: 0.9720\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9792\n",
      "Epoch 00083: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0617 - acc: 0.9792 - val_loss: 0.1306 - val_acc: 0.9667\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9801\n",
      "Epoch 00084: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0613 - acc: 0.9801 - val_loss: 0.1183 - val_acc: 0.9681\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9805\n",
      "Epoch 00085: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0583 - acc: 0.9805 - val_loss: 0.1106 - val_acc: 0.9704\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9808\n",
      "Epoch 00086: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0589 - acc: 0.9808 - val_loss: 0.1105 - val_acc: 0.9697\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9807\n",
      "Epoch 00087: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0578 - acc: 0.9807 - val_loss: 0.1339 - val_acc: 0.9674\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9810\n",
      "Epoch 00088: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0584 - acc: 0.9810 - val_loss: 0.1156 - val_acc: 0.9725\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9817\n",
      "Epoch 00089: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0543 - acc: 0.9817 - val_loss: 0.1180 - val_acc: 0.9697\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9817\n",
      "Epoch 00090: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0555 - acc: 0.9817 - val_loss: 0.1110 - val_acc: 0.9711\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9838\n",
      "Epoch 00091: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0494 - acc: 0.9838 - val_loss: 0.1462 - val_acc: 0.9672\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9825\n",
      "Epoch 00092: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0540 - acc: 0.9825 - val_loss: 0.1198 - val_acc: 0.9720\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9827\n",
      "Epoch 00093: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0524 - acc: 0.9827 - val_loss: 0.1164 - val_acc: 0.9700\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9819\n",
      "Epoch 00094: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0536 - acc: 0.9819 - val_loss: 0.1270 - val_acc: 0.9690\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9843\n",
      "Epoch 00095: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0487 - acc: 0.9843 - val_loss: 0.1467 - val_acc: 0.9686\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9840\n",
      "Epoch 00096: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0482 - acc: 0.9840 - val_loss: 0.1368 - val_acc: 0.9688\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9835\n",
      "Epoch 00097: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0502 - acc: 0.9835 - val_loss: 0.1168 - val_acc: 0.9716\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9837\n",
      "Epoch 00098: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0507 - acc: 0.9837 - val_loss: 0.1414 - val_acc: 0.9669\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9843\n",
      "Epoch 00099: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0470 - acc: 0.9843 - val_loss: 0.1214 - val_acc: 0.9711\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9859\n",
      "Epoch 00100: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0445 - acc: 0.9859 - val_loss: 0.1307 - val_acc: 0.9706\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9854\n",
      "Epoch 00101: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0461 - acc: 0.9854 - val_loss: 0.1158 - val_acc: 0.9706\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9835\n",
      "Epoch 00102: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0488 - acc: 0.9835 - val_loss: 0.1358 - val_acc: 0.9700\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9854\n",
      "Epoch 00103: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0428 - acc: 0.9854 - val_loss: 0.1378 - val_acc: 0.9702\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9856\n",
      "Epoch 00104: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0440 - acc: 0.9856 - val_loss: 0.1267 - val_acc: 0.9720\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9848\n",
      "Epoch 00105: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0441 - acc: 0.9848 - val_loss: 0.1322 - val_acc: 0.9704\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9852\n",
      "Epoch 00106: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0443 - acc: 0.9852 - val_loss: 0.1255 - val_acc: 0.9713\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9863\n",
      "Epoch 00107: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0423 - acc: 0.9863 - val_loss: 0.1241 - val_acc: 0.9713\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9857\n",
      "Epoch 00108: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0435 - acc: 0.9857 - val_loss: 0.1433 - val_acc: 0.9697\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9864\n",
      "Epoch 00109: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0429 - acc: 0.9864 - val_loss: 0.1475 - val_acc: 0.9702\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9863\n",
      "Epoch 00110: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0397 - acc: 0.9863 - val_loss: 0.1297 - val_acc: 0.9716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9867\n",
      "Epoch 00111: val_loss did not improve from 0.09871\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0429 - acc: 0.9867 - val_loss: 0.1351 - val_acc: 0.9727\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1672 - acc: 0.9551\n",
      "Loss: 0.16716184404844017 Accuracy: 0.9551402\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3769 - acc: 0.2215\n",
      "Epoch 00001: val_loss improved from inf to 1.55977, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/001-1.5598.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 2.3768 - acc: 0.2215 - val_loss: 1.5598 - val_acc: 0.5029\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4840 - acc: 0.5209\n",
      "Epoch 00002: val_loss improved from 1.55977 to 1.05271, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/002-1.0527.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4840 - acc: 0.5209 - val_loss: 1.0527 - val_acc: 0.6744\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0658 - acc: 0.6533\n",
      "Epoch 00003: val_loss improved from 1.05271 to 0.67370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/003-0.6737.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0658 - acc: 0.6534 - val_loss: 0.6737 - val_acc: 0.7820\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8521 - acc: 0.7260\n",
      "Epoch 00004: val_loss improved from 0.67370 to 0.56686, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/004-0.5669.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8520 - acc: 0.7260 - val_loss: 0.5669 - val_acc: 0.8130\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7249 - acc: 0.7680\n",
      "Epoch 00005: val_loss improved from 0.56686 to 0.49394, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/005-0.4939.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7249 - acc: 0.7680 - val_loss: 0.4939 - val_acc: 0.8414\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.7984\n",
      "Epoch 00006: val_loss improved from 0.49394 to 0.40761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/006-0.4076.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6289 - acc: 0.7984 - val_loss: 0.4076 - val_acc: 0.8672\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8230\n",
      "Epoch 00007: val_loss improved from 0.40761 to 0.35129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/007-0.3513.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5551 - acc: 0.8230 - val_loss: 0.3513 - val_acc: 0.8898\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.8460\n",
      "Epoch 00008: val_loss improved from 0.35129 to 0.33068, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/008-0.3307.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4855 - acc: 0.8460 - val_loss: 0.3307 - val_acc: 0.8966\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8665\n",
      "Epoch 00009: val_loss improved from 0.33068 to 0.31461, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/009-0.3146.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4314 - acc: 0.8665 - val_loss: 0.3146 - val_acc: 0.9040\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8745\n",
      "Epoch 00010: val_loss improved from 0.31461 to 0.30410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/010-0.3041.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3971 - acc: 0.8745 - val_loss: 0.3041 - val_acc: 0.9068\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8854\n",
      "Epoch 00011: val_loss improved from 0.30410 to 0.22693, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/011-0.2269.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3660 - acc: 0.8854 - val_loss: 0.2269 - val_acc: 0.9269\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8986\n",
      "Epoch 00012: val_loss did not improve from 0.22693\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3258 - acc: 0.8986 - val_loss: 0.2752 - val_acc: 0.9115\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9036\n",
      "Epoch 00013: val_loss improved from 0.22693 to 0.20145, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/013-0.2015.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3109 - acc: 0.9036 - val_loss: 0.2015 - val_acc: 0.9359\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9116\n",
      "Epoch 00014: val_loss did not improve from 0.20145\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2812 - acc: 0.9116 - val_loss: 0.2147 - val_acc: 0.9331\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9178\n",
      "Epoch 00015: val_loss did not improve from 0.20145\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2650 - acc: 0.9178 - val_loss: 0.2709 - val_acc: 0.9157\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9184\n",
      "Epoch 00016: val_loss improved from 0.20145 to 0.18249, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/016-0.1825.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2628 - acc: 0.9184 - val_loss: 0.1825 - val_acc: 0.9408\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9298\n",
      "Epoch 00017: val_loss improved from 0.18249 to 0.17570, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/017-0.1757.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2284 - acc: 0.9298 - val_loss: 0.1757 - val_acc: 0.9446\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9310\n",
      "Epoch 00018: val_loss improved from 0.17570 to 0.15848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/018-0.1585.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2256 - acc: 0.9310 - val_loss: 0.1585 - val_acc: 0.9509\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9343\n",
      "Epoch 00019: val_loss did not improve from 0.15848\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2101 - acc: 0.9344 - val_loss: 0.1984 - val_acc: 0.9394\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9383\n",
      "Epoch 00020: val_loss did not improve from 0.15848\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2026 - acc: 0.9382 - val_loss: 0.1680 - val_acc: 0.9502\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9405\n",
      "Epoch 00021: val_loss did not improve from 0.15848\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1872 - acc: 0.9405 - val_loss: 0.1779 - val_acc: 0.9450\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9442\n",
      "Epoch 00022: val_loss improved from 0.15848 to 0.14734, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/022-0.1473.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1792 - acc: 0.9442 - val_loss: 0.1473 - val_acc: 0.9564\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9454\n",
      "Epoch 00023: val_loss improved from 0.14734 to 0.14579, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/023-0.1458.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1678 - acc: 0.9454 - val_loss: 0.1458 - val_acc: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9489\n",
      "Epoch 00024: val_loss improved from 0.14579 to 0.13930, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/024-0.1393.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1593 - acc: 0.9489 - val_loss: 0.1393 - val_acc: 0.9571\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9510\n",
      "Epoch 00025: val_loss improved from 0.13930 to 0.12054, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv_checkpoint/025-0.1205.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1566 - acc: 0.9510 - val_loss: 0.1205 - val_acc: 0.9625\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9536\n",
      "Epoch 00026: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1455 - acc: 0.9536 - val_loss: 0.1410 - val_acc: 0.9592\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9523\n",
      "Epoch 00027: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1489 - acc: 0.9523 - val_loss: 0.1348 - val_acc: 0.9585\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9562\n",
      "Epoch 00028: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1336 - acc: 0.9562 - val_loss: 0.1360 - val_acc: 0.9585\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9599\n",
      "Epoch 00029: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1275 - acc: 0.9599 - val_loss: 0.1326 - val_acc: 0.9630\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9604\n",
      "Epoch 00030: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1234 - acc: 0.9604 - val_loss: 0.1480 - val_acc: 0.9560\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9620\n",
      "Epoch 00031: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1154 - acc: 0.9620 - val_loss: 0.1369 - val_acc: 0.9588\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9630\n",
      "Epoch 00032: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1153 - acc: 0.9630 - val_loss: 0.1357 - val_acc: 0.9634\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9650\n",
      "Epoch 00033: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1075 - acc: 0.9650 - val_loss: 0.1261 - val_acc: 0.9641\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9656\n",
      "Epoch 00034: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1075 - acc: 0.9656 - val_loss: 0.1496 - val_acc: 0.9606\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9688\n",
      "Epoch 00035: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0958 - acc: 0.9688 - val_loss: 0.1328 - val_acc: 0.9639\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9690\n",
      "Epoch 00036: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0944 - acc: 0.9690 - val_loss: 0.1418 - val_acc: 0.9595\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9707\n",
      "Epoch 00037: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0893 - acc: 0.9707 - val_loss: 0.1488 - val_acc: 0.9585\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9705\n",
      "Epoch 00038: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0911 - acc: 0.9705 - val_loss: 0.1714 - val_acc: 0.9541\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9711\n",
      "Epoch 00039: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0868 - acc: 0.9711 - val_loss: 0.1292 - val_acc: 0.9658\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9725\n",
      "Epoch 00040: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0826 - acc: 0.9725 - val_loss: 0.1402 - val_acc: 0.9602\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9735\n",
      "Epoch 00041: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0811 - acc: 0.9735 - val_loss: 0.1308 - val_acc: 0.9648\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9757\n",
      "Epoch 00042: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0724 - acc: 0.9757 - val_loss: 0.1680 - val_acc: 0.9595\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9754\n",
      "Epoch 00043: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0743 - acc: 0.9754 - val_loss: 0.1316 - val_acc: 0.9632\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9764\n",
      "Epoch 00044: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0720 - acc: 0.9764 - val_loss: 0.1354 - val_acc: 0.9637\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9779\n",
      "Epoch 00045: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0651 - acc: 0.9779 - val_loss: 0.1513 - val_acc: 0.9634\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9784\n",
      "Epoch 00046: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0658 - acc: 0.9784 - val_loss: 0.1312 - val_acc: 0.9653\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9785\n",
      "Epoch 00047: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0628 - acc: 0.9785 - val_loss: 0.1319 - val_acc: 0.9669\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9797\n",
      "Epoch 00048: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0621 - acc: 0.9797 - val_loss: 0.1428 - val_acc: 0.9641\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9805\n",
      "Epoch 00049: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0595 - acc: 0.9805 - val_loss: 0.1420 - val_acc: 0.9655\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9797\n",
      "Epoch 00050: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0599 - acc: 0.9797 - val_loss: 0.1373 - val_acc: 0.9672\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9808\n",
      "Epoch 00051: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0577 - acc: 0.9808 - val_loss: 0.1511 - val_acc: 0.9658\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9805\n",
      "Epoch 00052: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0595 - acc: 0.9805 - val_loss: 0.1323 - val_acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 00053: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.1511 - val_acc: 0.9648\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9825\n",
      "Epoch 00054: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0513 - acc: 0.9825 - val_loss: 0.1428 - val_acc: 0.9711\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9821\n",
      "Epoch 00055: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0542 - acc: 0.9821 - val_loss: 0.1493 - val_acc: 0.9655\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9843\n",
      "Epoch 00056: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0469 - acc: 0.9843 - val_loss: 0.1465 - val_acc: 0.9651\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9841\n",
      "Epoch 00057: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0490 - acc: 0.9841 - val_loss: 0.1404 - val_acc: 0.9674\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9842\n",
      "Epoch 00058: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0481 - acc: 0.9842 - val_loss: 0.1495 - val_acc: 0.9697\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9843\n",
      "Epoch 00059: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0475 - acc: 0.9843 - val_loss: 0.1591 - val_acc: 0.9632\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9848\n",
      "Epoch 00060: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0437 - acc: 0.9848 - val_loss: 0.1683 - val_acc: 0.9651\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9856\n",
      "Epoch 00061: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0429 - acc: 0.9856 - val_loss: 0.1652 - val_acc: 0.9662\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9861\n",
      "Epoch 00062: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0426 - acc: 0.9861 - val_loss: 0.1806 - val_acc: 0.9618\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9872\n",
      "Epoch 00063: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0406 - acc: 0.9872 - val_loss: 0.1650 - val_acc: 0.9646\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9862\n",
      "Epoch 00064: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0417 - acc: 0.9863 - val_loss: 0.1624 - val_acc: 0.9648\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9873\n",
      "Epoch 00065: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0389 - acc: 0.9873 - val_loss: 0.1606 - val_acc: 0.9676\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9873\n",
      "Epoch 00066: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0411 - acc: 0.9873 - val_loss: 0.1792 - val_acc: 0.9632\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9877\n",
      "Epoch 00067: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0390 - acc: 0.9877 - val_loss: 0.1528 - val_acc: 0.9625\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9878\n",
      "Epoch 00068: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0372 - acc: 0.9878 - val_loss: 0.1662 - val_acc: 0.9641\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9876\n",
      "Epoch 00069: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0357 - acc: 0.9876 - val_loss: 0.1650 - val_acc: 0.9655\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9891\n",
      "Epoch 00070: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0341 - acc: 0.9891 - val_loss: 0.1767 - val_acc: 0.9653\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9888\n",
      "Epoch 00071: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0344 - acc: 0.9888 - val_loss: 0.1934 - val_acc: 0.9637\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9864\n",
      "Epoch 00072: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0416 - acc: 0.9864 - val_loss: 0.1621 - val_acc: 0.9667\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9910\n",
      "Epoch 00073: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0286 - acc: 0.9910 - val_loss: 0.1696 - val_acc: 0.9697\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9889\n",
      "Epoch 00074: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0347 - acc: 0.9889 - val_loss: 0.1756 - val_acc: 0.9700\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9904\n",
      "Epoch 00075: val_loss did not improve from 0.12054\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0311 - acc: 0.9904 - val_loss: 0.1492 - val_acc: 0.9697\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1960 - acc: 0.9389\n",
      "Loss: 0.19602461982429462 Accuracy: 0.9389408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_conv_3_VGG_DO_075_DO_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "    \n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 611us/sample - loss: 2.0562 - acc: 0.3641\n",
      "Loss: 2.0561687808913236 Accuracy: 0.36407062\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 928us/sample - loss: 1.7243 - acc: 0.4665\n",
      "Loss: 1.724294582061926 Accuracy: 0.46645898\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.3509 - acc: 0.5850\n",
      "Loss: 1.3508993964576523 Accuracy: 0.5850467\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8876 - acc: 0.7697\n",
      "Loss: 0.8876171379569658 Accuracy: 0.7696781\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6308 - acc: 0.8706\n",
      "Loss: 0.6307582460088522 Accuracy: 0.8706127\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2933 - acc: 0.9205\n",
      "Loss: 0.2933371344567707 Accuracy: 0.9204569\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1740 - acc: 0.9464\n",
      "Loss: 0.1739721952522655 Accuracy: 0.94641745\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1672 - acc: 0.9551\n",
      "Loss: 0.16716184404844017 Accuracy: 0.9551402\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1960 - acc: 0.9389\n",
      "Loss: 0.19602461982429462 Accuracy: 0.9389408\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6326 - acc: 0.6648\n",
      "Loss: 1.632649170225902 Accuracy: 0.6647975\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9869 - acc: 0.7915\n",
      "Loss: 0.9868532168407183 Accuracy: 0.79148495\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6370 - acc: 0.8694\n",
      "Loss: 0.6370343017367683 Accuracy: 0.8693666\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3421 - acc: 0.9227\n",
      "Loss: 0.34211312928203175 Accuracy: 0.9227414\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1985 - acc: 0.9526\n",
      "Loss: 0.1984875632022524 Accuracy: 0.952648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1917 - acc: 0.9570\n",
      "Loss: 0.1917211537721334 Accuracy: 0.9570094\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2165 - acc: 0.9624\n",
      "Loss: 0.21652678627228772 Accuracy: 0.96240914\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
